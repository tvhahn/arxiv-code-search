id,pattern,token_count,update_date,label,para
1702.03196,data,195,,,"WebQuestionsGraphQuestionsMethodendeesendeesSINGLEEVENT48.545.646.315.98.811.4DEPTREE48.845.946.416.08.311.3CCGGRAPH49.5––15.9––UDEPLAMBDA49.546.147.517.79.512.8UDEPLAMBDASRL49.846.247.017.79.112.7Table3:F1-scoresonthetestdata.baselinecannothandlecompositionalquestions,orthosewithaggregationorcomparison.DEPTREEAnungroundedgraphisobtaineddi-rectlyfromtheoriginaldependencytree.Aneventiscreatedforeachparentanditsdependentsinthetree.Eachdependentislinkedtothiseventwithanedgelabeledwithitsdependencyrelation,whiletheparentislinkedtotheeventwithanedgelabeledarg0.Ifawordisaquestionword,anadditionalTARGETpredicateisattachedtoitsentitynode.CCGGRAPHThisistheCCG-basedsemanticrepresentationofReddyetal.(2014).NotethatthisbaselineexistsonlyforEnglish.UDEPLAMBDASRLThisissimilartoUDEP-LAMBDAexceptthatinsteadofassumingnsubj,dobjandnsubjpasscorrespondtoarg1,arg2andarg2,weemploysemanticrolelabelingtoidentifythecorrectinterpretation.WeusedthesystemsofRothandWoodsend(2014)forEnglishandGer-manandBjrkelundetal.(2009)forSpanishtrainedontheCoNLL-2009dataset(Hajietal.,2009).84.5ResultsTable3showstheperformanceofGRAPHPARSERwiththesedifferentrepresentations.Hereandinwhatfollows,weuseaverageF1-scoreofpredictedanswers(Berantetal.,2013)astheevaluationmet-ric.WeﬁrstobservethatUDEPLAMBDAconsis-tentlyoutperformstheSINGLEEVENTandDEP-TREErepresentationsinalllanguages.ForEnglish,performanceisonparwithCCG-GRAPH,whichsuggeststhatUDEPLAMBDAdoesnotsacriﬁcetoomuchspeciﬁcityforuniversal-ity.Withbothdatasets,resultsarelowerforGer-mancomparedtoSpanish.ThisagreeswiththelowerperformanceofthesyntacticparserontheGermanportionoftheUDtreebank.WhileU-DEPLAMBDASRLperformsbetterthanUDEP-8Theparseraccuracies(%)are87.33,81.38and79.91forEnglish,GermanandSpanishrespectively.MethodGraphQ.WebQ.SEMPRE(Berantetal.,2013)10.835.7JACANA(YaoandVanDurme,2014)5.133.0PARASEMPRE(BerantandLiang,2014)12.839.9QA(Yao,2015)–44.3AQQU(BastandHaussmann,2015)–49.4AGENDAIL(BerantandLiang,2015)–49.7DEPLAMBDA(Reddyetal.,2016)–50.3STAGG(Yihetal.,2015)–48.4(52.5)BILSTM(T¨ureandJojic,2016)–24.9(52.2)MCNN(Xuetal.,2016)–47.0(53.3)AGENDAIL-RANK(Yavuzetal.,2016)–51.6(52.6)UDEPLAMBDA17.749.5Table4:F1-scoresontheEnglishGraphQuestionsandWebQuestionstestsets(resultswithadditionaltask-speciﬁcresourcesinparentheses).LAMBDAonWebQuestionsforEnglish,wedonotseelargeperformancegapsinothersettings,sug-gestingthatGRAPHPARSERiseitherabletolearncontext-sensitivesemanticsofungroundedpredi-catesorthatthedatasetsdonotcontainambiguousnsubj,dobjandnsubjpassmappings.Finally,whiletheseresultsconﬁrmthatGraphQuestionsismuchhardercomparedtoWebQuestions,wenotethatbothdatasetspredominantlycontainsingle-hopquestions,asindicatedbythecompetitiveperfor-manceofSINGLEEVENTonbothdatasets.Table4comparesUDEPLAMBDAwithprevi-ouslypublishedmodelswhichexistonlyforEn-glishandhavebeenmainlyevaluatedonWeb-Questions.Theseareeithersymboliclikeours(ﬁrstblock)oremployneuralnetworks(secondblock).Resultsformodelsusingadditionaltask-speciﬁctrainingresources,suchasClueWeb09,Wikipedia,orSimpleQuestions(Bordesetal.,2015)areshowninparentheses.OnGraphQuestions,weachieveanewstate-of-the-artresultwithagainof4.8F1-pointsoverthepreviouslyreportedbestresult.OnWebQuestionsweare2.1pointsbelowthebestmodelusingcomparableresources,and3.8pointsbelowthestateoftheart.MostrelatedtoourworkistheEnglish-speciﬁcsystemofReddyetal. (2016).Weattributethe0.8pointdifferenceinF1-scoretotheiruseofthemoreﬁne-grainedPTBtagsetandStanfordDependencies.5RelatedWorkOurworkcontinuesthelongtraditionofbuildinglogicalformsfromsyntacticrepresentationsiniti-atedbyMontague(1973).TheliteratureisrifewithattemptstodevelopsemanticinterfacesforHPSG(Copestakeetal.,2005),LFG(KaplanandBresnan,1982;Dalrympleetal.,1995;CrouchandKing,2006),TAG(KallmeyerandJoshi,2003;GardentandKallmeyer,2003;NessonandShieber,2006),andCCG(BaldridgeandKruijff,2002;Bosetal.,2004;Artzietal.,2015).Unlikeexistingsemanticinterfaces,UDEPLAMBDAusesdependencysyn-tax,awidelyavailablesyntacticresource.Acommontrendinpreviousworkonseman-ticin"
1702.03196,data,204,,,"eaveragedstructuredperceptron(Collins,2002)fromtrainingdataconsistingofquestion-answerpairs,usinganswerF1-scoreastheobjective.4.2DatasetsWeevaluateourapproachontwopublicbench-marksofquestionansweringagainstFreebase:WebQuestions(Berantetal.,2013),awidelyusedbenchmarkconsistingofEnglishquestionsandtheiranswers,andGraphQuestions(Suetal.,2016),arecentlyreleaseddatasetofEnglishquestionswithboththeiranswersandgroundedlogicalforms. WhileWebQuestionsisdominatedbysimpleentity-attributequestions,GraphQuestionscontainsalargenumberofcompositionalquestionsinvolvingaggregation(e.g.HowmanychildrenofEddardStarkwereborninWinterfell?)andcomparison(e.g.InwhichmonthdoestheaveragerainfallofNewYorkCityexceed86mm?).Thenumberoftraining,developmentandtestquestionsis2644,1134,and2032,respectively,forWebQuestionsand1794,764,and2608forGraphQuestions.Tosupportmultilingualevaluation,wecreatedtranslationsofWebQuestionsandGraphQuestionstoGermanandSpanish.ForWebQuestionstwoprofessionalannotatorswerehiredperlanguage,whileforGraphQuestionsweusedatrustedpoolof20annotatorsperlanguage(withasingleannotatorperquestion).ExamplesoftheoriginalquestionsandtheirtranslationsareprovidedinTable1.4.3ImplementationDetailsHereweprovidedetailsonthesyntacticanalyzersemployed,ourentityresolutionalgorithm,andthefeaturesusedbythegroundingmodel.DependencyParsingTheEnglish,Spanish,andGermanUniversalDependencies(UD)treebanks(v1.3;Nivreetal2016)wereusedtotrainpartofspeechtaggersanddependencyparsers.WeusedabidirectionalLSTMtagger(Planketal.,2016)andabidirectionalLSTMshift-reduceparser(Kiper-wasserandGoldberg,2016).Boththetaggerandparserrequirewordembeddings.ForEnglish,weusedGloVeembeddings(Penningtonetal.,2014)trainedonWikipediaandtheGigawordcorpus.ForGermanandSpanish,weusedSENNAem-beddings(Collobertetal.,2011;Al-Rfouetal.,2013)trainedonWikipediacorpora(589MwordsGerman;397MwordsSpanish).6MeasuredontheUDtestsets,thetaggeraccuraciesare94.5(En-glish),92.2(German),and95.7(Spanish),withcorrespondinglabeledattachmentparserscoresof81.8,74.7,and82.2.EntityResolutionWefollowReddyetal.(2016)andresolveentitiesinthreesteps:(1)potentialen-tityspansareidentiﬁedusingsevenhandcraftedpart-of-speechpatterns;(2)eachspanisassociatedwithpotentialFreebaseentitiesaccordingtotheFreebase/KGAPI;and(3)the10-bestentitylink-inglattices,scoredbyastructuredperceptron,are6https://sites.google.com/site/rmyeid/projects/polyglot.WebQuestionsenWhatlanguagedothepeopleinGhanaspeak?deWelcheSprachewirdinGhanagesprochen?es¿Cu´aleslalenguadeGhana?enWhowasVincentvanGoghinspiredby?deVonwemwurdeVincentvanGoghinspiriert?es¿Qu´einspir´oaVanGogh?GraphQuestionsenNASAhashowmanylaunchsites?deWievieleAbschussbasenbesitztNASA?es¿Cu´antossitiosdedespeguetieneNASA?enWhichloudspeakersareheavierthan82.0kg?deWelcheLautsprechersindschwererals82.0kg?es¿Qu´ealtavocespesanm´asde82.0kg?Table1:Examplequestionsandtheirtranslations.kWebQuestionsGraphQuestionsendeesendees189.682.886.747.239.939.51095.791.294.056.948.451.6Table2:Structuredperceptronk-bestentitylinkingaccuraciesonthedevelopmentsets.inputtoGRAPHPARSER,leavingtheﬁnaldisam-biguationtothesemanticparsingproblem.Table2showsthe1-bestand10-bestentitydisambiguationF1-scoresforeachlanguageanddataset.FeaturesWeusefeaturessimilartoReddyetal.(2016):basicfeaturesofwordsandFreebasere-lations,andgraphfeaturescrossingungroundedeventswithgroundedrelations,ungroundedtypeswithgroundedrelations,andungroundedanswertypecrossedwithabinaryfeatureindicatingiftheanswerisanumber.Inaddition,weaddfeaturesencodingthesemanticsimilarityofungroundedeventsandFreebaserelations.Speciﬁcally,weusedthecosinesimilarityofthetranslation-invariantem-beddingsofHuangetal.(2015).74.4ComparisonSystemsWecomparedUDEPLAMBDAtofourversionsofGRAPHPARSERthatoperateondifferentrepresen-tations,inadditiontopriorwork.SINGLEEVENTThismodelresemblesthelearning-to-rankmodelofBastandHaussmann(2015).Anungroundedgraphisgeneratedbycon-nectingallentitiesinthequestionwiththeTARGETnode,representingasingleevent.Notethatthis7http://128.2.220.95/multilingual/data/."
1702.03196,github,15,,,"SivaReddy,OscarT¨ackstr¨om,MichaelCollins,TomKwiatkowski,DipanjanDas,MarkSteedman,andMirellaLapata.2016.TransformingDependencyStructurestoLogicalFormsforSemanticParsing.TransactionsoftheAssociationforComputationalLinguistics4:127–140.SebastianSchusterandChristopherD.Manning.2016.EnhancedEnglishUniversalDependencies:AnIm-provedRepresentationforNaturalLanguageUnde"
1702.03196,github,265,,,"6.QuestionAnsweringonFreebaseviaRelationExtractionandTextualEvi-dence.InProceedingsoftheAssociationforCompu-tationalLinguistics.AssociationforComputationalLinguistics,Berlin,Germany,pages2326–2336.XuchenYao.2015.LeanQuestionAnsweringoverFreebasefromScratch.InProceedingsofNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics.pages66–70.XuchenYaoandBenjaminVanDurme.2014.Infor-mationExtractionoverStructuredData:QuestionAnsweringwithFreebase.InProceedingsoftheAs-sociationforComputationalLinguistics.pages956–966.SemihYavuz,IzzeddinGur,YuSu,MudhakarSrivatsa,andXifengYan.2016.ImprovingSemanticParsingviaAnswerTypeInference.InProceedingsoftheEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputationalLinguistics,Austin,Texas,pages149–159.Wen-tauYih,Ming-WeiChang,XiaodongHe,andJianfengGao.2015.SemanticParsingviaStagedQueryGraphGeneration:QuestionAnsweringwithKnowledgeBase.InProceedingsoftheAssociationforComputationalLinguistics.pages1321–1331.JohnM.ZelleandRaymondJ.Mooney.1996.Learn-ingtoParseDatabaseQueriesUsingInductiveLogicProgramming.InProceedingsofAssociationfortheAdvancementofArtiﬁcialIntelligence.pages1050–1055.LukeS.ZettlemoyerandMichaelCollins.2005.Learn-ingtoMapSentencestoLogicalForm:StructuredClassiﬁcationwithProbabilisticCategorialGram-mars.InProceedingsofUncertaintyinArtiﬁcialIn-telligence.pages658–666. UniversalSemanticParsing:SupplementaryMaterialSivaReddy†OscarT¨ackstr¨om‡SlavPetrov‡MarkSteedman††MirellaLapata†††StanfordUniversity‡GoogleInc.††UniversityofEdinburghsivar@stanford.edu,{oscart,slav}@google.com,{steedman,mlap}@inf.ed.ac.ukAbstractThissupplementarymaterialtothemainpaper,providesanoutlineofhowquantiﬁ-cationcanbeincorporatedintheUDEP-LAMBDAframework.1UniversalQuantiﬁcationConsiderthesentenceEverybodywantstobuyahouse,1whosedependencytreeintheUniversalDependencies(UD)formalismisshowninFig-ure1(a).Thissentencehastwopossiblereadings:either(1)everypersonwantstobuyadifferenthouse;or(2)everypersonwantstobuythesamehouse.Thetwointerpretationscorrespondtothefollowinglogicalforms:(1)∀x.person(xa)→[∃zyw.wants(ze)∧arg1(ze,xa)∧buy(ye)∧xcomp(ze,ye)∧house(wa)∧arg1(ze,xa)∧arg2(ze,wa)];(2)∃w.house(wa)∧(∀x.person(xa)→[∃zy.wants(ze)∧arg1(ze,xa)∧buy(ye)∧xcomp(ze,ye)∧arg1(ze,xa)∧arg2(ze,wa)]).In(1),theexistentialvariablewisinthescopeoftheuniversalvariablex(i.e.thehouseisdependentontheperson).Thisreadingiscommonlyreferredtoasthesurfacereading.Conversely,in(2)theuniversalvariablexisinthescopeoftheexistentialvariablew(i.e.thehouseisindependentoftheperson).Thisreadingisalsocalledinversereading.Ourgoalistoobtainthesurfacereadinglogicalformin(1)withUDEPLAMBDA.Wedonotaimtoobtaintheinversereading,althoughthisispossiblewiththeuseofSkolemization(Steedman,2012).InUDEPLAMBDA,lambdaexpressionsforwords,phrasesandsentencesarealloftheformλx.....Butfrom(1),itisclearthatweneedtoexpressvariablesboundbyquantiﬁers,e.g.∀x,whilestillprovidingaccesstoxforcomposition.Thisdemandsachangeinthetypesystemsincethe1ExampleborrowedfromSchusterandManning(2016).Everybodywantstobuyahousensubjxcompdobjdetmarkroot(a)Originaldependencytree.EverybodywantstobuyahousensubjxcompdobjdetmarkrootΩΩbindnsubj(b)Enhanceddependencytree.Everybodywantstobuyahousensubj:univxcompdobjdetmarkrootΩΩbindnsubj(c)Enhanceddependencytreewithuniversalquantiﬁcation.Figure1:ThedependencytreeforEverybodywantstobuyahouseanditsenhancedvariants.samevariablecannotbelambdaboundandquanti-ﬁerbound—thatiswecannothaveformulasoftheformλx...∀x....Inthismaterial,weﬁrstderivethelogicalformfortheexamplesentenceusingthetypesystemfromourmainpaper(Section1.1)andshowthatitfailstohandleuniversalquantiﬁ-cation.Wethenmodifythetypesystemslightlytoallowderivationofthedesiredsurfacereadinglogicalform(Section1.2).Thismodiﬁedtypesys-temisastrictgeneralizationoftheoriginaltypesystem.2Fancelluetal.(2017)presentanelaboratediscussiononthemodiﬁedtypesystem,andhowitcanhandlenegationscopeanditsinteractionwithuniversalquantiﬁers.2Notethatthistreatmenthasyettobeaddedtoourimplementation,whichcanbefoundathttps://github.com/sivareddyg/udeplambda."
1702.03196,github,324,,,"languagesanddatasets.ForEnglish,itachievesthestrongestresulttodateonGraphQuestions,withcompetitiveresultsonWebQuestions.Ourimple-mentationandtranslateddatasetsarepubliclyavail-ableathttps://github.com/sivareddyg/udeplambda.2DEPLAMBDABeforedescribingUDEPLAMBDA,weprovideanoverviewofDEPLAMBDA(Reddyetal.,2016)onwhichourapproachisbased.DEPLAMBDAconvertsadependencytreetoitslogicalforminthreesteps:binarization,substitution,andcom-position,eachofwhichisbrieﬂyoutlinedbelow.Algorithm1describesthestepsofDEPLAMBDAinlines4-6,whereaslines2and3arespeciﬁctoUDEPLAMBDA.BinarizationAdependencytreeisﬁrstmappedtoaLisp-styles-expressionindicatingtheorderofsemanticcomposition.Figure1(b)showsthes-expressionforthesentenceDisneywonanOs-carforthemovieFrozen,derivedfromthedepen-dencytreeinFigure1(a).Here,thesub-expression(dobjwon(detOscaran))indicatesthatthelogi-calformofthephrasewonanOscarisderivedbycomposingthelogicalformofthelabeldobjwiththelogicalformofthewordwonandthelogicalformofthephraseanOscar,derivedanalogously.Thes-expressioncanalsobeinterpretedasabi-narizedtreewiththedependencylabelastherootnode,andtheleftandrightexpressionsassubtrees.Acompositionhierarchyisemployedtoimposeastricttraversalorderingonthemodiﬁerstoeachheadinthedependencytree.Asanexample,wonhasthreemodiﬁersinFigure1(a),whichaccordingtothecompositionhierarchyarecomposedintheorderdobj>nmod>nsubj.Inconstructionslikecoordination,thisorderingiscrucialtoarriveatthecorrectsemantics.Lines7-17inAlgorithm1describethebinarizationstep.SubstitutionEachsymbolinthes-expressionsissubstitutedforalambdaexpressionencodingitssemantics.Wordsanddependencylabelsareassigneddifferenttypesofexpressions.Ingeneral,wordshaveexpressionsofthefollowingkind:ENTITY⇒λx.word(xa);e.g.Oscar⇒λx.Oscar(xa)EVENT⇒λx.word(xe);e.g.won⇒λx.won(xe)FUNCTIONAL⇒λx.TRUE;e.g.an⇒λx.TRUEHere,thesubscripts·aand·edenotethetypesofindividuals(Ind)andevents(Event),respec-tively,whereasxdenotesapairedvariable(xa,xe)DisneywonanOscarforthemovieFrozenpropnverbdetpropnadpdetnounpropnnsubjdobjnmoddetcasedetcompoundroot(a)ThedependencytreeforDisneywonanOscarforthemovieFrozenintheUniversalDependenciesformalism.(nsubj(nmod(dobjwon(detOscaran))(case(det(comp.Frozenmovie)the)for))Disney)(b)Thebinarizeds-expressionforthedependencytree.λx.∃yzw.won(xe)∧Disney(ya)∧Oscar(za)∧Frozen(wa)∧movie(wa)∧arg1(xe,ya)∧arg2(xe,za)∧nmod.for(xe,wa)(c)Thecomposedlambda-calculusexpression.Figure1:Themappingofadependencytreetoitslogicalformwiththeintermediates-expression.oftypeInd×Event.Roughlyspeaking,propernounsandadjectivesinvokeENTITYexpressions,verbsandadverbsinvokeEVENTexpressions,andcommonnounsinvokebothENTITYandEVENTex-pressions(seeSection3.3),whileremainingwordsinvokeFUNCTIONALexpressions.DEPLAMBDAenforcestheconstraintthateverys-expressionisofthetypeη=Ind×Event→Bool,whichsimpli-ﬁesthetypesystemconsiderably.Expressionsfordependencylabelsgluethesemanticsofheadsandmodiﬁerstoarticulatepredicate-argumentstructure.Theseexpressionsingeneraltakeoneofthefollowingforms:COPY⇒λfgx.∃y.f(x)∧g(y)∧rel(x,y)e.g.nsubj,dobj,nmod,advmodINVERT⇒λfgx.∃y.f(x)∧g(y)∧reli(y,x)e.g.amod,aclMERGE⇒λfgx.f(x)∧g(x)e.g.compound,appos,amod,aclHEAD⇒λfgx.f(x)e.g.case,punct,aux,mark.AsanexampleofCOPY,considerthelambdaexpressionfordobjin(dobjwon(detOscaran)):λfgx.∃y.f(x)∧g(y)∧arg2(xe,ya).Thisexpres-siontakestwofunctionsfandgasinput,wherefrepresentsthelogicalformofwonandgrepre-sentsthelogicalformofanOscar.Thepredicate-argumentstructurearg2(xe,ya)indicatesthatthearg2oftheeventxe,i.e.won,istheindividualya,i.e.theentityOscar.Sincearg2(xe,ya)mimicsthedependencystructuredobj(won,Oscar),werefertotheexpressionkindevokedbydobjasCOPY. Expressionsthatinvertthedependencydirec-tionarereferredtoasINVERT(e.g.amodinrun-ninghorse);expressionsthatmergetwosubexpres-sionswithoutintroducinganyrelationpredicatesarereferredtoasMERGE(e.g.compoundinmovieFrozen);andexpressionsthatsimplyreturnthepar-entexpressionsemanticsarereferredtoasHEAD(e.g.caseinforFrozen).Whilethisgeneralizationappliestomostdependencylabels,severallabelstakeadifferentlogicalformnotlistedhere,someofwhicharediscussedinSection3.3.Sometimesthemappingofdependencylabeltolambdaexpres-sionmaydependonsurroundingpart-of-speechtagsordependencylabels.Fore"
1702.03196,github,403,,,"UniversalSemanticParsingSivaReddy†∗OscarT¨ackstr¨om‡SlavPetrov‡MarkSteedman††MirellaLapata†††StanfordUniversity‡GoogleInc.††UniversityofEdinburghsivar@stanford.edu,{oscart,slav}@google.com,{steedman,mlap}@inf.ed.ac.ukAbstractUniversalDependencies(UD)offerauni-formcross-lingualsyntacticrepresentation,withtheaimofadvancingmultilingualap-plications.Recentworkshowsthatse-manticparsingcanbeaccomplishedbytransformingsyntacticdependenciestolog-icalforms.However,thisworkislim-itedtoEnglish,andcannotprocessde-pendencygraphs,whichallowhandlingcomplexphenomenasuchascontrol.Inthiswork,weintroduceUDEPLAMBDA,asemanticinterfaceforUD,whichmapsnaturallanguagetologicalformsinanalmostlanguage-independentfashionandcanprocessdependencygraphs.Weper-formexperimentsonquestionansweringagainstFreebaseandprovideGermanandSpanishtranslationsoftheWebQuestionsandGraphQuestionsdatasetstofacilitatemultilingualevaluation.ResultsshowthatUDEPLAMBDAoutperformsstrongbase-linesacrosslanguagesanddatasets.ForEnglish,itachievesa4.9F1pointimprove-mentoverthestate-of-the-artonGraph-Questions.1IntroductionTheUniversalDependencies(UD)initiativeseekstodevelopcross-linguisticallyconsistentannota-tionguidelinesaswellasalargenumberofuni-formlyannotatedtreebanksformanylanguages(Nivreetal.,2016).Suchresourcescouldadvancemultilingualapplicationsofparsing,improvecom-parabilityofevaluationresults,enablecross-linguallearning,andmoregenerallysupportnaturallan-guageunderstanding.∗WorkdoneattheUniversityofEdinburghSeekingtoexploitthebeneﬁtsofUDfornatu-rallanguageunderstanding,weintroduceUDEP-LAMBDA,asemanticinterfaceforUDthatmapsnaturallanguagetologicalforms,representingun-derlyingpredicate-argumentstructures,inanal-mostlanguage-independentmanner.Ourframe-workisbasedonDEPLAMBDA(Reddyetal.,2016)arecentlydevelopedmethodthatconvertsEnglishStanfordDependencies(SD)tologicalforms.TheconversionprocessisillustratedinFigure1anddiscussedinmoredetailinSection2.WhereasDEPLAMBDAworksonlyforEnglish,U-DEPLAMBDAappliestoanylanguageforwhichUDannotationsareavailable.1Moreover,DEP-LAMBDAcanonlyprocesstree-structuredinputswhereasUDEPLAMBDAcanalsoprocessdepen-dencygraphs,whichallowtohandlecomplexcon-structionssuchascontrol.ThedifferenttreatmentsofvariouslinguisticconstructionsinUDcomparedtoSDalsorequiredifferenthandlinginUDEP-LAMBDA(Section3.3).OurexperimentsfocusonFreebasesemanticparsingasatestbedforevaluatingtheframework’smultilingualappeal.Weconvertnaturallanguagetologicalformswhichinturnareconvertedtoma-chineinterpretableformalmeaningrepresentationsforretrievinganswerstoquestionsfromFreebase.Tofacilitatemultilingualevaluation,weprovidetranslationsoftheEnglishWebQuestions(Berantetal.,2013)andGraphQuestions(Suetal.,2016)datasetstoGermanandSpanish.WedemonstratethatUDEPLAMBDAcanbeusedtoderivelogicalformsfortheselanguagesusingaminimalamountoflanguage-speciﬁcknowledge.Asidefromdevel-opingtheﬁrstmultilingualsemanticparsingtoolforFreebase,wealsoexperimentallyshowthatU-DEPLAMBDAoutperformsstrongbaselinesacross1Asofv1.3,UDannotationsareavailablefor47languagesathttp://universaldependencies.org. languagesanddatasets.ForEnglish,itachievesthestrongestresulttodateonGraphQuestions,withcompetitiveresultsonWebQuestions.Ourimple-mentationandtranslateddatasetsarepubliclyavail-ableathttps://github.com/sivareddyg/udeplambda.2DEPLAMBDABeforedescribingUDEPLAMBDA,weprovideanoverviewofDEPLAMBDA(Reddyetal.,2016)onwhichourapproachisbased.DEPLAMBDAconvertsadependencytreetoitslogicalforminthreesteps:binarization,substitution,andcom-position,eachofwhichisbrieﬂyoutlinedbelow.Algorithm1describesthestepsofDEPLAMBDAinlines4-6,whereaslines2and3arespeciﬁctoUDEPLAMBDA.BinarizationAdependencytreeisﬁrstmappedtoaLisp-styles-expressionindicatingtheorderofsemanticcomposition.Figure1(b)showsthes-expressionforthesentenceDisneywonanOs-carforthemovieFrozen,derivedfromthedepen-dencytreeinFigure1(a).Here,thesub-expression(dobjwon(detOscaran))indicatesthatthelogi-calformofthephrasewonanOscarisderivedbycomposingthelogicalformofthelabeldobjwiththelogicalformofthewordwonandthelogicalformofthephraseanOscar,derivedanalogously.Thes-expressioncanalsobeinterpretedasabi-narizedtreewiththedependencylabelastherootnode,andtheleftandrightexpressionsassubtrees.Acompositionhierarchyisemployedtoimposeastricttraversalorderingonthemodiﬁerstoeachheadinthedependencytree.Asanexample,wonhasthreemodiﬁersinFigure1(a),whichaccordingtothecompositionhierarchyarecomposedintheorderdobj>nmod>nsubj.Inconstructionslikecoordination,thisorderingiscrucialtoarriveatthecorrectsemantics.Lines7-17inAlgorithm1describethebinarizationstep.SubstitutionEachsymbolinthes-expressionsissubstitutedforalambdaexpressionencodingitssemantics.Wordsanddependencylabelsareassigneddifferenttypesofexpressions.Ingeneral,wordshaveexpressionsofthefollowingkind:ENTITY⇒λx.word(xa);e.g.Oscar⇒λx.Oscar(xa)EVENT⇒λx.word(xe);e.g.won⇒λx.won(xe)FUNCTIONAL⇒λx.TRUE;e.g.an⇒λx.TRUEHere,thesubscripts·aand·edenotethetypesofindividuals(Ind)andevents(Event),respec-tively,whereasxdenotesapairedvariable(xa,xe)DisneywonanOscarforthemovieFrozenpropnverbdetpropnadpdetnounpropnnsubjdobjnmoddetcasedetcompoundroot(a)ThedependencytreeforDisneywonanOscarforthemovieFrozenintheUniversalDependenciesformalism.(nsubj(nmod(dobjwon(detOscaran))(case(det(comp.Frozenmovie)the)for))Disney)(b)Thebinarizeds-expressionforthedependencytree.λx.∃yzw.won(xe)∧Disney(ya)∧Oscar(za)∧Frozen(wa)∧movie(wa)∧arg1(xe,ya)∧arg2(xe,za)∧nmod.for(xe,wa)(c)Thecomposedlambda-calculusexpression.Figure1:Themappingofadependencytreetoitslogicalformwiththeintermediates-expression.oftypeInd×Event.Roughlyspeaking,propernounsandadjectivesinvokeENTITYexpressions,verbsandadverbsinvokeEVENTexpressions,andcommonnounsinvokebothENTITYandEVENTex-pressions(seeSection3.3),whileremainingwordsinvokeFUNCTIONALexpressions.DEPLAMBDAenforcestheconstraintthateverys-expressionisofthetypeη=Ind×Event→Bool,whichsimpli-ﬁesthetypesystemconsiderably.Expressionsfordependencylabelsgluethesemanticsofheadsandmodiﬁerstoarticulatepredicate-argumentstructure.Theseexpressionsingeneraltakeoneofthefollowingforms:COPY⇒λfgx.∃y.f(x)∧g(y)∧rel(x,y)e.g.nsubj,dobj,nmod,advmodINVERT⇒λfgx.∃y.f(x)∧g(y)∧reli(y,x)e.g.amod,aclMERGE⇒λfgx.f(x)∧g(x)e.g.compound,appos,amod,aclHEAD⇒λfgx.f(x)e.g.case,punct,aux,mark.AsanexampleofCOPY,considerthelambdaexpressionfordobjin(dobjwon(detOscaran)):λfgx.∃y.f(x)∧g(y)∧arg2(xe,ya).Thisexpres-siontakestwofunctionsfandgasinput,wherefrepresentsthelogicalformofwonandgrepre-sentsthelogicalformofanOscar.Thepredicate-argumentstructurearg2(xe,ya)indicatesthatthearg2oftheeventxe,i.e.won,istheindividualya,i.e.theentityOscar.Sincearg2(xe,ya)mimicsthedependencystructuredobj(won,Oscar),werefertotheexpressionkindevokedbydobjasCOPY."
1702.03196,github,555,,,"1.1WithOriginalTypeSystemWewillﬁrstattempttoderivethelogicalformin(1)usingthedefaulttypesystemofUDEPLAMBDA.Figure1(b)showstheenhanceddependencytreeforthesentence,whereBINDhasbeenintroducedtoconnecttheimpliednsubjofbuy(BINDisex-plainedinthemainpaperinSection3.2).Thes-expressioncorrespondingtotheenhancedtreeis:(nsubj(xcompwants(mark(nsubj(dobjbuy(dethousea))Ω)to))(BINDeverybodyΩ)).Withthefollowingsubstitutionentries,wants,buy∈EVENT;everybody,house∈ENTITY;a,to∈FUNCTIONAL;Ω=λx.EQ(x,ω);nsubj=λfgx.∃y.f(x)∧g(y)∧arg1(xe,ya);dobj=λfgx.∃y.f(x)∧g(y)∧arg2(xe,ya);xcomp=λfgx.∃y.f(x)∧g(y)∧xcomp(xe,ya);mark∈HEAD;BIND∈MERGE,thelambdaexpressionaftercompositionbecomes:λz.∃xywv.wants(ze)∧everybody(xa)∧arg1(ze,xa)∧EQ(x,ω)∧buy(ye)∧xcomp(ze,ye)∧arg1(ye,va)∧EQ(v,ω)∧arg1(xe,ya)∧house(wa)∧arg2(ye,wa).Thisexpressionencodesthefactthatxandvareinuniﬁcation,andcanthusbefurthersimpliﬁedto:(3)λz.∃xyw.wants(ze)∧everybody(xa)∧arg1(ze,xa)∧buy(ye)∧xcomp(ze,ye)∧arg1(ye,xa)∧arg1(xe,ya)∧house(wa)∧arg2(ye,wa).However,thelogicalform(3)differsfromthedesiredform(1).Asnotedabove,UDEPLAMBDAwithitsdefaulttype,whereeachs-expressionmusthavethetypeη=Ind×Event→Bool,cannothandlequantiﬁerscoping.1.2WithHigher-orderTypeSystemFollowingChampollion(2010),wemakeaslightmodiﬁcationtothetypesystem.Insteadofusingexpressionsoftheformλx....forwords,weuseeitherλf.∃x....orλf.∀x....,wherefhastypeη.AsarguedbyChampollion,thishigher-orderformmakesquantiﬁcationandnegationhandlingsoundandsimplerinNeo-Davidsonianeventsemantics.Followingthischange,weassignthefollowinglambdaexpressionstothewordsinourexamplesentence:everybody=λf.∀x.person(x)→f(x);wants=λf.∃x.wants(xe)∧f(x);to=λf.TRUE;buy=λf.∃x.buy(xe)∧f(x);a=λf.TRUE;house=λf.∃x.house(xa)∧f(x);Ω=λf.f(ω).Hereeverybodyisassigneduniversalquantiﬁersemantics.SincetheUDrepresentationdoesnotdistinguishquantiﬁers,weneedtorelyonasmall(language-speciﬁc)lexicontoidentifythese.Toencodequantiﬁcationscope,weenhancethela-belnsubjtonsubj:univ,whichindicatesthatthesubjectargumentofwantscontainsauniversalquantiﬁer,asshowninFigure1(c).Thischangeofsemantictypeforwordsands-expressionsforcesustoalsomodifytheseman-tictypeofdependencylabels,inordertoobeythesingle-typeconstraintofDEPLAMBDA(Reddyetal.,2016).Thus,dependencylabelswillnowtaketheformλPQf....,wherePistheparentex-pression,Qisthechildexpression,andthereturnexpressionisoftheformλf.....Followingthischange,weassignthefollowinglambdaexpres-sionstodependencylabels:nsubj:univ=λPQf.Q(λy.P(λx.f(x)∧arg1(xe,ya)));nsubj=λPQf.P(λx.f(x)∧Q(λy.arg1(xe,ya)));dobj=λPQf.P(λx.f(x)∧Q(λy.arg2(xe,ya)));xcomp=λPQf.P(λx.f(x)∧Q(λy.xcomp(xe,ya)));det,mark=λPQf.P(f);BIND=λPQf.P(λx.f(x)∧Q(λy.EQ(y,x))).Noticethatthelambdaexpressionofnsubj:univdiffersfromnsubj.Inthefor-mer,thelambdavariablesinsideQhavewiderscopeoverthevariablesinP(i.e.theuniversalquantiﬁervariableofeverybodyhasscopeovertheeventvariableofwants)contrarytothelatter.Thenews-expressionforFigure1(c)is(nsubj:univ(xcompwants(mark(nsubj(dobjbuy(dethousea))Ω)to))(BINDeverybodyΩ)).Substitutingwiththemodiﬁedexpressions,andperformingcompositionandsimpliﬁcationleadstotheexpression:(6)λf.∀x.person(xa)→[∃zyw.f(z)∧wants(ze)∧arg1(ze,xa)∧buy(ye)∧xcomp(ze,ye)∧house(wa)∧arg1(ze,xa)∧arg2(ze,wa)].Thisexpressionisidenticalto(1)exceptfortheoutermosttermλf.Byapplying(6)toλx.TRUE,weobtain(1),whichcompletesthetreatmentofuniversalquantiﬁcationinUDEPLAMBDA.ReferencesLucasChampollion.2010.Quantiﬁcationandnegationineventsemantics.BalticInternationalYearbookofCognition,LogicandCommunication6(1):3.FedericoFancellu,SivaReddy,AdamLopez,andBon-nieWebber.2017.UniversalDependenciestoLogi-calFormswithNegationScope.arXivPreprint."
1706.03171,data,48,,,"[12] X. Jin, R. Krishnan, and R. Sandhu, “A uniﬁed attribute-based access control model covering DAC, MAC and RBAC,” in IFIP Annual Conference on Data and Applications Security and Privacy. Springer, 2012, pp. 41–55."
1710.02907,code,41,,,"[27] R. R. S. Tomar and K. Jain, “Lossless image compression using differential pulse code modulation and its application,” in Communication Systems and Network Technologies (CSNT), 2015 Fifth International Conference on."
1710.02907,data,7,,,3 Zipper Transformation for N-dimensional Image Data
1710.02907,data,11,,,Figure 3: Zipper and Inverse-zipper Transformation for N-dimensional image data
1710.02907,data,20,,,"[14] G. Hinton and R. Salakhutdinov, “Reducing the dimensionality of data with neural networks,”"
1710.02907,data,50,,,"[11] H. Patel, U. Itwala, R. Rana, and K. Dangarwala, “Survey of lossless data compression algorithms,” in International Journal of Engineering Research and Technology, vol. 4, no. 04 (April-2015). ESRSA Publications, 2015."
1710.02907,data,51,,,"[6] Z. Xiong, X. Wu, S. Cheng, and J. Hua, “Lossy-to-lossless compression of medical volumetric data using three-dimensional integer wavelet transforms,” Medical Imaging, IEEE Transactions on, vol. 22, no. 3, pp. 459–470, 2003."
1710.02907,data,59,,,"The main idea in this transformation is to ﬁrst implement Discrete Fourier Transform (DFT) on the input array. For image data, it must be noted that DFT extracts the frequency component of each pixel. For a given set of real valued discrete samples f (n), the DFT is given as:"
1710.02907,data,75,,,"between interlacing and concatenating ZT, and lastly (iii) generalizing the proposed compression scheme to N-dimensional data. The rest of the paper is organized as follows. Section 2 describes the implementation of the zipper, inverse zipper transformation; section 3 presents the N-dimensional zipper transformation and zipper-based Huffman coding and section 4 discusses the experimental designs and presents the results. Finally, conclusions are drawn in section 5."
1710.02907,data,76,,,"0501001502468Block sizeCR ZTint−ZTDCTFWHT0501001502468Block sizeCR ZTint−ZTDCTFWHT0501001502468Block sizeCR ZTint−ZTDCTFWHT0501001502468Block sizeCR ZTint−ZTDCTFWHT050100150020406080100Block sizeRT (s) ZTint−ZTDCTFWHT050100150020406080100Block sizeRT (s) ZTint−ZTDCTFWHT0501001500102030Block sizeRT (s) ZTint−ZTDCTFWHT050100150050100150Block sizeRT (s) ZTint−ZTDCTFWHTzipper transformation in the proposed compression scheme on grayscale images and also to demonstrate the exactness in performance of both concatenating-zipper and interlacing-zipper transforms. The data sets used were lena.jpg, elaine.gif, cameraman.png, man.tif, and couple.png as shown in Fig. 6."
1710.02907,data,78,,,"The new lossless compression scheme proposed in this paper is implemented in two stages - the zipper transformation stage and the Huffman coding stage. It was shown that the proposed scheme has the capability of reducing the Huffman coding table while improving the compression ratio. From the experimental results, it is shown that the proposed methods outperform the DCT and FWHT-based methods in compressing image data. In other words, the proposed method indeed"
1710.02907,data,124,,,"This method was proposed in 1952 by David Huffman to compress data by reducing the amount of bits necessary to represent a string of symbols. The Huffman coding technique is a commonly used scheme for data compression because it is very simple and efﬁcient. It uses the statistical information about the distribution of the data to be encoded. Besides, an identical coding table is used in both the encoder and the decoder. Huffman coding use codewords with variable length, and with shortest codewords for most frequently used characters. The ﬂowchart of the Huffman coding is given below in Fig 5. More formally, for a discrete random variable X with underlying ﬁnite set with n elements"
1710.02907,data,187,,,"In this section, the pixel intensities of image data are transformed using the ZT and the output is encoded using efﬁcient coding scheme. The basic idea of this approach is to obtain a different distribution in the transformed domain that could make symbol coding more efﬁcient. By virtue of this transformation, the transformed image has a lower entropy, and hence, coding scheme such as Huffman coding is able to encode this stream of data with codewords of variable length. At the receiving end, the decoder can then use the encoded information and a lookup table (or dictionary) to retrieve the information back. It is worthy to know that these encoding and decoding operations must also be lossless [35]. It is remarked that other efﬁcient coders such as Lempel-Ziv-Welch (LZW) algorithm can also be used in place of Huffman coding. The inverse zipper transformation can then be utilized to recover the original image with no loss incurred. The schematic of the whole process is as shown in Fig 4."
1710.02907,data,263,,,"concatenating zipper transform. In fact, the interlacing zipping operation is abstracted from the way two separate entities can be fastened and tightened with a zipping tool. In the case of interlacingZT, the imaginary parts of the complex numbers in the upper half of the symmetry are stripped off and interlaced with their corresponding real counterparts. In this conﬁguration, the structural arrangement of the imaginary parts and real part guides against arbitrary variation of pixel intensities in the spatial domain. In the concatenating ZT (or simply ZT) on the other hand, the imaginary parts of the complex numbers in the upper half of the symmetry are stripped off and concatenated with their corresponding real counterparts. It is noted that this transformation is near-lossless. Also, the two-dimensional zipper transformation is implemented as a two 1D ZT in sequence by ﬁrst performing a 1D ZT on columns and then doing another 1D ZT on rows. This procedure is reversed in the inverse zipper transformation. Fig. 1 illustrates the zipper and inverse-zipper transformations and Fig. 2 illustrates the implementation of interlacing-zipper and inverse-interlacing-zipper transformations using a 10 1 vector. It is remarked that the number of elements in the vector before ZT stays the same after the transformation, that is, ZT preserves the dimensionality of the input data. For images, the input vector illustrated in Figs. 1 and 2 is the columns or rows of the multidimensional array representing the image of interest."
1710.02907,data,338,,,"Lossless compression scheme is important in many critical application domains such as biomedical image storage, art images, remote sensing, security and defense, just to mention a few [1, 2, 3]. In teleradiological applications, one of the key players for effective diagnosis as well as the treatment is the quality of the image [4]. Since in most medical applications, data is often acquired and stored digitally, therefore, the demand for storage and transmission of multidimensional medical images has sprung up some interests in this knowledge domain [5, 6]. Medical images are often very large in size and number, and any means to compress them can lead to reduction of storage cost and increased transmission throughput since data are often stored on servers and relayed to clients on demand. For instance, a 256 MB is required for a 512 512 16-bit computed tomography (CT) image data [7]. To visualize and analyze this data is mostly computationally inefﬁcient and daunting. The primary objective of compression in medical applications is reducing large volume of data to be stored and transmitted while preserving crucial diagnostic information [7]. Even though the cost of storage and transmission of digital signal has plummeted, however, the demand for lossless compression of medical images is increasing [8]. From a general viewpoint, image compression can be broadly divided into two main categories: lossy and lossless compression. Lossy compression deals with compression schemes that have tolerance for some certain amount of error, that is, the compressed and the decompressed images may not be identical. In contrast, lossless compression encodes all the information from the original image and therefore, the decompressed image is identical to the original image. Lossy compression is sometimes tolerated in some medical applications as long as the necessary and required diagnostic"
1710.02907,"data, dataset",210,,,"that 3D images can be viewed as time sequence of radiographic images or volume of tomographic slice image of a static object or tomographic slice images of a dynamic object [4, 36]. The 3-D images considered in this work can be visualized as a stack of 2D image slices of a progressive variation of static object. In a way, 3D medical image data is abstracted as a 3D rectangular block of voxels, where each voxel has its assigned value [4]. The 3-D medical image data samples considered in this paper are: scan 1 [37], scan 2 is a 3D MRI image of the circle of willis and other cerebral arteries [38], and scan 3 is the axial T2-weighted MR image of a normal brain at the level of the lateral ventricles. These images are shown in Fig. 7a, b, and c respectively. In the second set of experiments, dataset 2 in Fig. 7 were utilized. In the last set of experiments, dataset 3 in Fig. 8 which comprises of popular benchmark images are used to show the effectiveness of the proposed heuristics."
1710.02907,"data, dataset",280,,,"Experiment 2: In this set of experiments, we evaluated the ability of the proposed method to compress high dimensional data. For these experiments, the proposed compression method was deployed on each dimension and the performance metrics are averaged over all the dimensions. In this regard, Dataset 2 ( comprising of three medical images and the color version of lena.jpg image) was used to benchmark ZT-based algorithm with those of DCT and FWHT. It can be observed in Figs. 11a and 12a that ZT-based transform have competitive performance on Scan1 data for block sizes 4 128, performance of ZT-based 8, 32 algorithm plummeted in comparison with DCT and FWHT-based approaches. Experimentation with Scan2 image shows that ZT consistently compresses the MRI data better and faster than its counterpart for most of the block sizes considered as shown in Figs. 11b and 12b. With block size of 128 128, the performance of FWHT improves and became comparable with ZT. Better performance in compression and speed of implementation is again observed on the average for ZT-based method using the Scan3 image, and the performance is more pronounced when the block size is 128 128 as shown in Figs. 11c and 12c. Again the proposed algorithm using color version of Lena image was compared with those of DCT and FWHT. It was observed that for most of the block sizes, ZT consistently compresses better and faster than its counterparts as shown in Figs. 11d and 12d. For block size 4 and 8, zipper transform has slightly better compression capability and"
1710.02907,"data, dataset",326,,,"It was observed in Figs. 9c and 10c that the performances of ZT and interlacing-ZT outweighs those of DCT and FWHT. For couple.png image data, we again observe that ZT and interlacing-ZT are superior to both DCT and FWHT in term of CR for most of the block sizes considered as shown in Figs. 9d. In fact, a close scrutiny of the results shows that the performance is more pronounced when the block size is 64. In addition to the poor compressing capability, implementing DCT and FWHT are more expensive in terms of running time than the ZT-based compression paradigms as shown in Fig. 10d. In Tables 1 and 2, we report the average length of the codewords and average entropy for Dataset 1 in Fig. 6 for all the four methods considered. Each image in Dataset 1 is transformed using DCT, FHWT, Zipper, and interlacing-Zipper then encoded using Huffman coding scheme. Entropy and average length of the codewords were recorded and averaged over all the images in Dataset 1. The most efﬁcient of the four methods is highlighted in bold fonts. Our observations for all the experiments reported in Tables 1 and 2 are that average length of the codeword decreases as the entropy decreases for all the four methods, and also the value of entropy is very close to the average codeword length. Closer look at each of the compression method reveals that the discrepancy between the average length of the codeword and the entropy is smallest for ZT-based algorithms compared to DCT and FWHT-based methods. In addition, the average length for zipper-based transforms is less than those of DCT and FWHT for almost all the block sizes tested. It is also remarked that both ZT and interlacing-ZT yield similar results which might indicate robustness of the proposed approach."
1710.02907,"data, dataset",335,,,"Experiment 1: In the ﬁrst set of experiments using Lena image, both the ZT-based and interlacingZT-based compression algorithms outperform both the DCT and FWHT counterparts in terms of CR for many block sizes as seen in Fig. 9a. On the average, both the ZT-based and interlacing-ZT-based compression algorithms signiﬁcantly compress the lena.jpg image better than DCT and FWHT. In addition to efﬁcient compression of Lena image, ZT and interlacing-ZT have faster implementations than DCT and FWHT as shown in Fig. 10a. The proposed algorithms were also tested using the Elaine.gif image and the compression ratio and running time are depicted in Figs. 9b and 10b respectively. Again, we observed that ZT and interlacing-ZT outperform their counterparts in both compressing capabilities and speed of implementation. In the next set of experiments, we benchmark the proposed schemes with DCT and FWHT in terms of CR and running time using the cameraman.png image data. It was observed in Figs. 9c and 10c that the performances of ZT and interlacing-ZT outweighs those of DCT and FWHT. For couple.png image data, we again observe that ZT and interlacing-ZT are superior to both DCT and FWHT in term of CR for most of the block sizes considered as shown in Figs. 9d. In fact, a close scrutiny of the results shows that the performance is more pronounced when the block size is 64. In addition to the poor compressing capability, implementing DCT and FWHT are more expensive in terms of running time than the ZT-based compression paradigms as shown in Fig. 10d. In Tables 1 and 2, we report the average length of the codewords and average entropy for Dataset 1 in Fig. 6 for all the four methods considered. Each image in Dataset 1 is transformed using DCT, FHWT, Zipper, and interlacing-Zipper then encoded using Huffman coding scheme."
1710.02907,"data, package",315,,,"information is preserved in the decompressed image [4, 6, 9, 10]. In medical imaging, lossy compression can sometimes achieve a minute compression before a good percentage of information is dropped. More compression can be achieved if some visible losses can be tolerated for clinical task purposes. There are still a lot of controversies as to what the real life applications of lossy compressions are, especially in the medical domain. One other approach to lossy compression is the machine learning approach where images are encoded with sparse feature representations using neural networks [14, 15, 16]. Generally speaking, lossless compression can be categorized into three broad categories, namely: predictive scheme with statistical modeling, transform based coding and ﬁnally, dictionary based coding. The predictive deals with using statistical method to evaluate the differences between pixels and their neighbors, and performed context modeling before coding. Whereas in transform based compression, pixel are transformed using frequency or wavelet transformation before modeling and coding. Dictionary based compression is the third category and it deals with replacing strings of symbols with shorter codes. It must be noted that dictionary based schemes are widely used for text compression [17]. An example of a dictionary based algorithm is the well known ZIP package. Other dictionary based compression algorithms for image data are the Lempel-Ziv-Welsh (LZW), Portable Network Graphics (PNG), Graphics Interchange Format, and so on. A good number of image data compression paradigms have been investigated in recent past. The least squares adaptive prediction scheme was proposed in [18] for lossless compression of natural images and it was shown that the novel scheme improves the computational complexity with negligible performance trade-off."
1710.02907,"data, package",336,,,"Whereas in transform based compression, pixel are transformed using frequency or wavelet transformation before modeling and coding. Dictionary based compression is the third category and it deals with replacing strings of symbols with shorter codes. It must be noted that dictionary based schemes are widely used for text compression [17]. An example of a dictionary based algorithm is the well known ZIP package. Other dictionary based compression algorithms for image data are the Lempel-Ziv-Welsh (LZW), Portable Network Graphics (PNG), Graphics Interchange Format, and so on. A good number of image data compression paradigms have been investigated in recent past. The least squares adaptive prediction scheme was proposed in [18] for lossless compression of natural images and it was shown that the novel scheme improves the computational complexity with negligible performance trade-off. Lossless compression based on adaptive spectral band re-ordering and adaptive backward previous closest neighbors (PCN) algorithms was also proposed in [19, 20] for hyperspectral images, and it was shown that the compression performance was greatly enhanced with the implementation of both the re-ordering of spectral band. The problem of compression in medical applications has also been studied in [21] where different types of compression standards on grayscale medical images were compared by highlighting the pros and cons of various compression methods. In [1], problems of video compression were addressed taking cognizance of the temporary spectral information. Also in [22], the possibility of using 3-D versions of the lossless JPEG spatial predictors was considered and the likelihood of using best predictor to encode the present frame was investigated. The spectral redundancy was also exploited by implementing the best predictor from one spectral component to another spectral component. It can be inferred from [22] that pixels in a given neighborhood are concurrent in adjoining color bands."
1710.02907,"data, package",339,,,"A good number of predictors were considered taking cognizance of the spatial redundancy. In addition, a variation of singular value decomposition (SVD) based image compression was proposed in [25] which is an extension on the conventional SVD-based compression. Image data was ﬁrst preprocessed using data independent permutation prior to computing SVD, and in the reconstruction algorithm, inverse permutation was used to post-process the output of the conventional SVD-based decompression algorithm. Many compression algorithms have been proposed for a wide range of imaging domains such as DNA microarray images [26] and multimedia image data [27]. Improving the lossless compression of images with sparse histogram was addressed in [29] and its robustness was shown on other types of images. Both lossy and lossless compression was investigated in a uniﬁed framework and a new cost effective coding strategy was proposed to enhance the coding efﬁciency. A motion-JPEG-LS based lossless scheme was also proposed in [8] and the work only explores high enough correlation between adjacent image frames in order to avoid possible coding loss and abrupt high computational cost. Wavelet-based lossy to lossless compression methods have also been addressed for ECG [30] and for volumetric medical images [31]. Lossless digital audio compression scheme was addressed in [32]. The main contribution of this paper is to propose new near-lossless compression heuristics and benchmark their performance with DCT and FWHT-based compression paradigms in term of computational complexity and compression ratio. The algorithms have been implemented taking into consideration the three important ﬁgure-of-merit namely: the compression efﬁciency, easiness of use, and speed of computation. The design requires no complicated parameter selection nor special digital signal processing hardware add-on. This paper considerably expands the scope of the ZT ﬁrst introduced in [33] by: (i) proposing new interlacing ZT (ii) showing the performance similarity"
1710.02907,"data, package",350,,,"In [1], problems of video compression were addressed taking cognizance of the temporary spectral information. Also in [22], the possibility of using 3-D versions of the lossless JPEG spatial predictors was considered and the likelihood of using best predictor to encode the present frame was investigated. The spectral redundancy was also exploited by implementing the best predictor from one spectral component to another spectral component. It can be inferred from [22] that pixels in a given neighborhood are concurrent in adjoining color bands. To improve on this, a different predictor for interband correlation was proposed in [23]. In [24], a simple context predictive scheme was also proposed where either intraframe or interframe coding is selected based on temporal and spatial variations, and the prediction of the current pixel was then computed. A good number of predictors were considered taking cognizance of the spatial redundancy. In addition, a variation of singular value decomposition (SVD) based image compression was proposed in [25] which is an extension on the conventional SVD-based compression. Image data was ﬁrst preprocessed using data independent permutation prior to computing SVD, and in the reconstruction algorithm, inverse permutation was used to post-process the output of the conventional SVD-based decompression algorithm. Many compression algorithms have been proposed for a wide range of imaging domains such as DNA microarray images [26] and multimedia image data [27]. Improving the lossless compression of images with sparse histogram was addressed in [29] and its robustness was shown on other types of images. Both lossy and lossless compression was investigated in a uniﬁed framework and a new cost effective coding strategy was proposed to enhance the coding efﬁciency. A motion-JPEG-LS based lossless scheme was also proposed in [8] and the work only explores high enough correlation between adjacent image frames in order to avoid possible coding loss and abrupt high computational cost."
1710.02907,dataset,2,,,4.1 Dataset
1710.02907,dataset,6,,,Figure 6: Image Dataset 1
1710.02907,dataset,6,,,Figure 7: Image Dataset 2
1710.02907,dataset,6,,,Figure 8: Image Dataset 3
1710.02907,dataset,15,,,"Table 1: Average Entropy for DCT, FWHT and ZT using Image Dataset 1"
1710.02907,dataset,15,,,"Table 2: Average Length for DCT, FWHT and ZT using Image Dataset 1"
1710.02907,dataset,15,,,"Table 3: Average Entropy for DCT, FWHT and ZT using Image Dataset 3"
1710.02907,dataset,41,,,"Figure 10: A plot of running time against block size for DCT, FWHT, Zipper, and interlace-zipper transform using (a) Lena (b) Elaine (c) Cameraman (d) Couple from Dataset 1"
1710.02907,dataset,41,,,"Figure 13: A plot of compression ratio against block size for DCT, FWHT, Zipper, and interlacing-zipper transform using (a) Pepper (b) Mandrill (c) Monarch (d) Tulips from Dataset 3"
1710.02907,dataset,41,,,"Figure 14: A plot of running time against block size for DCT, FWHT, Zipper, and interlacing-zipper transform using (a) Pepper (b) Mandrill (c) Monarch (d) Tulips from Dataset 3"
1710.02907,dataset,41,,,"Figure 9: A plot of compression ratio against block size for DCT, FWHT, Zipper, and interlace-zipper transform using (a) Lena (b) Elaine (c) Cameraman (d) Couple from Dataset 1"
1710.02907,dataset,42,,,"Figure 11: A plot of compression ratio against block size for DCT, FWHT, Zipper, and interlacing-zipper transform using (a) Scan1 (b) Scan2 (c) Scan3 (d) Lena color from Dataset 2"
1710.02907,dataset,42,,,"Figure 12: A plot of running time against block size for DCT, FWHT, Zipper, and interlacing-zipper transform using (a) Scan1 (b) Scan2 (c) Scan3 (d) Lena color from Dataset 2"
1710.02907,dataset,43,,,Experiment 2. This experiment aimed to compare the proposed compression scheme with two existing schemes using multi-dimensional images. Dataset 2 in Fig. 7 and dataset 3 in Fig. 8 were used to benchmark the proposed with other schemes.
1710.02907,dataset,64,,,"05010015020406080Block sizeRT (s) ZTDCTFWHT05010015020406080100Block sizeRT (s) ZTDCTFWHT0501001500204060Block sizeRT (s) ZTDCTFWHT050100150050100Block sizeRT (s) ZTDCTFWHT0501001502468Block sizeCR ZTDCTFWHT05010015002468Block sizeCR ZTDCTFWHT0501001502468Block sizeCR ZTDCTFWHT0501001502468Block sizeCR ZTDCTFWHT050100150050100Block sizeRT (s) ZTDCTFWHT050100150050100150Block sizeRT (s) ZTDCTFWHT050100150020406080Block sizeRT (s) ZTDCTFWHT050100150020406080Block sizeRT (s) ZTDCTFWHTTable 4: Average Length for DCT, FWHT and ZT using Image Dataset 3"
1710.02907,dataset,233,,,"running time than DCT, however as the block size increases, performance of ZT became more pronounced. In the last set of experiments, we evaluated the performance of the proposed algorithm on popular color image collection tagged in this paper as Dataset 3 and the performance is compared with those of DCT and FWHT. It is remarked that ZT-based algorithm outperforms its counterpart on the average in terms of compression ratio and algorithmic running time (in seconds) for all the four color images in this particular dataset as shown in Figs. 13 and 14 respectively. Also, Tables 4 and 3 respectively give the synopsis of the average length of the codewords and average entropy of the compressed version of the images in Dataset3 using ZT, DCT, and FWHT. Once again, the most efﬁcient of the three methods is highlighted in bold fonts. There is strong correlation between the average codeword and average entropy for all the three heuristics as inferred in Tables 3 and 4. Again, the disparity between the average length of the codeword and the entropy is least in ZT-based algorithms in comparison with DCT and FWHT-based methods. Furthermore, the average length and entropy for zipper-based transforms is less than those of DCT and FWHT for almost all the block sizes tested."
1710.02907,github,85,,,"In order to benchmark the proposed scheme with other methods, we also implemented DCT and FWHT-based compression algorithms. We also utilized the running time to compare the proposed method with those of DCT and FWHT. In this work, we deﬁne the running time as the time elapsed between the zipper transformation and the inverse transformation including Huffman coding and decoding as shown in Fig. 4. The MATLAB implementation of these algorithms can be downloaded from https://github.com/babajide07/Zipper-Transformation."
1802.03417,code,125,,,"In the game we created for testing purposes, the goal of the human player is to get to a particular position and stay there for a known number of turns. In contrast, the goal of the AI-controlled character is to prevent that from happening by touching its opponent. We chose this speciﬁc objective because it was directly affected by the quality of the estimations and because it did not require that we code a complicated strategy surrounding the AI’s movement. We invented a pirate concept to intrigue the players who ran tests on our game. In this concept, the player is a pirate trying to steal a treasure from the captain on a remote island."
1802.03417,data,157,,,"We tested our AI under different scenarios to determine whether or not the AI could learn from the its experiences to estimate the position of mobile agent more accurately. To this end, we used the mean distance through out a game between the real position and the estimate as our statistic. We wanted to compare the performance of our AI with adaptive memory with an AI similar to Hladky’s [2], which is not capable of machine learning, using the same statistic. We couldn’t compare the precision of our estimates with estimates produced by a more recent AI using reinforcement learning since those AI’s aren’t actually trying to predict their opponent’s location. We played multiple games using different strategies against various AI’s to collect data. For testing purposes we created multiple variations of the experiment."
1802.03417,"data, code",159,,,"For this experiment, we also had to code a path-ﬁnding algorithm. Even though our primary goal was to estimate the position of a mobile agent, we needed the AI to move autonomously in order to obtain data and conduct tests. Using equation 7, the AI can ﬁnd the most probable hidden state and deﬁne it as its estimate of its opponent’s position at each turn. This position is generally deﬁned as the goal in the path-ﬁnding literature. Considering each state as a node in a graph, we implemented the Dijkstra’s algorithm [12] to let the AI determine the shortest path to that estimate and start moving towards it. At each turn, the AI determines a new goal, calculates the distance between the goal and each of its surrounding positions, and chooses a position to move from among these surrounding positions."
1805.08801,data,15,,,"data. Magnetic resonance in medicine, 44(4):625–632, 2000."
1805.08801,data,16,,,[35] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne.
1805.08801,data,18,,,"Proceedings of the 2017 SIAM International Conference on Data Mining, pages 180–188. SIAM, 2017."
1805.08801,data,38,,,"In this paper, we develop a computational framework for analyzing the neuroimages in PPMI data based on Graph Convolutional Networks (GCN) [24]. Our framework learns pairwise relationships with the following steps."
1805.08801,data,38,,,"[22] Srikanth Ryali, Kaustubh Supekar, Daniel A Abrams, and Vinod Menon. Sparse logistic regression for whole-brain classiﬁ cation of fmri data. NeuroImage, 51(2):752–764, 2010."
1805.08801,data,41,,,"driven without utilization of any clinical domain knowledge. The clinical data such as Electronic Health Records are not considered in the analysis of the disease. In the future, we will continue our research speciﬁcally along these directions."
1805.08801,data,50,,,"[18] Zilong Bai, Peter Walker, Anna Tschiffely, Fei Wang, and Ian Davidson. Unsupervised network discovery for brain imaging data. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 55–64. ACM, 2017."
1805.08801,data,58,,,"[2] Kenneth D Kochanek, Sherry L Murphy, Jiaquan Xu, and Betzaida Tejada-Vera. Deaths: ﬁnal data for 2014. National vital statistics reports: from the Centers for Disease Control and Prevention, National Center for Health Statistics, National Vital Statistics System, 65(4):1–122, 2016."
1805.08801,data,62,,,"Our goal is to learn a feature representation for each subject by fusing its BCGs and the shared BGG, which captures both the local traits of each individual subject and the global traits of the population of subjects. Speciﬁcally, we develop a customized Multi-View Graph Convolutional Network (MVGCN) model to learn feature representations on neuroimaging data."
1805.08801,data,73,,,"[4] Ivo D Dinov, Ben Heavner, Ming Tang, Gustavo Glusman, Kyle Chard, Mike Darcy, Ravi Madduri, Judy Pa, Cathie Spino, Carl Kesselman, et al. Predictive big data analytics: a study of parkinsons disease using large, complex, heterogeneous, incongruent, multi-source and incomplete observations. PloS one, 11(8):e0157077, 2016."
1805.08801,data,76,,,"We propose a multi-view graph convolutional network method called MVGCN in this paper, which can directly take brain graphs from multiple views as inputs and do prediction on that. We validate the effectiveness of MVGCN on real-world Parkinson’s Progression Markers Initiative (PPMI) data for predicting the pairwise matching relations. We demonstrate that our proposed MVGCN can not only achieve good performance, but also discover interesting predictive patterns."
1805.08801,data,114,,,"In recent years, with the arrival of the big data era, many computational approaches have been developed for neuroimaging analysis [15, 16, 17]. Different from conventional hypothesis driven radiology methods, these computational approaches are typically data driven and hypothesis free – they derive features and evidences directly from neuroimages and utilize them in the derivation of clinical insights on multiple problems such as brain network discovery [18, 19] and imaging genomics [20, 21]. Most of these algorithms are linear [22] or multilinear [23], and they work on a single modality of brain images."
1805.08801,data,149,,,"Abstract. Parkinson’s Disease (PD) is one of the most prevalent neurodegenerative diseases that affects tens of millions of Americans. PD is highly progressive and heterogeneous. Quite a few studies have been conducted in recent years on predictive or disease progression modeling of PD using clinical and biomarkers data. Neuroimaging, as another important information source for neurodegenerative disease, has also arisen considerable interests from the PD community. In this paper, we propose a deep learning method based on Graph Convolutional Networks (GCN) for fusing multiple modalities of brain images in relationship prediction which is useful for distinguishing PD cases from controls. On Parkinson’s Progression Markers Initiative (PPMI) cohort, our approach achieved 0.9537 ± 0.0587 AUC, compared with 0.6443 ± 0.0223 AUC achieved by traditional approaches such as PCA."
1805.08801,data,160,,,"The underlying rationale of the proposed method is modeling the multiple brain connectivity networks (BCGs) and a brain geometry graph (BGG) based on the common ROI coordinate simultaneously. Since BCGs are non-Euclidean, it is not straightforward to use a standard convolution that has impressive performances on the grid. Additionally, multi-view graph fusion methods [37] allow us to explore various aspects of the given data. Our non-parametric view pooling is promising in practice. Furthermore, the pairwise learning strategies can satisfy the “data hungry” neural networks with few acquisitions [26]. Our work has demonstrated strong potentials of graph neural networks on the scenario of multiple graph-structured neuroimages. Meanwhile, the representations learned by our approach can be straightforwardly interpreted. However, there are still some limitations. The current approach is completely data (a) Top-10 similar ROI for PD group"
1805.08801,"data available, data",92,,,"• C2: Pairwise Matching. Training deep learning model requires a large amount of training data, but usually very few data are available from clinical practice. We take advantage of the pairwise relationships between subjects to guide the process of deep learning [26, 17]. Similarity is an important type of pairwise relationship that measures the relatedness of two subjects. The basic assumption is that, if two subjects are similar, they should have a high probability to have the same class label."
1805.08801,"data, data https",37,,,"[7] University of california, san francisco and weill cornell medicine researchers named winners of 2016 parkinson’s progression markers initiative data challenge. https://www.michaeljfox.org/foundation/publication-detail. html?id=625&category=7."
1805.08801,"data, database, data https",166,,,"The work is supported by NSF IIS-1716432 (FW), NSF IIS-1650723 (FW), NSF IIS-1750326 (FW), NSF IIS-1718798 (KC), and MJFF14858 (FW). Data used in the preparation of this article were obtained from the Parkinson’s Progression Markers Initiative (PPMI) database (http://www.ppmi-info.org/data). For up-to-date information on the study, visit http://www.ppmi-info.org. PPMI – a public-private partnership – is funded by the Michael J. Fox Foundation for Parkinson’s Research and funding partners, including Abbvie, Avid, Biogen, Bristol-Mayers Squibb, Covance, GE, Genentech, GlaxoSmithKline, Lilly, Lundbeck, Merk, Meso Scale Discovery, Pﬁzer, Piramal, Roche, Sanoﬁ, Servier, TEVA, UCB and Golub Capital. The authors would like to thank the support from Amazon Web Service Machine Learning for Research Award (AWS MLRA)."
1805.08801,"data, dataset",62,,,"In order to evaluate the effectiveness of our proposed approach, we conduct extensive experiments on real-life Parkinsons Progression Markers Initiative (PPMI) data for relationship prediction and compare with several state-of-the-art methods. In the following, we introduce the datasets used and describe details of the experiments. Then we present the results as well as the analysis."
1805.08801,"data, dataset",197,,,"Data Description. We consider the DTI acquisition on 754 subjects, where 596 subjects are Parkinson’s Disease (PD) patients and the rest 158 are Healthy Control (HC) ones. Each subject’s raw data were aligned to the b0 image using the FSL2 eddy-correct tool to correct for head motion and eddy current distortions. The gradient table is also corrected accordingly. Non-brain tissue is removed from the diffusion MRI using the Brain Extraction Tool (BET) from FSL. To correct for echo-planar induced (EPI) susceptibility artifacts, which can cause distortions at tissue-ﬂuid interfaces, skull-stripped b0 images are linearly aligned and then elastically registered to their respective preprocessed structural MRI using Advanced Normalization Tools (ANTs3) with SyN nonlinear registration algorithm. The resulting 3D deformation ﬁelds are then applied to the remaining diffusion-weighted volumes to generate full preprocessed diffusion MRI dataset for the brain network reconstruction. In the meantime, 84 ROIs are parcellated from T1-weighted structural MRI using Freesufer4 and each ROI’s coordinate is deﬁned using the mean coordinate for all voxels in that ROI."
1805.08801,"data, dataset provided",144,,,"Quite a few computational studies have been conducted on PPMI data in recent years. For example, Dinov et al. [4] built a big data analytics pipeline on the clinical, biomarker and assessment data in PPMI to perform various prediction tasks. Schrag et al. [5] predicted the cognitive impairment of the patients in PPMI with clinical variables and biomarkers. Nalls et al. [6] developed a diagnostic model with clinical and genetic classiﬁcations with PPMI cohort. We also developed a sequential deep learning based approach to identify the subtypes of PD on the clinical variables, biomarkers and assessment data in PPMI, and our solution won the PPMI data challenge in 2016 [7]. These studies provided insights to PD researchers in addition to the clinical knowledge."
1805.08801,github,9,,,2http://www.fmrib.ox.ac.uk/fsl 3http://stnava.github.io/ANTs/ 4https://surfer.nmr.mgh.harvard.edu
1805.08801,"github, code available, data, code",229,,,"Experimental Settings. To learn similarities between graphs, brain networks in the same group (PD or HC) are labeled as matching pairs while brain networks from different groups are labeled as non-matching pairs. Hence, we have 283, 881 pairs in total, with 189, 713 matching samples and 94, 168 non-matching samples. 5-fold cross validation is adopted in all of our experiments by separating brain graphs into 5 stratiﬁed randomized sets, where 1fold held-out data used for the testing sample pairs and the rest used for training. Using the coordinate information of ROIs in DTI, we construct a 10-NN BGG in our method, which has 84 vertices and 527 edges. For graph convolutional layers, the order of Chebyshev polynomials s = 30 and the output feature dimension Fout = 128 are used. For fully connected layers, the number of feature dimensions is 1024 in the baseline of one fully connected layer, and those are set as 1024 and 64 for the baseline of two layers. The Adam optimizer [34] is used with the initial learning rate 0.005. The above parameters are optimal settings for all the methods by performing cross-validation. MVGCN code and scripts are available on a public repository (https://github.com/sheryl-ai/MVGCN)."
1806.02891,data,95,,,"Whole networks have been studied to characterize their learning ability. It has been demonstrated [21] that the same classiﬁcation networks that successfully generalize meaningful labels are also able to memorize data with meaningless random labels. Conversely, it has been observed [18] that networks are susceptible to being fooled on tiny perturbations of test inputs. These observations have challenged traditional assumptions about what a network learns, and why. Our current work contributes to the effort to characterize the mechanisms deep networks use to achieve generalization."
1806.02891,database,27,,,"[23] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba. Places: A 10 million image database for"
1806.02891,dataset,4,,,3.1 Models and Datasets
1806.02891,dataset,218,,,"We revisit the importance of the individual units in Convolutional Neural Networks (CNNs) for visual recognition. By conducting unit ablation experiments on CNNs trained on large scale image datasets, we demonstrate that, though ablating any individual unit does not hurt overall classiﬁcation accuracy, it does lead to signiﬁcant damage on the accuracy of speciﬁc classes. This result shows that an individual unit is specialized to encode information relevant to a subset of classes. We compute the correlation between the accuracy drop under unit ablation and various attributes of an individual unit such as class selectivity and weight L1 norm. We conﬁrm that unit attributes such as class selectivity are a poor predictor for impact on overall accuracy as found previously in recent work [11]. However, our results show that class selectivity along with other attributes are good predictors of the importance of one unit to individual classes. We evaluate the impact of random rotation, batch normalization, and dropout to the importance of units to speciﬁc classes. Our results show that units with high selectivity play an important role in network classiﬁcation power at the individual class level. Understanding and interpreting the behavior of these units is necessary and meaningful."
1806.04699,data,49,,,"In this paper, we focus on weakly-labeled event detection. It presents a challenge that is relevant to many applications, because collecting labeled data is often prohibitively costly. Nevertheless, we believe the main contributions of this paper easily apply to the strongly-labeled scenario too."
1806.04699,data,120,,,"A more intrinsic rationale is that capsule routing can be considered as an attention mechanism. The idea of attention is to focus on the most salient parts of an input via weighting. It has been very successful in numerous applications, including machine translation [17], image captioning [18], and, notably, sound event detection [11], [12]. Attention is particularly useful for SED when training data is weakly labeled; ground truths for the onset and offset times are not available, so the learning algorithm must localize sound events without supervision. Routing implements attention by weighting the association between lower- and higher-level capsules."
1806.04699,"data available, data, dataset",127,,,"In this paper, we have proposed a neural network architecture based on capsule routing for the detection of sound events. The motivation was that capsules can learn to identify global structures in the data that alternatives such as convolutional networks cannot. Our system was evaluated on a weakly-labeled dataset from Task 4 of the DCASE 2017 challenge. We found that the method was considerably less prone to overﬁtting compared to other architectures. For the audio tagging subtask, we achieved a best-in-class F-score of 58.6%, while for the event detection subtask, an F-score of 46.3% and an error rate of 0.76. These are promising results, and suggest that capsule routing should be further investigated."
1806.04699,"data, dataset",54,,,"The dataset used in the experiments has a large amount of class imbalance, which can lead to bias in the classiﬁcation. To alleviate this issue, we used the data balancing technique suggested in [12] to ensure that every mini-batch contains a fair number of samples from each class."
1806.04699,database,46,,,"[16] J. Deng, W. Dong, R. Socher et al., “ImageNet: A large-scale hierarchical image database,” in 2009 IEEE Conf. Comput. Vis. Pattern Recognit., Miami, FL, 2009, pp. 248–255."
1806.04699,dataset,50,,,"[4] A. Mesaros, T. Heittola, A. Diment et al., “DCASE 2017 challenge setup: Tasks, datasets and baseline system,” in Proc. Detection, Classiﬁcation Acoust. Scenes, Events 2017 (DCASE2017), Munich, Germany, 2017."
1806.04699,dataset,87,,,"It is hypothesized that capsule routing will perform well for SED. One of the reasons is contemporary in that current datasets are relatively small, which means training is prone to overﬁtting. To compare with image recognition, ImageNet [16] has more than 14 million training samples, while most environmental sound datasets have thousands. Indeed, we demonstrate this issue in Section IV-A for a number of architectures. By utilizing capsules, we show that overﬁtting can be mitigated."
1806.04699,dataset,89,,,"These results demonstrate that a dynamic routing mechanism can improve the generalization abilities of a neural network. Although it remains to be seen, we are conﬁdent that this applies to other datasets too. Investigating deeper layers of capsules or different capsule networks, such as convolutional capsule networks [29], [30], is a natural direction to take in the future. It is also of interest to explore different routing algorithms, such as that proposed in [29]."
1806.04699,dataset,94,,,"The notion of a capsule was ﬁrst introduced in [13] and very recently revisited in [14] with the addition of a routing mechanism. Simply put, a capsule represents a set of properties for a particular entity. The authors of [14] found that routing with capsules performed better than the state-of-the-art for digit recognition using the MNIST dataset [15]. The motivation for capsule routing is that it implicitly learns global coherence by enforcing part-whole relationships to be learned. For instance, a"
1806.04699,dataset,122,,,"Compared to speech and music recognition, the general characteristics of environmental sounds are much broader, which means it is difﬁcult to apply domain-speciﬁc knowledge. Thus, it is important that the method used is able to perform well despite little a priori knowledge. Supervised deep learning methods have largely satisﬁed this requirement, producing stateof-the-art results consistently in this task [9]–[12]. On the other hand, problems such as overﬁtting have not been completely eliminated, and this is especially severe for smaller datasets. To overcome this, we propose a neural network architecture based on grouping activation units into capsules and using a procedure called routing during inference."
1806.04699,"dataset provided, used dataset, dataset",201,,,"To evaluate the performance of the proposed method, we used the weakly-labeled dataset provided for Task 4 of the DCASE 2017 challenge [4]. This dataset is comprised of 17 sound event classes, of which nine are warning sounds and eight are vehicle sounds. It is divided into a training set, a validation set, and an evaluation set, where the former contains 51,172 audio clips. Each clip is up to ten seconds in duration, and corresponds to one or more sound events that may overlap. For this dataset, two tasks were evaluated: audio tagging and sound event detection. The former is for detecting which sound events occur in an audio clip, while the latter also requires providing onset and offset times. For both tasks, performance was evaluated using micro-averages of precision, recall, and F-scores. For SED, a segment-based error rate with a onesecond time resolution was computed too. We used the sed eval toolbox [22] for evaluation of the SED task. The reader is referred to [22] for a description of these metrics."
1806.04699,github,7,,,2Code available online: https://github.com/turab95/gccaps
1811.11012,data,20,,,"generally focus on real BSM data, in the future we will run the performance analyses with real data."
1811.11012,data,27,,,individual phase. Removing this display bug from the research application will simply be a matter of modifying the way the SPaT data structure is interpreted.
1811.11012,data,28,,,1) Architecture and Data Flow: The National Transportation Communications for Intelligent Transportation System Protocol (NTCIP) is a common protocol used by many trans 10
1811.11012,data,33,,,"[2] “Safety pilot model deployment one day sample data, from Ann Arbor, Michigan,” U.S. Dept of Transportation Intelligent Transportation Systems Joint Program Ofﬁce, 2014."
1811.11012,data,40,,,Intersection Viewer Version 3 provides a working example architecture and foundation for other applications which require a centralized server to host SPaT data. It can be modiﬁed and improved to assist with the development of centralized V2I applications.
1811.11012,data,40,,,"the conﬁning rectangle for generated data are as follows, as seen in Figure 4: (42.356186, -83.522030), (42.356186, -83.816270), (42.226673, -83.522030), (42.226673, -83.816270)."
1811.11012,data,42,,,An intuitive visual simulator has been developed to assist transportation researchers and analysts to study BSM data. This section has documented the application and shared the details of its performance analysis. The simulation successfully generates time-ordered displays that represent vehicle
1811.11012,data,45,,,1) Concept and Purpose: The purpose of this section’s application is to ingest BSM data and use it to display a simulation of vehicle activity and connectivity with respect to time. The simulator takes as input Basic Safety Messages from
1811.11012,data,45,,,"Also, the application presently sends only the light data, which contains the light statuses of the intersection. Eventually, the application should also send the “map data,” which communicates other details about the intersection such as its location."
1811.11012,data,46,,,"The application described in this report is intended to assist an analyst in visualizing location-related aspects of BSM data, speciﬁcally vehicle locations and vehicle connectivity partitions. To display the latter, it is necessary to calculate the partitions using a multi-hop partitioning algorithm."
1811.11012,data,47,,,"minor (see below for qualiﬁcation). The laptop received the SPaT data and, using the previously discussed method, sent it to the AWS server. Multiple cell phones then accessed the data over LTE, displaying the intersection’s state live."
1811.11012,data,58,,,"The details related to connection between the trafﬁc controller and the local laptop/“server” did not change. For example, just as with IV v2, in IV v3 the trafﬁc controller must be conﬁgured to submit data to the IP address of the server. Refer to the server documentation in section 2."
1811.11012,data,58,,,"portation devices. Just as with IV v2, the third iteration of IV is designed to interpret SPaT data sent over UDP in NTCIP. Testing was, again, done with a Siemens m60 series trafﬁc controller. This was the same type of controller used by the local Johnson City, Tennessee Trafﬁc Division."
1811.11012,data,66,,,"A Siemens m60 series trafﬁc controller was used, which is a modern controller employed by various municipalities. Both the IP address of the trafﬁc controller and the single IP address to which it sends SPaT data are conﬁgurable within the controller’s setup menu. While running, the trafﬁc controller constantly transmits SPaT packets ten times per second over the wire."
1811.11012,data,69,,,"2) Results and Conclusions: The completion time was measured in milliseconds and logged for various parts of the simulation process. Also logged was the number of individual input vehicles within the BSM timestamp. After numerous runs with different input CSVs, the log ﬁles were combed for the data and patterns were observed; see Figures 7, 8, 9, and 10."
1811.11012,data,76,,,"IV v1-v3 are all limited in that they can only interpret the SPaT data from eight-phase intersection conﬁgurations. The graduate students will likely want to remove this limitation by giving the application the ability to display other types of intersections. Lastly, the CSS of the client page could be enhanced to make the HTML display more user-friendly on mobile devices. See Figure 14 for the current view on a mobile device."
1811.11012,data,79,,,"2) Testing with a Live Intersection: Once ready, the IV v3 application was tested at the Johnson City, Tennessee Trafﬁc Division. The trafﬁc engineers directed the trafﬁc controller for the intersection of Indian Ridge Rd and West Market St to send its SPaT data to the IP address of the researchers’ laptop. This intersection was largely a classic 8-phase intersection, with W Market being the major and Indian Ridge being the"
1811.11012,data,79,,,"Again, for clarity: under multi-hop communication vehicles can communicate directly, but they may also do so indirectly via one or more intermediary vehicles through which data and messages can pass (or ‘hop’). In the output of the algorithm, all vehicles that can communicate with each other by one of these two means are grouped together as a partition. The result is a collection of partitions. [27]"
1811.11012,data,80,,,"Currently, the application parses SPaT data from the controller into an arbitrary, homegrown data structure; the client machines are set up to interpret that format. SAE J7235 is a new standardized format that has been recently developed by the Society of Automotive Engineers to make SPaT communications uniform. A new task is to update the message format of the IV v3 application to the new standard message format for SPaT data, SAE J7235."
1811.11012,data,80,,,"The server machine receives SPaT data from the trafﬁc controller. It performs the translation from a byte stream to a C# object that is interpreted by the server’s GUI. The server then has the option to forward the SPaT to a single speciﬁed client machine ten times per second. The client builds the GUI in the same way that the server does, such that the GUI is potentially visible on both devices."
1811.11012,data,82,,,"1) General Observations: After several hours of uninterrupted testing, the cell phone used was observed to have a high temperature. It may be beneﬁcial to observe the extent to which the client’s resources are used by the Ajax calls, which currently pull data from the server at a rate of once per millisecond. It is probable that a lower update frequency would reduce the cost of the software while not introducing noticeable latency."
1811.11012,data,82,,,"Surprisingly, the delay between the laptop’s display and the cell phones’ displays was, to the human eye, observable but not signiﬁcant. It was estimated at less than 0.2 seconds. This meant that the data was traveling from the laptop to the AWS server instance in Oregon, being requested by the phones, and being transmitted to and displayed by the phones with, from a human visual perspective, practically no latency."
1811.11012,data,85,,,"[24] M. A. Hoque, P. Pfeiffer, S. Gabrielle, E. Hall, and E. Turbyﬁll, “Safaritaxi: Secure, autonomic, fault-resilient, and intelligent taxi hailing system,” in Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence & Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress (DASC/PiCom/DataCom/CyberSciTech), 2017 IEEE 15th Intl. IEEE, 2017, pp. 438445."
1811.11012,data,98,,,"positions and connectivity statuses. The complexity of the partitioning algorithm has been identiﬁed as the application’s bottleneck when the number of vehicles increases within a conﬁned space. In a future build of the tests, it will be a goal to compare the effects of vehicle population size with those of vehicle population density. For example, keeping vehicle density constant while increasing vehicle population may produce interesting results. Another advancement would be to conduct the performance analyses with real data instead of generated data using hardware-in-the-loop simulation [28]."
1811.11012,data,107,,,"IV v3 allows users to remotely monitor the status of a roadway intersection using the browser of a personal device, such as a laptop or mobile phone. As a web app, this implementation can be used on the web browser of any device without the need to install additional software. Simple asynchronous HTTP requests are made to send and fetch the data. Whereas IV v2 was limited to a single client with a known IP address, in IV v3 any number of clients and any platform which utilizes HTTP can access the data structures used to represent the SPaT data."
1811.11012,data,107,,,"The server was conﬁgured to transfer the data to AWS at a higher frequency than the previous egress frequency. That is, it no longer pauses for 0.1 second before transmitting. Instead, whenever a new SPaT conﬁguration is detected the server makes an asynchronous HTTP call and waits for a response. It was noted that if a signiﬁcant delay was allowed, and if HTTP calls were made too often, the server would gradually fall farther and farther behind the trafﬁc controller, eventually resulting in all remote clients displaying inaccurate views. (Removing this delay from the server also decreased"
1811.11012,data,108,,,"Though IV v1 and IV v2, described below, are conﬁned to desktops on a local wired network, they later became part of a system to broadcast SPaT data over the Internet–like the applications of Audi and California Riverside. This would allow for multiple off-site mobile clients to receive the data for display. It would enable a driver to quickly view the status of a trafﬁc signal from anywhere with a mobile Internet connection. The desktop products, described here in section 2, served as both a prototype and a benchmark for the development and testing of the next IV iteration."
1811.11012,data,112,,,"The server of IV v2 transmits the SPaT in a similar fashion to the way in which the trafﬁc controller transmits the data; that is, the server sends the original byte stream out a speciﬁed port to a pre-deﬁned IP address at a rate of once per decisecond. This means that the server and client must each know the other’s IP addresses and use a common port number. Additionally, whereas the server requires a known IP, the client must connect before communication can begin. The client, in turn, mimics the way in which the server receives and interprets SPaT data."
1811.11012,data,120,,,"roads and often share the same road. They do not have the freedom of going everywhere. Thus, in reality, vehicles will be less sparse and less dispersed than they are in the artiﬁciallygenerated data. This means that, if real BSM data had been used for performance analysis, there likely would have been more or less partitions than were observed. The number of partitions could have thereby had an impact on the time to calculate the partitions. There is therefore a certain degree of uncertainty regarding the extent to which the simulation will perform similarly for real data. Since this application is intended to assist transportation researchers and analysts, who"
1811.11012,data,120,,,"the artiﬁcial data allowed vehicles to disperse themselves randomly. In real data, vehicles would limit themselves to the roads, effectively reducing the space between vehicles. Thus for real data the density at which there would have been only a single partition would have been considerably less than that of this artiﬁcial data. This is especially true when the roads are less like a grid and more like a highway, or when the DSRC transmission range is increased by ﬂat land and few obstacles. 3) Limitations: The program written to artiﬁcially generate BSM data permitted vehicles to be randomly generated anywhere in the rectangular area. In reality, vehicles stay on"
1811.11012,data,125,,,"In addition to its position, each pin has a color and a character(s). Pins with matching color-character combinations are recognized to be in the same connectivity partition–they are able to send data to each other via multi-hop communication (see Figure 2). For example, if two pins are within the DSRC range of each other, they can communicate with a single hop; they will therefore share the same color-character combination for each timestamp in which they remain within range. If two pins are not within the DSRC range, then they cannot communicate unless there exists a middle pin that they can least one mutuallyboth reach via DSRC. If there is at"
1811.11012,data,141,,,"The server application of IV v2 utilizes the C# implementation of the UDP Client to read in SPaT data streams. In the SPaT packets, bytes 211, 213, and 215 (representing red, yellow, and green lights respectively) are relevant to the display. In IV v2, however, byte 211 was ignored due to the assumption that all lights which do not possess a yellow or green light at a given time should be red. (This assumption is only valid in the very limited scope of a regular 8-phase intersection. It was not made by IV v3.) Also note that while a SPaT object is deﬁned as containing 245 bytes, the last four bytes are only appended in trafﬁc intersections which contain pedestrian call buttons."
1811.11012,data,152,,,"The Basic Safety Message (BSM) is a standardized communication packet that is sent every tenth of a second between connected vehicles during vehicle-to-vehicle (V2V) communication. Indeed, one of the primary requirements for all vehicle safety applications is the mandatory broadcast of Basic Safety Messages (BSMs) at this time interval. BSMs contain a wide range of data describing the sending vehicle’s state in time, such as speed, location, and the status of the turn signal [?]. When vehicles and human/non-human drivers have more information about the driving environment around them (such as is provided by BSMs), it follows that they can make safer decisions on the road and drive with higher efﬁciency. In general, vehicle-to-vehicle (V2V) and vehicle-to-infrastructure communications like BSM have major implications for improving automotive transportation."
1811.11012,data,173,,,"Intersection Viewer Version 3 is not intended as a ﬁnished product, but rather a proof of concept and prototype for further development. A team of ETSU graduate students is currently developing an app that should give drivers foreknowledge about the upcoming status of intersections–similar to those developed in by California Riverside and others. Part of their project requires a central server to relay SPaT data to mobile devices over the wireless mobile network. AWS was discussed as the potential host for the centralized portion of their application, but there were concerns about latency. (Minimizing latency is critical to delivering timely notiﬁcations to drivers.) The low latency of IV v3 has now demonstrated that AWS and the mobile network are suitable communication mediums for their application. Hence, the researchers at East Tennessee State University’s Vehicular Networking Lab plan to use IV Version 3 as a starting point for the AWS-hosted, centralized portion of their safety and efﬁciency V2I application."
1811.11012,data,195,,,"This section of the technical report is focused on two similar desktop applications, which will here be referred to as Intersection Viewer (abbreviated IV) Version 1 and Intersection Viewer Version 2. IV v1 (developed by Nick Hodge and J.T. Blevins, with maintenance by Noah Carter) parses SPaT data and displays it in an intuitive graphical user interface. IV v2 (developed by Jacob Hoyos, Matthew Dale, and Noah Carter) goes a step further by not only displaying the GUI but also acting as a server and relaying the SPaT data to a single speciﬁed client machine over a local network. In IV v2, said client then displays the GUI based on the received SPaT data. The intended use of these applications was to serve as a learning experience and a prototype for the application described in section 3 of this report, IV v3. In IV v3 (itself yet another prototype), the client-server relationship was transitioned to the cloud, and made accessible from all personal devices over LTE (and the Internet generally)."
1811.11012,data,259,,,"simulation as a whole and the underlying multi-hop partitioning algorithm (as implemented in JavaScript) as the number of vehicles increased within a conﬁned space. The result was a greater understanding of some of the application’s performance limitations. The multi-hop partitioning algorithm was identiﬁed as taking the bulk of the computation time. Future performance analysis should study the effects of keeping vehicle density constant as vehicle count changes (see below). 1) Process: The performance testing was done by artiﬁcially generating BSM data ﬁles and measuring the simulation’s performance in milliseconds when run on these ﬁles. Each generated ﬁle had a different number of vehicles per timestamp (N ); some had as few as 1 vehicle and others had as many as 200 vehicles per timestamp. All vehicles’ positions were constrained within a ﬁxed rectangular area of 348.16 km in Ann Arbor, Michigan (where the Safety Pilot study took place). The vehicles would appear at random points within the green rectangle in Figure 4. Rather than only appearing on road surfaces, to simplify the generation process they could appear at random anywhere within this rectangle. Also, their positions in one timestamp did not inﬂuence their positions in the next timestamp. In the generated data, vehicles were conﬁned to this given territory. Thus, as the number of vehicles was increased their density (in vehicles per kilometer) rose quickly."
1811.11012,"data available, dataset",149,,,"During studies of vehicle connectivity, BSMs can be collected and aggregated into datasets. Currently, many BSM datasets are available from the connected vehicle testbeds of the U.S. Department of Transportation. (A rudimentary analysis of one such dataset can be found near the end of this paper.) However, without a proper visualization tool, it can be very difﬁcult to quickly and visually analyze the spatiotemporal distribution of the vehicles. To ﬁll this need, a web application has been collaboratively developed (by Salman Ahmed and Noah Carter) which can ingest a raw BSM dataset and display a time-based simulation of vehicle movement. The simulation has also been designed to display the adhoc network connectivity of the vehicles, each of which is assumed to utilize multi-hop communication over Dedicated Short Range Communication (see below)."
1811.11012,"data available, publicly available, data",73,,,"1) BSM Data Analysis: As stated above, some BSM databases are publicly available and are interesting subjects for data analysis. Our previous research has utilized BSM data for analyzing mobility patterns and developing various safety-critical as well as mobility applications in connected vehicle environments [13]–[25]. This includes, for example, the Safety Pilot Model Deployment, in which tens of Michigan"
1811.11012,"data, data https",78,,,"Just as in Version 2, in IV v3 the local server (laptop) can run without a destination to which it egresses SPaT data. Also, it can still transfer the data over HTTP to a local machine running the appropriate ASP.NET environment. However, by default it is conﬁgured to send data to an existing remote, public ASP.NET controller. (Obviously, such remote calls require an Internet connection.)"
1811.11012,"data, data https",249,,,"The following describes the data ﬂow in IV v3. (See also Figure 14.) The trafﬁc controller continuously transmits live SPaT data in UDP over Ethernet. A laptop (or other serverlike device) is connected to the same local network. The laptop receives and parses the SPaT data into a data structure which can be understood by the client. The human-readable parsed data and the visual GUI representation originally designed in IV v1 are displayed on the laptop. Meanwhile, upon receiving a changed SPaT conﬁguration the laptop transmits the clientbound data structure to a server in Oregon via an HTTP request. The server, which is hosted by Amazon Web Services (AWS), contains an ASP.NET controller which receives the updated data structure and saves it to a ﬁle (via an overwrite) on the AWS server. Next, various client machines (such as phones, tablets, or desktops) navigate to a public URL (also hosted by AWS) in their web browsers. The returned HTML page contains an Ajax call that repeatedly requests the saved, client-readable SPaT data structure from the AWS server. Upon receiving the data ﬁle, the client’s page will, if necessary, use JavaScript to interpret the returned data structure and update a visual display of the intersection. That visual display represents the status of the 8-phase intersection."
1811.11012,"data, dataset",70,,,"volunteers’ vehicles were mounted with BSM-broadcasting devices. (See the ﬁnal section of this technical report for a basic analysis of that particular dataset.) One aspect of such data analysis is visualization and simulation. Allowing an analyst to see the data in a helpful and intuitive format enables the analyst to make more informed and strategic decisions [2], [26]."
1811.11012,"data, dataset provided",139,,,"An environmental research group from the University of California Riverside developed a similar application using SPaT data and LTE. It provided drivers advance notice about the upcoming trafﬁc signal timing and contrasted the fuel consumption of an informed driver with that of an uninformed driver [?]. Yet another similar pollution-reducing application was developed with support from the USDOT. It relayed realtime SPaT information to drivers so they could make informed choices [33]. Ahmed et. al proposed an advisory system that generates live trafﬁc signal information and a speed advisory to help the driver reach a destination intersection on time [21]. Like previous works, Ahmed et. al used SPaT information for their advisory algorithm. However, instead of LTE/4G/4G, DSRC technology was employed."
1811.11012,dataset,13,,,Fig. 4: Geographical location of the sample dataset used for simulation
1908.09262,data,36,,,"4. Hollingsworth, K.G.: Reducing acquisition time in clinical MRI by data undersampling and compressed sensing reconstruction. Physics in Medicine and Biology 60(21), R297–R322 (oct 2015)"
1908.09262,data,85,,,"The GAN [6] consists of a generator (G) and discriminator (D). The generator (G) in GAN learns the mapping between two data distributions with the help of discriminator. In the case of MRI reconstruction, the goal of the generator is to learn the mapping between the data distribution of the ZF image (xu) and FS image (xf ). The discriminator learns to distinguish between the generated and target reconstruction."
1908.09262,data,127,,,"2. Caballero, J., Bai, W., Price, A.N., Rueckert, D., Hajnal, J.V.: Application-driven mri: Joint reconstruction and segmentation from undersampled mri data. In: Golland, P., Hata, N., Barillot, C., Hornegger, J., Howe, R. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2014. pp. 106–113 (2014) 3. Dedmari, M.A., Conjeti, S., Estrada, S., Ehses, P., St¨ocker, T., Reuter, M.: Complex fully convolutional neural networks for mr image reconstruction. In: Machine Learning for Medical Image Reconstruction. pp. 30–38 (2018)"
1908.09262,data,143,,,"produce images with excellent spatial resolution and soft tissue contrast. The major advantages of MRI include its non-invasive nature and the fact that it does not use radiation for imaging. However, the major drawback of MRI is the long acquisition time, which causes discomfort to patients and hinders applications in time critical diagnoses. This relatively slow acquisition process could result in signiﬁcant artefacts due to patient movement and physiological motion. The slow acquisition time of MRI can be attributed to data samples not being collected directly in the image space but rather in k-space. k-space contains spatial-frequency information that is acquired line-by-line by the MRI hardware. In order to accelerate the MRI acquisition process, various methods ranging from Partial Fourier Imaging, Compressed Sensing and Dictionary Learning have been developed [4]."
1908.09262,data,209,,,"We propose a novel GAN architecture called Reconstruction Global-Local GAN (Recon-GLGAN). The idea is inspired from a GAN based work [5] in the context of image inpainting. The idea behind Recon-GLGAN is to capture both the global and local contextual features. Recon-GLGAN consists of a generator and a context discriminator. The generator (G) tries to learn the mapping between data distribution of ZF image xu and FS image xf with the help of the context discriminator which can extract global and local features and classify it as real/fake. The context discriminator consists of three components: global feature extractor, local feature extractor and classiﬁer. The global feature extractor (ΨG) takes the entire image as input while the local feature extractor (ΨL) takes the region of interest (ROI) (Φ) from the entire image. The classiﬁer network (ΨC) takes the concatenated feature vector (ΨG(x)||ΨL(x)) to classify the input image as real/fake. The overview of the proposed architecture is shown in Figure 2. The joint optimization of the generator and context discriminator parameters is given by:"
1908.09262,data,229,,,"In the GANCS work [10], the generator is a residual network, the discriminator is a general deep network classiﬁer and a combination of L1 and adversarial loss constitutes the loss function. Similarly, another work ReconGAN [11] uses a multi-stage network as a generator; a simple deep network classiﬁer for the discriminator, and a combination of MSE loss in the image and frequency domains, adversarial loss constitute the loss function. The addition of the frequency domain loss adds data consistency. DAGAN [15] is another work which uses U-Net as a generator, a deep learning classiﬁer as the discriminator with a combination of MSE loss in the image and frequency domains, adversarial loss and perceptual loss as the loss function. It showed that incorporating the perceptual loss term improved the reconstructed image quality in terms of the visually more convincing anatomical or pathological details. CDFNet [3] proposed the use of a combination of MSE loss in the image and frequency domains along with the Structural Similarity Index Measure (SSIM) as a loss function. This can be extended to a GAN setup. We will refer to this setup as ComGAN. SEGAN [8] proposed a generator network called SU-Net and used a general deep network"
1908.09262,dataset,2,,,3.1 Dataset
1908.09262,dataset,28,,,Automated Cardiac Diagnosis Challenge (ACDC) [1] is a cardiac MRI segmentation dataset. The dataset has 150 and 50 patient records for training and
1908.09262,dataset,139,,,"most widely used segmentation network U-Net [12]. U-Net is trained on the FS images to produce multi-class (LV, RV and MC) segmentation outputs. Since the ground truth segmentation masks are unavailable for the test set of the ACDC dataset, we instead use the outputs of the FS images in the test set as ground truth. The reconstructed images from GAN and Recon-GLGAN are passed to the UNet and the corresponding segmentation masks are obtained. The obtained segmentation masks for sample images are shown in Figure 5. It is evident from the ﬁgure that our network’s performance is closest to FS followed by GAN and ZF images. The same are quantiﬁed using the segmentation metrics Dice and Hausdorﬀ for the sample images in Figure 6."
1908.09262,"github, code available, code",9,,,(cid:63) Code available at https://github.com/Bala93/Recon-GLGAN
1908.09262,publicly available,78,,,"The MR images in training set have their corresponding segmentation masks whereas the segmentation masks for MR images in test set are not publicly available. The dimensions of the ROI is set to 60×60 based on a study of the sizes of the segmentation masks in the training set. In the training phase, the center of the ROI for each slice is the midpoint of the closest bounding box of the corresponding segmentation mask."
1912.01136,code,16,,,minimize the effort in the infrastructure-level software development by facilitating the code reusability and modularity.
1912.01136,code,70,,,"In the Algorithms 2 and 3, the pseudocodes for computing the solutions (4.33b) and (4.35b) via sparse Cholesky method are shown, respectively. For the sake of simplicity, they were here split into two different algorithms. Clearly, in the code computation they represent a whole algorithm since the solution (4.33b) is mandatorily required for computing (4.35b)."
1912.01136,data,3,,,5.3 Data Analysis
1912.01136,data,4,,,5.3 Data Analysis .
1912.01136,data,35,,,"- If an experiment works, something has gone wrong. - In any collection of data, the ﬁgure most obviously correct, be yond all need of checking, is the mistake."
1912.01136,data,40,,,"Abraham. Savitzky and M. J. E. Golay. Smoothing and differentiation of data by simpliﬁed least squares procedures. Analytical Chemistry, 36(8):1627–1639, 1964. doi: 10.1021/ac60214a047. URL http://dx.doi.org/10.1021/ ac60214a047."
1912.01136,data,46,,,"Hyeong Ryeol Kam, Sung-Ho Lee, Taejung Park, and Chang-Hun Kim. Rviz: a toolkit for real domain data visualization. Telecommunication Systems, 60(2): 337–345, 2015. doi: 10.1007/s11235-015-0034-5. URL http://dx.doi. org/10.1007/s11235-015-0034-5."
1912.01136,data,47,,,"Jos´e A. Romagnoli and Mabel Cristina S´anchez. 10 estimation of measurement error variances from process data. 2(Supplement C):183 – 197, 1999. ISSN 1874-5970. doi: https://doi.org/10.1016/S1874-5970(00)80025-9. URL http://www.sciencedirect.com/science/article/pii/ S1874597000800259."
1912.01136,data,49,,,"Even if the uniform/non-uniform density assumption is an important part of this analysis (there is an outstanding debate among the scientiﬁc community), the complexity and data collection time are the most likely reasons for the lack of the use of Hatze-like model in biomechanical research."
1912.01136,data,49,,,"Given a human body model composed by interconnected segments, the common mathematical approach iteratively solves the Newton-Euler equations for each model segment. The approach differs depending on the available input data, following two different paths [Winter, 1990] (see Appendix A)."
1912.01136,data,50,,,"The IK tool solves a weighted least-squares problem by means of a general quadratic programming solver. The solver minimizes, for each timestamp in the task range, the difference between the position of the markers on the model and the experimental data. The Matlab API yields to"
1912.01136,data,51,,,"with six F/T sensors placed in the upper arms, in the upper legs and in the ankles. Internal joint torques and external wrenches are estimated through a whole-body estimation algorithm [Nori et al., 2015b]. Robot data were collected at a frequency of 100 Hz."
1912.01136,data,52,,,"Fig. 5.2 (a) An example of a wired IMU. (b) Reference frame of the IMU. (c) Xsens motion capture setup: 17-wired IMUs embedded in a wearable lycra suit, an Access Point connected to a laptop used for the data acquisition."
1912.01136,data,55,,,"J.Y. Keller, M. Zasadzinski, and M. Darouach. Analytical estimator of measurement error variances in data reconciliation. Computers and Chemical Engineering, 16(3):185 – 188, 1992. ISSN 0098-1354. doi: http://dx.doi.org/10. 1016/0098-1354(92)85005-S. URL http://www.sciencedirect.com/ science/article/pii/009813549285005S."
1912.01136,data,57,,,"J. Chen, A. Bandoni, and J.A. Romagnoli. Robust estimation of measurement error variance/covariance from process sampling data. Computers and Chemical Engineering, 21(6):593 – 600, 1997. ISSN 0098-1354. doi: http://dx.doi.org/ 10.1016/S0098-1354(96)00295-5. URL http://www.sciencedirect. com/science/article/pii/S0098135496002955."
1912.01136,data,75,,,⇒ parse Xsens data : suits{t} ⇒ compute sensor position : suits{t}.sensor goto Algorithm 1 ⇒ create URDF model : models{t}.urdf ⇒ create OSIM model : models{t}.osim ⇒ compute IK : qs{t} ⇒ compute Savitzky-Golay ﬁltering : ⇒ wrap available measurements in ys{t} ⇒ compute MAP : ds{t} goto Algorithm 3
1912.01136,data,77,,,"The module provides the ﬁnal estimation for the vector d given as inputs the previous two modules output data. In order to proper cluster data into y vector, it needs the human state and the forces readings both expressed in humans frames. Then, together with the human state (q, ˙q) and some information extracted from the URDF model, the module launches the MAP algorithm with the Cholesky factorization."
1912.01136,data,77,,,"• A version that solves iteratively the Newtonian physics from the top-most to the bottom-most segment (i.e., top-down approach) by the means of only angular acceleration measurements. However, the joint torques estimation is highly sensitive to uncertainties in acceleration data since they are retrieved by a double differentiation of the positions and, therefore, this method tends to produce noisy and non-reliable estimations [Cappozzo et al., 1975]."
1912.01136,data,81,,,"As suggested by the name, the module has to provide in real-time the human state (q, ˙q) . A pre-built URDF model of the human and the Xsens motion capture data (coming here from a YARP driver) are the inputs of the module. The information coming from the motion capture system has to be converted in a representation compatible with our human dynamic model represented in Equation (4.22), as follows."
1912.01136,data,81,,,"Data for the algorithm validation were collected at University of Waterloo (ON, Canada). The setup encompassed i) the Xsens wearable suit (in the look-andhoop strap version) for the human motion tracking , ii) the ftShoes prototype for the ground reaction computation. Two portable AMTI force plates and a printed ﬁxture of the shoes to be applied on the force plates (Figure 5.5a) were additionally required for the shoes validation."
1912.01136,data,86,,,"• A modiﬁed version that encompasses additional measurements in the form of ground reaction forces. The method, also called bottom-up, starts at the bottom-most segment where further boundary conditions are provided (typically ground reaction forces from force plates) and proceeds towards the top-most segment. Typically it produces more precise estimations but the introduction of additional data provides more equilibrium conditions to be satisﬁed: the problem arises at the top-most segment where the physics conditions are not satisﬁed anymore."
1912.01136,data,105,,,"As previously said, the novelty of the MAP framework consists in replacing the classical RNEA boundary conditions with readings coming from different types of sensors. In this Section, we discuss the beneﬁts of the multi-sensor data fusion for solving the estimation problem by characterizing the effects of the data fusion on the covariance of the estimator. The beneﬁt of the MAP is such that the more sensors we use in the estimation, the better the estimation itself will be. Let us consider writing Equations (4.33a) and (4.35a) in a more compact form, such as"
1912.01136,data,110,,,"Another important investigation will concern the estimation of the human inertial parameters (mass, CoM, inertias). The use of the anthropometric tables is currently the most used tool for estimating such values. Even though the tables allow to scale the model along with the subject, this could be a rough approximation for the non-standard population samples (children, elderly, obese, individual with prostheses) and this is pushing the scientiﬁc community towards the development of new alternative ways to estimate these parameters directly from data [Venture et al., 2009], [Robert et al., 2017]."
1912.01136,data,121,,,"is necessary to compute it through the estimation Algorithm 1 (Chapter 3). This step is mandatory since sensors position has to be encoded in the nonstandard extension of the URDF (see Section B.1 of Appendix B). It is worth remarking that, at this stage, a URDF template exists, built according to the human whole-body modelling in Figure B.2. The dimension of the URDF elements (cylinders, parallelepipeds and spheres) is estimated from IMUs data by making in this way the model scalable with the subject proportions. This is possible thanks to the Xsens trajectories acquisition of ‘ﬁctitious’ markers positioned at several anatomical known bony landmarks."
1912.01136,data,124,,,"The analysis, performed in the previous section, is here shown in a variant that encompasses the robot iCub. Data were collected at Istituto Italiano di Tecnologia, on a 48-DoF human model (d ∈ R1248). The experimental setup encompassed i) the Xsens suit for the motion tracking, ii) two standard AMTI OR6 force platforms to acquire the ground reaction wrenches, iii) the F/T sensors of the robot arms. ftShoes were not used in the experiment since they did not exist at that time. Kinematic data were acquired at a frequency of 240 Hz. Each platform acquired a sample at a frequency of 1 kHz by using AMTI acquisition units."
1912.01136,data,128,,,"In this Chapter, a ﬁrst attempt to aim at advancing the current state of the art in pHRI is presented through the design of an estimation tool for monitoring the dynamics of the human in a real-time domain. From a theoretical point of view, the online design does not differ from the version implemented in the previous chapter. However the logic in which input data have to be acquired and the way in which they have to be processed are completely different in the real-time context. Here the idea is to exploit a middleware already developed for the robot iCub to perform the online human dynamics estimation. To this scope the software implementation in Figure 5.1 was revisited and modiﬁed accordingly."
1912.01136,data,135,,,"When an additional measurement (e.g., f ) is added into the computation, the system becomes overdetermined. It is evident at the top-most segment (in the case of bottom-up) or at the bottom-most segment (in the case of top-down) where the physics condition are not satisﬁed anymore. The solution is obtained by discarding one set of 6 equations (e.g., (A.12) or (A.17) for the top-down, (A.22) or (A.23) for the bottom-up) by strongly conditioning the ﬁnal result of the computation. An important drawback of this criterion is that all the other variables (both forces and kinematic data) that entered in the discarded set of equations will remain unused in the computation."
1912.01136,data,138,,,"YARP allows to visualize the real-time human dynamics estimation (whether a pHRI is occurring or not) by means of some structured modules and a ROSbased visualizer (gray part in the Figure 6.1). The human-viz module in composed of different YARP submodules that are in charge of reading information from the Xsens system, the human state (q, ˙q), the estimated vector d, and of publishing information to be sent to the visualization tool in the form of ROS-topic messages. ROS messages (including those coming from the human-forces-provider module) are visualized in the toolkit RViz [Kam et al., 2015]. The advantage in adopting this tool is due to its versatility and agnosticism to the type of the data structure or algorithm."
1912.01136,data,144,,,"The MAP algorithm is performed twice by including as set of sensors ﬁrstly Equation (5.6a) and then Equation (5.6b). The attempt is to prove that, by adding (e.g., increasing) progressively different sensors data, the variance associated to the estimated dynamic variables consequently decreases at each MAP computation, by making the estimation more reliable. By passing progressively from CASE 1 to CASE 2 the variance associated to the torques decreases. Figure 5.17 shows the decreasing behaviour of the 5-subjects mean torque variances from CASE 1 (in orange) to CASE 2 (in violet) for both left and right ankle, hip joints, respectively and L5S1, L4L3, L1T12 joints. The two plots are for the tasks T1 (5.17a) and T2 (5.17b)."
1912.01136,data,145,,,"All the above-listed points are used to obtain Equation (4.14) for the measurements. They are typically kinematic readings, force sensing data (e.g., the ground reaction forces but in general the external forces acting on the model links) and joint accelerations that are here considered as acquired from a class of ‘ﬁctitious’ DoF-acceleration sensors. By exploiting the information coming from the model and the state, the pipeline block of the MAP computation provides the estimation of d given y (see Algorithm 3, Chapter 4). It is worth to highlight that Figure 5.1 does not refer to the temporal aspect of the data ﬂow. For a better understanding of the steps sequentiality refer to the Algorithm 4, where a situation with S subjects performing T different tasks is considered."
1912.01136,data,157,,,"The labelling of the human model links is inherited by Xsens biomechanical model. For each link, geometry, dimensions, inertial parameters, origin and orientation have to be mandatorily speciﬁed. For the geometry, the links are modelled as parallelepiped boxes, cylinders and spheres. In order to make the model scalable with the subject proportions, the dimension of each shape is obtained via the Xsens IMUs readings3. Inertial parameters are computed by exploiting the anthropometric table [Winter, 1990] [Herman, 2007]: for retrieving the mass of each link the weight of the subject (and thus mtot) is ﬁrstly measured and then the mass of each link is obtained by applying tabulated data. Inertia moments are computed as described in Table 3.1. The origin and orientation of the frame associated to each link are deﬁned in the following way:"
1912.01136,data,165,,,"Xsens data were acquired at 240 Hz through the MVN Studio software, ftShoes data at 100 Hz through the YARP driver. Data were synchnonized and downsampled in post-processing phase by means of Matlab. A 48-DoF model template was used both for the URDF and for the OpenSim model, (d ∈ R1248). The IK was computed by using the OpenSim API for Matlab and a Savitzky-Golay ﬁltering (3-th order, 57-elements moving window) was used for retrieving joint velocities ˙q and accelerations ¨q. The MAP settings were tuned as follows: Σd = 104 (i.e., no reliable prior on vector d), ΣD = 10−4 (i.e., high reliable prior on the dynamic model), Σy composed of each sensor variance submatrix (IMUs ≈ 10−3, joint acceleration ≈ 10−3, external forces on the feet ≈ 10−3 and on the other links of the model ≈ 10−6)."
1912.01136,data,172,,,"The least-squares approach became very popular for solving the human ID estimation since it is a versatile tool for addressing optimization problems solution. Since it is historically well-known that ID accuracy is closely related to the reliability of the accelerations data [Challis and Kerwin, 1996], a weighted least-squares optimization approach is adopted in [Cahou¨et et al., 2002] to retrieve optimum acceleration estimates from all the available imperfect and redundant measurements (positions by markers and forces by force plates). Since the considered cost function does not contain information on the joint torques, the method is not so robust to errors in the joint torque estimation. In [Riemer and Hsiao-Wecksler, 2008], a similar methodology is used but combining two important factors: i) a cost function that minimizes the difference between the measured ground reaction forces and the forces estimated via a top-down approach; ii) an optimization criterion for the measured joint angles."
1912.01136,data,224,,,"In general, when an additional measurement is introduced into the computation, a further constraint has to be deﬁned at the ﬁnal segment (both for top-down or bottom-up methods) in order to satisfy the equations of motion. This is due to the fact that the system of equations becomes overdetermined. The solution is retrieved by discarding 6 equations and it is strongly dependent on which equations are discarded, as recognized by [Kuo, 1998] that treats the problem of ﬁnding the joint torques as a least-squares problem. The idea here is to ﬁnd a set of joint torques that best agrees with the available data and to exploit the redundancy of the system when some measurements are missing or unreliable. The main drawback is that it is applicable only to a 2D ﬁxed-base system. In a subsequent study [van den Bogert and Su, 2008], a further development of [Kuo, 1998] is presented, not restricted to 2D ﬁxed-base system but particularly tailored for gait analysis. The method solves ID problems where the number of the unknown variables does not exceed the degrees of freedom (DoFs) of the system by employing a weighting covariance matrix derived via Monte Carlo simulation of measurement errors."
1912.01136,data,260,,,"Chapter 5 presents a software implementation for the MAP algorithm speciﬁcally tailored for Matlab ofﬂine validation procedures. The Chapter shows how the algorithm is able to estimate human kinematic and dynamic variables. The estimation capability of the algorithm is validated through a comparison between those variables that can be measured and estimated at the same time. Although Section 5.3 leads to suitable estimations, the method questionability lies the over-reliance of the solution on the chosen measurement covariances. Their initial setting was manually tuned by using datasheet values (when available) and kept constant for all the setups. This may have altered the goodness of the results. The problem equally affects the real-time YARP estimation since, at the current stage, even there the measurement covariances are deﬁned in a ﬁxed initial conﬁguration ﬁle. The next forthcoming work goes towards the direction of a sort of data-driven covariance estimation as it is well-known that the measurement covariances may vary when the process is at different operating conditions [Romagnoli and S´anchez, 1999]. The Expectation-Maximization (EM) algorithm is one solution for the data-driven estimation [Keller et al., 1992] [Chen et al., 1997] [Kulathinal et al., 2002]. By starting from a known initial covariance of the measurement (e.g., from the sensor datasheet), the covariance Σy|d (Equation (4.35a)) could be optimized until the EM does not increase the likelihood anymore."
1912.01136,"data, code",62,,,"Kinematic data were acquired at a frequency of 240 Hz, force plates and ftShoes data at 100 Hz. The synchronization between the motion capture system and the force plates was guaranteed by a synch station provided by Xsens. Readings from the shoes1 were synchronized by code. Data with different acquisition rates were linearly interpolated before being processed."
1912.01136,"data, dataset",184,,,"The author of a more recent study [Robertson, 2013] classiﬁes into two macrocategories the methodologies for estimating body inertial parameters: regression equations models and geometric approximation models. The former technique is based on equations derived from cadaveric measurements, computed radiographs, or magnetic resonance imaging of a ﬁnite set of specimens. The author reveals the weakness of this approach since it extrapolates data from an average and has little accuracy for modelling a subject that falls outside the proportions of the initial dataset. The other approach consists of a collection of in situ measurements to be mapped into a geometric model to derive mass and properties from certain shape and density assumptions. Here too, the accuracy of the method depends on the complexity of the model and limited to the numbers of measurements. To this purpose, the Hatze model [Hatze, 1980] consists of a 17-segment geometrical model of the body but requires 242 anthropometric measurements, which would imply a relatively substantial amount of time from each subject."
1912.01136,database,16,,,"body human motion database. Robotics (ICAR), pages 329–336, July 2015."
1912.01136,database,33,,,"G. Guerra-Filho and A. Biswas. The human motion database: A cognitive and parametric sampling of human motion. In Face and Gesture 2011, pages 103– 110, March 2011."
1912.01136,database,41,,,"H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. Hmdb: A large video database for human motion recognition. In 2011 International Conference on Computer Vision, pages 2556–2563, Nov 2011."
1912.01136,database,48,,,"J. Wojtusch and O. von Stryk. Humod - a versatile and open database for the investigation, modeling and simulation of human motion dynamics on actuation level. In 2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids), pages 74–79, Nov 2015."
1912.01136,dataset,124,,,"A Root Mean Square Error (RMSE) analysis (Table 5.2) has been performed for the above described dataset, for validating the three components of the forces (fx, fy,fz) and moments (mx, my, mz). Figure 5.6 shows the forces (on the left column) and moments (on the right column) for validating the left ftShoe reading (in human left foot frame) w.r.t. the gold standard provided by its coupled force plate FP1, for subject S1, tasks T1-T2. Similarly, Figure 5.7 validates the right ftShoe w.r.t. the force plate FP2, for the same subject, in the same tasks."
1912.01136,dataset,157,,,"As in Section 5.3.2, even here the MAP algorithm was able to estimate d for all the links/joints in the model. As in the previous dataset, the MAP results are here shown for one subject (i.e., S1). Figure 5.20a shows the comparison for the external forces measured (in red) and estimated (in blue) for the right foot5 and for both the human hands since during the interaction there are forces acting on them. The same comparison concerning the linear acceleration for the right foot, upper leg and hand, respectively, is shown in Figure 5.20b. Similar analysis in Figure 5.21a for the joint acceleration, for the right ankle and the hips. Again here, like in the previous case, the MAP algorithm is able to give us a suitable estimation of the human joint torques, Figure 5.21b."
1912.01136,github,2,,,//github.com/RealTimeBiomechanics.
1912.01136,github,2,,,//github.com/claudia-lat/MAPest.
1912.01136,github,2,,,//github.com/robotology/human-dynamics-estimation.
1912.01136,github,11,,,RTbiomechanics. Github repository for real-time biomechanics. URL https:
1912.01136,github,14,,,HDE. Github repository for human dynamics estimation. 2017. URL https:
1912.01136,github,15,,,MAPest. Github repository for human map ofﬂine estimation. 2016. URL https:
1912.01136,"github, code, open-source code, open-source",31,,,"All the software related to this Chapter has been released as an open source code and it is hosted on GitHub in the Robotology organization [HDE, 2017]."
1912.01136,"github, data",136,,,"Chapter 6 endeavours to design a C++ based tool for monitoring the real-time dynamics of a human being physically involved in a collaborative task with a robot. This Chapter, in a sense, describes the real-time evolution of the previous ofﬂine framework and proposes a preliminary validation analysis with very promising results. The real-time context automatically yields to a different way for retrieving the human state and the joint accelerations w.r.t. the ofﬂine case. Currently, a straightforward way for computing q and ˙q from data has been already implemented in Section 6.1. However, we are still investigating on the real-time way for obtaining ¨q. Some solutions are suggested in [van den Bogert et al., 2013] and in the GitHub repository [RTbiomechanics]."
1912.01136,"github, data, code",85,,,"In this Section the software architecture of the MAP framework is presented (see pipeline in Figure 5.1). It was implemented in Matlab and created speciﬁcally for ofﬂine validations. The code related to the MAP ofﬂine computation is hosted on GitHub in a dedicated public repository [MAPest, 2016]. The starting point of the analysis consists in the IMUs readings. Kinematic data are parsed into a compatible Matlab format and then processed as in the following steps."
1912.01136,open-source,52,,,"S. L. Delp, F. C. Anderson, A. S. Arnold, P. Loan, A. Habib, C. T. John, E. Guendelman, and D. G. Thelen. Opensim: Open-source software to create and anIEEE Transactions on Biomedical alyze dynamic simulations of movement. Engineering, Nov 2007."
1912.01136,provide implementation,77,,,"This Chapter presents the software implementation of the MAP solution and several ofﬂine validation procedures to assess the effectiveness of the presented estimation algorithm. Since the sensors play a paramount role in the analysis, they will be detailed described in a dedicated section. A new type of wearable force sensing is also introduced in this Chapter and an investigation for its validation w.r.t. the force gold standard is provided to the reader."
1912.01136,publicly available,69,,,"• Collection of human motions: movements of a human agent (i.e., the teacher in the imitation process) are retrieved with motion capture techniques and clustered in motion databases. Some publicly available examples are [GuerraFilho and Biswas, 2011] [Kuehne et al., 2011] [Wojtusch and von Stryk, 2015] [C. Mandery and Asfour, 2015]."
1912.09582,data,2,,,2.1 Data
1912.09582,data,5,,,2 Pre-training data and parameters
1912.09582,data,5,,,3 Tasks and test data
1912.09582,data,30,,,We are grateful to Daniel de Kok for sharing the Wikipedia data. BERTje was trained with Cloud TPUs from Google’s TensorFlow Research Cloud (TFRC).
1912.09582,data,31,,,"To evaluate the effectiveness of BERTje for use on downstream tasks, the model is ﬁne-tuned for several NLP tasks. We use annotated data from three sources for this."
1912.09582,data,34,,,Semantic Roles and Spatio-Temporal Relations The results in Table 3 show that BERTje outperforms multilingual BERT for the semantic role labeling (SRL) and spatio-temporal relation (STR) based test data.
1912.09582,data,44,,,"BERT was pre-trained with two objectives: next sentence prediction (NSP) and masked language modeling (MLM). Based on ﬁndings after the initial release of BERT, we made modiﬁcations in the pre-training data generation procedure for both tasks."
1912.09582,data,55,,,"The results in Table 1 and Table 3 both show a similar pattern where the scores on training data are higher than the development and test results. This indicates that BERTje may be prone to overﬁtting just like other models. Therefore, hyper-parameter tuning for speciﬁc tasks may help to improve performance."
1912.09582,data,60,,,"However, for these tasks the model has not really improved after the 850K checkpoint. For evaluation of the SRL data, the CoNLL-2002 evaluation script is used in order to take chunk overlap of multi-token expressions into account. The results on these tasks are stand-alone since we are not aware of the existence of similar systems."
1912.09582,data,73,,,"To demonstrate the effectiveness of using multi-genre data in a monolingual model, and to equip NLP research on Dutch with a high-performing model, we developed a Dutch BERT model which we call BERTje.5 In this paper we describe the training process of BERTje and evaluate its performance by ﬁne-tuning the model on several Dutch NLP tasks. We compare the performance on all tasks to that achieved using multilingual BERT."
1912.09582,data,79,,,"For each of the previously described tasks, three models are ﬁne-tuned: multilingual BERT base, BERTje at the 850K checkpoint (BERTje850k) and the fully trained BERTje model (1M checkpoint). All models are ﬁne-tuned for four epochs on the training data for each task with the same hyperparameters. Longer training has shown degradation of performance on some validation data and increase of performance after the fourth epoch has not been observed."
1912.09582,data,92,,,"Third, we evaluate on several classiﬁcation tasks that originate from the SoNaR-1 corpus of written Dutch (Delaere et al., 2009). We evaluate on token-level NER (6 labels), coarse POS tags (12 labels) and ﬁne-grained POS tags (241 labels in total of which 223 are present in the training data). The ﬁne-grained POS tags contain many labels that are used only once. In addition to NER and POS tags, we extract three other annotation types from SoNaR-1:"
1912.09582,data,116,,,"Named-Entity Recognition Table 1 shows the span-based F1 scores of the ﬁne-tuned models. For both the CoNLL-2002 data as well as the SoNaR-1 data, it is clear that BERTje outperforms the multilingual BERT model. Additionally, the BERTje model has improved after the 850K checkpoint. Our models do not outperform the state-of-the-art test score of 90.9% of Wu and Dredze (2019) on the CoNLL-2002 test data. This model is a well optimized ﬁne-tuned large multilingual BERT model. Based on the performance difference between multilingual BERT and BERTje, it is likely that replicating their approach with a monolingual Dutch BERT model would improve the state-of-the-art performance."
1912.09582,"data, data https",92,,,"architecture have been developed for Italian (Polignano et al., 2019), French (Le et al., 2019), German,2 Finnish (Virtanen et al., 2019), and Japanese.3 The Italian model is pre-trained on Twitter data, which may not be representative for general use of language and is only trained on the MLM objective, as the NSP task is barely applicable to tweets. The other models are pre-trained on a combination of Wikipedia with additional data from for instance online news articles.4"
1912.09582,"data, dataset",83,,,"To facilitate comparison and due to limited resources, we opt to train a single Dutch BERT-based model that is architecturally equivalent to the BERTBASE model with 12 transformer blocks (Devlin et al., 2019). However, the pre-training data is of course different and other pre-training data generation modiﬁcations were made based on later derivations of BERT. Nevertheless, we aimed to collect a dataset of similar size and diversity as used for the English BERT model."
1912.09582,"data, dataset",96,,,"First, we use the Dutch CoNLL-2002 named-entity recognition (NER) data (Tjong Kim Sang, 2002). This is a four-class BIO-encoded named-entity classiﬁcation task with the following four classes: person, organisation, location and miscellaneous. Second, we evaluate on the 16 universal part-of-speech (POS) tags in the Lassy Small treebank (van Noord et al., 2013) part of Universal Dependencies v2.5 (Zeman et al., 2019). Both datasets are already split into train, development, and test sets."
1912.09582,"data, dataset",101,,,"Documents that originate from chats or Twitter were removed from the SoNaR corpus because of quality considerations. We also removed the Wikipedia documents from SoNaR to avoid overlap with the full Wikipedia dump. Finally, in order to avoid any overlap with texts that we want to use as test data, we removed all documents from SoNaR-500 that are included in the manually annotated SoNaR-1 and Lassy Small (van Noord et al., 2013) datasets. As a result, the ﬁnal pre-training dataset contains 12GB of uncompressed text which amounts to about 2.4B tokens."
1912.09582,dataset,13,,,for small datasets–a case with Dutch book reviews. arXiv preprint 1910.00896.
1912.09582,dataset,15,,,Table 4: Sentiment Analysis accuracy scores on the 110k Dutch Book Reviews Dataset.
1912.09582,dataset,53,,,"Like BERT, we constructed a WordPiece vocabulary with a vocabulary size of 30K tokens. A SentencePiece model (Kudo and Richardson, 2018) was created for this based on the raw pre-training dataset. The resulting vocabulary is translated to WordPiece format for compatibility with the original BERT model."
1912.09582,dataset,55,,,"For the Lassy Small dataset, BERTje outperforms the 95.98% accuracy score achieved by UDPipe 2.0 (Straka, 2018). These scores are not strictly comparable, since they evaluate on UD 2.2, while we evaluate on UD 2.5; however, the differences can be assumed to be minimal."
1912.09582,dataset,56,,,"Sentiment Analysis Table 4 shows the sentiment analysis accuracy scores on the 110k Dutch Book Reviews Dataset. Without hyperparameter tuning, BERTje comes close to the 93.8% score that van der Burgh and Verberne (2019) obtain with manual hyperparameter tuning of an ULMFiT model (Howard and Ruder, 2018)."
1912.09582,github,3,,,1https://github.com/google-research/bert/blob/master/multilingual.md
1912.09582,github,26,,,"2https://deepset.ai/german-bert 3https://github.com/cl-tohoku/bert-japanese 4A monolingual Dutch model has also been made available at http://textdata.nl, but this this model was consistently"
1912.09582,"github, dataset",114,,,"The transformer-based pre-trained language model BERT has helped to improve state-of-the-art performance on many natural language processing (NLP) tasks. Using the same architecture and parameters, we developed and evaluated a monolingual Dutch BERT model called BERTje. Compared to the multilingual BERT model, which includes Dutch but is only based on Wikipedia text, BERTje is based on a large and diverse dataset of 2.4 billion tokens. BERTje consistently outperforms the equally-sized multilingual BERT model on downstream NLP tasks (part-of-speech tagging, named-entity recognition, semantic role labeling, and sentiment analysis). Our pre-trained Dutch BERT model is made available at https://github.com/wietsedv/bertje."
1912.09582,"used dataset, dataset",73,,,"Each of the previous tasks describe a low level linguistic type of information. However, we want to test BERTje on a more high-level, downstream task, such as sentiment analysis. For this, we use the 110k Dutch Book Reviews Dataset (van der Burgh and Verberne, 2019), a balanced collection of positive and negative reviews which lends itself to a binary sentiment classiﬁcation task."
2004.14247,code,152,,,"6G communication systems require ﬂexibility in codeword length and coding rate. The most commonly used coding schemes today are Turbo codes, Polar codes, and low-density parity check codes (LDPC). Their performance has already been pushed towards the limit using 16 nm and 7 nm technology [93]. Trade-oﬀs must be made between parallelization, pipelining, iterations, and unrolling, linking with the code design and decoder architecture. The future performance and eﬃciency improvements could come from CMOS scaling, but the picture is quite complex [94]. Indeed, trade-oﬀs must be made to cope with issues such as power density/dark silicon, interconnect delays, variability, and reliability. Cost considerations render the picture even more complex: cost due to silicon area and due to manufacturing masks, which explode at 7 nm and below."
2004.14247,code,175,,,"Gbps. Infrastructure links are even more demanding since they aggregate user throughput in a given cell or virtual cell, which is expected to increase due to spatial multiplexing. It is diﬃcult to achieve such a high throughput, only relying on the progress of integrated circuit manufacturing technology within ten years. Solutions must be found from the algorithm side as well. Both code design and corresponding encoding/decoding algorithms need to be taken into account to reduce the decoding iterations and improve the decoder’s parallelism level. Moreover, it is vital for the decoder to achieve reasonably high energy eﬃciency. To maintain the same energy consumption as in current devices, the energy consumption per bit needs to be reduced by 1-2 orders of magnitude. Implementation considerations such as area eﬃciency (in Gbps/mm2), energy eﬃciency (in Tb/J), and absolute power consumption (W) put huge challenges on code design, decoder architecture, and implementation [93]."
2004.14247,code,284,,,"The channel coding scheme used in 6G high-reliability scenarios must provide a lower error ﬂoor and better “waterfall” performance than that in 5G. Short and moderate length codes with excellent performance need to be considered. Polar codes, due to their error correction capabilities and lack of error ﬂoor, might be the preferred choice in 6G. However, state-of-the-art CRC-aided successive cancellation list decoding doesn’t scale up well with throughput due to the serial nature of the algorithm. Hence, iterative algorithms that can be parallelized need to be developed [95]. In [96], a new variant of the multi-trellis BP decoder is proposed, which permutes only a subgraph of the original factor graph. This enables the decoder to retain information of variable nodes in the subgraphs, which are not permuted, reducing the required number of iterations needed in-between the permutations. As a result, the proposed decoder can perform permutations more frequently, hence being more eﬀective in mitigating the eﬀect of cycles, which cause oscillation errors. Work in [97] proposes some new algorithms to process new types of node patterns that appear within multiple levels of pruned sub-trees, and it enables to process certain nodes in parallel. Furthermore, modiﬁed polar code constructions can be adopted to improve the performance of iterative algorithms, potentially with the aid of machine learning such as deep unfolding [98, 99]. These approaches could lead to better communication performance but will require signiﬁcant advances in understanding the behavior, robustness, and generalization of neural networks."
2004.14247,"code, provide implementation",155,,,"The non-orthogonal access and interference management provided by massive MIMO were considered suﬃcient in 5G; thus, neither NOMA nor RS was included in the ﬁrst releases. However, there are prospective 6G use cases where interference management at the protocol level can potentially provide suﬃciently large gains to outweigh the increased implementation complexity. In massive connectivity scenarios, where many devices transmit small packages intermittently, grant-free access using code-domain NOMA or similar protocols can be very competitive. In mmWave or THz communications where the controllability of the beamforming is limited by hardware constraints (e.g., phased arrays), NOMA and RS can enable multiple users to share the same beam [112]. There is also a need for multiple access schemes in VLC, where coherent adaptive beamforming might be practically impossible. NOMA and RS might be the required solution [113]."
2004.14247,data,17,,,"sur Cell-free massive MIMO, suitable modulation techniques ML/AI based model-free, data-driven learning and optimization techniques"
2004.14247,data,31,,,"9Algorithm Department, ZTE Corporation, China (email: hu.yuzhou@zte.com.cn). 10School of Electrical and Data Engineering, Faculty of Engineering and Information Technology, University of"
2004.14247,data,31,,,"• High-rate physical layer: The high-rate physical layer corresponds to a communication system whose objective is to maximize the data rate for a given band, i.e., the spectral"
2004.14247,data,39,,,"From 4G to 5G, the peak data rate has increased by 10-100 times, and this trend is likely to continue with 6G. The throughput of a single decoder in a 6G device will reach hundreds of"
2004.14247,data,44,,,"• Extreme data rates: Peak data rates up to 1 Tbps are envisaged for both indoor and outdoor connectivity. The user-experienced data rate, which is guaranteed to 95% of the user locations, is envisioned to reach 1 Gbps."
2004.14247,data,45,,,Peak data rateKPIExperienced data rateReliabilityLatency5G20 Gb/s0.1 Gb/s1 ms1-10-56G1 Tb/s1 Gb/s100 s1-10-9Connection densityPeak spectral efﬁciencyExperienced spectral efﬁciencyArea trafﬁc capacity30 b/s/Hz0.3 b/s/Hz106 devices/km210 Mb/s/m260 b/s/Hz3 b/s/Hz107 devices/km21 Gb/s/m2JitterEnergy efﬁciencynot speciﬁednot speciﬁed1 s1 Tb/JMobility500 km/h1000 km/hMaximum bandwidth1 GHz100 GHzFigure 4: Potential spectrum regions for 6G.
2004.14247,data,51,,,"• Enhanced hotspot: An enhanced hotspot entails a high-rate downlink from the AP to several user devices, with short coverage and low receiver complexity constraints. Envisaged applications are hotspots delivering high-speed data to demanding applications such as highdeﬁnition video streaming and enhanced Wireless LAN (WLAN)."
2004.14247,data,53,,,"Methods to overcome the challenge of oﬄine model training, which causes performance degradation due to the discrepancies between the real channels and training channels, need to be considered. ML implementations with online training and constructing training data to match real-world channel conditions are potential future directions in this regard."
2004.14247,data,60,,,"• Short-range device-to-device communications: A symmetric high-rate point-to-point link with very stringent energy and complexity constraints is considered here. The use case focuses on data exchange between user devices that are within a short distance, with limited involvement of the network infrastructure. This use case also includes inter/intrachip communications and wireless connectors, among others."
2004.14247,data,61,,,"• Other use cases: Some other applications that 6G is expected to enable or vastly enhance internet access on planes, wireless brain-computer interface based applications, include: broadband wireless connectivity inside data centers, Internet of Nano-Things and Internet of Bodies through smart wearable devices and intrabody communications achieved by implantable nanodevices and nanosensors [5]."
2004.14247,data,64,,,"The wireless technology becomes more complicated for every generation with many sophisticated interconnected components to design and optimize. In recent research, there have been an increasing interest in utilizing machine learning (ML) techniques to complement the traditional model-driven algorithmic design with data-driven approaches. There are two main motivations for this [114]: modeling or algorithmic deﬁciencies."
2004.14247,data,67,,,"JointmulticastingEdge CloudData ""shower""(data kiosk)Multi-hop topologyMulti-connectivityTransmit-receivepoint (TRP)Network-assistedcontent deliveryLocation-specificcacheDevice-to-deivcecommunicationMulti-user immersiveapplicationsVariable space-timecontextAntennaDynamic cachereplacementBlockageDynamic mobilityGroup mobilityHigh-endeyewearMultiple usersConsumer applicationsIndustrial applications© 6G Flagshipstorage capacity (it is one of the cheapest network resources) makes CC especially beneﬁcial given the uniqueness of this use case, where the popularity of limited and location-dependent content becomes much higher than in any traditional network."
2004.14247,data,74,,,"The operation in existing bands will be enhanced in 6G with respect to the KPIs described above, but not all targets are expected to be reached in all frequency bands. For example, low frequency bands are often preferable in terms of spectral eﬃciency, reliability, mobility support, and connectivity density. In contrast, high frequency bands are often preferable in terms of peak data rates and latency."
2004.14247,data,75,,,"In summary, it is desirable that idle/inactive UEs can harness the beneﬁts of the cell-free architecture as much as the active UEs can. To realize this, it is imperative to redesign the procedures of cell search and random access. To reduce the latency and improve the resource utilization eﬃciency, NOMA-enabled two-step RACH [46] or autonomous grant-free data transmission [47] should be investigated in cell-free networks."
2004.14247,data,75,,,"ticularly, when there are more users that require simultaneous service than the APs is able to separate by beamforming. When some of those users are requesting the same data at the same time, broadcast and multicast are suitable transport mechanisms for large-scale delivery, since they permit to transmit the same content to a vast number of devices within the covered area at the same time and with a predeﬁned quality-of-service."
2004.14247,data,78,,,"even in very high mobility scenarios up to 1000 km/h [3, 4]. Moreover, higher data rates are required due to the increased number of sensors in vehicles that are needed to assist autonomous driving. Other autonomous mobility solutions like drone-delivery systems and drone swarms are also evolving in diﬀerent application areas such as construction, emergency response, military, etc. and require improved capacity and coverage [3]."
2004.14247,data,83,,,"Recent and emerging wireless technologies at millimeter and THz frequencies are primarily targeting to improve the wireless connectivity where it is already relatively good; that is, at short distances to the access network and to serve densely packed users and devices with even more capacity. The rush towards higher peak data rates has the by-product of increasing the digital divide by oﬀering most gain to the users and devices which are physically near to an abundant radio access infrastructure."
2004.14247,data,83,,,"There are also situations where the data-driven approach leads to highly complex algorithms, both when it comes to computations, run-time, and the acquisition of the necessary side information. This is called an algorithmic deﬁciency and it can potentially be overcome by ML techniques, which can learn how to take shortcuts in the algorithmic design. This is particularly important to obtain eﬀective signal processing for latency-critical applications and when jointly optimizing many blocks in a communication system."
2004.14247,data,89,,,"This white paper explores the road to implementing broadband connectivity in future 6G wireless systems. Diﬀerent categories of use cases are considered, from extreme capacity with peak data rates up to 1 Tbps, to raising the typical data rates by orders-of-magnitude, to support broadband connectivity at railway speeds up to 1000 km/h. To achieve these goals, not only the terrestrial networks will be evolved but they will also be integrated with satellite networks, all facilitating autonomous systems and various interconnected structures."
2004.14247,data,92,,,"This white paper has been written by researchers from academia, research centers, industry, and standardization bodies. It extends the work in [1] with a more speciﬁc focus on broadband connectivity, which might be the most important use case in 6G (though far from the only one). In particular, it describes the physical (PHY) layer and medium-access control (MAC) layer methodologies for achieving 6G broadband connectivity with very high data rates, up to the Tbps range."
2004.14247,data,94,,,"Cell-free massive MIMO can be deployed in any frequency band, including below-6 GHz, mmWave, sub-THz, and THz bands. In the latter cases, the APs can serve each UE using a bandwidth of 100 GHz or higher, which yields extremely high data rates over short distances and low mobility. The spatial diversity gains of the cell-free architecture become particularly evident in such scenarios because the signal from a single AP is easily blocked, but the risk that all neighboring APs are simultaneously blocked is vastly lower.20"
2004.14247,data,96,,,"Terahertz-band communication [60–62] is envisioned as a 6G technology able to simultaneously support higher data rates (up to around 1 Tbps) and denser networks (towards of billions of interconnected devices). For many years, the lack of compact, energy-eﬃcient device technologies which are able to generate, modulate, detect, and demodulate THz signals has limited the feasibility of utilizing this frequency range for communications. However, major progress in electronic, photonic, and innovative plasmonic device technologies is ﬁnally closing the THz gap."
2004.14247,data,100,,,"The emergence and need for 6G technology will be governed by the unprecedented performance requirements arising from exciting new applications foreseen in the 2030 era, which existing cellular generations will not be able to support. This white paper focuses on applications that require broadband connectivity with high data rates, in combination with other specialized characteristics. The following is a list of the potential new use cases and applications in 6G which will help to understand the key requirements of future 6G systems. A summary of those discussed use cases is presented in Figure 1."
2004.14247,data,101,,,"• Smart rail mobility: This is a required component in a paradigm where infrastructure, trains, passengers, and goods are seamlessly connected with high data rates. Railway communications are evolving from only supporting critical signaling applications to providing several bandwidth-intensive applications, such as on-board and wayside high-deﬁnition video surveillance, broadband connectivity for passengers, broadcasting of passenger information, and remote driving or control. These applications need to be deployed in at least ﬁve scenarios: train-to-infrastructure, inter-wagon (the train backbone), intra-wagon, inside the station, and infrastructure-to-infrastructure."
2004.14247,data,102,,,"There are many practical constraints. For VLC systems, intensity modulation/direct detection (IM/DD) is the most practical scheme. The intensity-modulating data signal must satisfy a positive-valued amplitude constraint. Hence, it is not possible to straightforwardly apply techniques used in RF communications. A VLC-enabled device inside a pocket or briefcase cannot be connected optically. A solution to this would be a hybrid optical-radio wireless network. A reconﬁgurable optical-radio network is a high-performance and highly ﬂexible communications system that can be adapted for changing situations and diﬀerent scenarios [86, 87]."
2004.14247,data,104,,,"in the order of 100 Gigasamples-per-second. Therefore, high-parallelized systems and eﬃcient signal processing are needed to make the most out of the THz band. Since channel coding is the most computationally demanding component of the baseband chain, eﬃcient coding schemes need to be developed for Tbps operations. Nevertheless, the complete chain should be eﬃcient and parallelizable. Therefore, algorithm and architecture co-optimization of channel estimation, channel coding, and data detection is required. The baseband complexity can further be reduced by using low-resolution digital-to-analog conversion systems, where all-analog solutions are also being considered."
2004.14247,data,104,,,"• Multi-sensory extended reality: Augmented, mixed, and virtual reality (AR/MR/VR) applications, capturing multi-sensory inputs and providing real-time user interaction are considered under this use case. Very high per-user data rates in Gbps range and very low latencies are required to deliver a fully immersive experience [3]. Remote connectivity and interaction powered by holographic communications, along with all human sensory input information, will further push the data rate and latency targets. Multiple-view cameras used for holographic communications will require data rates on the order of terabits per second [4]."
2004.14247,data,126,,,"• Industrial automation and robotics: Industry 4.0 envisions a digital transformation of manufacturing industries and processes through cyber-physical systems, internet-of-things In order to achieve high(IoT) networks, cloud computing, and artiﬁcial intelligence. precision manufacturing, automatic control systems, and communication technologies are utilized in the industrial processes. Ultra-high reliability on the order of 1-10−9 and extremely low latency around 0.1-1 ms round-trip time are expected in communications, along with real-time data transfer with guaranteed microsecond delay jitter in industrial control networks [4]. While 5G initiated the implementation of Industry 4.0, 6G is expected to uncover its full potential supporting those stringent requirements by the novel, disruptive technologies brought by 6G."
2004.14247,data,128,,,"Fully immersiveAR/MR/VRHolographiccommunications withmulti-sensory inputcapturing> 1 Tb/s data ratesMulti-SensoryExtended RealityIndustrial AutomationEnhanced Industry 4.0High precisionmanufacturingUltra-high reliability andextremely low latenciesReal-time operationswith microsecond jitterSmart transportationwith fully autonomousvehiclesDrone-delivery systemsDrone swarmsUltra-high reliability andextremely low latenciesHigh capacity andextended coverageAutonomous MobilityExtreme CapacityxHaulEnhanced HotspotHigh-speed downloadHigh-deﬁnition videostreamingEnhanced Wireless LAN(WLAN)100+ Gbps per cell10+ meters rangeIndoorLow receiver complexityconstraintsConnector lessapplicationsInter-chipcommunicationIntra-chipcommunication2+ meters rangeIndoorHigh energy andcomplexity constraintsDevice-to-DeviceEnhanced broadbandﬁxed accessData aggregation of largenumber of users1 Tb/s transport200+ meters rangeIndoor / outdoorNo energy or complexityconstraintsSmart Rail MobilityFully interconnectedinfrastructure, trains,passengers and goodsHD video surveillanceHigh-data rateconnectivity forpassengersPassenger informationRemote drivingConnectivityanywhere, anytimeacross the globeReduced global digitaldivide10 Mb/s data rate inevery populated areaof the worldCost-efﬁcient design toenable actualdeployment targetsConnectivity inRemote AreasFigure 2: An example setup of an extreme capacity xHaul network in an indoor environment [2]."
2004.14247,data,146,,,"There are several practical challenges with implementing robust interference management at the protocol level. One is the error propagation eﬀects that appear when applying superposition coding and successive interference cancellation to ﬁnite-sized data blocks. Another issue is the high complexity in jointly selecting the coding rates of all messages, to enable successful decoding wherever needed, and conveying this information to the users. Implementing adaptive modulation and coding is nontrivial in fading environments with time-varying interference and beamforming, even if it is carried out on a per-user basis. With NOMA and RS, the selection of modulation/coding becomes coupled between the users, making it an extremely challenging resource allocation problem. Scheduling and ARQ retransmissions are other tasks that become more complex when the user data transmission is coupled. Hence, there is a large need to study"
2004.14247,data,164,,,"GHz spectrum will deﬁne the wide-area coverage and spatial multiplexing, a signiﬁcant eﬀort is expected in sub-THz and THz bands to achieve short-range connectivity with data rates in the vicinity of 1 Tbps. Novel coding, modulation and waveforms will be needed to support extreme data rates with manageable complexity and robustness to the hardware impairments that increase with the carrier frequency. In situations where the beamforming capabilities oﬀered by the ultramassive MIMO technology are insuﬃcient to manage interference in the spatial domain, coding methods based on rate-splitting or broadcasting can be also utilized. The eﬃciency can be also improved by making use of caching techniques. Moving on to photonics-based methods, one key development can be in holographic radio. This will revolutionize the way the communication is carried out until now. Seamless integration between satellites and terrestrial networks will ensure that large populations outside the urban centers will be reached by high-quality broadband connectivity."
2004.14247,data,164,,,"There is a long experience in operating wireless communication systems in the sub-6 GHz spectrum. Moving from sub-6 GHz to mmWave introduced several technical challenges from initial access to beamforming implementation since fully digital solutions take time to develop. The development of 5G has led to large innovations in these respects. It becomes even more challenging when going to higher frequencies. The main reason for moving to higher frequency bands is to greatly expand the available spectrum. In the sub-THz to THz region, hardware constraints such as the speed of data converters and computational complexity will cause challenges to make eﬃcient use of wide bandwidths. Moreover, investigations into new waveforms, mitigation of hardware impairments, as well as new materials to implement devices operating in that spectrum are required. This makes the sub-THz to THz region a new area where many open research problems exist from the hardware to the physical layer protocols."
2004.14247,data,166,,,"There are diﬀerent ways to implement holographic radios for the purpose of joint imaging, positioning, and wireless communications [23]. However, extreme broadband spectrum and holographic RF generation and sensing will generate massive amounts of data. These are challenging to process to perform critical tasks with low-latency and high reliability. Thus, machine learning might be required to operate the system eﬀectively. To meet the 6G challenges of energy eﬃciency, latency, and ﬂexibility, a hierarchical heterogeneous optoelectronic computing and signal processing architecture will be an inevitable choice [24]. Fortunately, holographic radios achieve ultra-high coherence and high parallelism of signals by coherent optical up-conversion of the microwave photonic antenna array, and this ultra-high coherence and high parallelism also facilitate the signal to be processed directly in the optical domain. However, it is challenging to adapt the signal processing algorithms of the physical layer to ﬁt the optical domain."
2004.14247,data,168,,,"The term free space optical (FSO) communication is utilized to refer to OWC systems in the infrared frequency range. Such systems are commonly found at the basis of long-range high-speed point-to-point links utilized, for example, in ultra-broadband wireless backhaul applications [82] and, to a lesser extent, for indoor communications [83]. Here, we focus on visible light communication (VLC), or OWC in the visible light spectrum, which is a promising technology to provide broadband local connectivity [84]. In VLC, all the baseband processing at the transmitter and the receiver is performed in the electrical domain and light-emitting diodes (LEDs) with a large ﬁeld of view (FoV) or laser diodes (LDs) with small FoV are used to encode and transmit data over the LOS/NLOS optical channel. Photodetectors at the receiver convert data-carrying light intensity back to electrical signals for baseband processing."
2004.14247,data,179,,,"Reducing the peak-to-average-power-ratio (PAPR) is another important technology direction in order to enable IoT with low-cost devices, edge coverage in THz communications, industrialIoT applications with high reliability, etc. There will be many diﬀerent types of demanding use cases in 6G, each having its own requirements. No single waveform solution will address the requirements of all scenarios. For example, as discussed also in Section 4.1, the high-frequency scenario is faced with challenges such as higher phase noise, larger propagation losses, and lower power ampliﬁer eﬃciency. Single-carrier waveforms might be preferable over conventional multicarrier waveforms to overcome these challenges [102]. For indoor hotspots, the requirements are instead the higher data rates and the need for ﬂexible user scheduling. Waveforms based on OFDM or on its variants exhibiting lower out-of-band emissions [103–105] will remain a good option for this scenario. 6G needs a high level of reconﬁgurability to become optimized towards diﬀerent use cases at diﬀerent time or frequency."
2004.14247,data,185,,,"The beamforming challenge for 6G is to make use of physically large panels, since the dimensionality of the beamforming is equal to the number of antennas, and the beamwidth is inversely proportional to the array aperture. With an ultra-high spatial resolution, each transmitted signal can be focused in a small region around the receiver, leading to a beamforming gain proportional to the number of antennas. An ultra-high level of spatial multiplexing is also possible since each signal is only strong in a small region around the receiver. This is particularly important to make eﬃcient use of the low bands, where the channel coherence time is large and, therefore, can accommodate the channel estimation overhead for many users. With a suﬃcient number of antennas, 1 MHz of spectrum in the 1 GHz band can give the same data rate as 100 MHz of spectrum in the 100 GHz band. The reason is that the lower band supports spatial multiplexing of 100 times more users since the coherence time is 100 times larger."
2004.14247,data,186,,,"Beyond the physical layer, new link and network layer strategies for ultra-directional THz links are needed. Indeed, the necessity for very highly directional antennas (or antenna arrays) simultaneously at the transmitter and at the receiver to close a link introduces many challenges and requires a revision of common channel access strategies, cell and user discovery, and even relaying and collaborative networks. For example, receiver-initiated channel access policies based on polling from the receiver, as opposed to transmitter-led channel contention, have been recently proposed [80]. Similarly, innovative strategies that leverage the full antenna radiation pattern to expedite the neighbor discovery process have been experimentally demonstrated [81]. All these aspects become more challenging for some of the speciﬁc use cases deﬁned in Section 2, for example, in the case of wireless backhaul, for which very long distances lead to very high gain directional antennas and, thus, ultra-narrow beamwidths, or smart rail mobility, where ultra-fast data-transfers can aid the intermittent connectivity in train-to-infrastructure scenarios.21"
2004.14247,data,188,,,"However, the performance of cell-free massive MIMO systems can be sensibly boosted by increasing the level of coordination among the APs, in particular, by sending the CSI to the CPU and perform joint combining/precoding [33, 49] as in conventional CoMP and network MIMO. In this case, the APs can be made smaller since most of the digital operations are carried out at the CPU. While the data rates can be increased, the drawback is higher computational complexity and latency, and the fronthaul signaling might also increase. If there are 64 antennas taking 20-bit samples, the total fronthaul capacity will far exceed 1 Gbps at the sampling rates of interest. This imposes signiﬁcant challenges for interconnection technologies, such as Common Public Radio Interface (CPRI) and on the I/O interface of application-speciﬁc integrated circuits (ASIC) or ﬁeld-programmable gate arrays (FPGA) [50]. A potential solution to this problem can be forming separate antenna clusters and having separate CPRI for each cluster. However, this increases the"
2004.14247,data,194,,,"Performance-wise, data throughput below 100 Mbps can be achieved with relatively simple optical transceivers and oﬀ-the-shelf components. Data rates of up to hundreds of Gbps have been demonstrated in laboratory conditions. Key optical components for VLC, such as LEDs and photodetectors, have been developed for decades, and they are typically low-cost standard components. VLC is not intended to replace but complement existing technologies. When it comes to use cases, VLC can be used for both conventional data services for consumers and support emerging applications and services such as smart city, smart buildings, factories of future, intelligent transportation systems (ITS), smart grid and the internet of things (IoT). The concept of light-based IoT (LIoT) exploits light not only to create optical links but also to harvest its energy [85, 86]. Thus, a LIoT node can be energy autonomous. VLC will also be useful in scenarios in which traditional RF communication is less eﬀective such as in-cabin internet service in airplanes, underwater communication, healthcare zones, etc."
2004.14247,data,315,,,"At mmWave frequencies and above, there is a need for dense network deployments to mitigate the reduced coverage at these bands due to constraints in the propagation environment and hardware limitations in the transceivers. Such dense deployments make backhauling/fronthauling challenging at these bands, in particular, it is expensive and cumbersome to roll out ﬁber links to all APs. However, the presence of very wide bandwidths at these carrier frequencies makes it possible to include the wireless backhaul/fronthaul in the same spectrum as the wireless access. For this reason, integrated access and backhaul (IAB) network conﬁgurations seem promising, where a few (potentially, ﬁber-connected) APs provide other APs as well as the mobile devices inside their cell area with wireless backhaul and access connections, respectively [13, 54]. IAB networks are diﬀerent from conventional relay networks, because the information load changes in diﬀerent hops. Particularly, as the number of mobile devices per hop increases, the APs need to transfer an aggregated data of multiple mobile devices accumulated from the previous hops. In addition, the interference on the links will depend on the information load. Some early work has been performed. It was demonstrated in [13, Sec. 7.6] how massive MIMO can be used for backhaul to small cells. In this framework, the access and backhaul are separated in space. In [55], an alternative framework for an IAB-enabled cellular network was developed and used to characterize the downlink rate coverage probability. In this framework, the bandwidth was divided between access and backhaul, either statistically or based on the dynamic network load. Given the fact that 6G networks will see an even greater level of densiﬁcation and spectrum heterogeneity"
2004.14247,data,344,,,"Massive MIMO is a cellular technology where the access points are equipped with a large number of antennas, which are utilized for spatial multiplexing of many data streams per cell to one or, preferably, multiple users. The massive MIMO technology has become synonymous with 5G, but the hardware implementation and algorithms that are used diﬀer to a large extent from what was originally proposed in [11] and then described in textbooks on the topic [12, 13]. For example, compact 64-antenna rectangular panels with limited angular resolution in the azimuth and elevation domains are being used instead of physically large horizontal uniform linear arrays with hundreds of antennas, which would lead to very narrow azimuth beams. Moreover, a beam-space approach is taken where 2D discrete Fourier transform (DFT) codebooks are used to describe grids of 64 beams in predetermined angular directions for rectangular panels, while only one of these 64 predeﬁned beams is selected for each user. This approach is only appropriate for line-ofsight (LOS) communication with calibrated planar arrays and widely spaced users. In general, non-LOS channels contain arbitrary linear combinations of those beams, the arrays might have diﬀerent geometry, and the array responses of imperfectly calibrated arrays are not described by a DFT. A practical reason for these design simpliﬁcations is that analog and hybrid beamforming were needed to come quickly to the market in 5G. However, fully digital arrays will be available for a wide range of frequencies (including mmWave) when 6G arrives and, therefore, we should utilize it to implement something capable of providing a performance substantially closer to what the massive MIMO theory suggests [12, 13]. Since the massive MIMO terminology has become diluted by many suboptimal design choices in 5G, we will use the term ultra-massive MIMO [14] in this paper to describe the 6G version of the technology."
2004.14247,"data available, data",144,,,"Coded caching (CC) was proposed in [123] as a way to increase the data rate, with the help of cache memories available throughout the network. It enables a global caching gain, proportional to the total cache size of all network users, to be achieved in addition to the local caching gain at each user. This additional gain is achieved by multicasting carefully created codewords to various user groups, such that each codeword contains useful data for every user in the target group. Technically, if K, M and N denote the user count, cache size at each user and the ﬁle library size respectively, using CC the total data size transmitted over the broadcast link can be reduced by a factor of 1 + t, where t = KM"
2004.14247,"data, code",208,,,"However, there are situations when the interference management provided by massive MIMO is insuﬃcient, for example, if the spatial resolution of the antenna panels is too limited [108], which can happen when the array is far from the users. In these cases, interference can also be managed at the protocol level, using so-called non-orthogonal multiple access (NOMA) schemes [109] or rate-splitting (RS) [110]. The NOMA techniques are mainly based on two domains: power and code [111]. Power-domain NOMA refers to the superposition of multiple messages using diﬀerent transmit power, such that users with higher SNRs can decode interfering signals before decoding their own signal, while users with lower SNRs can treat interference as noise. Code-domain NOMA refers to the use of non-orthogonal spreading codes, which provide the users with higher SNRs after despreading, at the expense of additional interference. RS is based on dividing the users’ messages into private and common parts. Each user decodes its private part and the common parts to extract its data. RS can be viewed as a generalization of power/code-domain NOMA."
2004.14247,"data, dataset provided",52,,,"overall complexity of the system. Over-the-air bi-directional signaling between the APs and the UEs might be utilized as a ﬂexible alternative to fronthaul signaling [51–53]. Furthermore, it is necessary to investigate scalable device coordination and synchronization methods to implement CSI and data exchange [44]."
2004.14247,"data, dataset provided",169,,,"• Extreme capacity xHaul: This use case refers to a ﬁxed symmetric point-to-point link targeting high data rates without energy or complexity constraints since no user devices are involved. This can only be enabled using a combination of high bandwidth and high spectral eﬃciency. The envisioned ultra-dense network topology in urban areas with extreme capacity and latency requirements makes ﬁber-based backhauling highly desirable but complicated due to the limited ﬁber network penetration (variable from country to country) and related extension cost. Hence, wireless infrastructure is needed as a ﬂexible complement to optical ﬁber deployment, both indoor and outdoor, to avoid any bottleneck in the backhaul (or xHaul). Ultra-high speed is required since backhaul aggregates the data rates of many user devices. The xHaul can also provide eﬃcient access to computing resources at the edge or in the cloud. Figure 2 depicts an example setup of an extreme capacity xHaul network in an indoor environment."
2004.14247,provide implementation,115,,,"The cell-free massive MIMO deployment can also provide support for the implementation of lowlatency, mission-critical applications. The availability of many APs, coupled with the rapidly decreasing cost of storage and computing capabilities, permits using cell-free massive MIMO deployments for the caching of content close to the UEs and for the realization of distributed computing architectures, which can be used to oﬄoad network-intensive computational tasks. Moreover, in low-demand situations, some APs can be partially switched oﬀ (control signals may still be transmitted) with a rather limited impact on the network performance, thus contributing to reducing the OPEX of mobile operators and their carbon footprint."
2005.07667,code,52,,,"[31] P. Yin and G. Neubig, “A syntactic neural model for general-purpose code gener.,” in Proc. of the 55th Annu. Meeting of the Assoc. for Comput. Linguistics (Vol. 1: Long Papers), pp. 440–450, 2017."
2005.07667,"code, data, database",239,,,"The possibility to use a natural language statement to query a database has the potential to attract a vast majority of users that are not proﬁcient in using query languages such as the Structured Query Language (SQL). This language is the main query language for relational databases currently in use. The problem of text to SQL mapping could be viewed as a Semantic Parsing problem [1], which is deﬁned as transforming a natural language input into a machine-interpretable representation. Semantic parsing is a long-standing question and is a well-studied problem in Natural Language Processing (NLP). As such, it has attracted much attention both from academia and from the industry, especially translating natural language into SQL queries. A large amount of the data in today’s age is stored in relational databases for applications ranging from ﬁnancial and e-commerce domains to medical domains. Therefore, it comes as no surprise that querying a database using natural language has many different applications. It also opens up the prospects of having self-serving dashboards and dynamic analytics, where people not accustomed to the SQL language could use it to get the most relevant information for their business. The task of translating natural language to SQL has many related tasks such as code generation and schema these tasks could be ultimately combined generation. All"
2005.07667,data,53,,,"[24] G. Huilin, G. Tong, W. Fan, and M. Chao, “Bidirectional attention for sql gener.,” in 2019 IEEE 4th Int. Conf. on Cloud Comput. and Big Data Anal. (ICCCBDA), pp. 676–682, IEEE, 2019."
2005.07667,"data, database",113,,,"[25] R. Cai, B. Xu, Z. Zhang, X. Yang, Z. Li, and Z. Liang, “An encoderdecoder framework translating natural lang. to database queries,” in Proc. of the 27th Int. Joint Conf. on Artif. Intell., pp. 3977–3983, 2018. [26] J. D. Lafferty, A. McCallum, and F. C. Pereira, “Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data,” in Proc. of the Eighteenth Int. Conf. on Mach. Learn., pp. 282–289, Morgan Kaufmann Publishers Inc., 2001."
2005.07667,"data, database, dataset",180,,,"For that purpose, in recent years, with the extensive development of deep learning techniques, especially convolutional and recurrent neural networks, the results are drastically improving. There have been quite a few researches attempting to generate data processing results by directly linking records in the tables to the semantic meaning of the natural language input, such as [6] and [7]. However, these attempts are not scalable to big tables and are not reusable when the database schema is changed. More recent approaches use only the natural language input and the database schema and metadata to generate the queries. We review the most recent approaches in our research. Furthermore, the release of large annotated datasets containing questions and the corresponding database queries has additionally enhanced the ability to use deep learning or supervised techniques to tackle this problem. This has enabled the problem to evolve into a more complex task where the approaches should be domain independent and involving multiple tables with complex queries."
2005.07667,"data, dataset",156,,,"The newest datasets, WikiSQL2 [15] and Spider3 [16], are cross-domain context-independent with a larger size. Also, newer datasets have a greater number of questions and more comprehensive queries. The size of the datasets is crucial for the proper model evaluation. Unseen complex queries in the test sets can evaluate the model generalization ability. Authors in [14] show that the generalisability of the systems is overstated by the traditional data splits. The WikiSQL dataset contains a large number of questions and SQL queries, yet these SQL queries are simple and concentrated on single tables [16]. The Spider dataset contains a more modest number of questions and SQL queries than WikiSQL. However, these questions are more complex, and the SQL queries include different SQL clauses such as join of tables and nested query [16]."
2005.07667,database,13,,,"[29] R. Shin, “Encoding database schemas with relation-aware self-attention"
2005.07667,database,14,,,academic publications Yelp website Internet Movie Database / student course information 138 different domains
2005.07667,database,54,,,"[10] L. R. Tang and R. J. Mooney, “Automated construction of database interfaces: Intergrating statistical and relational learning for semantic parsing,” in 2000 Joint SIGDAT Conf. on Empirical Methods in Natural Lang. Process. and Very Large Corpora, pp. 133–141, 2000."
2005.07667,database,128,,,"The progressions in the NLP area are reﬂected in the designed models of this problem. The encoder-decoder framework is incorporated to translate the natural language into an SQL query. The encoder serves for natural language processing, whereas the decoder predicts the BNF representation of the SQL output. The sketch-based approach is introduced for SQL representation for eliminating the ordering effect of sequence generation. Additional efforts incorporate attention to the bidirectional LSTM network with the sketch-based method. The augmented pointer network is also combined in the novel models. The Relation-Aware Self-Attention approach is an improvement on already existing methods to overcome several limitations. It includes a relationship graph of the database schema and self attention to encode more complex relationships."
2005.07667,database,161,,,"Seq2SQL7 [15] method consists of two parts: augmented pointer generator network and main Seq2SQL model. The augmented pointer network generates the content of the SQL query token-by-token by copying from the input sequence. The input sequence x is composed of the following tokens: words in the question, column names in the database tables and SQL clauses. The network encodes x with two-layer bidirectional LSTM network using the embeddings of its words. Next, a pointer network [22] is applied. The decoder is a two-layer unidirectional LSTM that generates one token at each timestep using the token generated in the previous step. It produces scalar attention score for each position of the input sequence. The token with the highest score is selected as next token. The second part, Seq2SQL, is composed of three different parts: Aggregation Operation, SELECT Column and WHERE Clause."
2005.07667,database,208,,,"The ﬁrst part, Aggregation Operation, classiﬁes aggregation operation of the query, if any. First, scalar attention score is computed for each token in the input sequence. The vector of scores is then normalized in order to produce a distribution is computed with a Multilayer over the input Perceptron (MLP) with cross-entropy loss. The second part, SELECT Column, points to a column in the input table. Each column name is ﬁrst encoded with LSTM network such that the last encoded state of the LSTM is assumed to be representation of the speciﬁc column. With the same architecture, representation for the input question is calculated. MLP with cross-entropy loss is applied to compute score for each column conditioned on the input representation. The last part, WHERE Clause, generates the conditions for the query. For this part, reinforcement learning is applied to optimize the expected correctness of the execution result. Next token is sampled from the output distribution. When the complete query is generated, it is executed against the database. The reward is: (1) -2 if the generated query is not a valid SQL"
2005.07667,database,258,,,"This approach explained in [29]8 is an improvement on the already existing methods so that it overcomes some crucial limitations such as: working in only one domain, working with one database schema, working with only one table or overlapping training and test sets. The main improvement of this approach focuses on the encoder part of the encoderdecoder framework already seen in previous approaches. To incorporate the relationships between schema elements in the encoder, the database schema is translated to a directed graph where each node represents either a table or a column and the edge represents the relationship between the elements. The label in the node represents the name of the table or column appropriately. The columns additionally have their type prepended. All the edges between the nodes are labeled as well to represent the exact relationship they represent. The relationships can be: (1) Column X Column Y relationship where X and Y belong to the same table or X is a foreign key for Y (or vice versa), (2) Column X Table T relationship (or vice versa) where X is the primary key of T or X is a column of T (but not a primary key), and (3) Table T Table R relationship where T has a foreign key column in R (or vice versa) or T and R have foreign keys in both directions."
2005.07667,"database, dataset",82,,,"The translation of a natural language to SQL queries is a problem of semantic parsing. There are several text-to-SQL datasets developed that include natural language questions that can be answered by executing an SQL query from a database. The progression of the datasets introduces a combination of different domains with multiple databases and tables. The increase in the size of the datasets is apparent. Also, the questions are becoming more complex and in more extensive number."
2005.07667,"database, dataset",167,,,"II. TEXT-TO-SQL DATASETS The datasets designed for semantic parsing of natural language sentences to SQL queries are composed of annotated complex questions and SQL queries. The sentences are questions for a speciﬁc domain, and the answers for these questions are derived from existing databases. Therefore, the particular question is connected with an SQL query. The execution of the SQL query extracts the answer from the existing database/s. Nowadays, there are several semantic parsing datasets developed for SQL query mapping. All of the different datasets vary in several aspects. Table I provides detailed statistics of the most used datasets among researchers. The early developed datasets concentrate on one domain and one database: ATIS [8], GeoQuery [9], Restaurants [8], [10], Academic [11], Scholar [12], Yelp [13], IMDB [13] and Advising1 [14]."
2005.07667,dataset,4,,,TABLE I TEXT-TO-SQL DATASETS
2005.07667,dataset,45,,,"Dataset ATIS [8] GeoQuery [9] Restaurants [10], [19] Academic [11] Scholar [12] Yelp [13] IMDB [13] WikiSQL [15] Advising [14] Spider [16]"
2005.07667,dataset,46,,,The SParC [17] and CoSQL [18] are the extension of the Spider dataset that are created for contextual cross-domain semantical parsing and conversational dialog text-to-SQL system. These new aspects open new and signiﬁcant challenges for future research in this domain.
2005.07667,dataset,77,,,"[16] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman, et al., “Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task,” in Proc. of the 2018 Conf. on Empirical Methods in Natural Lang. Process., pp. 3911–3921, 2018."
2005.07667,dataset,89,,,"This survey aims to overview some of the latest methods and models proposed in the area of SQL query generation from natural language. We describe models with various architectures such as convolutional neural networks, recurrent neural networks, pointer networks, reinforcement learning, etc. Several datasets intended to address the problem of SQL query generation are interpreted and brieﬂy overviewed. In the end, evaluation metrics utilized in the ﬁeld are presented mainly as a combination of execution accuracy and logical form accuracy."
2005.07667,dataset,107,,,"There is no single metric for evaluation of the text-to-SQL model. One strategy is to estimate the correctness of the result for the question. This metric is called execution accuracy [15]. It compares the result from the generated SQL query and the result from the ground truth query. Then it returns the number of correct matches divided by the total number of examples in the dataset. One shortcoming of this approach is that it does not eliminate the cases when a completely different query is giving the same result as the expected, for example, the NULL result."
2005.07667,dataset,145,,,"In this paper, we provide an extensive research on the most used datasets, as well as the most recent approaches applied on these datasets to handle the problem of generating SQL queries from natural language input. The main motivation of this paper is to provide a comprehensive explanation and analysis of the most recent methods to handle the task of generating SQL using natural language as well as the different datasets and evaluation techniques used. The rest of the paper is organized as follows. Section II describes the different datasets that have been used in the approaches described. A comprehensive explanation of the different methods for generating SQL queries from text is presented in Section III. The different evaluation methods used for this problem are outlined in Section IV. Finally, Section V concludes the paper."
2005.07667,github,8,,,"5https://github.com/guotong1988/NL2SQL, last visited: 05.05.2020"
2005.07667,github,8,,,"7https://github.com/salesforce/WikiSQL, last visited: 05.05.2020"
2005.07667,github,8,,,"8https://github.com/rshin/seq2struct, last visited: 05.05.2020"
2005.07667,"github, data",32,,,"1https://github.com/jkkummerfeld/text2sql-data, last visited: 05.05.2020 2https://github.com/salesforce/WikiSQL, last visited: 05.05.2020 3https://github.com/taoyds/spider, last visited: 05.05.2020 4https://github.com/xiaojunxu/SQLNet, last visited: 05.05.2020"
2005.07667,publicly available,85,,,"With the rise of deep learning techniques, there is extensive ongoing research in designing a suitable natural language interface to relational databases. Mostly, the models in this area rely upon the encoder-decoder framework that is widely used in the ﬁeld of natural language processing. The following subsections present some of the models utilized in the ﬁeld. Some of the models described in this paper are publicly available which enables other researchers to evaluate or build other models upon them."
2005.08622,data,29,,,"Silla, C.N., Freitas, A.A., 2011. A survey of hierarchical classiﬁcation across different application domains. Data Mining and Knowledge Discovery 22, 31–72."
2005.08622,data,111,,,"Fourth experiment HDL is designed to maximize the learning capacity and to extract the hierarchical structure from the labelled data. Our intuition is that our model, lead to different losses at any level, with the power to reduce intra-variance and to maximize inter-variance, can obtain better accuracy than a classical convolutional neural network. To prove this, we conduct six different experiments using a classic ResNet18 and our HDL on Animals Taxonomy8 using two learning rate and a batch size of 64. The results in Fig. 2 and 9,10 clearly conﬁrm our expectations. In all cases, the accuracy is higher than a"
2005.08622,"data available, data, used dataset, dataset",35,,,"1. In the ﬁrst one, we use a well-known dataset (VQA-Med 2019) to test our approach with biomedical real images, also in the case we have few data available."
2005.08622,"data available, dataset",29,,,2. In the second we test the capability of abstraction of our approach on a synthetic dataset created in the context we have thousand of instances available.
2005.08622,"data available, dataset",30,,,"To evaluate the proposed method, we created our own datasets as there is no a standard benchmarked dataset on hierarchical multi-label images classiﬁcation, available in the literature."
2005.08622,"data, python, dataset",139,,,"This data set is created from Flickr animals images, the hierarchy represents a small taxonomy with class, family and species as in Fig. 6. The selected class is mammalia and reptilia. The second level of hierarchy is the family, in particular felidae and ursidae for mammalia and crocodyle, iguanidae, emydidae and pythonidae for reptilia. The last hierarchy level represent the species( example of images in ﬁg. 5) as malaysia tiger, felis catus known as cat, ailuropoda melanoleuca known as giant panda, ursus maritimus known as polar bear, python molurus known as green python, trachemys scripta as small turtle, iguana iguana and crocodylus niloticus well known as nilus crocodile. A whole representation of the dataset is in Fig. 3."
2005.08622,"data, python, dataset",263,,,"We build our hierarchical multi-label classiﬁer model as an extension on a Resnet-18, but is it possible to apply to any Convolutional Neural networks. We implement our extension in Python using Pytorch framework. Fig.1 shows the architecture used for experiments. The size of the input images is re-scaled to 64x64x3 for Geometry dataset and 256x256x3 for VQA-Med 2019 and Animals Taxonomy8 datasets. We do not apply any preprocessing of images as data augmentation, rotation or normalization. The kernel size of the ﬁrst convolutional layers is 7x7 with a stride of 2 pixels, followed by a normalization of layer and a non-linear layer with ReLu activation function. A max-pooling operation over 3x3 regions and a stride of 2 pixels. Then, we have four blocks of Convolution, with 64, 128, 256, 512 numbers of plans respectively and apply an adaptive average pooling over 1x1 region. Finally, we add three fully connected linear layer, where each layer corresponding to the total number of concepts in our hierarchical dataset. In the forward process, we take the output after the adaptive average pooling and apply Center loss function and for each linear layers we apply softmax function and then cross-entropy loss. The total loss will be the sum of the local loss per layers. Our network was trained with Adam optimizer Kingma and Ba (2014). The batch-size used, learning rate, epochs are described jointly with the results for each dataset."
2005.08622,dataset,1,,,Datasets
2005.08622,dataset,3,,,3. Datasets
2005.08622,dataset,10,,,Fig. 2: Some sample of geometric dataset.
2005.08622,dataset,10,,,Fig. 6: Animals Taxonomy8 dataset classiﬁcation hierarchy.
2005.08622,dataset,21,,,"3. In the third, we extract hierarchical structure on the realimages dataset contains images of three types of animal"
2005.08622,dataset,37,,,"We have created a synthetic geometric shapes dataset which contains 2 different shapes (Triangle, Square, some image sample into ﬁg. 2) at the ﬁrst level of our hierarchy. Each shape has"
2005.08622,dataset,39,,,"depth, indeed CIFAR100 contains only two levels of depth (Super Class, Classes) and other datasets with many depths ﬁnd applicability only in text classiﬁcation or in bioinformatics, where the inputs are not images."
2005.08622,dataset,52,,,"Fig. 4: Four images extracted from VQA-Med 2019 Abacha et al. (2019) dataset (synpic371, synpic10103, synpic16486, synpic48315). The labels separated by comma belong to the sub-categories of the three main classes following the order: Modality, Plain, Organ."
2005.08622,dataset,60,,,"G|×1 is the bias vector with φ linwhere W 1 ear activation function and d be the number of features. Then, we add a linear layer l for each hierarchical level in a generic dataset and we perform the cross-entropy loss to maximize the inter-class variance. Precisely, we apply softmax function from logits of layer ll"
2005.08622,dataset,82,,,"This study is placed in the sub-category of multi-label classiﬁcation called Structure output learning. In according with experimental results at Tables 1, 2, we achieved good results on three different datasets ﬁnding the way to exploit the dependency among classes and make accurate predictions, reducing the misclassiﬁcation than a classic ResNet18. The main reason we have created these datasets is to prove our proposal in the ﬁeld of computer vision and with more than 2 levels of"
2005.08622,dataset,90,,,"In literature, multi-label classiﬁcation is an important ﬁeld in machine learning and it is strongly related to many realworld applications for example, in biomedical images annotation, document categorization and whatever problem which the instances inside the classes are not disjoint but they keep a hierarchical structure. In this work, we have conducted four empirical studies on different datasets to prove by experimental results the effectiveness and robustness of our proposed model, that can be applied as an extension to any Convolutional Neural Network."
2005.08622,dataset,112,,,"Fig. 8: Training losses Animals Taxonomy8 with lr = 0.005. We emphasize the different descent of losses. This is due to the number of concepts to distinguish from each layer. Each line represents the loss for each layer. For this dataset, we design our model with a shape of 6,8,2 to distinguish Family, Species and Classes respectively. As we show also in 1, the line yellow that represents linear layer 3 with 2 concepts (Mammals or Reptiles) has more descent power, indicating that our model quickly learns a few concepts rather than many as red line or green."
2005.08622,python,1,,,Python
2005.08622,python,11,,,MammalsFelidaePolar bearTigerCatOrsidaeGiant PandaPythons AA. CrocodileReptilesGreen IguanaTurtles ACrocodyliaPythonaeIguanaTestudinesTurtles BPythons BXX5
2005.08622,"python, dataset",50,,,ReptiliaMammaliaFelidaeUrsidaeFelis catusAiluropodamelanoleucaMalaysia TigerUrsusmaritimusCrocodylidaeIguanaiguanaCrocodylusniloticusPython molurusClassFamilySpeciesPythonidaeChordataIguanidaeEmydidaeTrachemysscripta020406080Epoch0.00.51.01.52.0LossModel lossesCenter LossCrossentropy Layer 1Crossentropy Layer 2Crossentropy Layer 3050100150200250Epoch0.00.51.01.52.0LossModel lossesCenter LossCrossentropy Layer 1Crossentropy Layer 2Crossentropy Layer 3Table 1: Accuracies comparison using three different datasets on different learning rate. We use a batch size of 32 on VQA-MED and 64 on the other datasets.
2005.08622,"used dataset, dataset",114,,,"• To prove the effectiveness of our hierarchical classiﬁcation approach we conduct empirical studies on three different datasets. First, we created Animals Taxonomy8 dataset based on real animal images from Flickr on three groups of taxonomy (Class, Family, Species) with their relative label annotations. Second, we used a well-known biomedical dataset (VQA-Med 2019) contains radiology real images on different levels of hierarchy and third, we created Geometry shapes annotations that contains thousands of shapes images on three depth hierarchy levels. Further, all datasets have a different number of instances (2.8k,8k,40k) useful to prove the robustness of our approach."
2005.08622,"used dataset, dataset",141,,,"Second experiment In a second experiment, we use a synthetic dataset with simple geometric shapes and several instances 7.10 times greater than VQA-Med 2019. Our intuition is that attribute more samples per classes can improve the training of our model and subsequently, to obtain better performance in terms of accuracy than the ﬁrst experiments. To prove this conjecture, we train with 20K instances our HDL and test it with 6k instances. The results in Table. 1 at the second row per tables, conﬁrm our expectations. The higher number of instances jointly with the simplicity of images allows the model to reach high accuracy starting from the ﬁrst ten epochs. Furthermore, we conduct three different runs with a learning rate of 0.005, 0.001, 0.01 using batch-size of 64."
2007.12442,code,42,,,"(such as Raspberry Pi units). In addition, the motivating usecases described in §III can beneﬁt of the additional security guarantees provided by MQT-TZ with no additional hardware and without changes to the client’s application code."
2007.12442,code,73,,,"Secure Messaging Libraries Using TEEs. MQT-TZ leverages the native support of MQTT to establish TLS channels between the client and the broker. TaLoS [12] can establish secure TLS termination inside Intel SGX enclaves. Deploying a complete TLS stack inside the TEE is unnecessary in our context, and it would yield a larger attack surface, as the code loaded in the TRUSTZONE must be fully trusted."
2007.12442,code,199,,,"First, the OP-TEE framework is sufﬁciently mature to allow rapid development cycles to quickly test with TRUSTZONE. Trusted Applications, as our LRU cache in the TEE or the reencryption TA, can achieve performances close to the corresponding non-secure ones. However, the crypto primitives they provide are slower than expected, especially when compared to other state-of-the-art libraries (i.e., OpenSSL [5]). Our experimental results highlight a slowdown of up to two orders of magnitude. We hope to mitigate these drawbacks exploiting hardware support for TRUSTZONE-speciﬁc Cryptographic Hardware Accelerators such as ARM CRYPTOISLAND [8]. the Second, we have shown in our evaluation that mosquitto MQTT broker has a small CPU footprint. However, code executed in TRUSTZONE can only be singlethreaded, negatively affecting scalability. While leveraging multi-threading in the broker is part of future work, this must be carefully handled. In fact, we expect the increased CPU usage to lead to higher memory consumption, as well as higher energy requirements. In this sense, deployers should carefully evaluate such trade-offs and decide on application-dependent requirements."
2007.12442,"code open-source, code",28,,,"these industrial scenarios against a powerful attacker with the available hardware, minimal additional software, and no changes to the application code running at the edge;"
2007.12442,data,27,,,"[58] P. Voigt and A. Von dem Bussche. The EU general data protection A Practical Guide, 1st Ed., Cham: Springer"
2007.12442,data,33,,,"[16] M. Berberich and M. Steiner. Blockchain technology and the GDPR - how to reconcile privacy and distributed ledgers. European Data Protection Law Review, 2:422, 2016."
2007.12442,data,34,,,Payload Re-encryption. We link mosquitto with a trusted application that transfers the encrypted data to TRUST Fig. 4. MQT-TZ: broker (above) and client (below) authentication.
2007.12442,data,39,,,"[19] C. Cuijpers and B.-J. Koops. Smart metering and privacy in Europe: Lessons from the Dutch case. In European data protection: Coming of age, pages 269–293. Springer, 2013."
2007.12442,data,42,,,"[55] V. Turner, J. F. Gantz, D. Reinsel, and S. Minton. The digital universe of opportunities: rich data and the increasing value of the internet of things. IDC Analyze the Future, 2014."
2007.12442,data,43,,,"Second, they do not encrypt data packets before transmission. As a consequence, an attacker with the ability to intercept or spoof the client-broker link would gain access and tamper all the information processed by the broker (AV-2)."
2007.12442,data,46,,,"preventing malicious actors from spooﬁng application data. However, the brokers still expose a great attack surface [53]. In particular, a powerful attacker with physical access to the broker node can intercept and tamper with all inbound and outbound trafﬁc."
2007.12442,data,51,,,"[49] C. Segarra, E. Muntan´e Calvo, M. Lemay, V. Schiavoni, and R. DelgadoGonzalo. Secure stream processing for medical data. In Proceedings of the 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC’19, 2019."
2007.12442,data,64,,,"[48] C. Segarra, R. Delgado-Gonzalo, M. Lemay, P.-L. Aublin, P. Pietzuch, and V. Schiavoni. Using trusted execution environments for secure stream processing of medical data. In Proceedings of the International Conference on Distributed Applications and Interoperable Systems (DAIS’19), volume 11534 of DAIS’19, pages 91–107, 2019."
2007.12442,data,71,,,"Client nodes continuously publish live monitoring data in a streaming fashion. These nodes can typically sustain a steady network throughput of hundreds of bytes per second. For instance, a publisher streaming an ECG will send at most 350 Bytes/s, and a full ﬂeet of publishers (e.g., a ﬂoor in a hospital) will amount to around 3-5 kBytes per second (see §VI)."
2007.12442,data,89,,,"LRU Cache in the TEE. Our evaluation (§VI) shows that retrieving the keys from secure storage is the most expensive operation when re-encrypting application data in the TEE. To mitigate this behavior, MQT-TZ embeds a lightweight LRU cache in the TEE that keeps the most recently used keys in the TA’s heap memory, and evicts the least used ones (LRU policy) to persistent secure storage. Access to the heap’s content is also hardware-protected by TRUSTZONE."
2007.12442,data,89,,,"Moreover, as our healthcare systems quickly transition towards personalized medicine [27], an erroneous diagnosis generated by tampered health data could result in life-threatening situations. On the privacy side, read access to this health data can be used by third parties for user proﬁling, which may be used by health insurance companies to raise premiums from the leaked information. Q3: What if an attacker deliberately push data to one patient’s HRV topic, invalidating all the further monitoring?"
2007.12442,data,94,,,"From the architectural perspective, the overall system can be summarized on three key parts: (1) several in-house smart sensors implementing a large variety of communication protocols (e.g. EnOcean [34], KNX [32], Zigbee [24]); (2) a back-end managed by a third-party (e.g., an untrusted cloud provider) where the data is streamed and stored in real time; and (3) a web-based front-end that visualizes and manages the network of in-house sensors."
2007.12442,data,94,,,"I. INTRODUCTION The Internet of Things (IoT) is an increasingly popular environment to deploy all kind of data sensors, gather the produced data, and process it. Examples include live heartrate data [48], smart-grids [31], or infrastructure management systems [33]. The scale of IoT deployments is expected to grow exponentially in the next decade, with each individual to own and control several connected things [55]. Efﬁcient communication between the things is hence of paramount importance."
2007.12442,data,109,,,"The architecture of MQT-TZ is presented in Figure 3. We extend the mosquitto MQTT broker to ARM TRUSTZONE. Speciﬁcally, MQT-TZ’s broker adds an encryption layer in MQTT’s payload using client-speciﬁc keys stored inside ARM’s secure storage [37]. Doing so, application data is only processed inside the TEE, where it gets re-encrypted. For the additional key-provisioning to address AV1 and AV2, we redeﬁne the client authentication in the mutual TLS handshake to prevent the REE from gaining access to clients’ keys. We also leverage Access Control Lists (ACLs) for ﬁne-grained"
2007.12442,data,116,,,"Threat Model. Figure 2 depicts the potential threats that we consider in our deployment scenario, including the several attack vectors at disposal of an attacker. Note that these are not ﬂaws in the MQTT protocol per se as they may be out of scope (e.g., transport layer security). We highlight ﬂaws in the way these tools and protocols are used in industrial settings. First, most IoT settings leveraging MQTT bypass client or broker authentication (AV-1). As a consequence, unauthorized parties can publish data to the brokers as long as their address is known. Similarly, attackers might try to impersonate brokers."
2007.12442,data,117,,,"StreamBox-TZ [40] is a secure stream analytics framework for TRUSTZONE, speciﬁcally targeting telemetry data. Rather, MQT-TZ focuses on secure end-to-end packet delivery, delegating all the application-speciﬁc processing to client nodes. TEE-Based Applications. Due to the additional security guarantees and resilience to stronger adversarial models, TEEs are currently being deployed across a plethora diverse scenarios. One important step for all the applications is the attestation protocol, e.g., a process used by participants to verity the integrity and validity of the trusted applications as well as the CPU executing those. While Intel SGX has native support for remote attestation [3], TRUSTZONE lacks clear"
2007.12442,data,123,,,"Dataﬂow. In a nutshell, data in MQT-TZ ﬂows as follows. Data travels two-fold encrypted from the client to the broker (Fig.3-(cid:202)). Once the client access is conﬁrmed (Fig.3-(cid:203)), the subscribers for the given topic are retrieved and the payload forwarded (Fig.3-(cid:204)). Then, encrypted data is transferred to the TEE (Fig.3-(cid:205)). The origin and destination client keys are retrieved ((cid:206)-(cid:208)). The payload is re-encrypted and sent back to the REE (Fig.3-(cid:209)) and to the subscriber (Fig.3-(cid:210))."
2007.12442,data,124,,,"Server nodes in the system are placed at the edge. This choice maximizes responsiveness (minimizing latency), reduces the attack surface, and avoids the transferring of data ownership. They receive such data to process, for instance performing aggregation, averaging, or detecting statistical deviations. Increasingly common in IoT deployments—given its reduced costs, high availability in the market, popularity across developers, and hardware features—we deploy our brokers over Raspberry Pi 3 Model B units (see §VI-C). Conveniently, this device embeds an ARM Cortex-A processor with native support for TRUSTZONE. The deployment scenario includes storage services in charge of collecting data to be processed ofﬂine at a later time."
2007.12442,data,130,,,"In this scenario, the ﬂow of information is typically mediated by MQTT brokers. More precisely, ECGs and other key signal features are sent to remote MQTT brokers for further processing like HRV monitoring and arrhythmia detection. Locating them at the edge of the network, rather than the cloud, guarantees that data ownership is not transferred, hence easing compliance with data protection regulations. If a health anomaly is detected, an alarm is relayed through an MQTT topic to which a particular doctor or emergency services is subscribed. This architecture raises similar privacy and security concerns as the smart building scenario (§III-A) since, vital signs, and in particular HRV signals, are very sensitive data as well."
2007.12442,data,141,,,"speciﬁcations for it. Fides [44] presents a ready-to-use Key attestation framework for Android’s TEE, and we intend to look further into it. A common domain of application for TEEs is web-based systems, as well as a common way for end-users to disclose personal data. In this context, SGX can protect the identity of the users against re-identiﬁcation attacks via browser extensions and privacy proxies [15, 42]. While MQTTZ targets scenarios where the identity of the users is not to be hidden (rather, the opposite, as in our MedTech scenario), a broker can indeed be compromised. Techniques such as SGXTor [30] could be explored in the context of TRUSTZONE and MQT-TZ to reduce the information shared with the brokers."
2007.12442,data,149,,,"One scalable and ﬂexible communication pattern, commonly adopted by IoT deployers, is the publish-subscribe paradigm. Speciﬁcally, a prominent choice in pub-sub IoT contexts is the Message Queuing Telemetry Transport (MQTT) [28], a topic-based [13] publish-subscribe protocol [23] designed for environments with limited memory and reduced network bandwidth. In a nutshell, a client publishes data to a topic (e.g., soccer, art, etc), while a set of brokers forward it to the nodes subscribed to that topic. Some deployments, including the ones that motivated our work (§III) operate in sensitive environments, for which the topics, the subscriptions, or the messages being routed require high security guarantees. Most MQTT implementations support TLS [21] for transport-level security in the client-broker link,"
2007.12442,data,156,,,"This practical experience report presents the motivation, design, implementation, evaluation, and the lessons learned while building MQT-TZ, a secure edge-based publish/subscribe middleware leveraging MQTT and ARM TRUSTZONE protecting IoT systems against a variety of attacks. The MQT-TZ broker exploits TRUSTZONE’s tamper-proof secure storage to store clients’ keys (i.e., publishers and subscribers) upon a successful handshake phase. Authenticating the data publisher is not only beneﬁcial for a trustworthy end-to-end communication, but could also be used as a digital signature when connecting a storage back-end to our secure broker. Additionally, the re-encryption of the data—decrypting with the publisher key and encrypting with the subscriber’s one—happens within the memory-protected TRUSTZONE, protecting against memorydumps. As detailed later (§V-B), we built MQT-TZ’s messaging broker on top of mosquitto, the standard de facto MQTT implementation."
2007.12442,data,162,,,"Layered Encryption & Access Control Mechanisms. After the handshake, MQT-TZ uses a two-layer encryption mechanism. First, the client-broker link is protected by TLS within mosquitto. Second, MQTT’s payload is encrypted using the clients’ symmetric key. Then, data is re-encrypted in the TEE (as detailed next) and sent to its destination over a mosquitto-TLS channel. Doing so, we achieve end-toend security relying on TRUSTZONE as a secure proxy. We also leverage Access Control Lists to limit the clients able to interact with the broker. These are currently stored in the REE, but if clients are deﬁned in advance, its contents could be measured during boot and stored also in SS, preventing an attacker from tampering with the lists. A3: ACLs prevent unauthorized entities to inject or subscribe to sensitive topics, and enable revoking access to clients controlled by attackers."
2007.12442,data,178,,,"Trusted Application. We rely on OP-TEE APIs to implement the payload re-encryption TA. Trusted applications implemented within this framework have two parts: (1) a host app that runs in the REE and acts as an entry point and bridge to the TEE, and (2) a trusted API in the TEE that exposes different functions. MQT-TZ intercepts all MQTT packets forwarded to the recipients, and feeds our host app with both client’s IDs and the encrypted data. Then, the payload re-encryption happens, using OP-TEE’s storage and cryptographic libraries. TRUSTZONE not only provides isolation between worlds, but also between different TAs. Hence, we use the same secure API to store new keys during the handshake. For key retrieval, we implement a new LRU cache in the TEE to store the most frequently used keys in the TA’s heap, while the remaining ones are evicted and ﬂushed in persistent secure storage [37]."
2007.12442,data,184,,,"In the context of medical technologies (MedTech), the monitoring of vital signs is increasingly off-loaded and outsourced to third-party untrusted data centers. The main reason is to exploit the economy of scale that comes with cloud computing. However, recent data protection regulations (e.g., GDPR [58]) have stressed the importance of ownership of the data and limited the scope of its use. Much research has been recently devoted to deal with such restrictions and on how to reconcile distributed systems with such legislation frameworks [16]. For instance, [49] showed that it is possible to deploy real timeprocessing of heart-rate variability (HRV, linked to physiological and mental stress [17, 52]) by exploiting Intel SGX TEE enclaves with reasonable overheads. However, the platform must deal with a large diversity of physiological and behavioral signals originating from wearable sensors [20, 46] such as electrocardiograms (ECG), electroencephalograms (EEG), or photoplethysmographams (PPG)."
2007.12442,data,202,,,"Re-Encryption TA. We begin by measuring the time required to re-encrypt a block of data inside or outside the TEE, one of MQT-TZ’s cornerstone operations. We include results for the hardware and the emulated environment using the powersave CPU governor, as it most closely matches the expected deployment settings. We breakdown these measures into four main components: the time it takes to retrieve each key (retrieve_dec_key, retriev_enc_key), and to use them (encrypt, decrypt). On the x-axis, we show results for different block sizes of data to re-encrypt, e.g., from 20 Bytes up to 20 kBytes. We compare the performance of MQT-TZ decrypting inside the REE (left-side vertical bars) or in the TEE (right-side bars). For encryption, we use AES in CBC mode with 32-Byte keys. Finally, we include results for the variants of the system that maintain the keys either in volatile memory as well as in secure storage, and the overall elapsed time (in ms) boxed. Figure 5 uses a stacked bar chart representation to present these results."
2007.12442,"data available, data",74,,,"Third, edge-based pub/sub middleware usually lacks any mechanism of access control, e.g., the topics are public. An attacker with knowledge of the publish/subscribe topics could inject carefully crafted information while receiving potentially sensitive data sent by the clients (AV-3). By leveraging access control mechanisms, we can also revoke access to byzantine nodes, hence providing a simple defense mechanism against replay attacks towards the broker."
2007.12442,"data, code",60,,,"Trusted execution environments (TEEs) are hardware-based security mechanisms that shield code and data on compromised systems. Examples include Intel Software Guard Extensions (SGX) [18], AMD Secure Encrypted Virtualization (SEV) [29] for server-grade processors, and ARM TRUSTZONE [6, 38, 41] for edge-based processors."
2007.12442,"data, code",92,,,"The broker authentication (Figure 4, top) relies on TLS handshake, supported by default in mosquitto, while the corresponding client step (Figure 4, bottom) uses MQTT. We choose the latter over TLS’s client side authentication to ensure that client’s data is only processed in clear inside the TEE. Alternatively, we would need to install a full TLS endpoint inside TRUSTZONE, leading to a very large attack surface as code in the TEE needs to be trusted."
2007.12442,"data, code",165,,,"Fig. 3. MQT-TZ architecture and ﬂow of operations. privileged processes running in the same node (e.g., in the form of malware as these brokers tend to be connected to the internet), malicious users (e.g., with SSH access to the node), or even malicious operating systems loaded by an advanced attacker at boot time. For instance, a high-privilege process running on the same broker machine could intercept all its tampering with the data it incoming and outgoing trafﬁc, processes. Note also that even when the client-broker link is encrypted (e.g., via TLS channels), processing data in clear at the broker constitutes a privacy and security risk (AV-4). To protect against an attacker patching our broker, implementation which could or rebooting with a different trick the untrusted code to authenticate malicious clients, we measure the REE code base at secure-boot time. to"
2007.12442,"data, code, open-source",337,,,"MQTT & mosquitto. The Message Queuing Telemetry Transport protocol (MQTT) is a lightweight client and server topic-based publish/subscribe messaging transport protocol [28, 60]. It is specially suited for constrained environments (e.g., IoT) where memory and bandwidth are very scarce resources. In the protocol, a client publishes a message to a topic. Dedicated processes (i.e., the brokers) forward it to every subscriber for that same topic. The protocol supports decentralized deployments, in which brokers are organized in a layout and messages are routed and forwarded along the established routing tree. The de facto standard open-source C-based implementation of MQTT is mosquitto, actively maintained by the Eclipse Foundation [54]. We implement MQT-TZ atop mosquitto and introduces additional security guarantees leveraging ARM TRUSTZONE, as described next. TRUSTZONE & OP-TEE. TRUSTZONE [1] is a feature implemented in ARM processors since Arm1176JZ-S (2004). It implements a trusted execution environment that is primarily used to guarantee system-wide hardware isolation. Data and code are shielded from compromised environments. The TRUSTZONE architecture physically separates the device in two distinct execution environments, i.e., the trusted side (TEE - secure world) and the untrusted side, i.e., Rich Execution Environment REE or normal world [6]. TRUSTZONE shields against an attacker with physical access to the device, as well as higher-privileged software or malicious kernels running in the REE. The former hosts the so-called Trusted Applications (TAs). Trusted applications can leverage additional TRUSTZONE-only services, such as tamper-proof persistent storage via specialized APIs. The integrity of a TRUSTZONE-enabled device can be guaranteed by a secure boot mechanism.1 Its root of trust builds on Hardware Unique Keys (HUK) embedded in the processor during manufacturing. A common assumption is that only trustworthy TAs are deployed inside the secure world."
2007.12442,"data, code, open-source",349,,,"In the protocol, a client publishes a message to a topic. Dedicated processes (i.e., the brokers) forward it to every subscriber for that same topic. The protocol supports decentralized deployments, in which brokers are organized in a layout and messages are routed and forwarded along the established routing tree. The de facto standard open-source C-based implementation of MQTT is mosquitto, actively maintained by the Eclipse Foundation [54]. We implement MQT-TZ atop mosquitto and introduces additional security guarantees leveraging ARM TRUSTZONE, as described next. TRUSTZONE & OP-TEE. TRUSTZONE [1] is a feature implemented in ARM processors since Arm1176JZ-S (2004). It implements a trusted execution environment that is primarily used to guarantee system-wide hardware isolation. Data and code are shielded from compromised environments. The TRUSTZONE architecture physically separates the device in two distinct execution environments, i.e., the trusted side (TEE - secure world) and the untrusted side, i.e., Rich Execution Environment REE or normal world [6]. TRUSTZONE shields against an attacker with physical access to the device, as well as higher-privileged software or malicious kernels running in the REE. The former hosts the so-called Trusted Applications (TAs). Trusted applications can leverage additional TRUSTZONE-only services, such as tamper-proof persistent storage via specialized APIs. The integrity of a TRUSTZONE-enabled device can be guaranteed by a secure boot mechanism.1 Its root of trust builds on Hardware Unique Keys (HUK) embedded in the processor during manufacturing. A common assumption is that only trustworthy TAs are deployed inside the secure world. The Open Portable Trusted Execution Environment (OP-TEE) [36] is an open-source runtime for TEE applications sponsored by the Linaro Foundation [4] with native support for TRUSTZONE. It is designed to run together with a nonsecure Linux kernel in the REE. Finally, OP-TEE is compliant with the GlobalPlatform’s speciﬁcations [25]."
2007.12442,"data, database",100,,,"ZONE’s secure world, where it retrieves the origin and destination keys from secure storage, and re-encrypts the information. Currently, topic subscription lists and MQTT metadata are stored in a dedicated database (MQTT DB) in the REE. We plan on shadowing these structures to keep them in the TEE since information like subscription patterns, subscriber distribution, or topic ﬁltering can be used as a side-channel to leak sensitive data. A4: The re-encryption of information inside TRUSTZONE prevents a physical attacker or privileged process from spoofing sensitive information."
2007.12442,"data, dataset",210,,,"Finally, we fully implement and deploy the vital signs monitoring scenario (§III-B). In this case, we are interested in understanding if in a real-world setting the MQT-TZ broker can efﬁciently (e.g., CPU processing) sustain the injected workload. We leverage real-world ECG datasets we collected on the ﬁeld. This deployment reproduces the layout of an hospital ﬂoor with 50 patients whose cardiac signals are constantly monitored. For the sake of simplicity, these signals are streamed toward one single MQT-TZ broker, although a federated deployment is also supported. We capture and measure the outbound network trafﬁc from each publisher using nethogs [22]. Figure 9 depicts the outbound throughput generated by each publisher in bytes per second. We use a stacked percentile representation with shades of grey to plot the minimum, 25th, the 50th (median), the 75th and the maximum across all the publishers. We observe that at any given time only a subset of the publishers actually emits data. A single subscriber streams at 350 Bytes/s in the worst case, and the full ﬂeet of publishers generates between 3 to 5 kBytes per second overall."
2007.12442,"data, open-source",237,,,"Abstract—The publish-subscribe paradigm is an efﬁcient communication scheme with strong decoupling between the nodes, that is especially ﬁt for large-scale deployments. It adapts natively to very dynamic settings and it is used in a diversity of real-world scenarios, including ﬁnance, smart cities, medical environments, or IoT sensors. Several of the mentioned application scenarios require increasingly stringent security guarantees due to the sensitive nature of the exchanged messages as well as the privacy demands of the clients/stakeholders/receivers. MQTT is a lightweight topic-based publish-subscribe protocol popular in edge and IoT settings, a de-facto standard widely adopted nowadays by the industry and researchers. However, MQTT brokers must process data in clear, hence exposing a large attack surface. This paper presents MQT-TZ, a secure MQTT broker leveraging ARM TRUSTZONE, a trusted execution environment (TEE) commonly found even on inexpensive devices largely available on the market (such as Raspberry Pi units). We deﬁne a mutual TLS-based handshake and a two-layer encryption for end-to-end security using the TEE as a trusted proxy. The experimental evaluation of our fully implemented prototype with micro-, macro-benchmarks, as well as with real-world industrial workloads from a MedTech use-case, highlights several tradeoffs using TRUSTZONE TEE. We report several lessons learned while building and evaluating our system. We release MQT-TZ as open-source."
2007.12442,dataset,51,,,"We present the experimental evaluation of the MQT-TZ prototype using micro-benchmarks and macro-benchmarks, as well as using real-world datasets from the MedTech scenario. Our intent is to validate the design of MQT-TZ, the efﬁciency of our implementation and to analyze the different trade-offs that the system incurs."
2007.12442,github,4,,,GitHub: Op-TEE OS
2007.12442,github,5,,,Time https://github.com/OP-TEE/optee os/blob/
2007.12442,github,49,,,"MQT-TZ’s broker is implemented in C. The current version of MQT-TZ adds 400 SLOC to mosquitto version 1.6.3 and the TA amounts to 1204 SLOC. The MQT-TZ TA relies on OP-TEE, version 3.5.0. The MQT-TZ prototype will be available from https://github.com/mqttz."
2007.12442,open-source,14,,,[54] The Eclipse Foundation. Eclipse Mosquitto - An open source MQTT
2007.12442,open-source,91,,,"Motivated by the lack of secure-by-design communication protocols for the edge and two real-world use-cases, we built MQT-TZ, a secure edge-based publish/subscribe middleware using MQTT and TRUSTZONE. We report on our experiences while building and evaluating our open-source prototype against a vanilla MQTT under real-world workloads. Despite the measured slowdown (up to 8× in some scenarios), our system scales and can be deployed in restricted, IoTbased settings, achieving dissemination delays in the orders of milliseconds, even when deployed in low-end devices"
2007.12442,"provide implementation, open-source",31,,,"• we provide insights regarding our open-source implementation of such design. In particular, we describe a novel caching mechanism that combines TRUSTZONE trusted application memory and persistent storage;"
2007.12442,python,82,,,"is a content-based publish/subscribe framework on top of SCONE [11], a compilation tool chain to securely run Linux containers inside SGX enclaves. PubSubSGX is implemented in Python. The notion of topic is a ﬁrstclass entity currently not supported by PubSub-SGX, making its adoption for our scenarios requiring relevant engineering efforts. Similar drawbacks exist in [43], a content-based routing mechanism for SGX on top of which privacy-preserving pub/sub framework can be implemented."
2007.13018,data,10,,,C. Improving generalization in low-data regime and transfer as evaluation
2007.13018,data,18,,,"[23] V. R. de Sa, “Learning classiﬁcation with unlabeled data,” in Advances"
2007.13018,data,22,,,"Index Terms—self-supervised learning, deep learning, federated learning, embedded intelligence, low-data regime, sensor analytics, learning representations."
2007.13018,data,30,,,TABLE VI: Fine-tuning transferred model with few-labeled data to improve recognition rate. We report weighted F-score averaged over 100 independent runs. T denotes a transfer learning.
2007.13018,data,37,,,"Flora D. Salim is with the School of Science, RMIT University, Melbourne Australia. She co-directs the RMIT Centre for Information Discovery and Data Analytics (CIDDA). E-mail: ﬂora.salim@rmit.edu.au."
2007.13018,data,40,,,"[16] J. Dean and S. Ghemawat, “Mapreduce: simpliﬁed data processing on large clusters,” Communications of the ACM, vol. 51, no. 1, pp. 107–113, 2008."
2007.13018,data,46,,,TABLE III: Assessing performance in a federated learning setting to determine SCN’s ability to learn representations from distributed data. The entries marked with FC (federated classiﬁer) denotes metrics when both representations and classiﬁer are learned in a federated context.
2007.13018,data,50,,,"[44] Z. Zhou, S. Yang, L. J. Pu, and S. Yu, “Ceﬂ: Online admission control, data scheduling and accuracy tuning for cost-efﬁcient federated learning across edge nodes,” IEEE Internet of Things Journal, pp. 1–1, 2020."
2007.13018,data,54,,,"In this work, we seek to learn representations from data produced by sensors (time-series) on edge as obtaining a large amount of such labeled data is time-consuming and extremely costly. To solve this problem, we utilize a contrastive objective between a raw and complementary view of the data acquired"
2007.13018,data,61,,,"[20] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efﬁcient learning of deep networks from decentralized data,” in Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2017. [Online]. Available: http://arxiv.org/abs/1602.05629"
2007.13018,data,71,,,"We consider learning sensory features from raw unlabeled data with a deep neural network Fθ (parameterized by θ), which transforms input from X into output in Z. Here, we refer to a vector obtained through applying a mapping function F : X (cid:55)→ Z from an arbitrary intermediate or penultimate layer of the network as ‘representation’ or ‘feature.’ Our"
2007.13018,data,76,,,"Fig. 3: Overview of federated learning framework. A central server dispatches a randomly initialized model and other training conﬁguration details to the selected clients’ devices, as depicted by dashed gray lines. The clients train local models on their private data and send the models back to the server illustrated with solid black lines. The models are aggregated to produce a uniﬁed model that is used for the end-task."
2007.13018,data,81,,,"[14] T. Baltruˇsaitis, C. Ahuja, and L.-P. Morency, “Multimodal machine learning: A survey and taxonomy,” IEEE transactions on pattern analysis and machine intelligence, vol. 41, no. 2, pp. 423–443, 2018. [15] A. Vulimiri, C. Curino, B. Godfrey, K. Karanasos, and G. Varghese, “WANalytics: Analytics for a geo-distributed data-intensive world.” in CIDR, 2015."
2007.13018,data,84,,,"the fusion of selfsupervision with federated learning could result in an effective method for learning from unlabeled, private, and diverse types of sensory data, which is crucial for several embedded (personalized) machine learning tasks. To achieve this objective, we develop a novel auxiliary task based on a wavelet transform, which we call scalogram-signal correspondence learning (SSCL). A deep temporal convolution network is trained to solve the speciﬁed task so as to learn representations"
2007.13018,data,129,,,"One of the most rudimentary forms of unsupervised feature discovery has been hand-crafted feature engineering, which turns out to be largely redundant due to its limited discriminative power for building high-performance models [22]. Another area of research that is considerably explored focuses on reconstruction based approaches for extracting lowdimensional embedding through learning from data with deep autoencoders [11]. The main drawback of these methods is that they may waste the network’s capacity to model low-level input details through predicting every bit of the signal. This is not needed if the aim is to learn discriminatory features that generalize well to the downstream (or end) tasks, e.g., sleep stage classiﬁcation with electrical brain activity signals."
2007.13018,data,145,,,"To address the aforementioned concerns, federated learning [20] is emerging as an effective way of collaboratively training shared models from distributed private data. However, existing exploration in this area is solely focused on learning supervised models for tasks where annotations can be easily acquired based on the user interaction, e.g., mobile keyword prediction [21]. The curation of strongly labeled data becomes infeasible as annotations can not be acquired easily for solving several important problems involving sensory inputs. Because apart from wearables, other sensors could be installed in remote locations, and expert-level domain knowledge could be required to annotate samples. Hence, in such cases, unsu Fig. 1: Illustration of a 30-seconds long electroencephalogram (EEG) signal and a corresponding scalogram extracted with a Morlet wavelet transform."
2007.13018,data,145,,,"objective is to learn general-purpose representations that can make subsequent tasks of interests easier to solve. To this end, numerous unsupervised methods are developed to leverage a large amount of unlabeled data for achieving better generalization. Moreover, the data required for model development could not only be unannotated but also distributed, without the option to accumulate it in a centralized repository due to privacy concerns and its ever-increasing size. To tackle the issue of learning models from decentralized user data, the ﬁeld of federated learning [20] is rapidly gaining momentum. Our work is intended to unify self-supervision with federated learning to realize the vision of on-device learning, with a focus on multi-sensor inputs. We describe the details of the essential building blocks of our approach and related work in the following subsections."
2007.13018,data,154,,,"In the past few years, several self-supervised methods have been developed for vision, audio, language modeling, and other domains. However, little to no attention is paid towards exploring other sensing modalities, such as electroencephalography, IMUs, and blood volume pulse. The prominent approaches for learning from traditional input modalities include, colorization of grayscale images [34], predicting relative location of an image patch [25], audio-visual synchronization [2], temporal alignment in videos through cycle-consistency [35], word2vec (and other variants) [3], signal transformation prediction [8], contrastive predictive coding [1], and robotic imitation learning via time-contrastive networks [27]. These are some of the many strategies proposed for learning from an unlimited amount of unlabeled audio, visual, and textual data."
2007.13018,data,155,,,"agree on sharing their data for learning, which is not ideal in several real-world contexts due to raising privacy issues and misaligned incentives for the user. Likewise, IoT devices produce an astonishing amount of data on a daily basis, and even if the data sharing takes place, its rapidly increasing size limits exploiting for learning models. Therefore, there is a need to develop unsupervised (or self-supervised) methods that can be used to learn general-purpose models from unlabeled data. It is particularly pertinent to on-device learning (such as a smartphone), without the need for data aggregation in a centralized server, and minimal to no human involvement in terms of the annotation process. Consequently, the unsupervised model can be used as a semantic feature extractor or initialization for efﬁciently adapting to an end-task of interest through ﬁne-tuning with few-labeled instances."
2007.13018,data,159,,,"Autonomous vehicles, wearables, smartphones, and IoT sensors are examples of modern distributed devices producing a wealth of data every second. This massive amount of data offers an excellent opportunity for learning models to solve a diverse range of tasks. The applications of interest include customized ﬁtness plans, personalized language models, and contextual awareness for driving automation. The growing computational power of edge devices allows us to leave the data decentralized and push the network computation to the client, which is also ideal from a privacy aspect. The expanding area of federated learning [20], [37], [38] explores developing methods to achieve the goal of learning from highly distributed and heterogeneous data through aggregating locally trained models on remote devices, such as smartphones and wearables. In this case, the intention is to minimize the following objective [37]:"
2007.13018,data,175,,,"A promising substitute is the emerging area of selfsupervised learning [23], which enables the learning of representations through solving an auxiliary task for which labels can be acquired from the data without any human intervention. In this case, several techniques are proposed mainly for audio, visual and textual data including estimation of missing input [24], prediction of contextually relevant information [25], recognizing degree of rotation applied on an image [26], contrastive predictive coding [1], synchronization of audio-visual inputs [2], and robotic imitation using multi-view videos [27]. Moreover, cross-modal learning is also utilized by specifying an appropriate loss term between different input modalities for training multimodal networks. However, to the best of our knowledge, previous work did not study self-supervised learning for other sensing modalities (e.g., electroencephalography, accelerometer, blood volume pulse, and others) as produced by a variety of IoT devices."
2007.13018,data,199,,,"Learning multi-sensor representations with deep networks requires a large amount of well-curated data, which is made difﬁcult by the diversity of device types, environmental factors, inter-personal differences, privacy issues, and annotation cost. We propose a self-supervised auxiliary task whose objective at a high level is to contrast or compare raw signals and their corresponding scalograms (which are a visual representation of the wavelet transform) so that a network learns to discriminate between aligned and unaligned scalogram-signal pairs. The rationale of the proposed approach is similar in spirit to crossview learning in the audio-visual domain [2]. However, it differs in a core way that we obtain aligned and unaligned views1 from the same modality with wavelet transform. In the absence of the semantic labels, our methodology can be leveraged to generate an endless stream of labeled data. Therefore, it can train the network without any human involvement, which is particularly attractive for on-device learning. In subsequent sections, we describe details of the correspondence learning, sample generation, preference of a loss function, and key network architectural properties."
2007.13018,"data available, data",23,,,pervised approaches provide a compelling substitute to learn from unlabeled data available in huge quantities as they do not require semantic labels.
2007.13018,"data available, data",114,,,"the existing techniques make a strong assumption that labeled training data are always accessible, or annotations can be extracted reliably, e.g., via user interaction with smartphone applications. However, for various problems involving sensory data (such as sleep stage scoring and context recognition), obtaining a large number of annotated examples in a real-world setting to train supervised models is prohibitively expensive and not feasible. This limits the applicability of current methods in learning from unlabeled data available from distributed IoT devices. The approach presented here is a step towards exploring self-supervised representation learning in a federated setting from unannotated multi-sensor data at the edge."
2007.13018,"data available, data",143,,,"We explore the effectiveness of the proposed technique for improving performance with few-labeled examples. We pre train a scalogram-contrastive network with the entire unlabeled data and use the model as initialization for learning a downstream task. We compare the performance with a standard supervised network trained only with certain labeled instances. Speciﬁcally, we use 5, 10, 20, and 40 labeled instances per class to learn the end-task model. Figure 6 and Table IV show an average F-score of 100 independent repetitions where different examples are sampled to train the network at each run. In all the cases, the results obtained with utilizing a selfsupervised network are better than the baseline, even when limited labeled data are available. This highlights that the SCN efﬁciently harnesses unlabeled data to learn generalized features."
2007.13018,"data available, data",181,,,"In Table II and Table III, we provide our key evaluation results in central and federated learning settings. First, we compare the performance of our approach with a) a supervised network trained end-to-end, b) an autoencoder, and c) a randomly initialized network in a central setting, i.e. when the entire data are available for learning on a server. We measure the quality of learned representations through a linear classiﬁer trained on-top of the frozen feature extractor, which is a standard evaluation protocol used in earlier work. In the federated setting (Table III, the supervised network is learned for each user, and the weights are aggregated to create a uniﬁed model. For an autoencoder and SCN, the pre-training is performed in a federated setting to learn representations, and a classiﬁer is trained in a standard way i.e., as if the data of end-task are available on the server. In addition, we also assess the performance when unsupervised networks are kept"
2007.13018,"data available, data, dataset",202,,,"Similarly, the self-supervised networks are also evaluated in terms of their usefulness in a transfer learning setting. Generally, this is achieved by treating a pre-trained model as a ﬁxed feature extractor, and a linear model is trained on top of it using a different dataset. Here, we assess the performance on activity recognition tasks with HHAR and MobitAct datasets. Table V provides these results and compares with the supervised network, transfer from supervised (Sup.), and SCN trained on the same source instances. In both cases, we see that the recognition improves relatively if the transferred embedding is from SCN compared to a supervised network. Finally, we also assess the performance of SCN when fewlabeled instances are available for ﬁne-tuning, but different unlabeled data are available for pre-training, as shown in Table VI. Similar to earlier semi-supervised evaluation, we ﬁnetune a pre-trained network end-to-end with 5, 10, 20, and 40 examples of each class from the target dataset. We notice a 2%−3% improvement in F-score over the supervised network when an SCN encoder is utilized."
2007.13018,"data, code",221,,,"Previous approaches to learning representations from timeseries of sensory modalities with deep networks can be mainly categorized into three areas: end-to-end training of supervised models with labeled data [4], [6], [7], [10], reconstruction of the actual input for pre-training [11]–[13], and utilizing self-learning with domain-speciﬁc transformations or crossmodal learning [14]. Primarily, the focus of these methods is to conduct training of predictive models on a central server in a data center. However, as mentioned earlier, the aggregation of continuously increasing data from distributed devices is practically infeasible, aside from privacy issues. Initially, geodistributed analytics [15], [16] and distributed learning [17]– [19] in a data center environment is studied to exploit data locality and reducing network costs through pushing code to the data on the edge which generally is a node in the data center which could be across the globe. Nevertheless, these methods do not address the fundamental problem of learning representations with deep networks from unlabeled and highly distributed data that resides on user devices, which can not be aggregated in a central environment for learning."
2007.13018,"data, data repository",93,,,"the aforementioned challenges motivate the following research questions: Can we train a deep network to extract useful sensory representations in an unsupervised manner without utilizing strong labels for a speciﬁc problem, such as activity recognition? Could it also be achieved without aggregating the local data samples from remote devices in a centralized repository, i.e., employing decentralized or ondevice learning? Can we improve the network generalization in a low-data regime through ﬁne-tuning it with few-labeled examples that potentially could be easily pooled from a group of users?"
2007.13018,"data, data repository",328,,,"the challenges associated with learning a generalizable model for a particular use case, consider this illustrative example. Let us assume that we aim to develop a robust sleep stage classiﬁcation model that can be used for a larger population of users. The standard methodology is to learn a supervised model and requires example-label pairs for providing supervision so that a model can differentiate among instances of multiple classes through learning underlying patterns in the input. The procedure begins with the data collection to monitor hundreds of users for electroencephalography (EEG) or other signals as they progress through various stages of sleep and accumulate the multi-sensor data in a centralized (data center) repository for further analysis. The next step is then to get the aggregated inputs annotated by the sleep expert (i.e., generally a professional trained in analyzing physiological signals) for the sleep classes, such as wake, N1, N2, N3, and rapid eye movement. Then, the learning and evaluation phase involves several iterations of improving the model performance. Lastly, the model is deployed in the wild for user monitoring. The process of model development, from data collection to annotation, could be extremely costly owing to the difﬁculty in setting up an experimental (data collection) protocol. Furthermore, the domain expertise required for the labeling could be severely limited. This problem is exacerbated by the need of supervised deep neural network models for a massive amount of labeled data to learn discriminative to inspect and features. It becomes painstakingly difﬁcult annotate hundreds of thousands of hours of multi-sensor data. Therefore, in practice, limited-sized sensor data are collected and labeled for learning the model, which could further affect its generalization. The important point to note here is that the explained strategy is only applicable when the users"
2007.13018,"data, data repository, dataset",278,,,"Abstract—Smartphones, wearables, and Internet of Things (IoT) devices produce a wealth of data that cannot be accumulated in a centralized repository for learning supervised models due to privacy, bandwidth limitations, and the prohibitive cost of annotations. Federated learning provides a compelling framework for learning models from decentralized data, but conventionally, it assumes the availability of labeled samples, whereas on-device data are generally either unlabeled or cannot be annotated readily through user interaction. To address these issues, we propose a self-supervised approach termed scalogramsignal correspondence learning based on wavelet transform to learn useful representations from unlabeled sensor inputs, such as electroencephalography, blood volume pulse, accelerometer, and WiFi channel state information. Our auxiliary task requires a deep temporal neural network to determine if a given pair of a signal and its complementary viewpoint (i.e., a scalogram generated with a wavelet transform) align with each other or not through optimizing a contrastive objective. We extensively assess the quality of learned features with our multi-view strategy on diverse public datasets, achieving strong performance in all domains. We demonstrate the effectiveness of representations learned from an unlabeled input collection on downstream tasks with training a linear classiﬁer over pretrained network, usefulness in low-data regime, transfer learning, and cross-validation. Our methodology achieves competitive performance with fullysupervised networks, and it outperforms pre-training with autoencoders in both central and federated contexts. Notably, it improves the generalization in a semi-supervised setting as it reduces the volume of labeled data required through leveraging self-supervised learning."
2007.13018,"data, dataset",47,,,"have made tremendous improvements in the last few years on challenging real-world tasks [1]–[4], thanks to the emergence of massive datasets. In particular, the wealth of sensory data from the Internet of Things (IoT) devices are"
2007.13018,"data, dataset",69,,,"Fig. 6: Effectiveness of self-supervised learning in a low-data regime. The SCN is pre-trained on unlabeled data and ﬁne-tuned end-to-end with few-labeled data points (i.e, 5, 10, 20, and 40 instances per class). On all the evaluated datasets, we notice a signiﬁcant performance improvement over a supervised baseline network, which is trained only with labeled inputs."
2007.13018,"data, dataset",98,,,"In all the cases, we use a random 70% − 30% split of the dataset (based on users such that there is no overlap in terms of users’ data) for training and evaluation, respectively. We also pick a 20% subset from training split as a validation set for hyperparameter tuning and model selection. Moreover, we also evaluate the performance of our approach with crossvalidation based on user split, i.e., leave-one-user-out. Table I summarizes the key characteristics of the datasets used in our evaluation."
2007.13018,"data, dataset",149,,,"only recently being leveraged for tackling important problems in understanding context, user monitoring, health, and other predictive analytics tasks, e.g., for emotional well-being [5], [6], sleep tracking [7], and physical activity detection [8]. The success is mainly attributed to the supervised methods that utilize labeled datasets for training models in a central environment. In contrast, learning models from unlabeled decentralized data still presents a major challenge. Obtaining large, well-curated sensory data from edge devices is especially difﬁcult owing to issues like user privacy, the prohibitive cost of labeling, bandwidth limitations, network connectivity, and the diversity of device types [9]. These factors make it signiﬁcantly challenging to harness abundant data on remote devices for learning semantic features with standard supervised approaches."
2007.13018,"data, dataset",153,,,"of subject data in a training set and achieves signiﬁcantly better results than an autoencoder. Notably, on Sleep-EDF, our methods achieve a mean kappa score of 0.83 as compared to 0.77 of a supervised network and 0.76 as reported in [7]. Likewise, our self-supervised technique performs better than the hand-designed features from wrist physiological signals on WESAD by achieving an F-score of 75.7 ± 0.13 as compared to 66.33 ± 0.36 [5]. Furthermore, we would like to highlight that a direct comparison of existing approaches on other datasets used in our study is not feasible due to the differences in reported metrics and used sensing modalities. Nevertheless, our results with cross-validation further indicate that self-supervised learning can be effectively utilized for sensor modeling tasks on a large scale and can be combined with active learning methods [54]."
2007.13018,"data, dataset",191,,,"In particular, we highlight that for federated learning, we utilize random partitioning of the training sets as in [20] to tackle the low number of users in the considered (existing) datasets. This choice might result in a decentralized IID (i.e., independent and identically distributed) dataset that could be unbalanced but does not suffer from extreme heterogeneity in terms of training instances per client as generally, the case is for non-IID data that typically varies heavily based on the users’ demographics, device usage, and other factors. However, we would again emphasize that our self-supervised technique does not depend on the user-generated labels for learning representations and could be easily applied to largescale datasets. However, as the end-task labels are required to evaluate the quality of learned features, the unavailability of massive multi-sensor labeled data is a critical limiting factor towards realizing the goal of assessment in the non-IID setting. We leave the evaluation of self-supervised features on a large pool of users with a greater variety of devices as future work."
2007.13018,"data, dataset",215,,,"On the evaluated datasets, we observe that the classiﬁers learned on-top of a ﬁxed randomly initialized network achieve F-score above 60% in most cases. It highlights the representational capacity of our architecture design that, without seeing any samples, the encoder can provide reasonable embedding for a linear classiﬁer. Notably, the SCN surpasses pre-training results with the autoencoder and on HHAR achieves better Fscore (82.7) than a supervised baseline (73.0). Particularly, we notice that the results obtained in a federated setting are close to those achieved with learning end-to-end models in a central setting, which hints towards the robustness of our approach in a federated environment. Similarly, when a linear classiﬁer is also trained in a federated setting, the performance of SCN is mainly consistent with the centralized classiﬁer, which is not the case for an autoencoder. Moreover, in Figure 4, we provide the t-SNE embedding of SCN on 1000 randomly selected instances from a test set of Sleep-EDF, WiFi-CSI, and HHAR. The distinct clusters of data points can be seen that are discovered entirely in an unsupervised manner. This further highlights the ability of SCN to learn meaningful representations."
2007.13018,"data, used dataset, dataset",274,,,"The electroencephalogram (EEG) and electrooculography (EOG) signals are used from the PhysioNet Sleep-EDF dataset [50], [51] for classifying sleep into ﬁve stages (i.e., Wake, N1, N2, N3, and Rapid Eye Movement). We preprocess these signals, which are recorded at 100Hz, as done in earlier work [7] and utilize 30-second epochs (segments). For activity classiﬁcation with smartphones, accelerometer, and gyroscope signals from HHAR [52] and MobiAct [53] datasets are used, which have 6 and 11 output classes, respectively. We segment the raw signals through a sliding window into a segment size of 400 samples with a 50% overlap. For device-free sensing of daily activities, we use the WiFi channel state information data [10] and follow identical preprocessing steps the signals are resampled from 1kHz with [10]. Notably, to 500Hz through uniform temporal downsampling with a rate of 2 for each of the 90 channels (i.e., 30 sub-carriers per antenna) to classify them into 7 classes. The WESAD dataset [5] is used for the detection of stress, normal, and amusement physiological states. Here, we use blood volume pulse, electrodermal activity, and temperature signals collected from a wrist wearable device at 64Hz, 4Hz, and 4Hz, respectively. Following [5], we extract 30-seconds segments and independently normalize each subject’s data before the model development phase."
2007.13018,dataset,1,,,Dataset
2007.13018,dataset,4,,,A. Datasets and Preprocessing
2007.13018,dataset,7,,,TABLE I: Summary of datasets.
2007.13018,dataset,30,,,"We experimented with learning models on 5 datasets from the following application areas: sleep stage scoring, human activity recognition, WiFi sensing, and physiological stress detection."
2007.13018,dataset,42,,,"[53] G. Vavoulas, C. Chatzaki, T. Malliotakis, M. Pediaditis, and M. Tsiknakis, “The mobiact dataset: Recognition of activities of daily living using smartphones.” in ICT4AgeingWell, 2016, pp. 143–151."
2007.13018,dataset,55,,,"network consists of modality-speciﬁc and fusion layers to learn specialized and joint embedding, respectively. In particular, we utilize the same network architecture for learning on different datasets unless mentioned otherwise. Likewise, only the features from the signal network are used for evaluation, discarding the scalogram network after pre-training."
2007.13018,dataset,121,,,"[4] V. Radu, C. Tong, S. Bhattacharya, N. D. Lane, C. Mascolo, M. K. Marina, and F. Kawsar, “Multimodal deep learning for activity and context recognition,” Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 1, no. 4, pp. 1–27, 2018. [5] P. Schmidt, A. Reiss, R. Duerichen, C. Marberger, and K. Van Laerhoven, “Introducing wesad, a multimodal dataset for wearable stress and affect detection,” in Proceedings of the 2018 on International Conference on Multimodal Interaction. ACM, 2018, pp. 400–408."
2007.13018,dataset,129,,,"The idea behind SSCL is to learn network parameters with a self-supervised objective that determines whether a raw signal and a scalogram correspond (or align) with each other or not. Given a multi-sensor dataset with ﬁxed-length input segments of multiple modalities D = {x1, x2, . . . , xM} of M instances, we train a multimodal contrastive network to achieve the objective of synchronizing representations of the raw input with their corresponding scalogram. Speciﬁcally, a time-series is segmented into a ﬁxed-sized input with a sliding window having a certain overlap between samples. Afterward, the scalogram sm of a signal xm can be generated with a speciﬁed wavelet transformation Ψ [29]. This procedure results"
2007.13018,dataset,182,,,"The ﬁeld of unsupervised learning deals with extracting disentangled representations that could be used for solving a wide variety of end-tasks. The most prominent approaches include principal component analysis, Boltzmann machine [31], autoencoders [11], generative adversarial networks [32], and autoregressive models [33]. Another emerging area of research for extracting unsupervised representations is ‘selfsupervision.’ It provides a general and powerful framework for learning with unlabeled inputs through solving pretext tasks. Here, a surrogate objective is speciﬁed in such a way that optimizing it would force the network to learn meaningful and usable features for the end-task. Speciﬁcally, given an unlabeled dataset D = {x1, x2, . . . , xM} with M instances. A surrogate task is designed that provides pseudolabels {y1, y2, . . . , yM} to learn Fθ (without the need of any strong class annotations) through minimizing a classiﬁcation, regression or metric loss L given by:"
2007.13018,"dataset provided, data, dataset",265,,,"non-corresponding randomly selecting scalogram-signal scalograms from outside the current input batch while keeping the raw input ﬁxed for positives and negatives. We preprocess the signals before computing scalogram or initiating network training as done in the previous works for each considered dataset; further details are provided in Section IV-A. We calculate summary statistics for znormalization from the training set. We use an Adam optimizer with a ﬁxed learning rate of 0.0001 for pre-training and 0.01 or 0.02 in case of learning a linear classiﬁer, which could also be decayed based on performance on the validation set. The network is trained with a batch size of 24, a dropout rate of 0.1, and L2 regularization rate of 0.0001. For federated learning simulation, we use the Tensorﬂow federated learning framework2. In this case, the networks are trained with a batch size of 12 for 5 local epochs using data of n randomly selected users (typically 10) at each training round with 30 − 50 rounds in total, depending on the dataset size. Speciﬁcally, in our experiments, we randomly divide the training set into multiple subsets (representing each client) that are used to train the models in a federated setting. We opt for this strategy due to fewer users in existing datasets. The availability of bigger datasets with a larger pool of users could be useful to evaluate self-supervised methods in the future. A high-level overview of federated learning is illustrated in Figure 3."
2007.13018,"publicly available, data, dataset",92,,,"We evaluate the effectiveness of our approach in multiple ways with several publicly available datasets from different domains. First, we probe the quality of representations with a linear classiﬁer trained on-top of a frozen feature extractor in both central and federated learning settings. Second, we examine whether scalogram-signal correspondence learning could be used to improve the recognition rate in the low-data regime. Finally, we determine the transferability of features on related datasets, followed by an evaluation with crossvalidation to determine robustness against subject variations."
2007.13018,"publicly available, data, dataset",259,,,"In this paper, we propose a self-supervised method for learning representations from unlabeled multi-sensor input data, which is typical in the IoT setting. Our method utilizes wavelet transform to generate a complementary view of the input (i.e., a scalogram) to deﬁne an auxiliary task of scalogram-signal correspondence. This procedure is speciﬁcally designed to work in federated learning setting to allow training networks with widely distributed and unannotated data as the labels can be readily extracted from the data without human-in-the-loop. We show the efﬁcacy of the developed technique on several publicly available datasets involving diverse sensory streams, such as electroencephalogram, blood volume pulse, and IMUs. Particularly, we evaluate the quality of learned features with a linear classiﬁer on an end-task and compare the performance with a fully-supervised network and pre-training with an autoencoder in both federated and central settings. Furthermore, we demonstrate an improved generalization in the low-data regime with self-supervision, i.e., when few labeled instances are used for ﬁne-tuning network on the desired end-task. Our generic self-supervised approach can be used efﬁciently to learn general-purpose deep feature extractors entirely ondevice without the need to transmit the actual data to the server. In future work, we plan to combine self-supervision with architecture search on larger datasets and evaluate our method in a non-IID setting for federated learning. Another avenue of future research is to explore the effectiveness of self-supervised pre-training for adversarial robustness in a federated setting."
2007.13018,"publicly available, data, dataset",332,,,"The presented auxiliary task can formally be seen as a binary classiﬁcation problem, and we employ a contrastive objective inspired by [30] for optimizing it (see Figure 2 for an overview) in both central and federated settings without involving a human in the data labeling process. Importantly, we would like to highlight that for the model to solve the deﬁned task successfully, it should learn the core semantics in shared input views through possibly relating frequency, scale, and other information present in the signal. The network captures meaningful latent relationships through correlating scalogram-signal inputs in the embedding space. Mainly, the representations that could emerge from the learning process are forms of invariances (such as sensor noise, subject-speciﬁc variations), which are essential tasks involving sensory data, e.g., stress detection with physiological signals. The key contributions of this work are three-fold: First, we propose a scalogram-signal correspondence learning framework for self-supervised learning from diverse sensory data. Second, to the best of our knowledge, we, for the ﬁrst time, propose to unify federated learning with self-supervision to learn from unlabeled and private data on edge devices. Third, we extensively assess the proposed method on several publicly available datasets from different domains with linear classiﬁcation protocol in central and federated contexts, lowdata regime (i.e., semi-supervised setting), and transfer learning including cross-validation. The SCN achieves competitive performance compared with fully-supervised networks that are trained entirely on labeled data and perform signiﬁcantly better than other approaches. Particularly, SCN ﬁne-tuning with fewlabeled instances, e.g., ﬁve or ten instances per class, improves the F-score by as much as 5%-6% in comparison to training from scratch. Our approach also works better than transferring supervised features, learned from the source data, between the related tasks."
2007.13018,"publicly available, data, dataset",346,,,"task is to determine The core idea behind our pretext if a given pair of scalogram-signal inputs are aligned or misaligned, i.e., whether a scalogram is the transformation of a given signal. The presented auxiliary task can formally be seen as a binary classiﬁcation problem, and we employ a contrastive objective inspired by [30] for optimizing it (see Figure 2 for an overview) in both central and federated settings without involving a human in the data labeling process. Importantly, we would like to highlight that for the model to solve the deﬁned task successfully, it should learn the core semantics in shared input views through possibly relating frequency, scale, and other information present in the signal. The network captures meaningful latent relationships through correlating scalogram-signal inputs in the embedding space. Mainly, the representations that could emerge from the learning process are forms of invariances (such as sensor noise, subject-speciﬁc variations), which are essential tasks involving sensory data, e.g., stress detection with physiological signals. The key contributions of this work are three-fold: First, we propose a scalogram-signal correspondence learning framework for self-supervised learning from diverse sensory data. Second, to the best of our knowledge, we, for the ﬁrst time, propose to unify federated learning with self-supervision to learn from unlabeled and private data on edge devices. Third, we extensively assess the proposed method on several publicly available datasets from different domains with linear classiﬁcation protocol in central and federated contexts, lowdata regime (i.e., semi-supervised setting), and transfer learning including cross-validation. The SCN achieves competitive performance compared with fully-supervised networks that are trained entirely on labeled data and perform signiﬁcantly better than other approaches. Particularly, SCN ﬁne-tuning with fewlabeled instances, e.g., ﬁve or ten instances per class, improves the F-score by as much as 5%-6% in comparison to training from scratch."
2008.01391,code,2,,,2.2.6 Code-Mixing
2008.01391,code,2,,,4.5 Code-Switching
2008.01391,code,2,,,5.5 Code-Switching
2008.01391,code,20,,,"Yoder MM, Rijhwani S, Rosé CP, Levin L (2017) Code-Switching as a Social Act:"
2008.01391,code,28,,,"Chanda A, Das D, Mazumdar C (2016) Columbia-Jadavpur submission for emnlp 2016 code-switching workshop shared task: System description. EMNLP 2016 p 112"
2008.01391,code,30,,,Ayeomoni MO (2006) Code-switching and code-mixing: Style of language use in childhood in Yoruba speech community. Nordic Journal of African Studies 15(1):90–99
2008.01391,code,35,,,"Riyadh RR, Kondrak G (2019) Joint approach to deromanization of code-mixed texts. In: Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, pp 26–34"
2008.01391,code,39,,,"Dhar M, Kumar V, Shrivastava M (2018) Enabling code-mixed translation: Parallel corpus creation and MT augmentation approach. In: Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing, Association"
2008.01391,code,40,,,"Menacer MA, Langlois D, Jouvet D, Fohr D, Mella O, Smaïli K (2019) Machine translation on a parallel code-switched corpus. In: Canadian Conference on Artiﬁcial Intelligence, Springer, pp 426–432"
2008.01391,code,40,,,"Priyadharshini R, Chakravarthi BR, Vegupatti M, McCrae JP (2020) Named entity recognition for code-mixed Indian corpus using meta embedding. In: 2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS)"
2008.01391,code,46,,,"Chan JYC, Cao H, Ching PC, Lee T (2009) Automatic Recognition of CantoneseEnglish Code-Mixing Speech. In: International Journal of Computational Linguistics & Chinese Language Processing, Volume 14, Number 3, September 2009, URL https://www.aclweb.org/anthology/O09-5003"
2008.01391,code,68,,,"Chakravarthi BR, Muralidaran V, Priyadharshini R, McCrae JP (2020b) Corpus creation for sentiment analysis in code-mixed Tamil-English text. In: Proceedings of the 1st Joint Workshop of SLTU (Spoken Language Technologies for Underresourced languages) and CCURL (Collaboration and Computing for UnderResourced Languages) (SLTU-CCURL 2020), European Language Resources Association (ELRA), Marseille, France"
2008.01391,code,75,,,"In work by Bloodgood and Strauss (2017), the authors translated lexicon induction for a heavily code-switched text of historically unwritten colloquial words via loanwords using expert knowledge with language information. Their method is to take word pronunciation (IPA) from a donor language and convert them into the borrowing language. This shows improvements in BLEU score for induction of Moroccan Darija-English translation lexicon bridging via French loan words."
2008.01391,code,133,,,"Song K, Zhang Y, Yu H, Luo W, Wang K, Zhang M (2019) Code-switching for enhancing NMT with pre-speciﬁed translation. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics, Minneapolis, Minnesota, pp 449–459, DOI 10. 18653/v1/N19-1044, URL https://www.aclweb.org/anthology/N19-1044 Susanto RH, Larasati SD, Tyers FM (2012) Rule-based machine translation between Indonesian and Malaysian. In: Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing, The COLING 2012 Organizing Committee, Mumbai, India, pp 191–200, URL https://www.aclweb.org/anthology/W12-5017"
2008.01391,code,155,,,"An SMT system with a code-switched parallel corpus was studied by Menacer et al. (2019) and Fadaee and Monz (2018) for Arabic-English language pair. The authors have manually translated or used back translation method to translate foreign words. The identiﬁcation of the language of the word is based on the orthography. Chakravarthi et al. (2018) used the same approach for Dravidian languages; they used the improved MT for creating WordNet, showing improvement in the results. For English-Hindi, Dhar et al. (2018) manually translated the code-switched component and shown improvements. Machine translation of social media was studied by Rijhwani et al. (2016) where they tackle the code-mixing for Hindi-English and Spanish-English. The same approach translated the main language of the sentence using Bing Translate API (Niu et al., 2018)."
2008.01391,"code, dataset",31,,,"Chakravarthi BR, Jose N, Suryawanshi S, Sherly E, McCrae JP (2020a) A sentiment analysis dataset for code-mixed Malayalam-English. In: Proceedings of the 1st"
2008.01391,"code, dataset",41,,,"Jose N, Chakravarthi BR, Suryawanshi S, Sherly E, McCrae JP (2020) A survey of current datasets for code-switching research. In: 2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS)"
2008.01391,data,32,,,"translation with monolingual pivot data. the 3rd Workshop on Neural Generation and Translation, Association for Computational Linguistics, Hong Kong, 10.18653/v1/D19-5610, URL 99–107, DOI https://www.aclweb.org/anthology/D19-5610"
2008.01391,data,38,,,"Rijhwani S, Sequiera R, Choudhury MC, Bali K (2016) Translating codemixed tweets: A language detection based system. In: 3rd Workshop on Indian Language Data Resource and Evaluation-WILDRE-3, pp 81–82"
2008.01391,data,43,,,"1 Unit for Linguistic Data, Insight Centre for Data Analytics, Data Science Institute, National University of Ireland Galway 2 Unit for Natural Language Processing, Insight Centre for Data Analytics, Data Science Institute, National University of Ireland Galway"
2008.01391,data,53,,,"Niu X, Denkowski M, Carpuat M (2018) Bi-directional neural machine translation with synthetic parallel data. In: Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, Association for Computational Linguistics, Melbourne, Australia, pp 84–91, DOI 10.18653/v1/W18-2710, URL https://www.aclweb.org/anthology/W18-2710"
2008.01391,data,56,,,"Li Z, Specia L (2019) Improving neural machine translation robustness via data augmentation: Beyond back-translation. In: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), Association for Computational Linguistics, Hong Kong, China, pp 328–336, DOI 10.18653/v1/D19-5543, URL https://www.aclweb.org/anthology/D19-5543"
2008.01391,data,58,,,"Poncelas A, Popovi´c M, Shterionov D, Maillette de Buy Wenniger G, Way A (2019) Combining PBSMT and NMT back-translated data for efﬁcient NMT. In: Natural Language Processing in a Deep Learning World, INCOMA Ltd., Varna, Bulgaria, pp 922–931, DOI 10.26615/978-954-452-056-4_107, URL https://www.aclweb.org/anthology/R19-1107"
2008.01391,data,60,,,"Fadaee M, Bisazza A, Monz C (2017) Data augmentation for low-resource neural machine translation. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Association for Computational Linguistics, Vancouver, Canada, pp 567–573, DOI 10.18653/v1/P17-2090, URL https://www.aclweb.org/anthology/P17-2090"
2008.01391,data,61,,,"Sennrich R, Haddow B, Birch A (2016) Improving neural machine translation models with monolingual data. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Berlin, Germany, pp 86–96, DOI 10.18653/v1/P16-1009, URL https://www.aclweb.org/anthology/P16-1009"
2008.01391,data,62,,,"Dou Q, Vaswani A, Knight K (2014) Beyond parallel data: Joint word alignment and decipherment improves machine translation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Doha, Qatar, pp 557–565, DOI 10.3115/v1/ D14-1061, URL https://www.aclweb.org/anthology/D14-1061"
2008.01391,data,69,,,"Chakravarthi BR, Arcan M, McCrae JP (2019a) Comparison of Different Orthographies for Machine Translation of Under-Resourced Dravidian Languages. In: 2nd Conference on Language, Data and Knowledge (LDK 2019), Schloss Dagstuhl– Leibniz-Zentrum fuer Informatik, Dagstuhl, Germany, OpenAccess Series in Informatics (OASIcs), vol 70, pp 6:1–6:14, DOI 10.4230/OASIcs.LDK.2019.6, URL http://drops.dagstuhl.de/opus/volltexte/2019/10370"
2008.01391,data,92,,,"The two core methodologies used in the development of machine translation systems - RBMT and SMT - come with their own shares of advantages and disadvantages. In the initial stages, RBMTs were the ﬁrst commercial systems to be developed. These systems were based on linguistic rules and have proved to be more feasible for resource-poor languages with little or no data. It is also relatively simpler to carry out error analysis and work on improving the results. Moreover, these systems require very little computational resources."
2008.01391,data,98,,,"In: J LanProceedings Language guage Resources Association (ELRA), Marrakech, Morocco, URL http://www.lrec-conf.org/proceedings/lrec2008/pdf/484_paper.pdf Tiedemann J, Cap F, Kanerva J, Ginter F, Stymne S, Östling R, Weller-Di Marco M (2016) Phrase-based SMT for Finnish with more data, better models and alternative alignment and translation tools. In: Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, Association for Computational Linguistics, Berlin, Germany, pp 391–398, DOI 10.18653/v1/W16-2326, URL https://www.aclweb.org/anthology/W16-2326"
2008.01391,data,106,,,"2019; Currey and Heaﬁeld, 2019) and unsupervised methods (Artetxe et al., 2019b; Pourdamghani et al., 2019; Artetxe et al., 2019a), which are described in detail in following sections. A large array of techniques have been applied to overcome the data sparsity problem in MT, and virtually all of them seem to be based on the ﬁeld of transfer learning from high-resource languages in recent years. Other techniques are based on lexical and semantic similarities of closely related languages which are more relevant to our survey on orthographic information in machine translation."
2008.01391,data,112,,,"Languages that seeks to survive in modern society need NLP, which requires a vast amount of data and linguistic knowledge to create new language technology tools for languages. Mainly it is a big challenge to develop MT systems for these languages due to the scarcity of data, speciﬁcally sentence aligned data (parallel corpora) in large amounts to train MT systems. For example, Irish, Scottish Gaelic, Manx or Tamil, Telugu, and Kannada belonging to the Goidelic and the Dravidian languages, respectively are considered as under-resourced language due to scarcely available machine -readable resources Alegria et al. (2011)."
2008.01391,data,119,,,"(OOV) problems. To remove the script barrier, Bhat et al. (2016) created machine transliteration models for the common orthographic representation of Hindi and Urdu text. The authors have transliterated text in both directions between Devanagari script (used to write the Hindi language) and Perso-Arabic script (used to write the Urdu language). The authors have demonstrated that a dependency parser trained on augmented resources performs better than individual resources. The authors have shown that there was a signiﬁcant improvement in BLEU (Bilingual Evaluation Understudy) (Papineni et al., 2002) score and have shown that the problem of data sparsity is reduced."
2008.01391,data,122,,,"Spelling errors are ampliﬁed in under-resourced setting due to the potential inﬁnite possible misspelling and leads to a large number of out-of-vocabulary. Additionally, under-resourced morphological rich languages have morphological variation, which causes orthographic errors while using character level MT. A shared task was organised by Li et al. (2019); to deal with orthographic variation, grammatical error and informal languages from the noisy social media text. Data cleaning was used along with suitable corpora to handle spelling errors. Belinkov and Bisk (2018) investigated noise in NMT, focusing on kinds of orthographic errors. Parallel corpora were cleaned before submitting to NMT to reduce the spelling and typographical errors."
2008.01391,data,132,,,"translation system (Utiyama and Isahara, 2007). Finally, using the source-target MT system to create more data and adding it back to the source-target model, which is called back-translation (Sennrich et al., 2016; Edunov et al., 2018). Back translation is simple and easy to achieve without modifying the architecture of the machine translation models. Back-translation has been studied in both SMT (Tiedemann et al., 2016; Ahmadnia et al., 2017; Poncelas et al., 2019) and NMT (Sennrich et al., 2016; Edunov et al., 2018; Hoang et al., 2018; Prabhumoye et al., 2018; Graça et al., 2019; Kim et al., 2019)."
2008.01391,data,141,,,"On the contrary, SMT systems need a large amount of data, but no linguistic theories, especially with morphologically rich languages such as Irish, Persian, and Tamil SMT suffer from out-of-vocabulary problems very frequently due to orthographic inconsistencies. To evade the problem, orthographic normalization was proposed to improve the quality of SMT by sparsity reduction (El Kholy and Habash, 2012). SMT learns from data and requires less human effort in terms of creating linguistics rules. SMT systems, unlike RBMT system, does not cause disambiguation problems. Even though SMT has lots of advantages over rule-based, it also has some disadvantages. Its is very difﬁcult to conduct error analysis with SMT and data sparsity another disadvantage faced by SMT (Costa-Jussa et al., 2012)."
2008.01391,data,149,,,"NMT with word embedding lookup ignores the orthographic representation of the words such as the presence of stems, preﬁxes, sufﬁxes and another kind of afﬁxes. To overcome these drawbacks, character-based word embedding was proposed by Kim et al. (2016). Character-based NMT (Costa-jussà and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2017; Cherry et al., 2018) were developed to cover the disadvantages of the languages which do not have explicit word segmentation. This enhances the relationship between the orthography of a word and its meaning in the translation system. For spelling mistake data for under-resourced languages, the quality of word-based translation drops severely, because every non-canonical form of the word cannot be represented. Character-level model overcomes the spelling and typological error without much effort."
2008.01391,data,160,,,"Transliteration is the conversion of the text from one orthography to another without any phonological changes. The best example of transliteration is named entities and generic words (Kumaran and Kellner, 2007). Data collected from social media are highly transliterated and contains errors thus, using these data for building a machine translation system for resource-poor languages causes errors. One of the primary forms that have a high chance of transliteration is cognates. Cognates are words from different languages derived from the same root. The concept cognate in NLP approaches are the words with similar orthography. Therefore, cognates have a high chance of transliteration. Though Machine translation has progressed a lot in recently, the method of dealing with transliteration problem has changed from a languageindependent manner to cognates prediction when translating between closely related languages, transliteration of cognates would help to improve the result for underresourced languages."
2008.01391,data,161,,,"The main goal of this survey is to shed light on how orthographic information is utilised in the MT system development and how orthography helps to overcome the data sparsity problem for under-resourced languages. More particularly, it tries to explain the nature of interactions with orthography with different types of machine translation. For the sake of simplicity, the analysis presented here in this article is restricted to those languages which have some form of internet resources. The survey is organised as follows: Section 2 explains the background information to follow this article. We present orthographic information in Section 2.2. Section 3 describes the challenges of automatically using orthographic information in RBMT outputs. Section 4 presents an analysis of orthographic information in SMT systems. Section 5 presents an analysis of orthographic information in NMT systems. This survey ends with a discussion of the future directions towards utilising the orthographic information."
2008.01391,data,165,,,"The pivot translation method could also be used to improve MT systems for underresourced languages. One popular way is training SMT systems using source-pivot or pivot-target language pair using sub words where the pivot language is related to source or target or both. The subwords units consisted of orthographic syllable and byte-pair-encoded unit. The orthographic unit is a linguistically motivated unit which occurs in a sequence of one or more consonants followed by a vowel. Unlike orthographic units, BPE (Byte Pair Encoded Unit) (Sennrich et al., 2016) is motivated by statistical properties of the text. It represents stable and frequent character sequences in the texts. As orthographic syllable and BPE are variable-length units and the vocabularies used are much smaller than morpheme and word-level model, the problem of data sparsity does not occur but provides an appropriate context for translation between closely related languages (Kunchukuttan et al., 2017)."
2008.01391,data,168,,,"A solution for the open vocabulary problems in NMT is to break up the rare words into subword units (Chitnis and DeNero, 2015; Ding et al., 2019) which has been shown to deal with multiple script languages ambiguities (Schuster and Nakajima, 2012; Wu et al., 2016). A simple and language-independent tokenizer was introduced for NMT and Multilingual NMT by Kudo and Richardson (2018); it is based on two subword segmentation algorithms, byte-pair encoding (BPE) (Sennrich et al., 2016) and a unigram language model (Kudo, 2018). This system also normalise semantically equivalent Unicode character into canonical forms. Subword segmentation and true-casing model will be rebuilt whenever the training data changes. The preprocessing tools introduced by OpenNMT normalises characters and separates punctuation from words, and it can be used for any languages and any orthography (Klein et al., 2017)."
2008.01391,data,207,,,"In recent years, NMT has improved translation performance, which has lead to a boom in NMT research. The most popular neural architectures for NMT are based on the encoder-decoder (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2016) structure and the use of attention or self-attention based mechanism (Luong et al., 2015; Vaswani et al., 2017). Multilingual NMT created with or without multiway corpora has been studied for the potential for translation between two languages without any direct parallel corpus. Zero-shot translation is translation using multilingual data to create a translation for languages which have no direct parallel corpora to train independently. Multilingual Neural Machine Translation with only monolingual corpora was studied by (Sen et al., 2019; Wang et al., 2019b). In Ha et al. (2016) and (Johnson et al., 2017), the authors have demonstrated that multilingual NMT improves translation quality. For this, they created a multilingual NMT without changing the architecture by introducing special tokens at the beginning of the source sentence indicating the source language and target language."
2008.01391,"data available, data",146,,,"Worldwide, there are around 7,000 languages (Abney and Bird, 2010; Hauksdóttir, 2014). However, most of the machine-readable data and natural language applications are available for very few popular languages, such as Chinese, English, French, or German. For other languages, resources are scarcely available and, for some languages, not at all. Some examples of these languages do not even have a writing system (Maxwell and Hughes, 2006; Krauwer, 2003; Alegria et al., 2011), or are not encoded in major schemes such as Unicode. Due to the unavailability of digital resources, many of these languages may go extinct. With each language that is lost, we lose connection with the culture of the people and characteristics of the languages."
2008.01391,"data, code",49,,,"Ranjan P, Raja B, Priyadharshini R, Balabantaray RC (2016) A comparative study on code-mixed data of Indian social media vs formal text. In: 2016 2nd International Conference on Contemporary Computing and Informatics (IC3I), pp 608– 611, DOI 10.1109/IC3I.2016.7918035"
2008.01391,"data, code",52,,,"Ranjan P, Raja B, Priyadharshini R, Balabantaray RC (2016) A comparative study on code-mixed data of Indian social media vs formal text. In: 2nd International Conference on Contemporary Computing and Informatics (IC3I), IEEE, pp 608– 611, URL https://ieeexplore.ieee.org/document/7918035"
2008.01391,"data, code",119,,,"Back transliteration from one script to native script in code-mixed data is one of the challenging tasks to be performed. Riyadh and Kondrak (2019) adopted three different methods to back transliterate Romanised Hindi-Bangla code-mixed data to Hindi and Bangla script. They have used Sequitur, a generative joint n-gram transducer, DTLM, a discriminate string transducer and the OpenNMT 2 neural machine translation toolkit. Along with these three approaches, they have leveraged target word lists, character language models, as well as synthetic training data, whenever possible, in order to support transliteration. At last, these transliterations are provided to a sequence prediction module for further processing."
2008.01391,"data, code",123,,,"The translation of technical documents such as KDE, GNOME, and Ubuntu translations have code-mixed data since some of the technical terms may not be known to voluntary annotators for translation. Code-mixing in the OpenSubtitles corpus is due to bilingual and historical reasons of native speakers (Chanda et al., 2016; Parshad et al., 2016). Different combinations of languages may occur while codemixing, for example, German-Italian and French-Italian in Switzerland, Hindi-Telugu in state of Telangana, India, Taiwanese-Mandarin Chinese in Taiwan (Chan et al., 2009). As a result of code-mixing of the script are also possible from a voluntary annotated corpus. This poses another challenge for MT"
2008.01391,"data, code",215,,,"From our comprehensive survey, we can see that orthographic information improves translation quality in all types of machine translation from rule-based to completely unsupervised systems like bilingual lexicon induction. For the rule-based machine translation, translation between the closely related language is simpliﬁed to transliteration due to the cognates. Statistical machine translation deals with data sparsity problem by using orthographic information. Since statistical machine translation has been studied a long time, most of the orthographic properties are studies for different types of languages. Even the recent neural machine translation and other methods still use preprocessing tools such as true-casers, tokenizers, and detokenizers that are developed for statistical machine translation. Recent neural machine translation is completely end-to-end, however, it suffers from data sparsity when dealing with morphologically rich languages or under-resourced languages. These issues are dealt by utilising orthographic information in neural machine translation. One such method which improves the translation is a transliteration of cognates. Codeswitching is another issue with under-resourced languages due to the data collected from voluntary annotator, web crawling or other such methods. However, dealing with code-switching based on orthography or using character-based neural machine translation has been shown to improve the results signiﬁcantly."
2008.01391,"data, code",224,,,"A signiﬁcant part of corpora for under-resourced languages comes from movie subtitles and technical documents, which makes it even more prone to code-mixing. Most of these corpora are movie speeches (Birch et al., 2019) transcribed to text, and they differ from that in other written genres: the vocabulary is informal, nonlinguistics sounds like ah, and mixes of scripts in case of English and native languages (Tiedemann, 2008; Ranjan et al., 2016; Jose et al., 2020; Chakravarthi et al., 2020b,a; Priyadharshini et al., 2020). Data augmentation (Fadaee et al., 2017; Li and Specia, 2019) and changing the foreign to native words using dictionaries or other methods have been studied. Removing the code-mixing word from the corpus on both sides was studied by Chakravarthi et al. (2018, 2019b) for English-Dravidian languages. Song et al. (2019) studied the data augmentation method, making code-switched training data by replacing source phrases with their target translation. Character-based NMT (Costa-jussà and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2017) can naturally handle intra-sentence codeswitching as a result of the many-to-one translation task."
2008.01391,"data, open-source data, open-source",108,,,"Open-source shallow-transfer MT engine for the Romance languages of Spain such as Spanish, Catalan and Galician developed by Armentano-Oller et al. (2006). They were regeneration of existing non-open-source engines based on linguistic data. The post-generator in the system performs the orthographical operation such as contraction and apostrophes to reduce the orthographical errors. The dictionaries were used for string transformation operations to the target language surface forms. Similarly, the translation between Spanish-Portugues used a post-generation module to performs orthographical transformations to improve the translation quality (Garrido Alenda et al., 2004; Forcada et al., 2011)."
2008.01391,database,216,,,"writing a language in the other scripts keeping the phonemic units intact. It is extensively used in speech processing research, text-to-speech, and speech database construction â ˘AˇT phonetic transcription to common script has shown to improve the results of machine translation (Chakravarthi et al., 2018). The authors focus on the multilingual translation of languages which uses different scripts and studies the effect of different orthographies to common script with multilingual NMT. Multiway NMT system was created for Czech and Polish with Czech IPA transcription and Polish transcription to a 3-way parallel text together to take advantage of the phonology of the closely related languages (Chen and Avgustinova, 2019). Orthographic correspondence rules were used as a replacement list for translation between closely related Czech-Polish with added back-translated corpus (Chen and Avgustinova, 2019). Dialect translation was studied by Baniata et al. (2018). To translate Arabic dialects to modern standard Arabic, they used multitask learning which shares one decoder for standard Arabic, while every source has a separate encoder. This is due to the non-standard orthography in the Arabic dialects. The experiments showed that for the under-resourced Arabic dialects, it improved the results."
2008.01391,open-source,48,,,"Forcada ML, Ginestí-Rosell M, Nordfalk J, Oâ ˘A ´ZRegan J, Ortiz-Rojas S, PérezOrtiz JA, Sánchez-Martínez F, Ramírez-Sánchez G, Tyers FM (2011) Apertium: a free/open-source platform for rule-based machine translation. Machine translation 25(2):127–144"
2008.01391,open-source,48,,,"Tyers FM, Nordfalk J, et al. (2009) Shallow-transfer rule-based machine translation for swedish to danish. In: Proceedings of the First International Workshop on Free/Open-Source Rule-Based Machine Translation, Universidad de Alicante. Departamento de Lenguajes y Sistemas Informáticos, pp 27–33"
2008.01391,open-source,71,,,"Koehn P, Hoang H, Birch A, Callison-Burch C, Federico M, Bertoldi N, Cowan B, Shen W, Moran C, Zens R, et al. (2007) Moses: Open source toolkit for statistical machine translation. In: Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions, Association for Computational Linguistics, pp 177–180"
2008.01391,open-source,117,,,"Allauzen C, Byrne B, Gispert Ad, Iglesias G, Riley M (2014) Pushdown automata in statistical machine translation. Computational Linguistics 40(3):687–723, DOI 10. 1162/COLI_a_00197, URL https://www.aclweb.org/anthology/J14-3008 Armentano-Oller C, Carrasco RC, Corbí-Bellot AM, Forcada ML, Ginestí-Rosell M, Ortiz-Rojas S, Pérez-Ortiz JA, Ramírez-Sánchez G, Sánchez-Martínez F, Scalco MA (2006) Open-source portuguese–spanish machine translation. In: Vieira R, Quaresma P, Nunes MdGV, Mamede NJ, Oliveira C, Dias MC (eds) Computational Processing of the Portuguese Language, Springer Berlin Heidelberg, Berlin, Heidelberg, pp 50–59"
2008.01391,"publicly available, code",73,,,"Code-mixing is a phenomenon which occurs commonly in most multilingual societies where the speaker or writer alternate between more than one languages in a sentence (Ayeomoni, 2006; Ranjan et al., 2016; Yoder et al., 2017; Parshad et al., 2016). Most of the corpora for under-resourced languages came from the publicly available parallel corpora which were created by voluntary annotators or aligned automatically."
2008.08134,data,5,,,6.2 Results on Real-World Data
2008.08134,data,6,,,6.1 Result Discussion on Artiﬁcial Data
2008.08134,data,17,,,in private data analysis. In: Theory of Cryptography. pp. 265–284 (2006)
2008.08134,data,46,,,"We conclude that RRMinHash with B = 2 is a good choice in all considered experiments on artiﬁcial data. For small privacy budget, large user vectors are needed to get small estimation errors. A larger privacy budget allows to accommodate smaller vectors."
2008.08134,data,92,,,"Abstract. This paper describes two locally-diﬀerential private algorithms for releasing user vectors such that the Jaccard similarity between these vectors can be eﬃciently estimated. The basic building block is the well known MinHash method. To achieve a privacy-utility trade-oﬀ, MinHash is extended in two ways using variants of Generalized Randomized Response and the Laplace Mechanism. A theoretical analysis provides bounds on the absolute error and experiments show the utility-privacy trade-oﬀ on synthetic and real-world data. The paper ends with a critical discussion of related work."
2008.08134,data,105,,,"Diﬀerential privacy conveys a precise mathematically deﬁnition of privacy. It says that a randomized algorithm is private if for two “neighboring” databases, there must be a “good enough” probability that the algorithm produces the same output. Here, a clean deﬁnition of neighboring is a key criterion and we will introduce our notion in the next section. While diﬀerential privacy usually works with a trusted data curator, the notion of local diﬀerential privacy describes the setting in which the user apply the randomized algorithm themselves. Thus, the curator never sees the original data."
2008.08134,data,106,,,"We summarize that there is a clear trade-oﬀ between the utility and privacy of the proposed mechanisms. The results on artiﬁcial and real-world data show that to ensure good utility under a small privacy budget, user vectors have to contain many items, say in the 100s. Many of the theoretical choices translated well into practice. Most interestingly, while the upper bounds in the theory section painted an unclear picture about the utility at a ﬁxed privacy budget, our empirical analysis clearly suggests that RRMinHash is both easier to implement and achieves higher utility for the same privacy budget."
2008.08134,data,333,,,"Privacy of user data is becoming an ever increasing need for organizations and users alike. Multiple large-scale privacy breaches in the last years showed how critical and vulnerable most of today’s infrastructure is [8]. In particular, there is dispute about the concept of a trusted data curator to whom users send their original data, and who uses this data to build models for diﬀerent tasks such as targeted advertisement. As Kearns and Roth put it in their recent book about ethical algorithms [10], “[to] make sure that the eﬀect of these models respect the societal norms that we want to maintain, we need to learn how to design these goals directly into our algorithms.” In pursue of this goal, the present paper studies how we can implicitly incorporate privacy into a similarity search system. The concept of diﬀerential privacy as introduced by Dwork et al. in [7] deﬁnes privacy in a precise mathematical way that often allows the design of eﬃcient randomized algorithms. In the case of an untrusted data curator, the concept can be extended to local diﬀerential privacy, where users themselves run randomized algorithms to make their data private before sending it to an untrusted curator. This paper proposes two randomized mechanisms when users have a collection of items and are interested in ﬁnding their similarity with other users under the Jaccard similarity in a private manner. The proposed algorithms build upon the papers [11,4,17] and a precise account of the relation will be given in the related work section at the end of this paper. In a nutshell, each user starts by applying MinHash as introduced by Broder [1] with the range compression of Li and K¨onig [12] (Section 3.1) to produce a sketch of their data. It is well known that"
2008.08134,"data available, data, data https, dataset",291,,,"Second, we study how well these algorithms work on real-world datasets. Following [17], we chose the MovieLens and Last.FM dataset available at https: //grouplens.org/datasets/hetrec-2011. We obtain a set representation by collecting all movies rated at least 4 (MovieLens, m = 65 536) and the top-20 artists (Last.FM, m = 18 739 ). The average set size is 178.1 (σ = 187.5) and 19.8 (σ = 1.78), respectively. To account for the inﬂuence of the size of the user vectors, we create diﬀerent versions of these datasets. From the MovieLens dataset, we make three versions containing all users that have at least 50, 100, and 500 entries, respectively. This results in datasets with 1636, 1205, and 124 users. From the Last.FM dataset, we collect all users that have at least 20 entries which amounts to 1860 users. For each dataset, we take 50 query points at random among all data points for which the 10-th nearest neighbor has at least Jaccard similarity 0.1. As quality measure, we use recall@k (R@k) which measures the average number of times that the (index of the) true nearest neighbor is found among the ﬁrst k nearest neighbors in the private vectors. (Note that the true vectors are not revealed, so there cannot be a re-ranking step as is tradition in nearest neighbor search.) Moreover, we report on the approximate similarity ratio, which is deﬁned as the ratio of the sum of similarities to the 10 original"
2008.08134,"data, data https",85,,,"3. Charikar, M.: Similarity estimation techniques from rounding algorithms. In: Proc. 34th ACM Symposium on Theory of Computing (STOC). pp. 380–388 (2002) 4. Dhaliwal, J., So, G., Parker-Wood, A., Beck, M.: Utility preserving secure private data release. CoRR abs/1901.09858 (2019), http://arxiv.org/abs/1901.09858 5. Dubhashi, D.P., Panconesi, A.: Concentration of Measure for the Analysis of"
2008.08134,dataset,8,,,Dataset Last.FM (τ = 20) MinHash
2008.08134,dataset,31,,,Table 1. Results on real-world datasets for diﬀerent quality measures and privacy budget ε of 4 and 8 (split up via “/” in individual cells).
2008.08134,dataset,37,,,"15. Riazi, M.S., Chen, B., Shrivastava, A., Wallach, D.S., Koushanfar, F.: Sub-Linear Privacy-Preserving Near-Neighbor Search with Untrusted Server on Large-Scale Datasets (2016), arXiv:1612.01835"
2008.08134,dataset,41,,,"Table 1 summarizes the observed results for runs on the Last.FM and MovieLens datasets. Again, we set MinHash in relation to RRMinHash and NoisyMinHash. Motivated by the observations above we only discuss the case B = 2."
2008.08134,dataset,57,,,"It is important that the range of potential prices [values] is independent of the actual bids [the hash values which were observed for the user]. Otherwise there would exist a price [value] with non-zero weight in one dataset and zero weight in a neighboring dataset, violating diﬀerential privacy."
2008.08134,dataset,174,,,"We observe that RRMinHash achieves equal or better quality than NoisyMinHash in all measurements, so we focus the comparison on MinHash and RRMinHash. First, we note that the datasets are rather diﬃcult. Even standard MinHash with B = 2 does not achieve close to perfect recall, which means that all vectors are rather close to each other. The Last.FM dataset provides very small user vectors. Accordingly there is a big diﬀerence between the quality achieved by the two algorithms. For a privacy budget of ε = 4, the quality is between a factor of around 10 (R@10) and of around 3 (R@100, Approx) worse if solving the similarity search task on private vectors. For a privacy budget of ε = 8, these factors shrink to 1.5-3. With regard to MovieLens, we observe that it is diﬃcult for MinHash to achieve high recall values for τ = 50. Results for RRMinHash are"
2008.08134,"github, python, code",58,,,"All algorithms described in this paper where implemented in Python 3. The code, raw results, and evaluation notebooks can be accessed at https://github. com/maumueller/ldp-jaccard. Due to space restrictions, we only present a few selected results. See the Jupyter notebook at the web page for additional plots and tables."
2009.00822,data,4,,,40 60 Data Points
2009.00822,data,12,,,Fig. 4: Plot of test error on time series data
2009.00822,data,40,,,"i = 1 · · · c} is the maximum distance of kth input vector from the ith cluster, vi and σi denotes the average distance and variance of each data points from the cluster hyperplane respectively."
2009.00822,data,43,,,In this paper we have presented a new framework for FCRM with an innovative student-t distribution based membership function (MF) for sparse data modelling [6]–[8]. The presented approach is described in following subsections.
2009.00822,data,43,,,"Through MAP estimate on the posterior of defuzziﬁed error function Eik(ζi), the upper and lower membership function for every data points in each cluster are obtained as similar to [3] and they are given as follows:"
2009.00822,data,54,,,"Here, uik denotes the membership value of kth data point in the ith cluster. The parameter ζi and ζi are estimated by SGD using the above objective function by appropriately ﬁnding uik and uik. Then the regression coefﬁcient (ζi) are obtained by a type reduction technique as follow:"
2009.00822,data,54,,,"The main motivation of inter type-2 fuzzy c-regression algorithm is to partition the set of n data points (xk, yk) (k = 1 · · · n) into c clusters. The data points in every cluster i can be described by a regression model as mxkm + bi"
2009.00822,data,109,,,"The MF developed in [5] is hyperplane shaped, which cannot successfully incorporate the relevant information of data distributions within different clusters. To overcome this issue, we proposed a modiﬁed Gaussian based MF combined with a student-t density function. The student-t distribution is widely used as a prior in Bayesian inference for sparse data modelling [7]. Here, we weigh the Gaussian and the student-t part by a hyper-parameter α. If the data we are modelling is very sparse then, α should be set very low so as to give more weight to student-t density membership value."
2009.00822,data,112,,,"(0, 40]. We have sampled 121 data points uniformly for this one dimensional function. As similar to previous case study, the number of rules is taken as four. The hyper-parameters are tuned through grid-search and ﬁnally ﬁxed as: m1 = 1.5, m2 = 7 and η = 3.14. The MSE of the proposed model is 2.4 × 10−3, which is lower in compare to modiﬁed inter type-2 FRCM (MIT2-FCRM) [6], which is 7.7 × 10−3 on the test data of 121 samples. The Table III provides a detailed comparison of performance with state-ofthe-art methods."
2009.00822,data,126,,,"hyperplane-shaped clustering becomes more popular [4], [5]. The architecture of these algorithms are robust in partitioning the data space, inferring estimates of outputs with the inputs and determining an optimum ﬁt for the regression model. Previously proposed techniques like fuzzy c-regression model (FCRM) and fuzzy c-mean (FCM) have been developed for type-2 fuzzy logic framework. Here, upper and lower membership values are determined by simultaneously optimizing two objective functions. Interval type-2 (IT2) FCRM, which was presented recently for the T-S regression framework has shown signiﬁcantly better performance in terms of error minimization and robustness in comparison of type-1 fuzzy logic [4]–[6]."
2009.00822,data,142,,,"error (MSE) is 0.008 on the test data, which is lower than state-of-the-art methods as shown in Table I. The absolute value of error as shown in Fig. 2 is also small in compare to the absolute house prices as shown in Fig. 1. We postulate this is due to the student-t MF used in our model, that which helps in robustly quantifying the effects of sparse data. Also, the higher test accuracy is due to greater generalization owing to L2 regularizer used in our model. The coefﬁcient of variation which is the ratio of explained variance to total variance is very high (0.85). This suggests that our model captures variations in the data robustly and is not susceptible to faulty performance in the presence of outliers."
2009.00822,data,146,,,"T HERE have been numerous research focusing on model ing of non-linear systems through their input and output mapping. In particular, fuzzy logic based approaches have been very successful in modeling of non-linear dynamics in the presence of uncertainties [1]. Type-1 fuzzy logic enables system identiﬁcation and modeling by virtue of numerous linguistic rules. Although this approach performs well, but due to the limitation of crisp membership values, its potential to handle the uncertainty in data is limited. Therefore, in order to successfully model the data uncertainty, a type-2 fuzzy logic was proposed, where membership values of each data points are themselves fuzzy. The type-2 fuzzy logic has been remarkably successful in past due to its robustness in the presence of imprecise and noisy data [2], [3]."
2009.00822,data,151,,,In the most of the literature the defuzziﬁcation of weights is computed before determining the model output ˆyk. The problem with these approaches are that they do not consider effect of model output which will affect the over all performance of the model. To overcome this problem we evaluated the y k (xk) and µAi(xk) using the KM and yk corresponding to the µ Ai algorithm [2]. The values of y and yk are optimized parallelly k until the convergence. The another advantage of this approach is that it become more robust in handling noise and provide a conﬁdence interval for every output data points. The model (xk) and output y and yk corresponding to the weights µ k Ai µAi(xk) are calculate using (1) and (2) as follows:
2009.00822,data,181,,,"where, v(k) = sin(2k/25) is the input for validation of model, z(k) is the model output whereas, z(k − 1), z(k − 2) and u(k) are the model inputs respectively. The hyper-parameters are tuned by grid search and ﬁnally set as: c = 4, m1 = 1.5, m2 = 7 and η = 3.14. The obtained MSE of the model on 500 test data points is 7.2 × 10−5 using only four rules which is much smaller compared to other models. Through simulations, we have shown that proposed model outperforms with other state-of-the-art model. The Fig. 3 shows that our model output closely tracks the actual output at every timestep. As observed in Fig. 4, the error ﬂuctuates with data point, but the absolute error is consistently less than 0.1 with no rapid surge at stationary points of time series data. This is a crucial"
2009.00822,data,210,,,"In this paper, we have combined the Gaussian and student-t density type membership function for an IT2 FCRM framework. This is a hyperplane-shaped membership function with relatively two different weighed terms. The student-t density part is weighed more if the data being modeled is sparse. The student-t distribution is a popular prior for sparse data modelling in Bayesian inference. Therefore, it is used in our model. The stochastic gradient descent (SGD) technique is used for optimization of consequent parameters and Karnik Mendel (KM) algorithm is applied for type reduction of estimated output. We have used L2 regularization of the regression coefﬁcients in the IT2 fuzzy c-means clustering for identiﬁcation of antecedent parameters. As demonstrated in the results section, the regularization helps our model against overﬁtting of the training data and increases the generalization on unseen data. In addition, an innovative scheme for optimizing the consequent parameters is also presented, wherein we do not perform type reduction of the type-1 weight sets prior for output estimation. Instead, we use KM algorithm for the output to infer an optimal interval type-1 fuzzy set and the set boundaries are optimized by SGD method."
2009.00822,"data, data https, dataset",157,,,"dataset (https://www.kaggle.com/lespin/house-prices-dataset) is used to predict the sale price of a particular property. Through experimentation, we have demonstrated the robustness of proposed method on this sparse data. The dataset is divided in training (70%) and testing (30%) sets and ﬁve-fold cross-validation is used while training. The hyper-parameters of the model are initially set as: c = 3, m1 = 1.6, m2 = 4.7, λ = 0.3, α = 0.15 and η = 3.7. It should be noted that the value of α = 0.15 is small because the dataset is sparse. So, in MF, the contribution of student-t function should be high, which is ensured by a smaller value of α i.e., larger value of 1 − α as deﬁned by (12) and (13). The mean square"
2009.00822,"data, dataset",147,,,"Abstract—Clustering techniques have been proved highly successful for Takagi-Sugeno (T-S) fuzzy model identiﬁcation. In particular, fuzzy c-regression clustering based on type-2 fuzzy set has been shown the remarkable results on non-sparse data but their performance degraded on sparse data. In this paper, an innovative architecture for fuzzy c-regression model is presented and a novel student-t distribution based membership function is designed for sparse data modelling. To avoid the overﬁtting, we have adopted a Bayesian approach for incorporating a Gaussian prior on the regression coefﬁcients. Additional novelty of our approach lies in type-reduction where the ﬁnal output is computed using Karnik Mendel algorithm and the consequent parameters of the model are optimized using Stochastic Gradient Descent method. As detailed experimentation, the result shows that proposed approach outperforms on standard datasets in comparison of various state-of-the-art methods."
2009.00822,dataset,5,,,A. House Prices Dataset
2009.00822,dataset,10,,,Table I: COMPARISON OF PERFORMANCE ON HOUSE PRICES DATASET
2009.00822,dataset,12,,,Fig. 2: Plot of test error for house prices dataset
2009.10156,code,83,,,"An alternative approach is to allow modellers to express the validity properties of a planning problem declaratively. An instance generator with those validity constraints integrated is then automatically created. Specifying those properties using a declarative modelling language provides ﬂexibility, as users can easily add or update the validity speciﬁcation without having to modify the generator software’s source code directly. This is the approach of the essencebased automated instance generation system that we propose to build upon."
2009.10156,dataset,199,,,"As described in Section 4, another group of validity constraints involves implicit requirements on structures of the underlying map in a planning problem, such as the connections between tiles in the floor-tile problem. It is possible to express grids quite simply if the relations used to express the grid structure make use of the geometry of the plane. However, automated instance generation should not restrict the choices made in modelling problems. In the IPC-14 benchmark dataset, all instances of floor-tile have the tiles forming a square-grid structure and tiles’ connections are represented using adjacency relations left, right, up and down. Validity constraints to ensure that these adjacency relations express a square-grid map cannot be eﬃciently modelled using PDDL due to the limited expressiveness of ﬁrst-order logic. Checking that the adjacency relations form a valid grid requires reconstructing geometric information about placement of tiles on the plane. Although it is possible to express the property that the adjacency relations form a grid for special cases, even this requires solving a challenging tiling problem, and the general case is currently open [15]."
2009.10156,package,50,,,"17. L´opez-Ib´a˜nez, M., Dubois-Lacoste, J., C´aceres, L.P., Birattari, M., St¨utzle, T.: The irace package: Iterated racing for automatic algorithm conﬁguration. Operations Research Perspectives 3, 43–58 (2016). https://doi.org/10.1016/j.orp.2016.09.002, http://iridia.ulb.ac.be/irace/"
2009.10156,package,88,,,"The planning task consists of selecting a sequence of actions in order to achieve a speciﬁed goal from speciﬁed starting conditions. This type of problem arises in many contexts. Consider, for example, the delivery of a set of packages by vehicle from a depot to a set of destinations. The allocation of packages and drivers to vehicles must be planned, as well as the route for each vehicle, while respecting package delivery deadlines, vehicle capacities and driver shift restrictions."
2009.10156,python,161,,,"transitions of the state variables between steps respect the implicit constraints. When a model and an instance respect all the implicit assumptions by the modeller we say that it is a valid problem. As an example, some implicit assumptions by the modeller in the floor-tile domain (Figure 2) are that in the initial state each robot must be at exactly one tile, or that any given cell can only have one unique cell on top of it. Moreover, in the IPC-14 published instances, the tiles’ structure (represented by up, down, right and left) always forms a square grid. In the automated planning community, sometimes when a problem is released, a Python or Java program is included to generate instances automatically. The validity properties of the problem are normally hard-coded in those programs, and therefore appear implicitly in the generated problem instances."
2010.05620,code,170,,,"the left part of Eq. 5. This is repeated for several steps (epochs), using one Monte Carlo sample between each gradient step as suggested by [26] and [21], and it worked well in our experiments. After training, we remove the stochastic part of the gates and use only the variables ix ∈ {1, ..., Dx} and iy ∈ {1, ..., Dy} such that zx iy > 0. Empirically, we observe that our method also works well with stochastic gradient descent, as long as the batch size is not too small. Alternatively, for small batches we can use a variant of the total correlation loss as was presented in [35] or [36]. In the Appendix section C, we present a pseudo-code of ‘0-DCCA , and extend our formulation for a multi-modal setting (more than two views)."
2010.05620,data,16,,,[51] Paul Horst. Generalized canonical correlations and their applications to experimental data.
2010.05620,data,29,,,"To generate samples from X, Y ∈ RD×N , we follow the procedure described in [12], considering data sampling from the following distribution 0 0"
2010.05620,data,29,,,"[58] Arthur Tenenhaus, Cathy Philippe, and Vincent Frouin. Kernel generalized canonical correlation analysis. Computational Statistics and Data Analysis, 90:114–131, 2015."
2010.05620,data,35,,,"Input: Coupled data, {X, Y }, regularization parameters λx, λy, number of epochs T . Initialize the gate parameters: µx for t = 1 to T do"
2010.05620,data,37,,,"[33] Oﬁr Lindenbaum, Uri Shaham, Jonathan Svirsky, Erez Peterfreund, and Yuval Kluger. Let the data choose its features: Diﬀerentiable unsupervised feature selection. arXiv preprint arXiv:2007.04728, 2020."
2010.05620,data,38,,,"[19] Kosuke Yoshida, Junichiro Yoshimoto, and Kenji Doya. Sparse kernel canonical correlation analysis for discovery of nonlinear interactions in high-dimensional data. BMC bioinformatics, 18(1):1–11, 2017."
2010.05620,data,39,,,"[16] Elena Parkhomenko, David Tritchler, and Joseph Beyene. Sparse canonical correlation analysis with application to genomic data integration. Statistical applications in genetics and molecular biology, 8(1), 2009."
2010.05620,data,40,,,"[20] Viivi Uurtio, Sahely Bhadra, and Juho Rousu. Sparse non-linear cca through hilbertschmidt independence criterion. In 2018 IEEE International Conference on Data Mining (ICDM), pages 1278–1283. IEEE, 2018."
2010.05620,data,43,,,"[6] Moshe Salhov, Oﬁr Lindenbaum, Yariv Aizenbud, Avi Silberschatz, Yoel Shkolnisky, and Amir Averbuch. Multi-view kernel consensus for data analysis. Applied and Computational Harmonic Analysis, 49(1):208–228, 2020."
2010.05620,data,44,,,"Figure 8: Classiﬁcation accuracy on the noisy seismic data. Performance is evaluated using linear SVM in the 3 dimensional embedding. Comparing performance of ‘0-DCCA for diﬀerent levels of sparsity, and using linear and nonlinear activation (tanh)."
2010.05620,data,45,,,Method Raw Data PCA [46] CCA [47] mod-SCCA [12] SCCA-HSIC [20] KCCA [3] grad-KCCA [48] multiview-ICA [49] NCCA [4] DCCA [7] DCCAE [34] ‘0-DCCA
2010.05620,data,54,,,"To demonstrate the computational eﬃciency of our method, we run ‘0-CCA for diﬀerent values of N and D and evaluate the empirical computational complexity of the method. In Fig. 9 we present the average runtime over 100 runs, the data is generates following Model I from Section 5.1."
2010.05620,data,54,,,"We train ‘0-DCCA to embed the data into a correlated 10 dimensional space while selecting subsets of input pixels. Our model selects 277, and 258 pixels from both modalities respectively (see bottom right corner of Fig. 5). Next, we evaluate the quality of learned embedding by"
2010.05620,data,68,,,"Sample a stochastic gate (STG) vectors zx, zy deﬁned based on equation 4. Apply the STG to the data ˆX = zx (cid:12) X and ˆY = zy (cid:12) Y . Compute the loss L = Eh the total correlation ¯ρ is based on Section 3.4. θy = θy − γ∇θy L, Update θx = θx − γ∇θxL,"
2010.05620,data,81,,,"Using Model I, we ﬁrst generate N = 400 samples, with D = 800, and estimate the canonical vectors based on CCA and ‘0-CCA. In Fig. 2, we present a regularization path of the proposed scheme. Speciﬁcally, we apply ‘0-CCA to the data described above using various values of λ = λx = λy. We present the ‘0 of active gates (by expectation) along with the empirical"
2010.05620,data,88,,,Figure 1: Illustration of ‘0-DCCA. Data from two views propagate through stochastic gates (deﬁned in Eq. 4). The gates output is fed into two neural sub-nets that have a shared loss (see Eq. 5). We compute this shared loss based on the neural sub-nets outputs (with dimension d = 3 in this example). Our shared loss combines a total correlation term with a diﬀerentiable regularization term which induces sparsity in the input variables.
2010.05620,data,121,,,"observations from the METABRIC data [44] and attempt to ﬁnd correlated representations to improve cancer sub-type classiﬁcation. The data consists of 1, 112 breast cancer patients which are annotated by 10 subtypes based on InClust [45]. We observe tow modalities, namely the RNA gene expression data, and Copy Number Alteration (CNA) data. The dimensions of these modalities are 15, 709 and 47, 127, respectively. We compute the ‘0-DCCA 10 dimensional embedding (and all baseline embeddings) and demonstrate using k-means and SVM that the representation identiﬁed using ‘0-DCCA can lead to more accurate cancer sub-type classiﬁcation (see Table 2)."
2010.05620,data,131,,,"The proposed method provides an eﬀective solution to the problems of sparse linear and nonlinear CCA. One advantage of the suggested method compared with existing sparse CCA solutions, is that it can embed data into a d > 1 dimensional space and learn the sparsity pattern simultaneously. The proposed ‘0-CCA problem albeit not being convex leads to an empirically stable solution across diﬀerent settings. Nonetheless, the method has some limitations, speciﬁcally, tuning λ requires a cross validation procedure, which could be costly in high dimensional regime. Another caveat of the existing approach is that it lacks guarantees when trained on small batches. In the future, we aim to extend the method to enable compatibility with small batch training."
2010.05620,data,137,,,"In this example, we use a learning rate of 0.5 with 2000 epochs. The number of neurons for the 3 hidden layers are: 500, 300, 100, with a tanh activation after each layer. The number of dimensions in the embedding (d = 10) was selected based on the number of classes in the data. Parameters are optimized manually to maximize the correlation on a validation set. We compare ‘0-DCCA to CCA [47], mod-SCCA [12], SCCA-HSIC [20], KCCA [3], NCCA [4], multiview-ICA [49], DCCA [7], and DCCAE [34]. We use the same SVM and k-means scheme as described in Section A.3."
2010.05620,data,164,,,"where f ( ˆX) = f (zx (cid:12) X|θx) ∈ Rd×N , and g( ˆY ) = g(zy (cid:12) Y |θy) ∈ Rd×N are modeled as deep networks with model parameters θ = (θx, θy), and gate parameters µ = (µx, µy). The functions f and g embed the data into a d-dimensional space. The functional ¯ρ(f ( ˆX), g( ˆY )) measures the total correlation between the two d dimensional outputs of the deep nets, this is the sum over d correlation values computed between pairs of coordinates. Exact details on the computation of this term appear in the following subsection. The regularization parameters λx, λy control the sparsity of the input variables. The vectors zx and zy are Bernoulli relaxed vectors, with elements deﬁned based on Eq. 4."
2010.05620,data,166,,,"Canonical correlation models have been widely used in biology [8], neuroscience [9], medicine [10], and engineering [11], for unsupervised or semi-supervised learning. By extracting meaningful dimensionality reduced representations, CCA improves downstream tasks such as clustering, classiﬁcation, or manifold learning. One key limitation of these models is that they typically require more samples than features, i.e., N > Dx, Dy. However, if we have more variables than samples, the estimation based on the closed-form solution of the CCA problem (in Eq. 1) breaks [12]. Moreover, in high dimensional data, often some of the variables do not measure the phenomenon that is common to both modalities (therefore are not correlated) and thus should be omitted from the transformations. For these reasons, there has been a growing interest in studying sparse CCA models."
2010.05620,data,177,,,"Figure 2: Left: Regularization path of ‘0-CCA on data generated from the linear model I (described in Section 5.1). Values on the left y-axis (green) represent the sum of active gates (by expectation). Values on the right y-axis (blue) represent the empirical correlation between T the estimated representations, i.e. ˆρ = ˆφ X T Y ˆη, where ˆφ and ˆη are the estimated canonical vectors. Dashed lines indicate the correct number of active coeﬃcients (10) and true correlation ρ (0.9). Note that for small values of λ = λx = λy, the model selects many variables and attains a higher correlation value; in this case, ‘0-CCA suﬀers from overﬁtting. Right: True canonical vector φ along with the estimated vectors using ‘0-CCA ( ˆφ) and CCA (ˆa). Due to the small sample size, CCA overﬁts and fails to identify the correct canonical vectors."
2010.05620,data,256,,,"Using the seismic data, we compare the performance of ‘0-DCCA with a linear and non-linear activation. In this example, we use a learning rate of 0.01 with 2000 epochs. The number of neurons for the ﬁve hidden layer are: 300, 200, 100, 50, and 40 respectively, with a tanh activation after each layer. The number of dimensions in the embedding (d = 3) was selected based on the number of classes in the data. Parameters are optimized manually to maximize the correlation on a validation set. In Fig. 8, we present SVM accuracy for diﬀerent levels of sparsity. The presented number of features is the average over both modalities, and SVM performance is evaluated using 5-folds cross validation. We compare ‘0-DCCA to CCA [47], mod-SCCA [12], SCCA-HSIC [20], KCCA [3], NCCA [4], multiview-ICA [49], DCCA [7], and DCCAE [34]. For all methods we use an embedding with dimension d = 3, and evaluate performance with k-means using 20 random initilizations, and using linear SVM by performing a 5-folds cross validation. For the kernel methods we evaluated performance by constructing a kernel using k = 5, 10, ..., 50, nearest neighbors and selected the value which maximized performance in terms of total correlation."
2010.05620,"data, dataset",116,,,"We validate the eﬀectiveness of the proposed approach on a wide range of tasks. First, using synthetic data, we demonstrate that ‘0-CCA correctly identiﬁes the canonical vectors in a challenging regime of N (cid:28) Dx, Dy. Next, using a coupled video dataset, we demonstrate that ‘0-CCA can identify the common information from high dimensional data, and embed it into correlated, low-dimensional representations. Then, we use noisy images from MNIST and multi-channel seismic data to demonstrate that ‘0-DCCA ﬁnds meaningful representations of the data even in a noisy regime. Finally, we use ‘0-DCCA to improve cancer sub-type classiﬁcation"
2010.05620,"data, dataset",150,,,"The proposed ‘0-CCA algorithm was designed and demonstrated on coupled datasets. However, in many applications, one may have access to multiple (>2) datasets. There are many real-life cases where more than two modalities are available such as hyper-spectral imaging, medical imaging and more. The problem of CCA was generalized decades ago to include multiple data sets by [51, 52, 53] and [54] to name some. These approaches were later summarized in [55]. Since then, many extensions have been proposed such as [56]. Some of the modern extensions of multi-view CCA comprise its regularised [57], kernelised [58] sparse [59] and deep [60] variants. In this section, we extend the proposed ‘0-DCCA to multi-view datasets."
2010.05620,"data, dataset",186,,,"This paper presents ‘0-CCA, a simple yet eﬀective method for learning correlated representation based on sparse subsets of the input variables by minimizing an ‘0 regularized loss. Our ‘0 regularization relies on a recently proposed Gaussian-based continuous relaxation of Bernoulli random variables, termed gates [21]. The gates are applied to the input features to sparsify the canonical vectors. The parameters of the gates and of the model are trained jointly via gradient descent to maximize the correlation between the representations of X and Y while simultaneously selecting only the subsets of most correlated input features. By modeling the transformations using two neural networks, our method provides a natural solution to sparse non-linear correlation analysis. We apply the proposed method to synthetic data and demonstrate that our approach can improve the estimation of the canonical vectors compared with existing sparse CCA models. Then, we use the proposed scheme on several real datasets and demonstrate that it leads to more reliable and interpretable representations than other linear and non-linear data fusion schemes."
2010.05620,"data, dataset",301,,,"Next, we evaluate the method using a dataset of seismic events studied by [41, 42]. Here, we focus on 537 explosions which are categorized into 3 quarries. Each event is recorded using two directional channels facing east (E) and north (N); these comprise the coupled views for the correlation analysis. Following the analysis by [41], the input features are sonogram representations of the seismic signal. Sonograms are time-frequency representations with bins equally tempered on a logarithmic scale. Each sonogram z ∈ R1157 with 89 time bins and 13 frequency bins. An example of sonograms from both channels appears in the top row of Fig. 6. We create the noisy seismic data by adding sonograms computed based on vehicle noise from 1. Examples of noisy sonograms appear in the middle row of Fig. 6. We hold out 20% of the data as a validation set, and train ‘0-DCCA to embed the data in 3 dimensions. In Table 2 we present the MI, k-means and SVM accuracies computed based on ‘0-DCCA embedding. Furthermore, we compare the performance with several other baselines. Here, the proposed scheme improves performance in all 3 metrics while identifying a subset of 17 and 16 features from channel E and N, respectively. The active gates are presented in the bottom row of Fig. 6. Our results indicate that even in the presence of strong noise, ‘0-DCCA correctly activates the gates in frequency bins that coincide with the energy stamps of the primary and secondary waves (P and S in the top left of Fig. 6)."
2010.05620,database,19,,,"[40] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT"
2010.05620,dataset,41,,,"Table 2: Evaluation of correlated embedding extracted from the Noisy MNIST, Seismic, and METABRIC (cancer type) datasets. The representation extracted by ‘0-DCCA leads to higher clustering and classiﬁcation accuracy compared with several baselines."
2010.05620,dataset,48,,,"Figure 5: Images from the coupled noisy MNIST dataset. In the bottom right of both panels, we presents the active gates (white values within a green frame). There are 277 and 258 active gates for view I and II, respectively."
2010.05620,dataset,112,,,"applying k-means to the stacked embedding of both views. We run k-means (with k = 10) using 20 random initializations and record the run with the smallest sum of square distances from centroids. Given the cluster assignment, k-means clustering accuracy (KM) and mutual information (MI) are measured using the true labels. Additionally, we train a Linear-SVM (SVM) model on our train and validation datasets. SVM classiﬁcation accuracy is measured on the remaining test set. The embedding provided by ‘0-DCCA leads to higher classiﬁcation and clustering results compared with several linear and non-linear modality fusion models appear"
2010.05620,dataset,209,,,"Canonical Correlation Analysis (CCA) models are powerful for studying the associations between two sets of variables. The canonically correlated representations, termed canonical variates are widely used in unsupervised learning to analyze unlabeled multi-modal registered datasets. Despite their success, CCA models may break (or overﬁt) if the number of variables in either of the modalities exceeds the number of samples. Moreover, often a signiﬁcant fraction of the variables measures modality-speciﬁc information, and thus removing them is beneﬁcial for identifying the canonically correlated variates. Here, we propose ‘0-CCA, a method for learning correlated representations based on sparse subsets of variables from two observed modalities. Sparsity is obtained by multiplying the input variables by stochastic gates, whose parameters are learned together with the CCA weights via an ‘0-regularized correlation loss. We further propose ‘0-Deep CCA for solving the problem of non-linear sparse CCA by modeling the correlated representations using deep nets. We demonstrate the eﬃcacy of the method using several synthetic and real examples. Most notably, by gating nuisance input variables, our approach improves the extracted representations compared to other linear, non-linear and sparse CCA-based models."
2010.05620,github,3,,,6https://github.com/aalto-ics-kepaco/scca-hsic
2010.05620,github,12,,,2https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html 3https://gist.github.com/yuyay/16ce4914683da30f87d0 4https://tomer.net.technion.ac.il/ﬁles/2017/08/NCCAcode_v3.zip 5https://github.com/adrianna1211/DeepCCA_tensorﬂow
2010.05620,provide implementation,33,,,"in Table 2. In the Appendix, we provide the implementation details and present experimental results demonstrating the performance of ‘0-DCCA for various values of λ = λx = λy."
2010.05620,"used dataset, dataset",253,,,"As an illustrative example, we use a dataset collected by [39] for multiview learning. The authors have generated two videos capturing rotations of 3 desk puppets. One camera captures two puppets, while the other captures another two, where one puppet is shared across cameras. A snapshot from both cameras appears in the top row of Fig. 3. All puppets are placed on spinning devices that rotate the dolls at diﬀerent frequencies. In both videos, there is a shared underlying parameter, namely the rotation of the common bulldog. We use a subset of the spinning puppets dataset, with 400 images from each camera. Each image has 240 × 320 = 76800 pixels (using a grayscaled version of the colored image); therefore, there are more features than samples, and direct application of CCA would fail. We apply the proposed scheme using λy = λx = 50, a linear activation and embedding dimension d = 2. ‘0-CCA converges to embedding with a total correlation of 1.99 using 372 and 403 pixels from views X and Y . The active gates are presented in the right panels of Fig. 3. In this example, the active gates highlight subsets of pixels that overlap with the common spinning Bulldog. This indicates the ability of ‘0-CCA to identify correlated variables in the regime of N < Dx, Dy."
2011.02427,data,35,,,"3. Bhavsar, A.V., Rajagopalan, A.N.: Range map superresolution-inpainting, and reconstruction from sparse data. Computer Vision and Image Understanding 116(4), 572–591 (2012)"
2011.02427,database,48,,,"30. Martin Koestinger, Paul Wohlhart, P.M.R., Bischof, H.: Annotated Facial Landmarks in the Wild: A Large-scale, Real-world Database for Facial Landmark Localization. In: Proc. First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies (2011)"
2011.02427,dataset,2,,,4.2 Datasets
2011.02427,dataset,9,,,Fig. 4: Results for Real-Degraded Dataset.
2011.02427,dataset,31,,,Real-Degraded Dataset: This dataset contains 15000 images from the Widerface Dataset. Performance on this dataset will dictate how eﬀective our method is in super-resolving real degraded facial images.
2011.02427,dataset,41,,,"1. Agustsson, E., Timofte, R.: Ntire 2017 challenge on single image super-resolution: Dataset and study. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops (July 2017)"
2011.02427,dataset,47,,,"9. Cao, Q., Shen, L., Xie, W., Parkhi, O.M., Zisserman, A.: Vggface2: A dataset for recognising faces across pose and age. In: International Conference on Automatic Face and Gesture Recognition (2018)"
2011.02427,dataset,52,,,"On the other hand, in unpaired SISR, only the LR images are available in the dataset. In [45], a CycleGAN [47] was trained to denoise the input image and another one to ﬁnetune a pretrained super-resolution network. In [29], a"
2011.02427,dataset,56,,,"We have a clean High-Resolution dataset Yc and a degraded Low-Resolution dataset Xd. We obtain clean Low-Resolution dataset, Xc, corresponding to Yc, by downsampling every image in Yc with a bicubic downsampling kernel. So every xc in Xc is a downsampled version of some yc in Yc, using the equation"
2011.02427,dataset,70,,,"where, k is the bicubic downsampling kernel and s is the scale factor. Following [6], we train a Degradation GAN, Gd to convert clean samples from Xc to look like they have been drawn from the degraded LR dataset Xd. We call this synthetic degraded LR dataset (cid:99)Xd and samples in this dataset (cid:99)xd. So,"
2011.02427,dataset,111,,,"Our work builds on the existing methods in robust feature learning. Haoliang et al. [27] extract robust features from multiple datasets of similar semantic contents by minimizing Maximum Mean Discrepancy (MMD) between features extracted from these datasets. Cemgil et al. [10], achieve robustness by forcing Entropy Regularized Wasserstein Distance to be low between features extracted from clean images and their noisy counterparts. None of these works handle Super-Resolution where rigorous compression using an autoencoder may hurt the reconstruction quality. We propose a method of incorporating robust feature learning in super-resolution without requiring any face speciﬁc prior information."
2011.02427,dataset,158,,,"Single Image Super-Resolution (SISR) is a highly ill-posed inverse problem. Traditional methods mostly impose handcrafted constraints as priors to restrict the space of solutions. Early works [3,4,37] of Super-Resolution used multiple shifted low-resolution images of the same scene to retrieve the latent high resolution image conditioned on the motion between the LR images [3]. The performance of these algorithms, however, are highly dependent on the motion estimates. To address this, in [35], a motion free super-resolution was attempted by analytically deriving the relation for the reconstruction of the super-resolved image from its blurred and downsampled versions. With the availability of Large-scale Image Datasets and the consistent success of Convolutional Neural Networks (CNNs), learning (rather than handcrafting) a prior from a set of natural images became a possibility. Many such approaches have been explored subsequently."
2011.02427,dataset,170,,,"In this paper, we focus on incorporating robustness to degradations in the task of tiny face super-resolution without the need of a face speciﬁc prior and without a dataset of degraded LR-clean HR image pairs. Premised upon the observation that humans are remarkably adept at registering diﬀerent degrdaded versions of the same image as visually similar images, we prepend a smooth feature extractor module to our Super-Resolution (SR) module. Since our feature extractor is smooth with respect to real degradations, its output does not vary wildly when we move from clean images to degraded images. The SR module which produces clean HR images from features extracted by the smooth feature extractor, thus, produce similar images regardless of the degradation. Features which remain smooth under degradations are also features that are common between clean and degraded LR. So, our network, in essence, learns to look at features which are similar between clean and degraded LR."
2011.02427,dataset,177,,,"We train our network for 4× super-resolution (s = 4). However, our robustness strategy is not scale dependent. For training our network, we used two datasets: one with degraded images and the other with clean images. To make the degraded image dataset, we randomly sample 153446 images from the Widerface [41] dataset. This dataset contains face images with a wide range of degradations such as: varying degrees of noise, extreme poses and expressions, occlusions, skew, non-uniform blur etc. We use 138446 of these images for training and 15000 for testing. While compiling the clean dataset, to make sure it is diverse enough in terms of poses, occlusions, skin colours and expressions, we combined the entire AFLW [30] dataset with 60000 images from CelebAMask-HQ [26] dataset and 100000 images from VGGFace2 [9] dataset. To obtain clean LR images, we simply downsample images from the clean dataset."
2011.02427,dataset,243,,,"Abstract. Real low-resolution (LR) face images contain degradations which are too varied and complex to be captured by known downsampling kernels and signal-independent noises. So, in order to successfully super-resolve real faces, a method needs to be robust to a wide range of noise, blur, compression artifacts etc. Some of the recent works attempt to model these degradations from a dataset of real images using a Generative Adversarial Network (GAN). They generate synthetically degraded LR images and use them with corresponding real high-resolution(HR) image to train a super-resolution (SR) network using a combination of a pixel-wise loss and an adversarial loss. In this paper, we propose a two module super-resolution network where the feature extractor module extracts robust features from the LR image, and the SR module generates an HR estimate using only these robust features. We train a degradation GAN to convert bicubically downsampled clean images to real degraded images, and interpolate between the obtained degraded LR image and its clean LR counterpart. This interpolated LR image is then used along with it’s corresponding HR counterpart to train the super-resolution network from end to end. Entropy Regularized Wasserstein Divergence is used to force the encoded features learnt from the clean and degraded images to closely resemble those extracted from the interpolated image to ensure robustness."
2011.02427,dataset,331,,,"Networks like [12], [5], [44], [42] rely on facial landmarks and heatmaps to impose additional constraints on the output whereas [14] leverage HR exemplars to produce high-quality HR outputs. On the other hand, networks like [20], [11] rely on pairs of LR and HR face images to perceptually super-resolve faces. Even though the above methods are somewhat robust to noise and occlusion, they are not equipped well enough to handle noises and blur which are as complex and as diverse as those in real images. [31] come close by attempting the problem of unsupervised class-speciﬁc deblurring but the blur under consideration was uniform and it still leaves the problem of denoising and super-resolution unaddressed. [43], [40] leverage capsule networks and transformative autoencoders to class-speciﬁcally super-resolve noisy faces but the noises are synthetic. As of yet, there seems to be no dataset with paired examples of degraded LR and clean HR images of faces available. As a result, in recent years, there has been a shift in face SISR methods from paired to unpaired. Recently, with the release of Widerface [41] dataset of real low-resolution faces and the wide availability of high resolution face recognition datasets such AFLW [30], VGGFace2 [9] and CelebAMask-HQ [26], Bulat et al. [6] propose a training strategy where a High-to-Low GAN is trained to convert instances from clean HR face images to corresponding degraded LR images and a Low-to-High GAN is then trained using synthetically degraded LR images and their clean HR counterparts. This method is highly eﬀective since it does not require facial landmarks or heatmaps for faces (as they are not available for real face images captured in the wild)."
2011.02427,dataset,333,,,"In paired SISR, corresponding pairs of LR and HR images are available and the network is evaluated on its ability to estimate an HR image given its LR counterpart. Most of the available deep paired SISR networks are trained under a setting where LR images are generated by downsampling HR images (from datasets such as Set5, Set14, DIV2K [1], BSD100 [2] etc) using a known kernel (often bicubic). These networks are trained using either a pixel wise Mean Squared Error (MSE) loss e.g. [15], [23], [28], L1 loss e.g. [46], Charbonnier loss e.g. [24] or a combination of pixel-wise L1 or L2 loss, perceptual loss [22] and adversarial loss [18] e.g. [25], [39], [12], [38]. Even though these networks perform really well in terms of PSNR and SSIM, and the GAN based ones produce images that are highly realistic, these networks often fail when they are applied on real images with unseen degradations such as realistic noise and blur. Image deblurring and denoising are challenging inverse problems in their own right and most works in these areas deal with uniform blur and ﬁxed noise models. In a recent work [33], the authors use deformable convolution layers to adaptively change the size of receptive ﬁeld to tackle non-uniform blur. However, this work deals with only clean images. To address this, RealSR [8] dataset was introduced in NTIRE 2019 Challenge [7] containing images taken at two diﬀerent focal lengths of a camera. Networks like [21], [16], [17] were trained on this dataset and are therefore robust to real degradations."
2011.02427,dataset,338,,,"We classify all the deep Single Image Super-Resolution (SISR) methods in two broad categories - (i) deep Paired SISR and (ii) deep Unpaired SISR. In paired SISR, corresponding pairs of LR and HR images are available and the network is evaluated on its ability to estimate an HR image given its LR counterpart. Most of the available deep paired SISR networks are trained under a setting where LR images are generated by downsampling HR images (from datasets such as Set5, Set14, DIV2K [1], BSD100 [2] etc) using a known kernel (often bicubic). These networks are trained using either a pixel wise Mean Squared Error (MSE) loss e.g. [15], [23], [28], L1 loss e.g. [46], Charbonnier loss e.g. [24] or a combination of pixel-wise L1 or L2 loss, perceptual loss [22] and adversarial loss [18] e.g. [25], [39], [12], [38]. Even though these networks perform really well in terms of PSNR and SSIM, and the GAN based ones produce images that are highly realistic, these networks often fail when they are applied on real images with unseen degradations such as realistic noise and blur. Image deblurring and denoising are challenging inverse problems in their own right and most works in these areas deal with uniform blur and ﬁxed noise models. In a recent work [33], the authors use deformable convolution layers to adaptively change the size of receptive ﬁeld to tackle non-uniform blur. However, this work deals with only clean images. To address this, RealSR [8] dataset was introduced in NTIRE 2019 Challenge [7] containing images taken at two diﬀerent focal lengths of a camera."
2011.02427,dataset,342,,,"General SR networks as the ones mentioned above, often produce undesired artifacts when applied on faces. Hence, paired face SR networks often rely on face-speciﬁc prior information to subdue the artifacts and make the network focus on important features. Networks like [12], [5], [44], [42] rely on facial landmarks and heatmaps to impose additional constraints on the output whereas [14] leverage HR exemplars to produce high-quality HR outputs. On the other hand, networks like [20], [11] rely on pairs of LR and HR face images to perceptually super-resolve faces. Even though the above methods are somewhat robust to noise and occlusion, they are not equipped well enough to handle noises and blur which are as complex and as diverse as those in real images. [31] come close by attempting the problem of unsupervised class-speciﬁc deblurring but the blur under consideration was uniform and it still leaves the problem of denoising and super-resolution unaddressed. [43], [40] leverage capsule networks and transformative autoencoders to class-speciﬁcally super-resolve noisy faces but the noises are synthetic. As of yet, there seems to be no dataset with paired examples of degraded LR and clean HR images of faces available. As a result, in recent years, there has been a shift in face SISR methods from paired to unpaired. Recently, with the release of Widerface [41] dataset of real low-resolution faces and the wide availability of high resolution face recognition datasets such AFLW [30], VGGFace2 [9] and CelebAMask-HQ [26], Bulat et al. [6] propose a training strategy where a High-to-Low GAN is trained to convert instances from clean HR face images to corresponding degraded LR images and a Low-to-High GAN is then trained using synthetically degraded LR images and their clean HR counterparts."
2011.09069,data,1,,,Data
2011.09069,data,26,,,This article performed a comprehensive analysis of altmetric data for a very large set of scholarly articles and attempted to answer the following research questions:
2011.09069,data,30,,,The variations in coverage and mentions between the two aggregators is also analysed across different subjects. We have used the data grouping into fourteen broad subjects. Table 6
2011.09069,data,30,,,"in varied coverage and altmetric scores captured by them. Therefore, researchers using altmetric data from these aggregators should keep these facts in mind during their analytical studies."
2011.09069,data,36,,,Several previous studies tried to analyze the data from different altmetric aggregators for different purposes ranging from assessing their accuracy to finding how much they agree on altmetric counts for same set of scholarly articles.
2011.09069,data,36,,,"Zahedi, Z., & Costas, R. (2018a). Challenges in the quality of social media data across altmetric data aggregators. Proceedings of 23rd International Conference on Science and Technology Indicators."
2011.09069,data,37,,,"Ortega, J. L. (2018a). Reliability and accuracy of altmetric providers: a comparison among Altmetric. com, PlumX and Crossref Event Data. Scientometrics, 116(3), 2123-2138."
2011.09069,data,37,,,"Ortega, J. L. (2020a). Altmetrics data providers: A meta-analysis review of the coverage of metrics and publication. El profesional de la información (EPI), 29(1)."
2011.09069,data,38,,,"Ortega, J. L. (2020b). Blogs and news sources coverage in altmetrics data providers: a comparative analysis by country, language, and subject. Scientometrics, 122(1), 555-572."
2011.09069,data,38,,,"licensing policies of the harvested platforms. Plum Analytics refreshes PlumX in every 3-4 hours to keep it most updated. The data can be accessed through end-user interfaces, widgets, and APIs of Plum analytics."
2011.09069,data,39,,,"Ortega, J. L. (2019a). The coverage of blogs and news in the three major altmetric data providers. In 17th International conference of the international society for scientometrics and informetrics, Rome, Italy."
2011.09069,data,40,,,"Bar-Ilan, J., Halevi, G. and Milojević, S., 2019. Differences between Altmetric Data Sources – A Case Study. Journal of Altmetrics, 2(1), p.1. DOI: http://doi.org/10.29024/joa.4"
2011.09069,data,42,,,"Zahedi, Z., & Costas, R. (2018b). General discussion of data quality challenges in social media metrics: Extensive comparison of four major altmetric data aggregators. PloS one, 13(5), e0197326."
2011.09069,data,42,,,"Zahedi, Z., Fenner, M., & Costas, R. (2015). Consistency among altmetrics data provider/aggregators: What are the challenges? Proceedings of Altmetrics workshop, 9 October 2015, Amsterdam Science Park, Amsterdam."
2011.09069,data,46,,,"The altmetric data obtained from the two aggregators for WoS publication records was analysed on six aspects: variation in coverage, difference in magnitude of mentions, correlations in mention values, variation across document types, variation by subjects and variation across publishers."
2011.09069,data,49,,,"Ortega, J.L., (2019b). Availability and Audit of Links in Altmetric Data Providers: Link Checking of Blogs and News in Altmetric.com, Crossref Event Data and PlumX. Journal of Altmetrics, 2(1), p.4. DOI: http://doi.org/10.29024/joa.14"
2011.09069,data,49,,,"The authors would like to acknowledge support of Stacy Konkiel, Director of Research Relations at Digital Science for providing access to Altmetric.com and Stephanie Faulkner, Director of Product Management and Operations at Elsevier Research Metrics for providing dashboard access to PlumX data for our research work."
2011.09069,data,53,,,"Secondly, the magnitude of mentions in the four platforms for the articles was analysed and the difference in magnitude of mentions in the data drawn from the two aggregators was computed. Statistical measures (mean and median) were computed for the differences in values from each of these platforms."
2011.09069,data,57,,,"The altmetric data captured by the two aggregators for the set of WoS articles was analysed to identify differences in coverage and magnitude of mentions across different platforms. The differences in mentions captured by the two aggregators were also analysed across different subjects, document types and publishers. The subsections below present these results."
2011.09069,data,65,,,"An altmetric aggregator is typically a platform which tracks and accumulates various types of events from different social media, academic social networks and other platforms for scholarly articles. Altmetric.com and PlumX are the two-current popular altmetric aggregators. We present below a very brief overview of the two aggregators, emphasizing on their coverage of sources and the data collection/ update process."
2011.09069,data,110,,,"based data collection has an inherent problem of duplicity since in WoS a paper is generally tagged under many WCs. Due to this duplicity, the downloaded data comprised of 3,545,720 publication records. This data comprised of the standard 67 fields- including TI (title), PY (publication year), DI (DOI), DT (document type), SO (publication name), DE (author keywords), AB (abstract) etc. This data was then processed to remove duplicate and incomplete records. After this process, we were left with 1,785,149 publication records with DOI."
2011.09069,data,176,,,"First of all, the difference in coverage of the two altmetric aggregators was identified. It was observed that out of the total set of 1,785,149 publication records, a total of 902,990 records are found to be covered by Altmetric.com (which is about 50.58% of the total data), and a total of 1,661,477 publication records were found covered in PlumX (which constitutes about 93.07% of the whole data). Figure 1 shows the overlap of coverage of the two aggregators. It can be seen that a total of 879,981 articles are commonly covered by the two aggregators. About 97.5% of articles covered by Altmetric.com are also covered by PlumX, whereas Altmetric.com covers only 53% of articles tracked by PlumX. The PlumX aggregator has 47% of articles uniquely covered. Thus, it is observed that PlumX has a higher overall coverage of articles (including uniquely covered articles) as compared to Altmetric.com, indicating wider coverage of PlumX."
2011.09069,data,191,,,"Some recent studies have analysed the coverage and data quality of the two altmetric aggregators for different samples of scholarly articles. While several studies (such as Zahedi, Fenner, & Costas, 2015; Zahedi & Costas, 2018a, 2018b; Ortega, 2018a; Bar-Ilan, Halevi, & Milojević, 2019) focused on analyzing the agreement/ disagreement and differences in values of the altmetric scores across different altmetric data providers; some others (such as Meschede & Siebenlis, 2018; Ortega, 2018b, 2018c) attempted to measure the correlations among altmetric values from different aggregators. Two recent studies (Ortega 2019a, 2020b) tried to analyze the altmetric biases with respect to country, language and subjects; while another study (Ortega, 2019b) analysed the inconsistencies in data obtained from different aggregators. Most of the previous studies, however, either analysed small samples of scholarly articles. Only few recent studies (Ortega, 2019a, 2019b, 2020b) have used a large data sample of 100,000 Crossref DOIs for analysis."
2011.09069,data,215,,,"The coverage difference, however, is not same across all the four platforms analysed. For example, in the case of Twitter platform, Altmetric.com is found to have better coverage and more average tweets than PlumX. This is interesting to observe given that both the aggregators collect information using the same API (GNIP) and that both update the data in real time. The previous studies by Ortega (2018a) have also found a similar pattern for Twitter, with Altmetric.com performing better than PlumX. One probable reason for such difference might be the fact that the two aggregators track articles by using different scholarly identifiers. Zahedi & Costas (2018b) have mentioned that the use of different scholarly identifiers could be one of the reasons for such variations. They have also reported that different handling policies of deleted tweets, protected tweets and suspended accounts by the two aggregators could be another reason for the differences observed. For example, PlumX removes all such tweets whereas Altmetric.com keep on counting the deleted ones. Thus, different handling of the tweet data appears to be the main reason for variations in coverage and values of tweets captured by the two aggregators."
2011.09069,data,257,,,"This article, therefore, attempts to revisit the question of coverage variation, differences in altmetric scores and correlations in values provided by the two aggregators by analysing a very large sample of scholarly articles. We analysed a set of 1,785,149 publication records (17 times higher than the largest data sample used earlier), which constitutes the complete publication record for the world for the year 2016 indexed in Web of Science. The data for the year 2016 was taken for the reasons of stability of bibliometric records and also for availability of longer period (2016 to 2019) for the altmetric events to accrue. Given that a recent study (Karmakar, Banshal & Singh, 2021) has observed that altmetric events accrue for a longer time period, it was important to take altmetric data for a longer period for an accurate analysis of the differences in coverage and altmetric values of the two aggregators. Further, since the publication records comprised of the whole of Web of Science publication records for the world for a complete year, they represented different geographies, publication types, subjects, and publishers. Therefore, the study could also compare differences in coverage and altmetric values obtained from the two aggregators across different document types, subjects and publishers. Our study is thus unique in terms of use of a very large data sample, longer altmetric data period and variety of publication records analysed."
2011.09069,data repos,1,,,Mendeley
2011.09069,data repos,4,,,(c) Mendeley
2011.09069,data repos,4,,,Twitter FB Mendeley Blog
2011.09069,data repos,9,,,Altmetric.com Twitter  FB Mendeley Blog  0.261 0.195 0.176 0.424
2011.09069,data repos,12,,,"# Mendeley 691,290 1,225,475 6,502 12,759 3,259 3,714 10,832 28,649 67,511 94,469"
2011.09069,data repos,28,,,"Thelwall, M. (2018). Early Mendeley readers correlate with later citation counts. Scientometrics, 115(3), 1231–1240. https://doi.org/10.1007/s11192-018-2715-9"
2011.09069,data repos,30,,,"# Mendeley 361,663 516,696 93,002 204,802 72,337 161,466 53,018 99,051 12,081 25,738 136,109 192,979 20,271 65,892 130,175 181,715 45,706 136,295 15,289 38,415 79,222 143,839 40,115 151,743 64,478 84,051 38,359 78,858"
2011.09069,data repos,37,,,"Thelwall, M. (2017). Are Mendeley reader counts high enough for research evaluations when articles are published? Aslib Journal of Information Management, 69(2), 174–183. https://doi.org/10.1108/AJIM-01-2017-0028"
2011.09069,data repos,44,,,"The article tried to present a comparative analysis of coverage and mentions captured by the two well-known altmetric aggregators, namely, Altmetric.com and PlumX, for four platforms- Twitter, Facebook, Mendeley and Blog. The variations in coverage and mention values"
2011.09069,data repos,45,,,"Thus, a perusal of the figures 4 (a) to (d) indicate that Altmetric.com captures more mentions per article in case of Twitter and Blog platforms, whereas PlumX captures more mentions per article for the Mendeley and Facebook platforms."
2011.09069,data repos,47,,,"sources in 2017, which increased to 1,021 papers in 2018. Further an increase in Mendeley reader counts and citations was also observed. They suggested using more than one aggregator to obtain altmetric indicators and to compare them in order to get reliable altmetrics."
2011.09069,data repos,83,,,"2https://www.Altmetric.com/, accessed on 10 January, 2020 3https://www.digital-science.com/, accessed on 10 January, 2020 4 https://help.altmetric.com/support/solutions/articles/6000240585-scholarly-identifiers-supported-by-altmetric, accessed on 25th February, 2021 5 https://help.altmetric.com/support/solutions/articles/6000235926-twitter, accessed on 25th February, 2021 6 https://www.altmetric.com/explorer/mention_sources?mention_sources_types%5B%5D=type%3Atw, accessed on 25th February, 2021 7 https://help.altmetric.com/support/solutions/articles/6000236722-mendeley, accessed on 25th February, 2021 8 https://help.altmetric.com/support/solutions/articles/6000235936-facebook, accessed on 25th February, 2021"
2011.09069,data repos,109,,,"values of mentions captured by the two aggregators across different platforms for the same set of commonly covered articles. The difference in mention value for the papers in Altmetric.com and PlumX is computed. Figure 3 shows the mean of differences in mentions in the four platforms as tracked by the two aggregators. It is observed that in case of Twitter and Blog platforms, the mean value of differences is positive indicating that Altmetric.com captures higher number of mentions as compared to PlumX. In the Facebook and Mendeley platforms, PlumX is found to have higher number of mentions tracked as compared to Altmetric.com."
2011.09069,data repos,121,,,"In case of Facebook platform, the coverage levels of the two aggregators are found to be quite similar, though PlumX has an edge over Altmetric.com. Ortega (2020a) has also found that in collecting mentions from Facebook and Mendeley, Altmetric.com has performed poorly as compared to PlumX. One possible reason for Altmetric.com recording slightly lesser Facebook mentions is that Altmetric.com captures only public pages19 and excludes likes, shares, and comments, whereas PlumX includes shares, likes, and comments along with the interactions in user’s closed network. A similar argument is also given by Zahedi & Costas (2018b) behind the low Facebook mentions captured by Altmetric.com."
2011.09069,data repos,125,,,"The increased social media attention to scholarly articles has resulted in creation of platforms & services to track the social media transactions around them. These platforms track the various events about scholarly articles in social media (such as Twitter, Blog, Facebook), academic social networks (such as Mendeley, Academia and ResearchGate) and various other networks. These newer kinds of metrics (aka altmetrics) present useful insight about the importance and impact of the articles. Altmetrics are now being collected and analysed for a variety of purposes ranging from early impact assessment to measure of correlations between altmetrics and citations. Some studies have even tried to propose that altmetrics could be an alternative to"
2011.09069,data repos,131,,,"The mean and median values of number of mentions for the four platforms as tracked by the two aggregators was computed. Table 3 shows the number of articles tracked by the two aggregators across the different platforms, along with the mean and median values of mentions. It can be observed that the mean value of mentions for Twitter and Blog platforms is higher in Altmetric.com, whereas the mean value of mentions for Facebook and Mendeley is higher in PlumX. In case of Facebook platform, PlumX platform has significantly higher value of mean and median of mentions as compared to Altmetric.com. It may, however, be noted that the mean/ median values are of different number of articles tracked by the two aggregators."
2011.09069,data repos,131,,,"We have tried to find out whether the difference in coverage of the two aggregators is similar across different platforms. Figure 2 shows a bar chart of article coverage of the two aggregators for four different platforms- Twitter, Facebook, Mendeley and Blog. It can be observed that PlumX has a better coverage for Mendeley platform, whereas Altmetric.com has an edge over PlumX in coverage in the Twitter and Blog platforms. The coverage for Facebook platform of the two aggregators is almost similar. The magnitude of coverage difference between the two aggregators is highest for Mendeley and lowest for Facebook. Thus, while PlumX has an overall higher coverage of articles, Altmetric.com has better coverage in two of the four platforms analysed."
2011.09069,data repos,137,,,"In terms of correlations between the mentions captured by the two aggregators, the correlation values are higher in case of Twitter and Mendeley platforms, indicating higher agreement between Altmetric.com and PlumX for these platforms. On the other hand, the correlation values are found to be in the lower range in case of Facebook and Blog platform, indicating higher differences in mentions captured by the two aggregators in these platforms. Zahedi & Costas (2018b) have also found highest inter-correlations across the aggregators for Mendeley platform. However, for Twitter platform, the current findings contradict with the finding of Meschede & Siebenlist (2018), where correlation for Twitter platform was found to be in the lower similarity group along with other platforms like Facebook and Blog."
2011.09069,data repos,137,,,"The histogram for the differences in mentions in the Mendeley platform is shown in Figure 4(c). Here, the plot is made for a total of 830,520 commonly covered articles in Mendeley platforms, that have at least one read recorded in both the aggregators. In this case too, it is seen that pattern is inclined more towards the negative side, indicating that PlumX captures more reads per article than Altmetric.com for majority of the articles. About 25% of articles have the same number of mentions recorded by the two aggregators. One example article would be article titled “Mastering the game of Go with deep neural networks and tree search” that has 39,621 records captured by PlumX but only 7,900 reads captured by Altmetric.com."
2011.09069,data repos,173,,,"We have also computed correlation between the mention values for different platforms across the two aggregators. The Spearman Rank Correlation Coefficient (SRCC) between mentions is computed for the articles commonly covered by the two aggregators. Table 4 shows the SRCC values in the article-mentions across the two aggregators. It can be observed that the correlation values for Twitter and Mendeley platforms are 0.823 and 0.95, respectively, indicating strong correlation. In case of Facebook and Blog platforms, these values are 0.272 and 0.424, respectively, indicating weaker correlation. Thus, it can be said that there is more agreement in mention-based ranks of articles in Twitter and Mendeley platforms, between the two aggregators. The mention values differ more in the other two platforms (Facebook & Blog). The inter-platform correlations across the two aggregators are also shown, all of which are less than 0.5, indicating weak positive rank correlations across the platforms in the two aggregators."
2011.09069,data repos,196,,,"The article analysed the coverage and altmetric scores captured by the two altmetric aggregators to identify the similarities and differences between them, and presents following useful results and conclusions. Firstly, PlumX has an overall higher coverage of articles than Altmetric.com and tracks a wider number of sources. Secondly, Altmetric.com captures more mentions per article in case of Twitter and Blog platform, whereas PlumX captures more mentions per article in case of Mendeley and Facebook platforms. Thirdly, correlation values indicate that Altmetric.com and PlumX agree more in their mention values for Twitter and Mendeley platforms but differ more in case of mention values for Facebook and Blog platforms. Fourthly, Altmetric.com is found to have better coverage of Twitter, whereas PlumX has better coverage of Facebook, across different document types, subjects and publishers. In case of Mendeley and Blog platforms, variations in patterns across different document types, subjects and publishers do not show a consistent pattern. Overall, the analytical results present a comprehensive account of the variations in coverage and mentions of the two aggregators across four different platforms."
2011.09069,data repos,199,,,"It would be interesting to check whether the coverage and mentions in different platforms as captured by the two aggregators vary across different document types. We have, therefore, analysed the coverage and mentions for the articles of different document types. These document types correspond to Article, Book, Book Chapter, proceedings paper and Review, as defined by the Web of Science. Table 5 shows the coverage and average mentions for Twitter, Facebook, Mendeley and Blog platforms for the different document types. It is observed that Altmetric.com has better coverage in Twitter and Blog platforms for almost all document types. In case of average mention values too, Altmetric.com has higher values across almost all document types in Twitter and Blog platforms. The PlumX platform is found to have better coverage and average mention values across almost all document types in the Facebook and Mendeley platforms. Thus, looking at the results across document types, it is seen that the overall trend of better coverage of Altmetric.com for Twitter and Blog and of PlumX for Facebook and Mendeley holds valid across different document types."
2011.09069,data repos,214,,,"The variations of coverage and mention values of the two aggregators in the four platforms across different document types, subjects and publishers also show interesting patterns. Out of these three aspects, only subject has been explored earlier by Ortega (2020b) for Blog and News mentions. The variations by publisher and document types have not been explored earlier. The present results show that in general Altmetric.com has better coverage and higher mention values than PlumX for Twitter platform across different document types, subjects and publishers. Similarly, PlumX platform is seen to have better coverage and higher mention values than Altmetric.com in case of Facebook, irrespective of the document type, subject and publisher. However, in case of Mendeley and Blog platforms, the coverage and average mention values of the two aggregators do not show a consistent pattern across all the document types, subjects and publishers. In some cases, PlumX has better coverage and higher mentions than Altmetric.com, while in several other cases Altmetric.com has better coverage and higher mentions. Thus, the variations in coverage and mentions across document types, subjects and publishers are more clearly seen in case of Mendeley and Blog platforms."
2011.09069,data repos,222,,,"In order to gain further insight into the difference in magnitude of mentions, we have also plotted the frequency of differences in mentions across different platforms by the two aggregators. Figure 4 (a) – (d) present the frequency values of differences in mentions in the two aggregators for the Twitter, Facebook, Mendeley and Blog platforms, respectively. Figure 4(a) shows the histogram for the differences in Twitter platform. These differences are for 565,445 commonly covered articles for Twitter platform, with at least one tweet captured in both the aggregators. It can be seen that a good percentage (approximately 50%) of papers has tweet difference equal to zero, indicating that both platforms record same number of tweets for these papers. However, the slope is inclined towards the positive side, indicating that Altmetric.com captures more tweets for a good number of the articles as compared to PlumX. We looked at some examples to verify this and found this valid. One example paper titled “When the Great Power Gets a Vote: The Effects of Great Power Electoral Interventions on Election Results” has 24,318 tweets captured by Altmetric.com, whereas PlumX captured only 493 tweets for this paper."
2011.09069,"data repos, data",73,,,"First of all, the coverage in different platforms of scholarly articles by the two aggregators was compared. The altmetric data for the articles in consideration was obtained from the two aggregators corresponding to the four platforms: Twitter, Facebook, Blogs and Mendeley. The percentage of articles covered in the four platforms as per the data from the two aggregators was identified and difference in coverage was compared."
2011.09069,"data repos, data",129,,,"In case of Mendeley platform, PlumX is found to have better coverage and higher number of reads as compared to Altmetric.com. Zahedi & Costas (2018b) have also observed similar pattern and reported that PlumX aggregates the reader counts of different versions of the same research output, resulting in overall higher Mendeley reads. Another possible reason may be the fact that PlumX and Mendeley are from the same parent company and hence they may have been better integrated together, which may allow more accurate and real time update of the data. Thus, a possibly better integration and recording counts of different versions of the same article by PlumX could be the main reasons for higher coverage of Mendeley by PlumX."
2011.09069,"data repos, data",200,,,"presents the coverage and average mention values in the four platforms. It is observed that in the Twitter platform, Altmetric.com has better coverage and higher mention values than PlumX for almost all subjects. In case of Facebook platform, PlumX has higher mention values for almost all subjects. The coverage, however, is not higher for PlumX in Facebook for all subjects as subjects like MED, AH, SS, BIO, and AGR has higher coverage by Altmetric.com. In Mendeley platform, PlumX has higher coverage in all subjects, but in terms of reads, PlumX captures higher reads only for MED, SS, BIO, GEO, and MUL subjects. In case of Blog platform, Altmetric.com has better coverage than PlumX across almost all subjects and the average mention values of Altmetric.com are higher except for PHY, ENV, MAT and ENG subjects. Thus, the analysis of data across different subjects shows an overall trend of better coverage of Altmetric.com of Twitter and Blog and PlumX of Facebook and Mendeley, except in case of some subjects where slightly different patterns are observed."
2011.09069,"data repos, data",235,,,"Meschede & Siebenlist (2018) explored about the relationship between the metrics across (inter-correlation) two aggregators PlumX and Altmetric.com, as well as between the metrics within the aggregator itself (intra-correlation). They analysed sample of 5,000 journal articles from six disciplines (‘Computer Science, Engineering and Mathematics’, ‘Natural Sciences’, ‘Multidisciplinary’, ‘Medicine and Health Sciences’, ‘Arts, Humanities and Social Sciences’ and ‘Life Sciences’) and analysed them for the eight common sources (‘Facebook’, ‘Blogs’, ‘Google+’, ‘News’, ‘Reddit’, ‘Twitter’, ‘Wikipedia’ and ‘Mendeley’) in both aggregators. The study showed that PlumX has higher overall coverage (99%) of the data chosen for analysis as compared to Altmetric.com (39%). The intra-correlation between the metrics within the same platforms are weak. They further observed that PlumX and Altmetric.com are highly intercorrelated in terms of Mendeley and Wikipedia (with correlation coefficient values 0.97 and 0.82 respectively) but weakly correlated for other sources- Facebook (0.29), Blogs (0.46), Google+ (0.28), News (0.11), Twitter (0.49), and Reddit (0.41)."
2011.09069,"data repos, data",236,,,"Abstract: The increased social media attention to scholarly articles has resulted in creation of platforms & services to track the social media transactions around them. Altmetric.com and PlumX are two such popular altmetric aggregators. Scholarly articles get mentions in different social platforms (such as Twitter, Blog, Facebook) and academic social networks (such as Mendeley, Academia and ResearchGate). The aggregators track activity and events in social media and academic social networks and provide the coverage and transaction data to researchers for various purposes. Some previous studies have compared different altmetric aggregators and found differences in the coverage and mentions captured by them. This paper attempts to revisit the question by doing a large-scale analysis of altmetric mentions captured by the two aggregators, for a set 1,785,149 publication records from Web of Science. Results obtained show that PlumX tracks more altmetric sources and captures altmetric events for a larger number of articles as compared to Altmetric.com. However, the coverage and average mentions of the two aggregators, for the same set of articles, vary across different platforms, with Altmetric.com recording higher mentions in Twitter and Blog, and PlumX recording higher mentions in Facebook and Mendeley. The article also analysed coverage and average mentions captured by the two aggregators across different document types, subjects and publishers."
2011.09069,"data repos, data",238,,,"Zahedi, Fenner & Costas (2015) explored the agreement/disagreement among metric scores across three altmetric providers namely, Mendeley, Lagotto, and Altmetric.com. They analysed 30,000 DOIs for the year 2013 in five common sources and analysed possible reasons for the differences. They found that Altmetric.com reports more tweets as compared to Lagotto and concluded that the data capture procedure of Altmetric.com, which includes tweets, public retweets, and comments in real-time, could be a probable reason for such differences. In later studies, Zahedi & Costas (2018a, 2018b) have analysed 31,437 PloSOne DOIs and explored the differences in metrics provided by four aggregators Crossref Event Data (CED), Altmetric.com, Lagotto, and Plum Analytics. They focused on the process of data collection used by different aggregators and that how different aggregators define metrics from the data collected. The results showed that Mendeley (r>0.8) and Twitter (0.5≤r≤0.9) have good agreement across aggregators, whereas Facebook (0.1≤r≤0.3) and Wikipedia (0.2≤r≤0.8) have the lowest agreement. They attributed this to the methods of tracking and processing data. For example, the effect of direct data collection or collection through third-party APIs, aggregation of data based on different versions, identifiers, types, etc., and the impact of frequency of"
2011.09069,"data repos, data",273,,,"Altmetric.com2 is one of the first altmetric aggregator platforms, that originated in 2011 through the efforts of Euan Adie and support of Digital Science3. It crawls over different social media, news, and blog platforms to collect views, mentions, comments etc. around a scholarly article. Altmetric.com makes use of various scholarly identifiers like DOI, PubMed ID, ISBN, Handle, arXiv ID, ADS ID, SSRN ID, RePEC ID, URN, Clinical Trials.gov record etc. to track altmetric events around the scholarly articles. It employs different APIs and tools to track altmetric events from various sources4. Multiple sources of information for the same scholarly articles are cross-checked and summed up into a single entry in the respective platforms. For example, it uses GNIP API to track twitter mentions in real time5. A list of 9,258,010 unique tweeters is indexed, providing 125,793,313 tweets6. Twitter mentions include tweets, retweets, and quoted tweets but do not incorporate any tweets with links to news stories or blog posts. Similarly, Mendeley API is used to capture Mendeley reads, and the data is updated on a daily basis. Along with total reads, geographic locations, user’s professional status and subject areas are also available7. In case of Facebook mentions, Altmetric.com captures only public pages, and the metric provided does not include likes, shares, and comments8. The data collection is done using Facebook Graph API. Currently, 297,808 Facebook sources are indexed in their"
2011.09069,"data repos, data",349,,,"We have also tried to see if the patterns of variations in coverage and mentions in the two aggregators change across different publishers. In order to analyse this, articles for the 16 most frequent publishers in the publication data are identified and analysed. Table 7 present the coverage and average mention values for data for these publishers in the four platforms for the two aggregators. In terms of number of journals for which data is covered, the PlumX aggregator has an edge over Altmetric.com. For example, PlumX covers 76 more journals of Springer than Altmetric.com, 34 more journals of Elsevier and 31 more journals of Taylor & Francis. It can be further observed that PlumX covers more than 90% of publication records for all Publishers except for Cambridge Univ Press (84.1%), whereas coverage of Altmetric.com varies between 25% to 86%. Altmetric.com has minimum coverage of about 25% for IEEE and highest coverage of 86.88% for PLoS publications. In terms of coverage and mentions for the four platforms, it is found that Altmetric.com has higher coverage in Twitter for almost all publishers. In Facebook platform, Altmetric.com shows higher coverage for all publishers except PLoS, Hindawi, and MDPI. In Mendeley platform, the coverage and average reads captured by PlumX are higher for all publishers except Springer, IEEE, Taylor & Francis, ACM and Hindawi. In case of Blog platform, Altmetric.com has in general better coverage and average mention values than PlumX, though for IEEE, IOP, Hindawi and Emerald publishers, the PlumX aggregators capture more mentions. Thus, in general it is observed that Altmetric.com has better coverage of Twitter and PlumX has better coverage of Facebook, irrespective of the publisher. However, in case of Mendeley and Blog platforms, the coverage and mention values of the two aggregators vary for different publishers, with no definite pattern observed across all publishers."
2011.09069,"data repos, data, dataset",142,,,"Ortega (2020a) has performed a meta-analysis over a set of 107 altmetric articles related to five altmetric aggregators, namely, Altmetric.com, Mendeley, PlumX, Lagotto, and ImpactStory, published during 2012-2019. The dataset consisted of papers that had either computed or published data useful in the computation of three metrics: coverage, platform wise coverage, and average mentions. The usage percentage of all the aggregators was explored. Almetric.com (54%) was found to be the most prevalent provider, followed by Mendeley (18%) and PlumX (17%). The analysis showed that Altmetric.com tracks more events for Twitter, News, and Blogs whereas PlumX performs well in Facebook and Mendeley platforms. The results exhibited gradual increase in tweet capture by PlumX."
2011.09069,"data repos, data, dataset provided",64,,,"Bar-Ilan, Halevi, & Milojević (2019) have analysed altmetric data of 2,728 JASIST articles and reviews, provided by Mendeley, Altmetric.com and PlumX in two different points of time 2017 and 2018. They observed increase in overlap in coverage of documents with Mendeley reader across the three sources over time. There were 874 papers commonly covered in all"
2011.09069,"data repos, data, used dataset, dataset",169,,,"This study, thus, presents a comprehensive account of variations in coverage and mention values captured by the two aggregators- Altmetric.com and PlumX- across four different platforms. The analysis used a significantly larger dataset as compared to all previous studies. Further, the study is also perhaps the first effort to have analysed the variations across different document types and publishers. The analytical results are interesting and useful and mainly confirm the findings of earlier studies, with some exceptions. The results have practical implications in terms of suggestion for use of a specific aggregator for data from different platforms. For example, based on the results, it can be suggested that one should use Altmetric.com for tracking tweets and Blog mentions, whereas PlumX should be used for Facebook mentions and Mendeley reads. Further, the study also highlights the fact that the two aggregators differ in their sources and data collection and update strategies, which in turn result"
2011.09069,"data, data https",49,,,"Banshal, S.K., Singh, V.K. & Muhuri, P.K. (2021). Can altmetric mentions predict later citations? A test of validity on data from ResearchGate and three social media platforms. Online Information Review, in press. DOI: 10.1108/OIR-11-2019-0364."
2011.09069,"data, data https",120,,,"9 https://www.altmetric.com/explorer/mention_sources?mention_sources_types%5B%5D=type%3Afb, accessed on 25th February, 2021 10 https://help.altmetric.com/support/solutions/articles/6000240275-attention-sources-update-frequencyand-collection-methods, accessed on 25th February, 2021 11https://www.altmetric.com/explorer/mention_sources?mention_sources_types%5B%5D=type%3Ablog, accessed on 22nd February, 2021 12 https://help.altmetric.com/support/solutions/articles/6000235927-blogs , accessed on 25th February, 2021 13https://www.Altmetric.com/about-our-data/the-donut-and-score/, accessed on 10 January, 2020 14https://www.Altmetric.com/research-access/, accessed on 10 January, 2020 15https://plumanalytics.com/learn/resources/plum-analytics-metrics-audit-log/, accessed on 10 December, 2019 16https://plumanalytics.com/learn/about-metrics/, accessed on 10 December, 2019 17https://plumanalytics.com/niso-altmetrics-working-group-on-data-quality/, accessed on 10 December, 2019 18 https://plumanalytics.com/plumx-facebook-altmetrics-measure-up/, accessed on 25th February, 2021"
2011.09069,"data, data repository, repo, data repos, dataset provided",347,,,"The next step was to obtain altmetric data for these publication records from the two aggregators- Altmetric.com and PlumX. In order to obtain altmetric data from Altmetric.com, a DOI look up was performed for all the DOIs in the WoS data. Out of the 1,785,149 publication records, a total of 902,990 records are found to be covered by Altmetric.com, which is about 50.58% of the total data. Altmetric.com had 46 fields in the data, including DOI, Title, Twitter mentions, Facebook mentions, News mentions, Altmetric Attention Score, OA Status, Subjects (FoR), Publication Date, URI, etc. Out of this, we mainly used data for Twitter, Facebook, Mendeley, and Blog platforms. The data from Altmetric.com was downloaded in the month of Oct. 2019. For obtaining data from PlumX, we contacted PlumX team to provide us with access to PlumX data for the 1,785,149 publication records, as we did not have access to PlumX. The PlumX team created a dashboard access for us for the concerned publication records. Out of the 1,785,149 publication records, a total of 1,661,477 publication records were found covered in PlumX, which constitutes about 93.07% of the total data. PlumX provides metrics in five categories from a wide range of source platforms. The PlumX data was downloaded in the month of Nov. 2019. This data included fields like DOI, Title, Year, Repo URL, Researcher Name(s), Captures:Readers:Mendeley, Social Media:Tweets:Twitter, Social Media:Shares, Likes & Comments:Facebook etc. This data also has a field named Plum stable url which redirects to the page from where one can get the actual tweets, blogs etc. We have focused our attention mainly to data for four platforms- Twitter, Facebook, Mendeley, and Blog platforms- from both the aggregators."
2011.09069,"data, database",172,,,"database, with 4,635,255 total mentions9. The update is performed daily for public pages and weekly for prioritized popular pages10. In case of Blog mentions, the data is updated daily by RSS feed from a manually curated list of 9,653 blog sources11 with direct links to scholarly identifiers12. Altmetric.com provides the facility to suggest any new blog source or Facebook page which is currently not indexed with them12. It also computes a weighted score based on mainly three factors- volume, sources, and authors - and provides a measure called ‘Altmetric Attention Score’. This score is represented as a colorful donut on the article details page13. Altmetric.com offers various services for institutions, funding agencies, researchers, and agencies involved in research & development (R&D). It provides a free API with rate limit to researchers on request14. It also provides an environment to obtain data in different formats, like JSON, CSV etc."
2011.09069,"data, dataset",88,,,"The results show that PlumX has an overall higher and wider coverage than Altmetric.com, with PlumX tracking about 93% articles as compared to Altmetric.com tracking about 50% articles. This is also confirmed by the findings of some previous studies (Meschede & Siebenlist, 2018; Ortega, 2018b; 2019a; Zahedi & Costas, 2018b), which in general found that PlumX has higher coverage of articles, with close to 95% articles of the data set being tracked."
2011.09069,"data, dataset",304,,,"A series of recent studies (Ortega, 2019a, 2019b, 2020b) analysed coverage of news and blog sources in three aggregators namely, Crossref event data, Altmetric.com, and PlumX, by taking 100,000 Crossref DOIs. The results showed that, the overlap of these sources across aggregators are comparatively low in numbers (Ortega, 2019a). As for example, Altmetric.com has a higher coverage for blog (37.8%) but only 7.8% of the publication set is commonly covered in the three aggregators. The coverage in one aggregator might be high for the same set of articles but the lower overlapping ratio shows that the sources covered in aggregators vary widely. Ortega (2020b) explored altmetric biases with respect to country, language and subjects with a dataset of 100,000 DOIs. Author has retrieved the sources which covered the randomly selected publication set and categorized them based on their regions, language and interest level. It was shown that, Altmetric.com is the most heterogeneous aggregator geographically and linguistically. However, PlumX had more coverage towards local news events, particularly for USA. Their conclusion served as evidence that English is the most prevailing language. From the same dataset, Ortega (2019b) extracted the blog and news links to verify the validity, coverage, and presence of the tracked blog mentions and news mentions of the scholarly articles. There were 51,000 news & blog links found in this extraction process, which were explored for their existence and it was found that almost one-third of the links are broken. This elaborate longitudinal study concluded that these mentions should be audited periodically as the aggregators are dependent on third-party providers."
2011.09069,"data, dataset provided",142,,,"citations for assessment of the impact of research (such as Haustein et al., 2014; Costas, Zahedi, & Wouters, 2015; Thelwall, 2017, 2018; Huang, Wang, & Wu, 2018; Thelwall & Nevil, 2018, Banshal, Singh & Muhuri, 2021). Due to importance and usefulness of the altmetric events, researchers are now increasingly using services of the altmetric aggregators for obtaining altmetric data for scholarly articles. Altmetric.com and PlumX are the two popular aggregators that track altmetric events around scholarly articles from a variety of platforms and provide the coverage and transaction data to researchers for various purposes. However, the outcome of any analysis of coverage and transaction data is likely to depend on the values provided by the aggregator used."
2011.09069,"data, publicly available, github, data repos, dataset provided",339,,,"PlumX is a suite of products launched by Plum analytics in 2016, initially with limited coverage15. Over time, these metrics evolved significantly in several ways and now it is a quite comprehensive altmetric aggregator. Plum Analytics considered as many as 67 different types of outputs to be tracked, which are named as ‘artifacts’. These artifacts include scholarly articles, books, book chapters, conference articles. In addition, it also includes speeches, visual arts, images, figures etc. Currently, PlumX track metrics over 180 million research outputs. It covers a very wide variety of social platforms, such as Twitter, Facebook, YouTube; online knowledge sharing mediums, such as StackExchange, Wikipedia, Github; and bibliographical data-based sites, such as Scopus, SciELO, RePEcetc. It organizes the captured data in five different types of metrics- usages, captures, mentions, citations, and social media16. The data for artifacts is collected from multiple platforms using different methods and tools. These include data provider APIs, third party APIs, FTP data transfers, OAI-PMH harvesting, Web crawlers, and RSS feeds17. For example, it updates twitter metrics in real time using GNIP API, whereas Facebook Graph API is used with daily update for Facebook data (Zahedi & Costas, 2018b). The Facebook metric provided by PlumX includes shares, likes, and comments along with the interactions in user’s closed network18. It is not very clear how PlumX collects information about Mendeley readers and Blog mentions. However, since Mendeley and PlumX both are owned by Elsevier, one may expect that PlumX may be getting direct access to Mendeley data (Zahedi & Costas 2018b). Various blog sources are mined to collect the blog mention data, but the list of blog sources is not disclosed in public domain."
2011.09069,"data, publicly available, github, data repos, dataset provided",342,,,"Over time, these metrics evolved significantly in several ways and now it is a quite comprehensive altmetric aggregator. Plum Analytics considered as many as 67 different types of outputs to be tracked, which are named as ‘artifacts’. These artifacts include scholarly articles, books, book chapters, conference articles. In addition, it also includes speeches, visual arts, images, figures etc. Currently, PlumX track metrics over 180 million research outputs. It covers a very wide variety of social platforms, such as Twitter, Facebook, YouTube; online knowledge sharing mediums, such as StackExchange, Wikipedia, Github; and bibliographical data-based sites, such as Scopus, SciELO, RePEcetc. It organizes the captured data in five different types of metrics- usages, captures, mentions, citations, and social media16. The data for artifacts is collected from multiple platforms using different methods and tools. These include data provider APIs, third party APIs, FTP data transfers, OAI-PMH harvesting, Web crawlers, and RSS feeds17. For example, it updates twitter metrics in real time using GNIP API, whereas Facebook Graph API is used with daily update for Facebook data (Zahedi & Costas, 2018b). The Facebook metric provided by PlumX includes shares, likes, and comments along with the interactions in user’s closed network18. It is not very clear how PlumX collects information about Mendeley readers and Blog mentions. However, since Mendeley and PlumX both are owned by Elsevier, one may expect that PlumX may be getting direct access to Mendeley data (Zahedi & Costas 2018b). Various blog sources are mined to collect the blog mention data, but the list of blog sources is not disclosed in public domain. The data harvesting is updated on different time periods, ranging from daily to monthly basis, based on the different"
2011.09069,"data, python",166,,,"Thirdly, the correlation between mention values from different platforms, as drawn from the two aggregators, was computed. For computing correlations, the options were to compute Pearson Correlation or Spearman Rank Correlation. However, as it has been observed in previous studies (such as Thelwall & Nevill, 2018) that the altmetric data are highly skewed, therefore, we have used Spearman Rank Correlation, which is more suitable for such skewed data. The Spearman Rank Correlation Coefficient (SRCC) was computed between the different types of mentions available from the two aggregators. The built-in function ‘corr’ available in pandas module of python programming language was used for this purpose, value ‘spearman’ passed as parameter to the function. The value of SRCC lies between -1 to +1, with positive values indicating positive correlation, value of 0 indicating no correlation, and negative value indicates negative correlation."
2011.09069,"download, data",143,,,"Since the focus of the work is comparing the coverage and scores of altmetrics provided by the two popular aggregators- Altmetric.com and PlumX, we analysed the variation in coverage and scores for a very large set of articles. The whole of the world’s research output for the year 2016 as indexed in Web of Science (WoS) was downloaded for the analysis. The download was performed in the month of Sep. 2019. The data for the year 2016 was taken for the reasons of stability of bibliometric records and also for availability of longer period (2016 to 2019) for the altmetric events to accrue. Since, WoS does not allow downloading data above 100,000 records, therefore the data is collected based on Web of Science Categories (WC). The WC"
2011.09069,"download, data repos, data",339,,,"For each platform, the difference in metrics across aggregators was quantified in terms of counting differences. Counting difference was computed by taking the sum of the differences in metrics provided by two aggregators at the document level and dividing by the number of publications that have non zero altmetric events and occur in both aggregators. They concluded that different aggregators should be used for data from different platforms, such as PlumX for Mendeley reads and Altmetric.com for tweets, news & blogs. In another study, Ortega (2018b) has grouped different altmetrics into three groups: social media, usage, citations using principal component analysis (PCA). In this study data from Altmetric.com, Scopus and PlumX for a set of 3,793 articles published in 2013, was used. Considering that the earlier studies provided evidence that some specific aggregators perform better for some specific data sources; they collected different indicators from different aggregators. These included tweets, Facebook mentions, news, blogs etc. from Altmetric.com; citations from Scopus; Wikipedia mentions from CED; and views & Mendeley reads from PlumX. Results showed that instead of using a single metric, such as altmetric score, one should consider the relatedness of metrics and their impact across different disciplines for evaluating research. In another study, Ortega (2018c) examined the emergence and evolution of five altmetrics (download, views, tweets, readers, and blog mention) along with bibliometric citations from the publication date of a document. The study also investigated the evolution of the relationships among these metrics by analyzing 5,185 papers from PlumX on a month-tomonth basis. The results showed that in a document’s entire life cycle, altmetric mentions are fast appearing ones, whereas citations appearance is slow. Based on the relationship analysis of metrics, the study suggested that the reader counts influence citations."
2011.09069,"download, data repos, data",348,,,"Ortega (2018a) analysed the difference in altmetric indicator counts in Crossref event data, Altmetric.com, and PlumX, using a sample 67,000 papers. For each platform, the difference in metrics across aggregators was quantified in terms of counting differences. Counting difference was computed by taking the sum of the differences in metrics provided by two aggregators at the document level and dividing by the number of publications that have non zero altmetric events and occur in both aggregators. They concluded that different aggregators should be used for data from different platforms, such as PlumX for Mendeley reads and Altmetric.com for tweets, news & blogs. In another study, Ortega (2018b) has grouped different altmetrics into three groups: social media, usage, citations using principal component analysis (PCA). In this study data from Altmetric.com, Scopus and PlumX for a set of 3,793 articles published in 2013, was used. Considering that the earlier studies provided evidence that some specific aggregators perform better for some specific data sources; they collected different indicators from different aggregators. These included tweets, Facebook mentions, news, blogs etc. from Altmetric.com; citations from Scopus; Wikipedia mentions from CED; and views & Mendeley reads from PlumX. Results showed that instead of using a single metric, such as altmetric score, one should consider the relatedness of metrics and their impact across different disciplines for evaluating research. In another study, Ortega (2018c) examined the emergence and evolution of five altmetrics (download, views, tweets, readers, and blog mention) along with bibliometric citations from the publication date of a document. The study also investigated the evolution of the relationships among these metrics by analyzing 5,185 papers from PlumX on a month-tomonth basis. The results showed that in a document’s entire life cycle, altmetric mentions are fast appearing ones, whereas citations appearance is slow."
2011.09069,"github, data repos",49,,,Source(s) Twitter  Facebook YouTube Reddit F1000 bit.ly Blog Figshare GitHub Mendeley Slideshare SoundCloud SourceForge Vimeo Stack Exchange Stack Overflow Wikipedia News Goodreads Amazon Delicious (historical only) CiteULike (historical only) Dryad DSpace SSRN EBSCO ePrints AiritiiRead eBooks Airiti Library WorldCat LinkedIn Google+ Pinterest
2011.09069,"github, data repos, data",287,,,"The two altmetric aggregators, Altmetric.com and PlumX, provide metrics based on data collected from various social media, bibliographic and policy document sources. Table 1 lists a total of 33 social media sources tracked by the two aggregators. Out of these 33 sources, 14 sources are captured in Altmetric.com, whereas PlumX tracks 28 sources. The platforms/ sources tracked by Altmetric.com are Twitter, Facebook, Youtube, Reddit, F1000, Blog, Mendeley, Stack Overflow, Wikipedia, News, CiteULike, LinkedIn, Google+, and Pinterest. Out of these 14 sources, PlumX tracks most except five sources- F1000, LinkedIn, Google+, Stack Overflow, and Pinterest. PlumX additionally tracks 19 social media sources that are not tracked by Altmetric.com. These sources are bit.ly, Figshare, Github, Slideshare, SoundCloud, SourceForge, Vimeo, Stack Exchange, Goodreads, Amazon, Delicious, Dryad, Dspace, SSRN, EBSCO, ePrints, AritiiRead eBooks, Ariti Library, WorldCat. Table 2 lists the bibliographic and policy document sources tracked by the two aggregators. Here, PlumX has a better coverage and also provides citation metrics. PlumX covers a total of 16 sources whereas Altmetric.com tracks only 6 sources. Thus, tables 1 and 2 suggest that PlumX covers more varied sources as compared to Altmetric.com. The two aggregators also differ in their strategies to capture altmetric events and update frequency. The study by Zahedi & Costas (2018b) provides a good overview of the main methods of collecting, tracking, and updating metrics by some popular altmetric aggregators."
2011.10298,data,16,,,"optimality across data distributions via homotopy methods. Learning Representations (ICLR), 2020."
2011.10298,data,43,,,"Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. on Knowl. and Data Eng., 22(10):1345–1359, October 2010. ISSN 1041-4347. doi: 10.1109/TKDE.2009.191. URL https://doi.org/10.1109/TKDE.2009.191."
2011.10298,data,176,,,"In this section, we empirically validate the theoretical results derived in Theorem 3.11. First, we consider a 1-dimensional toy regression problem to illustrate and visualize some of the basic properties of H-SGD. In this easy scenario, the introduced assumptions can be trivially veriﬁed by inspection (see the Figures 3- 4 in the Appendix). We then move to more complex and high-dimensional scenarios where the assumptions can not be veriﬁed. Inspired by Finn et al. (2017) and Gargiani et al. (2020), we consider the task of regressing with a neural network from input to output of a sinusoidal wave. Finally, we also consider a non-convex classiﬁcation task based on the combination of logistic regression with a non-linear model for moon-shaped binary data (Chapelle et al., 2006). In all the considered scenarios, we compare the numerical performance of H-SGD with those of SGD and tune the step-size based on the performance of the latter."
2011.10298,data,198,,,"First-order stochastic methods for solving large-scale non-convex optimization problems are widely used in many big-data applications, e.g. training deep neural networks as well as other complex and potentially non-convex machine learning models. Their inexpensive iterations generally come together with slow global convergence rate (mostly sublinear), leading to the necessity of carrying out a very high number of iterations before the iterates reach a neighborhood of a minimizer. In this work, we present a ﬁrst-order stochastic algorithm based on a combination of homotopy methods and SGD, called Homotopy-Stochastic Gradient Descent (H-SGD), which ﬁnds interesting connections with some proposed heuristics in the literature, e.g. optimization by Gaussian continuation, training by diffusion, mollifying networks. Under some mild assumptions on the problem structure, we conduct a theoretical analysis of the proposed algorithm. Our analysis shows that, with a speciﬁcally designed scheme for the homotopy parameter, H-SGD enjoys a global linear rate of convergence to a neighborhood of a minimum while maintaining fast and inexpensive iterations. Experimental evaluations conﬁrm the theoretical results and show that H-SGD can outperform standard SGD."
2011.10298,data,306,,,"Our second experiment is inspired by Finn et al. (2017) and Gargiani et al. (2020) and focuses on studying the numerical performance of H-SGD on the task of regressing from input to output of a sinusoidal function corrupted by Gaussian noise. In particular, the input data are sampled uniformly from the interval [−1, 1] and yj = sin(10 · xj) + (cid:15)j with (cid:15)j ∼ N (0, 0.1) for all j = 1, . . . , 500. The regressor is a feedforward neural network with two hidden layers, each of 10 units, and hyperbolic tangent as activation function. As for the previous benchmark, we use the mean squared error as loss. We employ the same values of step-size α and mini-batch M for both H-SGD and SGD, where the step-size value is tuned based on the numerical performance of SGD for the selected mini-batch size (M = 5). Regarding H-SGD, we set yj,0 = x2 j + (cid:15)j with (cid:15)j ∼ N (0, 0.01) and employ the same homotopy mapping as in Equation 16. As shown in Figure 6 in the Appendix, also in this scenario H-SGD shows a superior numerical performance than SGD, i.e. H-SGD reaches a loss of 10−1 roughly 4 times faster and achieves convergence more than 2 times faster than SGD. The superior numerical performance of H-SGD can be attributed to its ability of tracking a solution across homotopy iterations (see Figure 10 in the Appendix) which ensures the method to always work in the vicinity of a minimizer."
2011.10298,dataset,134,,,"We start with an easy regression problem motivated by Mobahi (2016): a 1-dimensional neural network with erf as activation function (see Figure 9 in Section B of the Appendix for a graphical representation). We generate a synthetic dataset of N = 100 samples, where xj ∈ [−1, 1], yj = 3 · xj + (cid:15)j and (cid:15)j ∼ N (0, 1). Regarding the choice of a value for the step-size, we use an estimate ˜L of the smoothness constant L and set α = 1/ ˜L. As loss, we use the mean squared error, which, composed with the regressing model, leads to the following non-convex optimization problem"
2011.10298,dataset,252,,,"where f : Rd → R is continuously differentiable, bounded below and not necessarily convex. In particular, we assume that we only have access to noisy function values and gradients of the objective function in equation 1 via a stochastic ﬁrst-order oracle, as in (Nemirovski et al., 2009) and (Ghadimi & Lan, 2013). Problems of this form typically arise in machine learning and deep learning applications, where the dimensionality of the datasets makes the full function and gradient evaluations too expensive. This class of problems is generally approximately solved by stochastic ﬁrst-order iterative algorithms, e.g. SGD (Bottou et al., 2018), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2015). At the iteration t, the algorithms of this class acquire a stochastic estimate of the function value f (wt, ξt) and the gradient g(wt, ξt) by calling the oracle with input wt, where ξt is a random variable, i.e. when the noise comes from subsampling as in the mini-batch scenarios, then ξt ∈ {0, 1}N with j=1 ξt,j · ∇fj(wt). In the case of SGD, for a given w0 ∈ Rd and (cid:107)ξt(cid:107)1 = M and g(wt, ξt) = 1 M"
2011.10298,dataset,272,,,"Finally, we test H-SGD also on a classiﬁcation benchmark with a logistic regression task. In particular, we use a 2-dimensional binary moon-shaped dataset (Chapelle et al., 2006) with 1000 samples corrupted by Gaussian noise. As the dataset is clearly not linearly separable, we opt for a cubic model, which, used in combination with the logistic regression framework, leads to a non-convex objective function where the optimization variable w is the collection of the model’s coefﬁcients. For both H-SGD and SGD we use a mini-batch size of 20 and tune the value of the step-size on the SGD’s performance for that mini-batch size. Regarding H-SGD, we use as source task the one obtained considering a linear instead of a cubic model, which results in a convex and hence “easy” optimization problem. We then gradually increase the non-linearity of the model, i.e. non-convexity of the problem, until reaching in the ﬁnal homotopy iteration the target problem with the desired cubic model. This homotopy map is obtained by multiplying the coefﬁcients of the non-linear terms in the model by λ as follows j,2 + c3 x2 j,1 + c2 x3 λ(c1 x3 j,2) + c7 xj,1 + c8 xj,2 + c9 . (17) For the homotopy parameter we adopt the increasing schedule that is suggested in Theorem 3.11. H-SGD outperforms SGD by reaching an error of 0.1 more than two times faster than SGD (see Figure 11 in the Appendix)."
2012.04966,data,2,,,Data type
2012.04966,data,4,,,A. Data collection
2012.04966,data,4,,,B. Data analysis
2012.04966,data,25,,,Testing strategies to support continuous integration for complex systems Loco Coco: Low Cost construction of Coordination and Communication networks from model-based systems engineering data
2012.04966,data,30,,,"TABLE II OVERVIEW OF DATA SOURCES (JP - JOURNAL PAPER, CP - CONF. PAPER, MT - MASTER’S THESIS, BT - BACHELOR THESIS)"
2012.04966,data,41,,,"• Use one cycle to generalize the artifact beyond the case company. I think that there is a high risk to gain little data of sufﬁcient quality, which must be carefully considered with respect to the potential gain."
2012.04966,data,46,,,"It is important to describe very clearly which data was collected when. Table III provides a proven structure for this purpose. Since the RQ typically should align with the phases of the regulative cycle, data collection is presented in relation to RQs."
2012.04966,data,54,,,"In one thesis [27], we aimed for generalization beyond the company context in the last iteration. While it worked ﬁne here, it also resulted in much additional work and a risk to generate less valuable data. Therefore, I have not raised this practice into the guidelines."
2012.04966,data,59,,,"[28] M. Mohamad, G. Liebel, and E. Knauss, “Loco coco: Automatically constructing coordination and communication networks from modelbased systems engineering data,” Information and Software Technology (IST), vol. 000, pp. 1–15, 2017. [Online]. Available: https://doi.org/10. 1016/j.infsof.2017.08.002"
2012.04966,data,63,,,"Survey participation was low, especially among students. For them, it is hard to judge whether guidelines are clear or helpful before doing a thesis. Participation of (potential) supervisors on PhD level and beyond was slightly higher, offering a good complement to other data collected in this study. Especially the free-text ﬁelds proved valuable feedback."
2012.04966,data,63,,,"• It is hard to fully leverage the different perspectives of all stakeholders to properly manage risks in thesis work. • It is hard to plan iterative research cycles and to frame good research questions. If research questions relate to only a speciﬁc iteration, there is a danger to not get enough data or good enough quality of data."
2012.04966,data,64,,,"(cid:6) When coming from an empirical background, it is tempting to press all content either in a ﬁndings section (usually any objective observations derived from data) and in a discussion section (to more subjectively discuss the implications of the ﬁndings). With design science thesis work, this results in very complicated descriptions in both sections."
2012.04966,data,73,,,"Still, by iteratively revisiting similar research questions, the quality of the data is very high and through the concrete artifact, researchers (i.e. theses workers) can provide deeper insights then with an interview-only study. In addition, the effort for managing iterations (as indicated in these guidelines) and the effort for designing the artifact must be considered, when coming to a fair assessment."
2012.04966,data,79,,,"It is difﬁcult to report on iterative work and to ﬁnd a good structure for the thesis. It is hard to fully leverage the different perspectives of all stakeholders to properly manage risks in thesis work. It is hard to plan iterative research cycles and to frame good research questions. If research questions relate to only a speciﬁc iteration, there is a danger to not get enough data or good enough quality of data."
2012.04966,data,82,,,"This section presents ﬁndings with respect to the research questions based on seven years of experience with running design science master’s theses. The ﬁndings relate to typical problems (RQ1), potential solutions (RQ2), and any evidence that such solutions actually solve the problems (RQ3). The knowledge for each research questions has evolved over the years and this section presents this evolution as a narrative supported by concrete data and experiences."
2012.04966,data,109,,,"For a certain type of problems, the design science research methodology is an excellent ﬁt [9]–[12]. Design science research is a constructive research methodology, which focuses on solving a concrete problem through the design of a tangible design artifact (e.g. a piece of software), while also exploring so-called knowledge-questions. In this way, it allows to work constructively on a relevant contribution towards a solution which is relevant to a company partner, and at the same time to collect empirical data about knowledge questions, which often are of high relevance to academia."
2012.04966,data,109,,,"These criteria have evolved over time, and a dialogue with the examiner is needed for each thesis. When asked about minimum requirements for passing grades, examiners have said things like “In a qualitative exploratory study, we expect 10 interviews of 60 minutes, recorded and transcribed, to form the data. Different methods should have comparable amounts of data and effort.” or “An acceptable thesis should be possible to publish at a conference, for a pass-with-honors, it should be possible to publish it at a journal.” This shows the emphasis on the third criteria, the methodology."
2012.04966,data,109,,,"This paper is written as a design science research paper with the guidelines for running a design science research master theses with industry as the artifact. This section presents the seven guidelines (G1-G7), i.e. the artifact, concisely. Section V elaborates how these guidelines follow from our research method and empirical data. While this order may appear backward, it often can be more elegant, since it allows to write the empirical results with the ﬁnal version of the artifact in mind. It also allows the reader to refer to the artifact section for guidance (see G7)."
2012.04966,data,122,,,"A major quality criteria for quality learning is the constructive alignment [17], which demands that assessment in pedagogics strongly relates to learning objectives. As argued in the introduction, the assessment of master’s theses focuses on academic contribution, thus leaving it entirely to the student and supervisor to balance assessment on academic quality with practical relevance of the work done during the thesis. Especially in applied ﬁelds (such as software engineering and requirements engineering), academic excellence can be related to applying strong empirical methods [4]. In order to get access to empirical data from practice, companies must be convinced that they will get value in return."
2012.04966,data,126,,,"For data analysis, information from different data sources was grouped along similar concerns with relevance to students and supervisor. During supervision in each cycle, I looked out for similarities in these concerns and provided examples from previous cycles as part of supervision meetings. This helped to reﬁne the advice and to see patterns of typical problems in such thesis projects, to list promising practices, and to learn about which guidelines actually work in favour of thesis projects. The results are reported in a detailed narrative style to make this iterative data analysis as transparent as possible. Visualizations and ﬁndings from the survey are provided to further illustrate the ﬁndings (mainly relevant for Research Question 3)."
2012.04966,data,136,,,"The results in this paper are based on personal experience from several student theses. Table II gives an overview of master’s theses as well as papers derived from them. Theses are grouped by years and each year is referred to as a cycle. Thus, experience from previous cycles is available for those that follow. For each master’s thesis, data sources include the author’s personal notes, discussions with students and colleagues, the actual theses and publications, and feedback from examiners and reviewers. Especially the personal notes and discussion are of informal and subjective nature, yet by triangulating them with the published theses and publications, they become reliable enough and valuable in the overall process of constructing useful guidelines."
2012.04966,data,144,,,"7 programs. In their learning outcomes, they explicitly list the skill to “contribute to research and development in software engineering” and the ability to “reﬂect on the possibilities and limitations of software engineering research, its role in society and the responsibility of the individual for how it is used”. These particular learning outcomes are to a large degree to be covered in the master’s thesis course, where students are supposed to make a research contribution, most often based on empirical data from companies. Both programs are taught at the Department of Computer Science and Engineering and share a small group of examiners for the master’s theses of both programs. Typically, examiners judge the quality of a master’s thesis based on the following high-level criteria:"
2012.04966,"data available, data",81,,,"4) Implications for examiners: It is hard to compare the amount of empirical data between design science research investigations. Usually, I found thesis and other empirical design science research interviews to be shorter and more structured. They can be less formal, since the artifact allows quick interaction with practitioners at random times. Analysis is often more straight forward than in an exploratory qualitative case study, since it is constrained by the artifact."
2012.04966,"dataset provided, data, provide implementation",65,,,TABLE III TEMPLATE FOR A TABLE THAT REPORTS WHICH DATA WAS COLLECTED IN WHICH CYCLE (G2) WITH RESPECT TO EACH RQ (OR CORRESPONDING PHASE OF REGULATIVE CYCLE (G3). THE GRAY AREA SHOWS THE SHIFTING EMPHASIS BETWEEN CYCLES (G5) AND THE CONTINUOUS WORK ON THE ARTIFACT (G1). TYPICAL EXAMPLES OF DATA COLLECTIONS ARE PROVIDED.
2012.04966,download,59,,,"[29] J. Simon and A. Balabhadrapatruni, “Test automation to enable continuous integration for an automotive platform: A design science study of software download function case,” Master’s thesis, Dept. of Computer Science and Engineering, Chalmers | University of Gothenburg, 2017, available at https://hdl.handle.net/20.500.12380/254873."
2012.08026,code,348,,,"In fact, these behaviors have been causing huge disasters. 2.5 million nonsmoker adults have died because they breathed the 1965 Surgeon’s Report [1]. secondhand smoke since Concretely, smoking during pregnancy results in more than 1,000 infants deaths annually; 34,000 premature deaths have been caused by secondhand smoke from heart disease each year in the United States among nonsmokers; Secondhand smoke causes more than 7,300 lung cancer deaths among U.S. nonsmokers each year [2]. Besides, calling in inappropriate environments also results in some irreparable damage. The National Safety Council reports that cell phone use while driving leads to 1.6 million crashes each year and nearly 390,000 injuries occur each year from accidents caused by texting and driving. In fact 94 percent of drivers support a ban on texting while driving. All in all, a detection system is urgently needed to determine whether people perform such dangerous behavior in certain environments to help the accidents and death caused by smoking and reduce calling.This report will introduce a practical pipeline to finish the behavior classification of smoking,calling ,normal and smokingcalling. In order to achieve high accuracy, we focus on deep learning methods with very deep convolutional neural networks such as inception V3. Since 2014,very deep convolutional neural networks have been proposed and become the must-have weapon for champions in all kinds of competition. This project tries to assess classification methods quantitatively and qualitatively. Since manual check for this dangerous behavior is laboursome and expensive, if such a system is deployed in a variety of public places that can be monitored, people who smoke will be given a warning and drivers will realize a risky behavior they do just now. In this way, second smoking will be reduced and help build a fresh environment to improve the quality of life and accidents will also be avoided when a system tries to hold back the calling behavior of some drivers."
2012.08026,code,350,,,"1. Background and Impact Code of conduct, that is some special restrictions of people's behavior in certain public scenes such as no smoking in gas stations, no calling for drivers, no taking photos in museums, etc. In fact, these behaviors have been causing huge disasters. 2.5 million nonsmoker adults have died because they breathed the 1965 Surgeon’s Report [1]. secondhand smoke since Concretely, smoking during pregnancy results in more than 1,000 infants deaths annually; 34,000 premature deaths have been caused by secondhand smoke from heart disease each year in the United States among nonsmokers; Secondhand smoke causes more than 7,300 lung cancer deaths among U.S. nonsmokers each year [2]. Besides, calling in inappropriate environments also results in some irreparable damage. The National Safety Council reports that cell phone use while driving leads to 1.6 million crashes each year and nearly 390,000 injuries occur each year from accidents caused by texting and driving. In fact 94 percent of drivers support a ban on texting while driving. All in all, a detection system is urgently needed to determine whether people perform such dangerous behavior in certain environments to help the accidents and death caused by smoking and reduce calling.This report will introduce a practical pipeline to finish the behavior classification of smoking,calling ,normal and smokingcalling. In order to achieve high accuracy, we focus on deep learning methods with very deep convolutional neural networks such as inception V3. Since 2014,very deep convolutional neural networks have been proposed and become the must-have weapon for champions in all kinds of competition. This project tries to assess classification methods quantitatively and qualitatively. Since manual check for this dangerous behavior is laboursome and expensive, if such a system is deployed in a variety of public places that can be monitored, people who smoke will be given a warning and drivers will realize a risky behavior they do just now."
2012.08026,"code, open-source",81,,,"3.2 Training Tricks and Implementation This program is built based on an open-source project doing classification of marine fish species [6] for fisheries regulation using keras deep learning API. This project improves the overall organization and flow. As the original code used deprecated code, (notably updating this project Model.fit_generators improve performance. This also allowed us to use some features that would greatly simplify other parts of the code: for example, the new"
2012.08026,data,158,,,"In Figure 2 two branches correspond to the reflectance and illumination, respectively. From the perspective of functionality, this network can be divided into three subnets: 1. Layer Decomposition, which is same as retinex   theory. In this net, reflectance graph of high light image can be used as a ground truth to lead enhancement of low light image’s reflectance graph. Of course, there is no real ground truth, a bright image is just a high-light input Figure;2. Illumination Guided Reflectance Restoration: the goal of this part is to restore the contaminated part by means of Illumination map guidance, since the retinex formula is related to Illumination map;3. Arbitrary Illumination Manipulation: a flexible mapping mechanism is proposed, which is derived from the image data and, of course, allows users to manually adjust the mapping."
2012.08026,"data, code",208,,,"The other major flow improvement and code simplification was the using ImageDataGenerator preprocessor, which allows Keras to automatically split the data while building the data generators, without the need for a separate script. The only preprocessing implemented within the Keras image generators was flipping images horizontally at random (to reduce right/left-handed bias in our set). The data still supports external preprocessing. The Model.fit function supports automatically calculating epoch steps based on batch size and total image count, which may result in misreported set sizes with irregular batch sizes. We used a relatively large initial learning rate, as the introduction of cosine decay means that we can use a much larger initial learning rate and let it drop over time, resulting in a reduction of training time by over a third. Finally, we implemented a callback function to automatically track the improvement of the model, and to stop training early if the validation accuracy starts to degrade, indicating that the model is starting to overtrain. Overall, these improvements and adjustments reduce the training process to a single script, down from an initial three, and dramatically reduce final accuracy. training"
2012.08026,dataset,10,,,Figure3. Some samples from this project's training dataset
2012.08026,dataset,10,,,datasets. Retrieved from https://dev.ehualu.com/dev/home/competition/competitionDe tail?competitionId=3
2012.08026,dataset,146,,,"2. Method Classification based on deep learning achieves high accuracy and scores in many competitions. Very deep and complicated convolutional neural networks have been proposed during recent several years and received wide attention and recognition. Next, this section is going to present the main classification network framework, inception V3 which was trained using a dataset of 1,000 classes by google from the original ImageNet dataset which was trained with over 1 million training images. Inception v3 is a widely-used image recognition model that has been shown to attain greater than 78.1% accuracy on the ImageNet dataset. The model is the culmination of many ideas developed by multiple researchers over the years. It is based on the original paper: “Rethinking the Inception Architecture for Computer Vision” by Szegedy, et al[3]."
2012.08026,"dataset provided, data, used dataset, dataset",136,,,"3. Prototype 3.1 Datasets This project’s dataset comprises of four classes, smoking behavior(1150 images), normal behavior(2625 images), calling behavior(2025 images), and smoking-calling behavior(1400 images).The dataset is provided by China eHualu group Co. ,Ltd. who crawls the data from the whole network for non-profit use[5]. Some images of the dataset have been shown as Figure3 below. From the Figure3, the dataset is really difficult and complicated  with all kinds of resolutions, different illumination, rotated and distance varying images, and a variety of sceneries. Once this project gets high accuracy in such tanglesome images, it is proved that the pipeline has powerful robustness and portability."
2012.08026,github,3,,,https://github.com/pengpaiSH/Kaggle_NCFM
2012.08026,github,3,,,https://github.com/wangmiaowei/CVDancer_Classification
2012.08026,github,3,,,https://github.com/zhangyhuaee/KinD_plus
2012.08026,"github, code",10,,,This project’s source code is released in Github
2012.08026,"github, open-source",218,,,"3.3 Image Enhancement  In order to increase the accuracy and efficiency of classification, this project tries to enhance the brightness of darker images to increase the accuracy of the classification results. To realize this function, this project has two steps to consider: (1) Find those relatively dark images. (2) Enhance the brightness of these images. For the first step, this project can calculate the proportion of pixels with lower brightness to all pixels. After several attempts, the project’s researchers believe that pixels with grayscale values below 50 can be regarded as the “darker pixels”. And if the proportion of darker pixels in an image is greater than 0.3, then it can be considered that this image needs a certain degree of brightness enhancement. The second step can be realized based on an open-source project on Github[7] which is an implement of the KinD network the method section has introduced. By the pretrained checkpoints, the brightness of images can be enhanced.  Two images with a dark pixel ratio of 0.3 and 0.6 is shown in Figure4(a) and the enhanced results of them are shown in Figure4(b)."
2101.00522,code,34,,,"Experiments were done on a Nvidia Titan Xp GPU. Code is provided in the supplementary material section of this submission, and will be made freely available online at a later date."
2101.00522,data,35,,,We developed a novel algorithm for performing unsupervised domain adaptation of semantic segmentation models in a source-free learning setting to preserve privacy of data for the source domain. After supervised training on a source
2101.00522,data,37,,,"Proof: Since we use the parameter ρ to estimate the prototypical distribution, the probability of predicting incorrect labels for the drawn pseudo-data points is 1 − ρ. We can deﬁne the following difference:"
2101.00522,data,41,,,"[38] Dingding Liu, Yingen Xiong, Kari Pulli, and Linda Shapiro. In Machine Estimating image segmentation difﬁculty. Learning and Data Mining in Pattern Recognition, volume 6871, pages 484–495, 08 2011. 1"
2101.00522,data,42,,,"[11] Jaehoon Choi, Taekyung Kim, and Changick Kim. Selfensembling with gan-based data augmentation for domain adaptation in semantic segmentation. In Proceedings of the IEEE international conference on computer vision, pages 6830–6840, 2019. 1"
2101.00522,data,79,,,"[28] Ali Emre Kavur, M Alper Selver, Oguz Dicle, Mustafa Barıs, and N Sinem Gezer. Chaos-combined (ct-mr) healthy abdominal organ segmentation challenge data, Apr 2019. 5 [29] Salome Kazeminia, Christoph Baur, Arjan Kuijper, Bram van Ginneken, Nassir Navab, Shadi Albarqouni, and Anirban Mukhopadhyay. Gans for medical image analysis. Artiﬁcial Intelligence in Medicine, 109:101938, 2020. 1"
2101.00522,data,79,,,"of data in the area of medical image segmentation is the existence of domain shift between different imaging modalities which results from using imaging devices that are based on totally different electromagnetic principles, e.g., CT vs MRI. When domain gap exists between the distributions of the training (source) and the testing (target) data, the performance of CNNs can degrade signiﬁcantly. This makes continual data annotation necessary for mode retraining."
2101.00522,data,97,,,"where P and Q are data distributions and (cid:104)P, oi(cid:105) and (cid:104)Q, oi(cid:105) denote the one-dimensional distributional slices, respectively. In Eq. (4) the term D1−W D denotes the 1D WD metric. Since WD has a closed form solution in the dimension of one, Eq. (4) can be computed efﬁciently. We present the pseudocode of the above described approach, called Source Free semantic Segmentation (SFS), in Algorithm 1."
2101.00522,data,129,,,"While existing UDA algorithms have been successful in reducing domain gap, the vast majority of these algorithms require sharing data between the source and target domains to enforce distribution alignment. This requirement limits the applicability of most existing works when sharing data may not be possible, e.g., sharing data is heavily regulated in healthcare domains due to privacy or security concerns. Until recently, there has been little exploration of UDA when access to the source domain is limited [33, 57, 49]. These works beneﬁt from generative adversarial learning to remember the source domain. However, both address UDA for only classiﬁcation tasks which limits their applicability to the problem of organ semantic segmentation [74]."
2101.00522,data,138,,,"To improve class separations in the prototypical distribution PZ, we only use high-conﬁdence samples in each class for estimating parameters of pk(·). We use a conﬁdence threshold parameter ρ, and discard all samples for which the classiﬁer conﬁdence on its prediction pij is strictly less than ρ. This step helps cancel out class outliers. Let Sρ = {(xs ij)|ψ(χ(φ(xij))) > ρ} be the source data pixels on which the classiﬁer φ assigns conﬁdence greater than ρ. Also, let Sρ,k = {(x, y)|(x, y) ∈ Sρ, y = k}. We can then generate empirical estimates for µk and Σk as:"
2101.00522,data,155,,,"We compare our approach to other state-of-the-art techniques that are developed for unsupervised medical image segmentation. To the best of our knowledge, no prior work addresses source-free UDA for semantic segmentation, so we compare our work against existing UDA techniques that need source data for alignment. We compare against adversarial approaches PnP-AdaNet [13], SynSeg-Net [25], AdaOutput [66], CycleGAN [77], CyCADA [23], SIFA [7], ARL-GAN [10], DSFN [79], SASAN [65], DSAN [21]. These works are recently developed methods for semantic segmentation that should serve as upper bounds for our method, because we do not process the source domain data directly. We reiterate that the advantage of our method is to preserve privacy and we do not claim best performance."
2101.00522,data,157,,,"for adaptation and 6 for evaluating generalization performance. The value range in the CT scans was ﬁrst clipped to [−125, 275] HU following literature [75]. The images were re-sampled to an axial view size of 256 × 256. Background was then cropped such that the distance between any labeled pixel and the image borders is at least 30 pixels, and scans were again resized to 256 × 256. Finally, each 3D scan was normalized independently to zero mean and unit variance, and values more than three standard deviation from the mean were clipped. Data augmentation was performed as follows on both the training MR and training CT instances to improve generalization: (1) random rotations of up to 20 degrees, (2) negating pixel values, (3) adding random Gaussian noise, (4) random cropping."
2101.00522,data,177,,,"UDA approaches have explored two main strategies to reduce the domain gap. A large number of works rely on generative adversarial networks (GANs) [72, 27]. The core idea is to train a GAN such that data points of both domains can be mapped into a domain-agnostic embedding space [23]. To this end, a cross-domain discriminator network is trained to classify whether an input data point belongs to the source domain or the target domain. The discriminator network receives its input from a feature generator network. The discriminator network is fooled by the feature generator network which is trained as an adversary to generate domain-agnostic features at its output. As a result, a classiﬁer network that is trained using the source domain annotated data at the output of the generator network would generalize well in the target domain [41, 14]. An area of weakness for GANs is mode collapse which happens and need for ﬁne-tuning the model hyper-parameters."
2101.00522,data,192,,,"Domain shift is a major area of concern, as data annotation is a challenging procedure even for the simplest semantic segmentation tasks [38]. Annotating medical images is also expensive, as annotation can be performed only by physicians, who undergo years of training to obtain domain expertise. Unsupervised domain adaptation (UDA) is a learning setting aimed at reducing domain gap without data annotation in the target domain. The goal is to adapt a source-trained model for improved generalization in the target domain using solely unannotated data [20, 68, 56, 71]. The core idea in UDA is to achieve knowledge transfer from the source domain to the target domain by aligning the latent features of the two domains in an embedding space. This idea has been implemented either using adversarial learning [23, 13, 67, 4], directly minimizing the distance of distributions of the latent features with respect to a probability metric [8, 62, 36], or a combination of the two [11, 58]."
2101.00522,data,209,,,"A large group of UDA algorithms select a probability distribution metric D(·, ·), e.g.SWD or KL-divergence, and then use the source and the target domain data points, N ] and X T = [xt X S = [xs N ], to minimize the loss term D(χ ◦ ψ(ps(·)), χ ◦ ψ(pt(·))) as a regularizer. However, this will have constrained the user to have access to the source domain data for computing D(χ ◦ ψ(ps(·)), χ ◦ ψ(pt(·))) that couples the two domains. In medical image segmentation and other similar avenues in which privacy or security are crucial, sharing the source domain data is not possible. As a result, UDA based on the above pipeline for the domain alignment of feature embed dings would be inoperative. To tackle this challenge, we provide a solution to align the two domains without sharing the source domain data, that beneﬁts from an intermediate distribution that is learned in the source domain."
2101.00522,data,232,,,"Consider a source domain DS = (X S, Y ) with annotated data and a target domain with unannotated data DT = (X T , Y ) that despite having different input spaces X S and X T , e.g., due to using different medical imaging techniques, share the same segmentation map space Y , e.g., the same tissue/organ classes. Following the standard UDA pipeline, the goal is to learn a segmentation mapping function for the target domain by transferring knowledge from the source domain. To this end, we must learn a function fθ(·) : {X S ∪ X T } → {Y } with learnable parameters θ, e.g., a deep neural network, such that given an input image x∗, the function returns a segmentation mask ˆy that best ap Figure 1: Diagram of our proposed method. We ﬁrst perform supervised training on source MR images. Using the source embeddings we characterize a prototypical distribution via a GMM distribution in the latent space. We then perform source free adaptation by matching the embedding of the target CT images to the learnt GMM distribution, and ﬁne tuning of the classiﬁer on GMM samples. Finally, we verify the improved performance that our model gains from model adaptation."
2101.00522,data,243,,,"Both of the above mentioned approaches have been found helpful in various medical semantic segmentation applications [24, 73, 5, 26]. However, both UDA strategies require direct access to the source domain data to compute their corresponding loss functions. To relax this requirement, UDA has been recently explored in a source-free setting in order to address applications in which the source domain is not directly accessible [33, 57]. Both Kundu et al. [33] and Saltori et al. [57] target image classiﬁcation, and beneﬁt from generative adversarial learning to generate pseudo-data points that are similar to the source domain data in the absence of actual source samples. While both approaches are suitable for classiﬁcation problems, extending them to semantic segmentation of medical images is not trivial. First, training models that can generate realistic medical images is considerably more challenging due to importance of ﬁne details. Second, one may argue that if the generated images are too similar to the real images, the privacy of human subjects in the training data may still be compromised. Our work is based on using a dramatically different approach. We develop a source-free UDA algorithm that performs the distribution alignment of two domains in an embedding space by using an intermediate prototypical distribution to relax the need for source data."
2101.00522,data,289,,,"Our proposed approach is based on using the prototypical distribution PZ as a surrogate for the learned distribution of the source domain in the embedding space. Upon training fθ using Eq. (1), the embedding space would become discriminative for the source domain. This mean that the source distribution in the embedding will be a multimodal distribution, where each mode denotes one of the classes. This distribution can be modeled as a Gaussian mixture model (GMM). To develop a source-free UDA algorithm, we can draw random samples from the GMM and instead of relying on the source data, align the target domain distribution with the prototypical distribution in the embedding space. In other words, we estimate the term D(χ ◦ ψ(ps(·)), χ ◦ ψ(pt(·))) with D(PZ (·), χ ◦ ψ(pt(·))) which does not depend on source samples. In our formulation, we use the Sliced Wasserstein Distance as the distribution metric for minimizing the domain discrepancy. A visual description for our approach is presented in Figure 1. More speciﬁcally, the feature extractor ψ ◦ χ will transform the input distribution ps(·) to the prototypical distribution PZ (·) = χ ◦ ψ(ps(·)) based on which the classiﬁer φ assigns the label probabilities. This distribution will have K modes. Our key idea is to approximate PZ (·) via a GMM with K components, each component encoding a class:"
2101.00522,"data available, data, dataset",196,,,"Convolutional neural networks (CNNs) have led to signiﬁcant improvements in tasks involving semantic segmentation of images. CNNs are vulnerable in the area of biomedical image segmentation because of distributional gap between two source and target domains with different data modalities which leads to domain shift. Domain shift makes data annotations in new modalities necessary because models must be retrained from scratch. Unsupervised domain adaptation (UDA) is proposed to adapt a model to new modalities using solely unlabeled target domain data. Common UDA algorithms require access to data points in the source domain which may not be feasible in medical imaging due to privacy concerns. In this work, we develop an algorithm for UDA in a privacy-constrained setting, where the source domain data is inaccessible. Our idea is based on encoding the information from the source samples into a prototypical distribution that is used as an intermediate distribution for aligning the target domain distribution with the source domain distribution. We demonstrate the effectiveness of our algorithm by comparing it to state-ofthe-art medical image semantic segmentation approaches on two medical image semantic segmentation datasets."
2101.00522,"data available, data, dataset",206,,,"SOTA semantic segmentation algorithms use deep neural network architectures to exploit large annotated datasets [40, 45, 37, 19]. These approaches are based on training a CNN encoder using manually annotated segmentation maps to learn a latent embedding of the data. An up-sampling decoder combined with a classiﬁer is then used to infer pixelwise estimations for the true semantic labels. Performance of such methods is high when large amounts of annotated data are available for supervised training. However, these methods are not suitable when the goal is to transfer knowledge between different domains [56, 46]. Model adaptation to target domains has been explored in both semisupervised and unsupervised settings. Semi-supervised approaches rely on the presence of a small number of annotated target data samples [44, 70]. For example, a weakly supervised signal on the target domain can be obtained using bounding boxes. However, manual data annotation of a small number of images is still a considerable bottleneck in the area of medical imaging because only trained professionals can perform this task. For this reason, UDA algorithms are more appealing for healthcare applications."
2101.00522,"data, dataset",62,,,"Figure 5: Segmentation maps of CT samples from the two datasets used for evaluation. The ﬁrst ﬁve columns correspond to cardiac images, while the last ﬁve columns correspond to abdominal images. From top to bottom: gray-scale CT images, source-only model predictions, post-adaptation model predictions, supervised predictions on the CT data, ground truth."
2101.00522,"data, dataset",103,,,"concerns making sharing data infeasible. Additionally, our work provides a method for semantic segmentation in a continual learning regime [54, 55]. Our approach is able to mitigate domain gap without having direct access to the source data. We learn a prototypical distribution for the source domain, and transfer knowledge between the source and target domains by distribution alignment between the learned prototypical distribution and the latent features of the target domain. We validate our algorithm on two medical image segmentation datasets, and observe comparable performance to SOTA methods based on joint training."
2101.00522,"data, dataset",117,,,"domain, our algorithm is able to generalize to new domains without having access to source samples. Our idea is based on estimating a prototypical distribution via a GMM and then use is to align two distributions indirectly. We provided theoretical analysis to demonstrated why our method is effective. We also empirically demonstrated that our algorithm is competitive on two real-world datasets when compared against state of the art approaches in medical semantic segmentation that require access to the source data. Moreover, given the source free nature of our adaptation approach, our algorithm is the ﬁrst of its kind for settings where privacy for source domain is a major concern."
2101.00522,"data, dataset",127,,,"Employing CNNs in semantic segmentation tasks has been proven to be extremely helpful in various applications, including object tracking [69, 2, 76], self-driving cars [31, 22], and medical image analysis [30, 59, 1, 29]. This success, however, is conditioned on the availability of huge manually annotated datasets to supervise the training of state-of-the-art (SOTA) networks structures [51, 64]. This condition is not always realized in practice, especially in ﬁelds such as medical image segmentation, where annotating data requires the input of trained experts and privacy regulations make sharing data for crowd-sourcing extremely restricted, and at times impossible. A characteristic"
2101.00522,"data, dataset",184,,,"CHAOS MR → Multi-Atlas Labeling Beyond the Cranial Vault: the second domain adaptation task consists of data frames from two different dataset. As source domain we, consider the the 2019 CHAOS MR dataset [28], previously used in the 2019 CHAOS Grad Challenge. The dataset consists of both MR and CT scans with segmentation maps for the following abdominal organs: liver, right kidney, left kidney and spleen. Similar to [7] we use the T2SPIR MR images as the source domain. Each scan is centered to zero mean and unit variance, and values more than three standard deviations away from the mean are clipped. In total, we obtain 20 MR scans, 16 of which we use for training and 4 for validation. The target domain is represented by the dataset which was presented in the MultiAtlas Labeling Beyond the Cranial Vault MICCAI 2015 Challenge [34]. We utilize the 30 CT scans in the training set which are provided segmentation maps, and use 24"
2101.00522,"data, dataset",217,,,"We analyze the problem in a standard PAC-learning setting. Consider that the set of classiﬁer sub-networks : Z → Rk, w ∈ RW } to be H = {ψw(·)|ψw(·) our hypothesis space. Let eS and eT denote the expected error of the optimal model that belongs to this space on the source and target domains, respectively. Let ψw∗ to be the model which minimizes the combined source and target expected error eC(w∗), deﬁned as: w∗ = arg minw eC(w) = arg minw{eS + eT }. This model is the best model within the hypothesis space in terms generalizability in both domains. Additionally, consider that ˆµS = 1 n))) and ˆµT = N 1 m=1 δ(χ(ψv(xt m))) are the empirical source distriM bution and the empirical target distribution in the embedding space, both built from the available data points. Let ˆµP = 1 n) denote the empirical prototypical Np distribution, built from the generated pseudo-dataset. Since we build our pseudo-data set based on points with conﬁdent labels, we can set: ρ = E"
2101.00522,dataset,3,,,6.1. Datasets
2101.00522,dataset,13,,,Input: target dataset DT = (X T ) Pseudo-Dataset Generation:
2101.00522,dataset,15,,,"Input: source domain dataset DS = (X S , Y S ),"
2101.00522,dataset,23,,,"Theorem 1 : Consider that we generate a pseudo-dataset using the prototypical distribution and preform UDA according to algorithm 1, then:"
2101.00522,dataset,23,,,"Theorem 1: Consider that we generate a pseudo-dataset using the prototypical distribution and preform UDA according to algorithm 1, then:"
2101.00522,dataset,28,,,Table 4: The percentage of shift in pixel labels during adaptation for the abdominal organ dataset. The same methodology as in Table 3 is used.
2101.00522,dataset,30,,,"In Eq. (12), eC(cid:48) denotes the joint optimal model true error for the source domain and the pseudo-dataset as the second domain."
2101.00522,dataset,36,,,"Figure 4: Learnt Gaussian embeddings on the cardiac dataset. From left to right we present samples from the learnt GMM when ρ = 0, ρ = .8 and ρ = .97 respectively."
2101.00522,dataset,53,,,"(cid:15) = 1e − 6 and decay of 1e − 6. We use the standard pixel-wise cross entropy loss, and batch size of 16. For the abdominal organ segmentation dataset, we observed better performance by using a weighted cross entropy loss, dropout rate of .5."
2101.00522,dataset,53,,,"proximates the ground truth segmentation mask y∗. Given the annotated training dataset {(xs, ys)}N i=1 in the source domain, it is straightforward to train a segmentation model that generalizes well in the source domain through solving an empirical risk minimization (ERM) problem:"
2101.00522,dataset,92,,,"We will use the same experimental setup and parsed dataset used by Dou et al. [15] for fair comparison. For the MRI source domain we use augmented samples from 16 MRI 3D instances. The target domain consists of augmented samples from of 14 3D CT images, and we report results on 4 CT instances, as proposed in by Chen et al. [7]. Each 3D segmentation map used for assessing test performance is normalized to have zero mean and unit variance."
2101.00522,dataset,100,,,"The ﬁrst term in Eq. 3 involves ﬁne-tuning the classiﬁer on samples of pseudo-dataset (Z P , Y P ) to ensure that it would continue to generalize well. The second term enforces the distributional alignment. As a result, the updated model will generalize on the target domain. Since the source samples are not used in Eq. 3, privacy of the source domain will also be preserved. Note that the source and target domains share initial similarities, otherwise domain alignment as described above may produce overlap of disjoint classes."
2101.00522,dataset,104,,,"Table 3: The percentage of shift in pixel labels during adaptation for the cardiac dataset. A cell (i, j) in the table has three values. The ﬁrst value represents the percentage of pixels labeled i that are labeled j after adaptation. The second value represents the percentage of switching pixels whose true label is i - lower is better. The third value represents the percentage of switching pixels whose true label is j - higher is better. Bolded cells denote label shift where more than 1% of pixels migrate from i to j."
2101.00522,dataset,124,,,"in Eq. (5) is minimized when the GMM is ﬁtted on the source domain distribution. The third term in the Eq. (5) upperbound is minimized because it is the second term in Eq. (3). The fourth term is small if we let ρ ≈ 1. The term eC(cid:48)(w∗) will be small if the domains are related to the extent that a joint-trained model can generalize well on both domains, e.g., there shouldn’t be label mismatch between similar classes across the two domains. The last term in Eq. (5) is negligible if the training datasets are large enough."
2101.00522,dataset,126,,,"We evaluate our algorithm on the following datasets. Multi-Modality Whole Heart Segmentation Dataset (MMWHS) [78]: this dataset consists of multi-modality whole heart images obtained on different imaging devices at different imaging sites. Segmentation maps are provided for 20 MRI 3D heart images and 20 CT 3D heart images which have domain gap. Following the UDA setup, we use the MRI images as the source domain and CT images as the target domain. We perform UDA with respect to four of the available segmentation classes: ascending aorta (AA), left ventricle blood cavity (LVC), left atrium blood cavity (LAC), myocardium of the left ventricle (MYO)."
2101.00522,dataset,137,,,"where, K denotes the number of segmentation classes, and W, H represent the width and the height the input images, respectively. Each pixel label y∗ ij will be represented as a one hot vector of size K and ˆyij is the prediction vector which assigns a probability weight to each label. Due to the existence of domain gap across the two domains, i.e. discrepancy between the source domain distribution ps(X) and the target domain distribution pt(X), the source-trained model in Eq. (1) may generalize poorly in the target domain. We want to beneﬁt from the information encoded in the target domain unannotated dataset {xt}M i=1 to improve the model generalization in the target domain."
2101.00522,dataset,177,,,"We provide qualitative assessment results. In Figure 5, we present the improvement in segmentation on CT scans from both the cardiac and abdominal organ datasets. In both cases, the supervised models are able to obtain a near perfect visual similarity to the ground truth segmentation mask. These represent the upper bound performance we compare against. Post-adaptation quality of the segmentation maps becomes much closer to the the supervised regime from a visual perspective. However, ﬁne details on image borders need more improvement- for example in images 2, 5, 6, 10. This is in line with the observed performance of our model when measured with ASSD. In conclusion, while we observe signiﬁcant gains with respect to the Dice coefﬁcient which directly measures the segmentation accuracy, the improvement in surface distance is not as large as in methods maintaining access to the source domain during adaptation. However, our performance is competitive given the advantage of providing privacy for the source domain."
2101.00522,dataset,182,,,"Figures 2 and 3 showcases the impact of our algorithm on the latent distribution of the two datasets. In Figure 2a, we record the latent embedding of the GMM prototypical distribution that is learned on the cardiac MR embeddings. Figure 2b exempliﬁes the distribution of the target CT samples before adaptation. As we can see from Table 1, the source-trained model is able to achieve some level of class separation without any adaptation, which is conﬁrmed in Figure 2b. Even so, we observe non-trivial overlap between the latent embeddings of two of the classes. In Figure 2c we observe that this overlap is reduced after adaptation. We also observe that the latent embedding of the target CT samples is shifted towards the prototypical distribution. For completeness, we repeat the same analysis for the organ segmentation dataset. We observe a similar behavior in the shift of the target embeddings based on the learned prototypical distribution, however compared to the heart segmentation dataset this shift is visually less pronounced."
2101.00522,dataset,182,,,"The quality of pixel label shift is analyzed in Tables 3 and 4. In Table 3 we observe that for the cardiac dataset there exists signiﬁcant inter-class label transfer, for approximately 20% of pixels, evenly distributed across classes. We see the majority of these shifts leading to an improvement in labeling accuracy, including all shifts where at least 1% of labels migrate, which is in line with our other reported results. These ﬁndings also corroborate with our observed embeddings. We can see from Table 3 that during adaptation there is signiﬁcant label migration between LVC and MYO, and this can be observed in the increased separation between the two classes in Figures 2b and 2c. For the abdominal organ dataset we observe signiﬁcantly less label shift between classes, with most of the activity involving previously labeled pixels being correctly le-labeled as Ignore after adaptation, or pixels initially in Ignore being correctly le-labeled to their appropriate class. This again supports the post adaptation improvement gains previously observed."
2101.00522,dataset,200,,,"Tables 2 and 1 summarize the segmentation performance for our method along with the baselines. We observe our method is comparable to SOTA approaches on the MMWHS dataset, despite the additional source-free constraint, and often outperforms other recently developed methods. We also note that our method outperforms all the listed methods on class AA, while having second best Dice scores on LAC, and competitive performance on the two remaining classes. However, we note that our method trails the other approaches when compared with respect to ASSD. This shows that our domain alignment approach successfully maps each class in the target embedding to its corresponding vicinity using the prototypical distribution, but lacks the reﬁnement offered by adversarial approaches. In our second segmentation task, we observe similar trends. We observe competitive performance against other reported results, especially on classes Liver, L. Kidney and Spleen. These results suggest that our method offers possibility of domain adaptation with competitive performance in a dataprivate regime. An idea to enhance performance in terms of ASSD is to use parametric distributions that can approximate the prototypical distribution more accurately."
2101.00522,github,3,,,1https://github.com/serbanstan/uda-medical-image-segmentation-sfs
2101.00522,"github, code available, code",7,,,Our code is available on github1.
2102.04190,data,2,,,Data Marshaling
2102.04190,data,16,,,"“ *TM supports soft- and hardware heterogeneity, but doesn't support data heterogeneity very"
2102.04190,data,19,,,"well, because it can't express complex data structures and therefore can't marshals these structures.”"
2102.04190,data,91,,,"There are two main types of properties, Object properties and Datatype properties. Object properties link an individual to an individual. Datatype properties link an individual to an XML Schema Datatype value (xml: integer) or an RDF literal (string). A third type of property known as Annotation properties is also exist, which can be used to add information (metadata) to classes, individuals and object/data type properties. Figure 6 shows a representation of some properties linking some individuals together"
2102.04190,"data, database",198,,,"Database middleware is responsible to establish communication among different applications and Local/remote database. However the database middleware cannot allow two way communi cation as well as transfer calls/objects between client and server(Linthicum,2019) It is often chosen to complement other middleware. types of Database middleware may be varying according to data source (flat file, relational DB, object database...), or connection type such as native middleware, call level interfaces (CLIs) and database gateways . Native middleware is a middleware created for a specific database, it provides the best performance and access to native database features. Call Level Interface or CLIs provide a single interface to several databases. CLIs, such as OLE DB, ODBC and JDBC, provide a single interface to several databases.  They are capable to translate the common interface call  into number of database dialects, and also translate the response back in an understandable form to the requesting application ( Microsoft Corporation.1992) Database gateways is capable to integrate different databases for access from a single application interface. Table 6: Database Middleware Characteristics:"
2102.04190,database,3,,,2.5 DATABASE MIDDLEWARE
2102.04190,database,4,,,Database Middleware Advantages:
2102.04190,database,4,,,Database Middleware Disadvantages:
2102.04190,database,6,,,"Database  Middleware ODBC, JDBC,"
2102.04190,database,20,,,"Ozsu M., and P. Valduriez,1991 ""Principle of Distributed Database Systems"", Prentice Hall inc, 1991."
2102.04190,database,21,,,"QUASIM, M.T, (2013) An Efficient approach for concurrency control in distributed database system. Indian Streams Research"
2102.04190,database,27,,,"Linthicum David, 2019 ""Database-Oriented Middleware"". DM Review Magazine November 1999 Issue, at: http://www.dmre view.com/toc.cfm?issueid=1255, accessed 2019"
2102.04190,database,30,,,"service, instead it is a middleware function that we could include it in our program to provide a service, e.g. database middleware services, agent middleware."
2102.04190,database,31,,,"QUASIM M. T.,2013 “Security Issues in Distributed Database System Model,” An Int. J. Adv. Comput. Technol., vol. 2, no. Xii,"
2102.04190,database,74,,,"In recent years' ontologies have become a topic of interest in computer science. Ontology gathers information about certain fields of interest. Ontology describes the concepts in the domain and also the relationships that hold between those concepts. In the next section I will present a middleware ontology structure that can be used for many purposes, including enterprise integration, database design, information retrieval, and information interchanges."
2102.04190,database,111,,,"Many classifications and definitions have been provided for middleware. There is an urgent need to shed light on middleware services and their factors. Some classification of middleware include (Chris, 2004; Cambel, 1999; Myerson, 2002) 1- Object oriented middleware (distributed object middleware). 2- Remote procedure call. 3- Message oriented middleware. 4- Database middleware 5- Transaction Middleware 6- Agent Based Middleware 7- Web Based Middleware However this is not the complete list, rather we can also find some other software included in the category , Now we will begin to briefly discuss each type of previous middleware services"
2102.05473,"data available, data",16,,,17 A. Ozaki and R. Peñaloza. Provenance in ontology-based data access. In Description Logics
2102.05473,database,45,,,"International Conference on Database Theory ICDT, pages 201–212, 2014. J. Doleschal, B. Kimelfeld, W. Martens, and L. Peterfreund. Weight annotation in information extraction. In International Conference on Database Theory, ICDT 2020, 2020. doi:10.4230/ LIPIcs.ICDT.2020.8."
2102.05473,database,170,,,"Some of the motivation for the study of semiring semantics comes from the successful development of semiring provenance in database theory and related fields (see e.g. [5, 6, 8, 13, 14, 17, 18, 19]), and the fact that the typical applications of provenance analysis, such as confidence scores, cost analysis, proof counting, and the understanding of evaluation strategies are of importance in many other areas of logic as well. However, semiring provenance analysis for database queries had originally been largely confined to positive query languages, such as conjunctive queries, positive relational algebra, and Datalog, and the treatment of negation poses non-trivial algebraic problems. Only recently, semiring semantics has been extended to logics with negation, and in particular to full first-order logic [11, 12], by means of new algebraic constructions based on quotient semirings. Semiring semantics has also been studied"
2102.05473,database,208,,,"Automata, pages 3–28. Springer, 2009. F. Geerts and A. Poggi. On database query languages for K-relations. J. Applied Logic, 8(2):173–185, 2010. J. Goodman. Semiring parsing. Computational Linguistics, 25:573–605, 1999. E. Grädel and Y. Gurevich. Metafinite model theory. Information and Computation, 140:26–81, 1998. E. Grädel and V. Tannen. Semiring provenance for first-order model checking. arXiv:1712.01980 [cs.LO], 2017. URL: http://www.logic.rwth-aachen.de/pub/graedel/ET17.pdf. E. Grädel and V. Tannen. Provenance analysis for logic and games. Moscow Journal of Combinatorics and Number Theory, 9(3):203–228, 2020. doi:10.2140/moscow.2020.9.203. T. Green, G. Karvounarakis, and V. Tannen. Provenance semirings. In Principles of Database Systems PODS, pages 31–40, 2007. T. Green and V. Tannen. The semiring framework for database provenance. In Proceedings of PODS, pages 93–99, 2017. 15 L. Hella. Logical hierarchies in PTIME. In Proceedings of LICS 92, pages 360–368, 1992. 16 M. Naaf. Semirings for Provenance Analysis of Fixed Point Logics and Games. Msc thesis,"
2102.06170,data,50,,,"[5] M. Geldenhuys, L. Thamsen, K. K. Gontarskay, F. Lorenz, and O. Kao, “Effectively testing system conﬁgurations of critical iot analytics pipelines,” 2019 IEEE International Conference on Big Data (Big Data), pp. 4157–4162, 2019."
2102.06170,data,53,,,"In our approach, we propose the generation of two twopopulation models. The ﬁrst for modeling performance as a predictor of Lavg and the second for modeling availability as a predictor of T RT . Concerning performance, data points are taken directly from proﬁling runs and used for modeling. The"
2102.06170,data,66,,,"[1] A. Toshniwal, S. Taneja, A. Shukla, K. Ramasamy, J. M. Patel, S. Kulkarni, J. Jackson, K. Gade, M. Fu, J. Donham, N. A. Bhagat, S. Mittal, and D. Ryaboy, “Storm@twitter,” Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data, 2014."
2102.06170,data,86,,,"[2] M. Zaharia, M. Chowdhury, M. Franklin, S. Shenker, and I. Stoica, “Spark: Cluster computing with working sets,” in HotCloud, 2010. [3] P. Carbone, A. Katsifodimos, S. Ewen, V. Markl, S. Haridi, and K. Tzoumas, “Apache ﬂink: Stream and batch processing in a single engine,” IEEE Data Eng. Bull., vol. 38, pp. 28–38, 2015."
2102.06170,data,92,,,"event stream. Our approach is aimed at use cases where the data stream is processed at event time and characterized by a fairly stable average input throughput. Although some variance in this rate is to be expected, use cases where the number of events entering the source operators is essentially random or ﬂuctuates wildly over time is not conducive to ﬁnding optimal conﬁgurations through proﬁling runs. Future work may focus on using the metrics gathered during proﬁling to optimize the fault tolerance mechanism of running analytics pipelines."
2102.06170,data,163,,,"Distributed Stream Processing (DSP) systems are critical to the processing of vast amounts of data in real-time. It is here where events must traverse a graph of streaming operators to allow for the extraction of valuable information. There are many scenarios where this information is at its most valuable at the time of data arrival and therefore systems must deliver a predictable level of performance. Examples of such scenarios include IoT data processing, click stream analytics, network monitoring, ﬁnancial fraud detection, spam ﬁltering, news processing, etc. In order to keep up with the ever increasing data processing demands, DSP systems have the ability to scale horizontally across a cluster of commodity nodes to provide additional computation capabilities. However, as DSP systems scale to ever larger sizes, the probability of individual node failures increases and the need for more efﬁcient fault tolerance mechanisms is attracting more attention."
2102.06170,data,210,,,"To meet the performance modeling requirements of DSP jobs, we use a technique for gathering metrics in environments that closely mirrors realistic conditions. By employing two key enabling technologies, i.e. OS-virtualization and container orchestration; we are able to replicate multiple pipelines in parallel while ensuring isolation from one another. At the same time, identical copies of the DSP job can be deployed within their own separate self-contained environments while possessing a unique variation of the system conﬁgurations. These technologies, when combined with Infrastructure-asCode processes, provide a mechanism for quickly reproducing any environment. Our previous work, Timon [5], introduced a tool for automatically achieving these goals. To ensure results are comparable and as close to how they would be under realistic conditions, each parallel deployment consumes the same data stream during proﬁling under normal expected loads. In order to select a good set of input conﬁguration variables for each parallel deployment, the solution space is evenly explored by selecting a minimum and maximum value for the CI after which a set of equidistant values are calculated between these extremes. The following metrics are gathered from each of the parallel deployments:"
2102.06170,data,295,,,"Building upon the analysis which was presented in the previous section, we introduce a new heuristic for estimating the minimum, average, and maximum T RT per data point gathered through proﬁling runs. Referring to Figure 1, the T RT is modeled as a decreasing geometric sequence where the 1st term is composed of the time periods vii through x multiplied by a common ratio U . The ﬁrst time period E is directly related to the checkpoint interval (CI) conﬁguration. The CI is deﬁned as the frequency with which the checkpoint process is initiated and is measured in milliseconds. Since we cannot predict exactly when the failure will occur between checkpoints, we can take a best, average, and worst case estimate. Therefore, zero in the best case scenario, the length of the CI divided by two in the average case, and the full length of the CI in the worst case. The second time period T is based on the heartbeat timeout conﬁguration variable which determines how long the system will wait until a node is timed out. In order to calculate the common ratio, a measure of the processing capacity utilization is required. We know that the system will use the maximum processing capacity of the system on top of what is already being utilized to catch-up after a failure has occurred. Therefore, knowing the average rate at which messages are processed (Iavg) and the maximum rate at which the system is capable of processing (Imax) can be combined to formulate the processing capacity utilization as a percentage (U) as seen in Equation 1."
2102.06170,github,14,,,"1https://prometheus.io, Accessed: Aug 2020 2https://github.com/morgel/IoTDV-experiment 3https://github.com/morgel/YSB-experiment"
2102.06170,"github, data, code",185,,,"Two experiments were conducted to evaluate the usefulness of Chiron. Source code for these experiments can be found on github23. The ﬁrst was the IoT Delivery Vehicles (IoTDV) Experiment where a simulator generates 500,000 delivery vehicle events per second which are submitted to Kafka to await processing. Each event contains information about the vehicle’s geo-location, speed, and crucially whether or not the vehicle is self-driven. The job consisted of the following streaming operations: read an event from Kafka; deserialize the JSON string; ﬁlter update events not within a certain radius of a designated observation geo-point where delivery vehicles are of a particular type, i.e. self-driving; take a 10 second window where all update messages are of the same vehicle ID and calculate the vehicles average speed; generate an alarm for vehicles which have exceeded the speed limit; enrich notiﬁcation with vehicle type information from data stored in system memory and write it back out to Kafka. The second is based on the Yahoo Streaming Benchmark experiment"
2102.10939,data,10,,,"noisy data,” arXiv preprint arXiv:1907.03692, 2019."
2102.10939,data,54,,,"[8] A. C. Gilbert, P. Indyk, M. Iwen, and L. Schmidt, “Recent developments in the sparse fourier transform: A compressed fourier transform for big data,” IEEE Signal Processing Magazine, vol. 31, no. 5, pp. 91–100, 2014."
2102.12411,code,100,,,"with ζN n bits. The labeling used is the binary-reﬂected Gray code [44]. The FEC parity bits together with ζN n bits serve as sign bits SI to yield the PAM symbol sequence 1)SI,iAI,i 1], where XI,i = ( X I = [XI,0, XI,1, . . . , XI,N n (i = 0, 1, . . . , N n 1). The same procedure is conducted in − the quadrature branch to generate X Q."
2102.12411,code,122,,,"[43] O. Geller, R. Dar, M. Feder, and M. Shtaif, “A shaping algorithm for mitigating inter-channel nonlinear phase-noise in nonlinear ﬁber systems,” J. Lightw. Technol., vol. 34, no. 16, pp. 3884–3889, May 2016. [44] F. Gray, “Pulse code communication,” US Patent 2 632 058, Mar. 1953. [45] Y. C. G¨ultekin, T. Fehenberger, A. Alvarado, and F. M. J. Willems, “Probabilistic shaping for ﬁnite blocklengths: Distribution matching and sphere shaping,” Entropy, vol. 22, no. 5, p. 581, May 2020."
2103.00265,data,13,,,over real data points sampled from the distribution p(x).
2103.00265,data,14,,,Then we can show the maximum likelihood estimation for a set of N data
2103.00265,data,14,,,"When we consider the expected value of the gradient over multiple data samples,"
2103.00265,data,15,,,"this issue in theory, while still learning an estimator of the data distribution."
2103.00265,data,16,,,"Perhaps most important, FID can be evaluated on the test data, so it can"
2103.00265,data,21,,,they encourage the production of data points which do not have density under any observed distribution p(x).
2103.00265,data,22,,,x) allows us to decompose the marginal log-likelihood of the data under the generative model in terms of the va |
2103.00265,data,24,,,"modeling the diﬀerence between a given generative model and the real data, and then encouraging the generative model to minimize that distance."
2103.00265,data,25,,,consequence of this is that the entropy of the true data distribution can be estimated by such a generative model if it maximizes likelihood.
2103.00265,data,25,,,"further limits the applicability to real data, where density is generally concentrated along speciﬁc low-dimensional manifolds (Bengio et al., 2012)."
2103.00265,data,26,,,deﬁnes a density for generative model and directly maximizes the value of this density on observed data points. A newer and quite distinctive approach involves
2103.00265,data,27,,,"Because the entropy of the true data distribution doesn’t depend on the estimator, the KL-divergence can be minimized by maximizing likelihood. Another useful"
2103.00265,data,27,,,While inception score has been shown to be highly correlated to visual sample quality (real data samples achieve better inception scores than any current models)
2103.00265,data,27,,,approach in which a statistical divergence is used which is continuous and diﬀerentiable even when the generating distribution and the real data distribution do not overlap.
2103.00265,data,29,,,is that a very high score can be achieved just by returning the samples from the training set. Thus a generative model which merely memorizes the training data
2103.00265,data,30,,,"noise injected. So long as suﬃcient noise is injected, this provides the generator with a density ratio which is well-deﬁned even when the support of the real data"
2103.00265,data,30,,,"the resulting gradient then has a particularly elegant form consisting of a positive phase, in which the function is maximized over samples from the real data, and a"
2103.00265,data,31,,,"C + Cw − (Heusel et al., 2017) studied several artiﬁcial deformations of the data and showed that FID scores gradually became worse with increasing corruption. More"
2103.00265,data,31,,,The key idea is that the score is high when the distribution of these activations for generated samples is close to the distribution of these activations for real data points.
2103.00265,data,31,,,Thus we can see that the KL-divergence decomposes into two terms 1.12: a crossentropy term (likelihood) and a term for the entropy of the true data distribution.
2103.00265,data,33,,,data by a model which produces only a single and clearly identiﬁable example of x) very diﬀerent each class that the classiﬁer is aware of. This would make p(y
2103.00265,data,33,,,discriminator’s score provides an estimate of how much the model’s density diﬀers from the true data density at a given point. If the discriminator is able to correctly
2103.00265,data,38,,,6 tive encouraging the generated images to diﬀer from styles that occur in the data and another objective encouraging the generated images to follow the data distribution. These approaches radically diﬀer from the probabilistic framework in that
2103.00265,data,38,,,"data distribution without adding noise (which tends to degrade sample quality). When the generator and the real data distribution do not have overlapping support, the KL-divergence is undeﬁned and the Jensen-Shannon divergence can be"
2103.00265,data,39,,,have enormous amounts of unlabeled data which can be used to train generative models without overﬁtting. We could also imagine that many of these generative models require learning features which are also useful for supervised learning. For
2103.00265,data,39,,,"is equivalent to learning a classiﬁer between the real data and the model’s distribution. Classiﬁers have been extremely successful in deep learning and methods for successfully training classiﬁers have been widely studied, and inductive biases"
2103.00265,data,40,,,"with these lofty ambitions, there are many practical motivations for generative models. One common argument is that humans may use generative models as a way of doing supervised and reinforcement learning while using less labeled data. Consider"
2103.00265,data,41,,,"models which can simulate from the dynamics of the world. We want models that can synthesize realistic looking data. However, before going further it is useful to understand the probabilistic interpretation of generative model, which gives a"
2103.00265,data,42,,,"methods of determining this integral over the entire space. Typically neural networks are evaluated at speciﬁc points from some distribution (for example, a data distribution), which is insuﬃcient for computing the integral over the entire space."
2103.00265,data,42,,,"normalization is that it enforces the lipschitz constraint at all points in the space, whereas gradient penalty only enforces the lipschitz constraint at points where it is applied, usually around data points or on linear interpolations between data points."
2103.00265,data,43,,,that are known to be good for classiﬁcation could also be good for determining the quality of generations. Another motivation for modeling the diﬀerence between a model and the data is that it allows the model to become sensitive to any clear
2103.00265,data,46,,,"would achieve a high inception score, without doing any learning. The inception score will give low scores to model which underﬁts and fails to achieve clear samples, but it does not penalize a model at all for memorizing the training data or failing"
2103.00265,data,47,,,"classify between real data points and generated data points reliably and in a way that generalizes, then it is a clear indicator that the generator is of poor quality. However, if the opposite is true, that the discriminator cannot classify between real"
2103.00265,data,48,,,"on two primary critiques: (1) that the gradient penalty only guarantees lipschitz continuity at the data points or wherever it is applied, and not everywhere in the input space and (2) that the gradient penalty has the eﬀect of pushing down the"
2103.00265,data,51,,,"from p(y) while having high entropy in p(y), and yet the model would clearly lack the diversity of real data, and would be a poor generative model from the statistical divergence perspective. Another limitation is that if the classiﬁer is vul |"
2103.00265,data,57,,,An alternative to the likelihood maximization approach involves studying a candidate generative model and the diﬀerences between the samples from this model and the original data. In practice this usually takes the form of estimating the density ratio between the generating distribution qθ(x) and the real data distribution p(x).
2103.00265,data,58,,,an approximation called contrastive divergence which replaces the model distribution qθ(x) in 1.28 with short gibbs sampling chains which start from the real data distribution. The discovery of a general way of deﬁning energy functions with deep networks which also allow for fast and exact sampling would make energy-based models signiﬁcantly more appealing.
2103.00265,data,60,,,"essentially require each z to lead to a p(x, z) with a large value. Intuitively, it is ﬁne if a few or even a single value of the latent variable explains our observed data, and it is not necessary for all of the z values to explain all of the data points."
2103.00265,data,68,,,"(Gutmann and Hyv¨arinen, 2010) proposed to use a ﬁxed generative model qθ(x) and learn a classiﬁer to distinguish between these samples and the real data distribution. Once this density ratio Dθ(x) is learned, the estimator can be sampled from by using importance sampling. A markov chain monte carlo method could also be used for sampling."
2103.00265,data,68,,,"refer to this as the “poverty of the stimulus” problem. An appealing hypothesis is that humans use unsupervised generative models to build robust representations of the world and then use those same representations to do supervised and rein forcement learning from small amounts of explicitly labeled data. Since humans are constantly absorbing perceptual data (sound, vision, touch), humans should"
2103.00265,data,79,,,"The maximum likelihood approach only requires that we be able to sample uniformly from the real data and evaluate the log-density of our estimating distribution qθ(x) at these points. What is the simplest choice for q, if we want to frame our problem in terms of optimizing over functions ? Indeed, q cannot simply be an arbitrary function, because it could simply assign a high value to every point in the"
2103.00265,"data, dataset",50,,,"what a classiﬁer has really learned about a type of data. Suppose we have a dataset where a model generates text captions from an image. Consider an image of a giraﬀe, which the model describes as “A giraﬀe walking next to tall green grass”."
2103.00265,dataset,41,,,"speciﬁcally: they studied artiﬁcally injecting independent noise into the images, removing random regions of the images, swirling the images, salt and pepper noise, and injecting examples from another dataset. On all of these increasing corruption"
2103.00265,dataset,42,,,"which makes them ill-suited to problems where very distinct points can have high density with regions of low density separating them. For example, nearly all natural distributions such as image and audio datasets are multimodal. Moreover, most of"
2103.00265,dataset,54,,,"example, we could consider the distribution over all human faces which can occur in reality to be p(x) and consider each face as a sample. If we have access to a recorded dataset (for example a set of faces), we may also choose to treat these"
2103.00265,dataset,150,,,"Under the maximum likelihood approach, a straightforward way of quantifying the success of the model is to compute the model’s average likelihood qθ(x) on datapoints from a held-out distribution (often referred to as a test dataset). This criteria has some desirable qualities: it is able to detect overﬁtting and it has a consistency guarantee. On the other hand, it has signiﬁcant limitations. One discussed by (Arjovsky et al., 2017) is that qθ(x) has a value approaching 0 at any points where p(x) has a value greater than zero, the log-likelihood approaches ne gative inﬁnity (intuitively this can be seen by observing that log(x) approaches negative inﬁnity as x approaches 0). This property is unlikely to be suitable for most applications."
2103.00265,dataset,169,,,"λ1 . The metric tensor is considered to be poorly conditioned if the condition number has a high value. (Odena et al., 2018) proposed to eschew the issue of computing the complete spectrum, which could be quite challenging, in favor of sampling random directions (essentially sampling small random values for εvk and empirically computing 1.45, and then adding a penalty to encourage these values to fall within a speciﬁc range. In practice they achieved good results by setting λmin to 1.0 and λmax to 20.0. Making λmax too small could have the eﬀect of making it too hard for the model to be responsive to the latent variables and setting λmin to be too large could have the eﬀect of making it impossible for the model to learn to give large regions in the space relatively constant density. In practice these hyperparameters would likely need to be tuned depending on the dataset in accordance with these concerns."
2103.07169,code,50,,,"69. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541–551 (1989)"
2103.07169,data,5,,,Useful in modelling sequential data
2103.07169,data,10,,,Perform FER based on dynamic data input i.e. videos
2103.07169,data,16,,,Learn the spatial features in an image i.e. perform FER based on static data input
2103.07169,data,29,,,"SVM: SVM was invented by Vapnik in 1995 [128]. In SVM [129], the training data can be separated by a hyperplane."
2103.07169,data,35,,,"46. Hinton, G., Salakhutdinov, R.: Reducing the dimensionality of data with neural networks. Science (New York, N.Y.) 313, 504–7 (2006). DOI 10.1126/science.1127647"
2103.07169,data,45,,,"and eﬀectively by taking into account their facial expression, body gestures, voice and words. Combining facial, audio, text and body gestures with physiological data could lead to a higher emotion recognition rate by machine learning algorithms than by humans."
2103.07169,data,45,,,from Information states in the preceding the time steps previous step) cannot be captured Diﬃcult to learn longterm temporal dependencies as gradients explode or vanish over many time steps [6] Requires a lot of data to prevent over-ﬁtting [138]
2103.07169,data,53,,,"107. Ringeval, F., Schuller, B., Valstar, M., Jaiswal, S., Marchi, E., Lalanne, D., Cowie, R., Pantic, M.: Av+ ec 2015–the ﬁrst aﬀect recognition challenge bridging across audio, video, and physiological data (2015)"
2103.07169,data,54,,,"28. Dhall, A., Goecke, R., Lucey, S., Gedeon, T.: Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark. In: 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), pp. 2106–2112 (2011)"
2103.07169,data,70,,,"1. Ahmed, T.U., Hossain, S., Hossain, M.S., ul Islam, R., Andersson, K.: Facial expression recognition using convolutional neural network with data augmentation. In: 2019 Joint 8th International Conference on Informatics, Electronics Vision (ICIEV) and 2019 3rd International Conference on Imaging, Vision Pattern Recognition (icIVPR), pp. 336–341 (2019)"
2103.07169,data,70,,,"In the deep learning algorithms, the input images are ﬁrst pre-processed by performing face alignment, data augmentation and normalization. Then the images are directly fed into deep networks like CNN, RNN etc. which predict the emotion of the images. The most commonly used classiﬁcation methods are explained in more detail below. They are arranged in the order in which they were invented."
2103.07169,data,86,,,"RNN: RNN [78] was introduced in the 1980s. RNN is a feed-forward neural network that has an edge over adjacent time steps, introducing a notion of time. Hence, RNN is mainly used for a dynamic data input that has a temporal sequence. In RNN, a state depends upon the current input as well as the state of the network at the previous time step, making it possible to contain information from a long time window."
2103.07169,data,107,,,"Breazeal et al. [10] presented a robot Leonardo that can imitate human facial expressions. They use neural networks to learn the direct mapping of a human’s facial expressions onto Leonardo’s own joint space. In Horii et al. [50], the robot does not directly imitate the human but estimates the correct emotion and generates the estimated emotion using RBM. RBM [46] is a generative model that represents the generative process of data distribution and latent representation, and can generate data from latent signals [123, 98, 117]."
2103.07169,data,112,,,"100. Nunes, A.R.V.: Deep emotion recognition through upper body movements and facial expression. Master’s thesis, Aalborg University (2019) 101. Nwosu, L., Wang, H., Lu, J., Unwala, I., Yang, X., Zhang, T.: Deep convolutional neural network for facial expression recognition using facial parts. In: 2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress, pp. 1318–1321 (2017)"
2103.07169,data,141,,,"Suggestion 3: Combine facial expression recognition with the data from other modalities such as voice, text, body gestures and physiological data to improve the emotion recognition rate. Although this overview focuses on facial expression recognition, it may be possible to control one’s face and not express the emotion one is truly experiencing. Some studies combine facial expression recognition with audio data, body gestures or physiological data for an improved emotion recognition [83, 41, 53]. There are very few studies that combine facial data with both audio and physiological data [107, 106] and studies that analyze all modalities (face, voice, text, body gestures and physiological signals) have not been found. Humans can recognize the emotion of a person quickly"
2103.07169,data,164,,,"20. Cohen, I.: Recognizing robotic emotions: facial versus body posture expression and the eﬀects of context and learning. Master’s thesis (2010) 21. Corneanu, C.A., Sim´on, M.O., Cohn, J.F., Guerrero, S.E.: Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and aﬀect-related applications. IEEE Transactions on Pattern Analysis and Machine Intelligence 38(8), 1548–1568 (2016) 22. Costa, S., Soares, F., Santos, C.: Facial expressions and gestures to convey emotions with a hu manoid robot. In: International Conference on Social Robotics, pp. 542–551. Springer (2013) 23. Dandıl, E., ¨Ozdemir, R.: Real-time facial emotion classiﬁcation using deep learning. Data Science and Applications 2(1), 13–17 (2019)"
2103.07169,"data, data https",37,,,"B.: Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data. Pattern Recognition Letters 66, 22–30 DOI https://doi.org/10.1016/j.patrec. (2015). 2014.11.007. Pattern Recognition in Human Computer Interaction"
2103.07169,"data, dataset",104,,,"Although the goal of this study is to perform FER in real-time and during HRI, the studies on real-time FER are compared with FER on predeﬁned datasets. FER has been carried out on static human images as well as on dynamic human video clips. While some studies, perform facial recognition on still images, others perform facial recognition on videos. In Datcu and Rothkrantz [24], they show that there is an advantage in using data from video frames over still images. This is because videos contain temporal information that is absent in still images."
2103.07169,"database, dataset",91,,,"Zhang et al. [146] used a deep convolutional network (DCN) that had an accuracy of 98.9% on CK+ dataset and 55.27% on Static Facial Expressions in the Wild (SFEW) dataset. Here, the same network produced very diﬀerent results for two diﬀerent datasets. SFEW [28] consists of close to a real-world environment extracted from movies. The database covers unconstrained facial expressions, varied head poses, large age range, occlusions, varied focus, diﬀerent resolution"
2103.07169,dataset,2,,,Dataset CK+/Oulu-Casia/MMI
2103.07169,dataset,5,,,4.1 FER on predeﬁned dataset
2103.07169,dataset,25,,,A supervised learning algorithm that can be used for classiﬁcation or regression on small datasets [129] Learn long-term temporal dependencies [38]
2103.07169,dataset,34,,,"In Table 3, various studies on facial expression recognition are listed. Here, studies with an accuracy of greater than 90% for facial expression recognition on predeﬁned datasets are selected."
2103.07169,dataset,57,,,"of faces, and close to real-world illumination. In Zhang et al. [146] the accuracy for ”in the wild” settings was considerably lower than on CK+ dataset, implying that the expression recognition algorithms can still not handle the variations in environment, head poses etc. in real-world settings."
2103.07169,dataset,68,,,"for comparison with Table 3c. Studies are arranged according to their accuracy level. It should be noted that these studies are carried out on predeﬁned datasets consisting of human images and videos and do not involve robots. There are a considerable number of studies that achieve accuracy greater than 90% on CK+, Jaﬀe and Oulu-Casia datasets on both still images and videos."
2103.07169,dataset,73,,,"27 studies on facial expression recognition during HRI were reviewed. Some of the studies have not been performed on a robot platform. These studies perform emotion recognition in real-time and mention HRI as their intended application. The studies are summarized in Table 2. Here, studies that perform facial expression recognition on predeﬁned datasets or studies that perform facial expression recognition but not in real-time were not included."
2103.07169,dataset,75,,,"Table 1 summarizes the major purpose, application areas, advantages, disadvantages and frequency of use for commonly used algorithms. For the frequency of use, only the number of papers that implement facial expression recognition during HRI or in real-time scenarios were counted. Although RNN has not been used for facial expression recognition during HRI or in real-time, some studies perform facial expression recognition on predeﬁned datasets using RNN."
2103.07169,dataset,75,,,"There are already studies having high accuracy (greater than 90%) in facial expression recognition on CK+, Jaﬀe and Oulu-Casia datasets. (see Tables 3a and 3b). The accuracies on CK+, Jaﬀe and Oulu-Casia datasets have been as high as 100%, 99.8% and 92.89% respectively. In comparison to this, the accuracy for facial expression recognition in real-time is not as high."
2103.07169,dataset,80,,,"It is easier to achieve high accuracy while performing emotion recognition on predeﬁned datasets as they are recorded under controlled environmental conditions. On the other hand, it is diﬃcult to achieve the same level of accuracy when performing emotion recognition in real-time when the movements are spontaneous. It should be noted that studies that perform facial expression recognition in real-time were carried out under controlled laboratory conditions with little variation in lighting conditions and head poses."
2103.07169,dataset,82,,,"Some studies perform facial expression recognition in the wild, but their accuracy is much less than the accuracy on predeﬁned datasets like CK+, Jaﬀe, MMI etc. To increase the eﬃciency of facial expression recognition in real-world scenarios, the performance of facial expression recognition in the wild needs to be improved. This can also be used to recognize facial expressions in real-time. Based on this, a direct adaptation of emotions would make HRI smoother."
2103.07169,dataset,131,,,"This overview focuses on two aspects: (1) recognition of human facial expressions and (2) generation of facial expressions by robots. The review framework (Figure 3) is based on these two streams. (1) Recognition of human facial expressions is further subdivided depending on whether the recognition takes place on (a) a predeﬁned dataset or in (b) real-time. (2) Generation of facial expressions by robots is also subdivided depending on whether the facial generation is (a) hand-coded or (b) automated, i.e., facial expressions of a robot are generated by moving the features (eyes, mouth) of the robot by hand-coding or automatically using machine learning techniques."
2103.07169,dataset,156,,,"As this study is about facial expressions in HRI, for a robot to be able to recognize emotion, emotion recognition has to be performed in real-time. Table 3c provides studies with facial expression recognition in real-time for HRI. Here, the accuracies are comparatively lower than the accuracies for predeﬁned datasets. As can be seen in Table 3c, only two studies have an accuracy greater than 90%. The robots that are used in the studies are either robotic heads or humanoid robots such as Pepper, Nao, iCub etc. Many studies that perform facial expression recognition in real-time use CNNs, making it a popular choice for facial expression recognition [8, 133, 15, 2, 2]. However, the highest accuracy is achieved by Bayesian and Artiﬁcial Neural Network (ANN) methods for facial expression recognition in real-time."
2103.07169,dataset,196,,,"Abstract Facial expressions are an ideal means of communicating one’s emotions or intentions to others. This overview will focus on human facial expression recognition as well as robotic facial expression generation. In the case of human facial expression recognition, both facial expression recognition on predeﬁned datasets as well as in real-time will be covered. For robotic facial expression generation, hand-coded and automated methods i.e., facial expressions of a robot are generated by moving the features (eyes, mouth) of the robot by hand-coding or automatically using machine learning techniques, will also be covered. There are already plenty of studies that achieve high accuracy for emotion expression recognition on predeﬁned datasets, but the accuracy for facial expression recognition in real-time is comparatively lower. In the case of expression generation in robots, while most of the robots are capable of making basic facial expressions, there are not many studies that enable robots to do so automatically. In this overview, state-of-the-art research in facial emotion expressions during human-robot interaction has been discussed leading to several possible directions for future research."
2103.07169,dataset,231,,,"This overview emphasizes the recognition of human facial expressions and the generation of robotic facial expressions. There are already plenty of studies having high accuracy for facial expression recognition on preexisting datasets. Accuracy on facial expression recognition in the wild is considerably lower than the experiments which have been conducted under controlled laboratory conditions. For human facial emotion recognition, future work would be to improve emotion recognition for non-frontal head poses in presence of occlusions (i.e. emotion recognition in the wild). It should be made possible to recognize emotions during speech as well emotions with varying intensities. In the case of facial expression generation in robots, robots are capable of making the basic facial expressions. Few studies perform autonomous facial generation in robots. In the future, there could be studies comparing robotic facial expressions with the robot’s bodily expressions and also with a combination of facial and bodily expressions to see if there is any diﬀerence in recognizing these. Robots should be able to express their emotion with partial bodily or facial gestures while speaking. They should also be express their emotions with various intensities instead of a single conﬁguration per emotion. Lastly, there is a need to go beyond the basic seven expressions for both facial expression recognition and generation."
2103.07169,dataset,264,,,Dataset Study CK+/MMI Mistry et al. (2017) [95] CK Kotsia and Pitas (2007) [66] Jaﬀe/CK Hossain et al. (2017) [51] CK+ Kar et al. (2017) [60] CK/Jaﬀe Mliki et al. (2015) [45] CK+/Jaﬀe Chen et al. (2017) [16] CK+ Zhang et al. (2016) [146] Mayya et al. (2016) [89] Jaﬀe/CK+ Minaee and Abdolrashidi (2019) [94] CK+/Jaﬀe Jaﬀe/CK+ Nwosu et al. (2017) [101] CK+/Oulu-Casia/Jaﬀe Yang et al. (2017) [140] CK+/Oulu-Casia/Jaﬀe Yang et al. (2018) [139] CK+/Oulu-Casia Ding et al. (2017) [29] CK+/Jaﬀe/ MMI Gogi´c et al. (2018) [39] CK+/Jaﬀe Kim et al. (2019) [62] Jaﬀe Hua et al. (2019) [52] CK+ Mannan et al. (2015) [85] KDEF/CK+ Ruiz-Garcia et al. (2018) [110] Jaﬀe Hamester et al. (2015) [44] CK+/MMI Meng et al. (2017) [93] CK+ Liliana et al. (2017) [77] CK+/Jaﬀe Ferreira et al. (2018) [35] CK+ Mollahosseini et al. (2016) [97] Jaﬀe/KDEF Yaddadenet al. (2016) [137]
2103.11630,code,11,,,// REPLACE LINE 3-4 IN ALG. 1 WITH FOLLOWING CODE
2103.11630,data,6,,,5https://movielens.umn.edu 6http://cru.uea.ac.uk/cru/data/hrg/tmc/
2103.11630,data,15,,,"2In the following, we preselect only one data point in the algorithm, in"
2103.11630,data,19,,,"Based on Lemma 3, it is time consuming for GREEDY to ﬁnd even the ﬁrst data point."
2103.11630,data,22,,,Lemma 3. The time complexity of determining the ﬁrst data point in GREEDY is at least O(n2).
2103.11630,data,24,,,"Submodularity: Given a data point q ∈ D\S2, according to the deﬁnition of HD(S, u), we have"
2103.11630,data,35,,,"• Weather6. It is 15-dimensional weather data of 566,268 points with 63,398 skyline points, each of which consists of average monthly precipitation totals and elevation at over half a million sensor locations."
2103.11630,data,39,,,"For convenience, we denote the maximum utility of data points in S2 ∪ {q} as Maxu, i.e., Maxu= maxp∈S2∪{q} u(p) and divide the discussion into 3 cases."
2103.11630,data,43,,,"sets chosen by a greedy algorithm, and qi denotes the data point selected in step i. Submodularity ratio [18]. The submodularity ratio of a nonnegative set function H(·) is the largest scalar γ, s.t."
2103.11630,data,44,,,"is still costly to determine even the ﬁrst data point. To reduce the cost, we adopt the same strategy as RDPGREED does by preselecting some data points and then integrating the GREEDY process. The details are shown in Algorithm 1."
2103.11630,data,66,,,"In this section, we conduct a theoretical analysis of the approximation ratio of the returned result set in PRESGREED, which ﬁlls the gap in the existing research. Our main theoretical result shows that the result set Sk of PRESGREED achieves a near-optimal solution for general monotone nonsubmodular functions with only one preselected data point. The details are shown in Theorem 1."
2103.11630,data,101,,,"Let D be a set of n d-dimensional points with positive real values. For each point p ∈ D, the value on the ith dimension is represented as p[i]. Before we introduce the problem, we ﬁrst present some related concepts [3]. Utility function. A utility function u is a mapping u: Rd + → R+. Given a utility function u, the utility of a data point p is denoted as u(p), which shows how satisﬁed the user is with the data point."
2103.11630,data,105,,,"The k-regret minimization problem is NP-hard, and a greedy strategy is often used [20], [19]. After adopting the concept of the happiness ratio, we show that the problem equals ﬁnding k data points that can maximize the minimum happiness ratio. The objective function HD(S, U) is monotonic but not submodular. The classical greedy algorithm [17] can solve the problem, which is critical to efﬁciently identify the data point with the largest marginal gain. We call the algorithm by directly using the classical greedy framework as GREEDY."
2103.11630,data,125,,,"As shown in [3], the data points with the maximum value(s) in the coordinate(s) are more representative2. Therefore, we preselect these data points for our PRESGREED algorithm. Then, our algorithm iteratively selects the point that contributes the most to the happiness ratio H and adds it into the result set (Lines 2-12). The happiness ratio is computed by using the linear programming tools in Line 5. Computing HS∪{p}(S, U) using an LP. Given a set S and a point p, we can compute HS∪{p}(S, U) using the linear program LP 4."
2103.11630,data,148,,,". p ∈ D \ S, let ∆S(p) = f ({p} ∪ S) − f (S) be the marginal gain of adding data point p to the set S. The function f is monotonically nondecreasing and submodular if i) ∆S(p) ≥ 0 and ii) for S1 ⊆ S2 and p /∈ S2, ∆S1 (p) ≥ ∆S2 (p) [16]. An equivalent form of a submodular function is expressed by f (S1) + f (S2) ≥ f (S1 ∪ S2) + f (S1 ∩ S2) compared to a modular function f which has the form f (S1) + f (S2) = f (S1 ∪ S2) + f (S1 ∩ S2)."
2103.11630,data,170,,,"According to Heuristic 1, a data point p ∈ D\Si−1 i.e., that contributes the most maximizing HD(Si−1 ∪ {p}, U), is the point satisfying HD(Si−1, U) = HSi−1∪{p}(Si−1, U). This can be done by computing HSi−1∪{p}(Si−1, U) for each point p and keeping the point with the minimum value, where HSi−1∪{p}(Si−1, U) is computed using the linear program and p · v is normalized due to the scale-invariance property of the k-regret query. The time complexity of the linear program is O(k2d) [15] because p(cid:48) ∈ S instead of p(cid:48) ∈ D. Otherwise, the time complexity will be O(n2d). Usually, |S| << |D|; thus, the time complexity decreases by a great extent."
2103.11630,data,199,,,"Proof. Based on the GREEDY framework, the ﬁrst data point selected is qf = arg maxp∈D HD({p}, U), where HD({p}, U) = inf u∈U maxp(cid:48) ∈D u(p(cid:48)) . When the utility function class U contains only one element u, i.e., HD({p}, U) = maxp(cid:48)∈D u(p(cid:48)) , then we only need to traverse all the data points in D to compute the HD({p}, U). However, when facing a class of utility functions (i.e., the inﬁnite number of functions), we need to employ a linear programming method to compute the ﬁrst data point. The time complexity of current state-of-the-art linear programming algorithms is at least O(n) (e.g., the time complexity of the Simplex method is O(n2d)). Therefore, the time complexity of determining the ﬁrst data point qf is at least O(n2)."
2103.11630,"data, database, dataset",238,,,"For summodularity aspect, submodular function maximization has numerous applications in machine learning and database systems [41], [42]. Though the problems are NPhard, the greedy method can return a result with 1 − 1/e approximation ratio [17]. By leveraging the marginal gain property, lazy-update method is proposed to accelerate the search [43]. Badanidiyuru and Vondr´ak [44] propose a centralized algorithm that achieves a (1 − 1/e − (cid:15)) approximation ratio using O(n/(cid:15) log(n/(cid:15))) function evaluations for general submodular functions, and a multistage algorithm is proposed to further accelerate the search. In [21], a linear time algorithm is proposed for cardinality constrained submodular maximization, which provides the same approximation ratio as [44] with n log 1 (cid:15) function evaluations. The accelerated greedy algorithms [43], [21] start from an empty set, and need more time for picking the ﬁrst data point. While the approaches in [44] require much more functions evaluations when n is large, and they are more effective when those submodular functions can be easily decomposed and approximated. In a word, all the approaches in above are not efﬁcient for large-scale datasets."
2103.11630,"data, dataset",46,,,"Description the whole dataset, a subset of D a positive integer, cardinality of D, dimensionality of D utility function a class of utility functions a data point in D, the utility of p happiness ratio of S under a speciﬁc utility u"
2103.11630,"data, dataset",134,,,"We can see that with 2 times, the minimum happiness ratios are equal to 1, which is the same as the result of PRESGREED. The average minimum happiness ratio of the 4 algorithms is 0.945, which is very close to 1. If we choose the maximum value among the results, it has a great chance to select a result as good as that of PRESGREED. In addition, the result of random sampling is also inﬂuenced by k. When sampling a subset R with size s from the dataset D, the probability of n)k a data point p being selected is P r(p) = 1 − (C s for sampling without replacement, which approaches 1 when k increases."
2103.11630,"data, dataset",139,,,"Datasets. We run our experiments on 1 synthetic and 4 realworld datasets, which are widely used in previous studies, e.g., [3], [4], [23], [5], [7], [6]. Moreover, similar to studies in the literature [3], [4], [23], [5], [6] on k-regret queries, we computed the skyline ﬁrst and then identiﬁed k points from it. The information about the datasets is summarized in Table VI. • Anti-correlated (synthetic dataset). The dataset is created by using the dataset generator in [2]. It is a 6-dimensional anti-correlated dataset with 10,000 data points and 5,531 skyline points."
2103.11630,"data, dataset",153,,,"To address these problems, Nanongkai et al. [3] introduced the k-regret minimization query. Given a positive integer k and a dataset D, it returns a set S of k data points that can minimize the user’s maximum regret ratio under a class of utility functions. The k-regret minimization query integrates the merits of top-k and skyline queries. It does not ask the user for any utility function and outputs a result with controllable i.e., only k data points. Due to the NP-hardness of size, the problem, in [3], RDPGREED was proposed based on the Ramer-Douglas-Peucker greedy framework. The greedy strategy has also been used in most follow-up studies, e.g., [4], [5], [6], [7], [8], [9]."
2103.11630,"data, dataset",160,,,"In terms of CPU time, by increasing k, the response time of all the algorithms increases since more data points are selected. GREEDY, HD-RRMS and EPS-KERNEL run much slower than the other algorithms on all the datasets. Under different settings of (cid:15) and λ, STOCPRESGREED constantly outperforms PRESGREED and achieves up to 10 times speedup. The HD-GREEDY algorithm is close to STOCPRESGREED, but it consumes a large amount of memory. Therefore, we omit it for the Movie and Weather datasets. The performance of STOCPRESGREED is close to SPHERE and even better in the Household dataset. However, SPHERE cannot work under the case of k < d. As can be observed, for different values of (cid:15) and λ, the maximum regret ratio reported by STOCPRESGREED is very close to that of PRESGREED but with much better performance in CPU time."
2103.11630,"data, dataset",174,,,"As shown, when k is small, the maximum regret ratios of our proposed algorithms especially STOCPRESGREED are worse than the others. The reason is that STOCPRESGREED uses greedy policy and sampling to select points. But with k increase, they are very close to each other except for the NBA and Household datasets. For the NBA dataset, our STOCPRESGREED performs better than HD-based algorithms and is comparable to others. In addition, when we need to select fewer data points in high-dimensional datasets (i.e., k < d), such as Movie and Weather, the SPHERE algorithm does not work under this case, but our STOCPRESGREED algorithm can still perform well. PRESGREED and STOCPRESGREED with different values of (cid:15) and λ are very close to each other and decrease with k. For the real datasets, the maximum regret ratio is much smaller than that on the anti-correlated dataset, and they generally decrease with the values of k."
2103.11630,"data, dataset",206,,,"Even though the PRESGREED algorithm can efﬁciently compute the result with theoretical guarantees, it is still not scalable when n is large. To scale for large datasets, a common method uses a sampling-based technique to reduce the number of data points evaluated [21] and returns a result with tight theoretical guarantees. Therefore, we propose a sampling-based method, STOCPRESGREED, which extends the proposed PRESGREED algorithm. The algorithm details are shown in Algorithm 2, which replaces lines 3-4 in Algorithm 1 with lines 1-3. The difference between STOCPRESGREED and PRESGREED is that PRESGREED ﬁnds a point from D\Si−1 directly, while STOCPRESGREED samples a subset R randomly and then ﬁnds the point in R which contributes the most to the value of the happiness ratio. To bound the quality of returned results for a sampling-based approach, a critical issue is to carefully choose the sample size. To do this, in each iteration of the algorithm, we sample a set R of size s = n λ−1+(cid:15) ) uniformly at random, where (cid:15) > 0 and λ ≥ 1, which is the sample factor."
2103.11630,"data, dataset",250,,,"In this paper, the main objective is to speed up existing greedy algorithms with theoretical guarantees. The developed techniques and theorems can also be used by the existing research if a similar greedy framework is involved. To simplify the theoretical analysis, we adopt the concept of the happiness ratio [14], which captures how happy a user can be after seeing k representative data points instead of the whole dataset. We show that minimizing the maximum regret ratio is equal to maximizing the user’s minimum happiness ratio. The minimum happiness ratio function is a monotone set function, which is composed of a class of submodular functions corresponding to a class of utility functions of the regret minimization query. We know that the function for computing the maximum of a class of submodular functions is generally not submodular, which means that the minimum happiness ratio function is also not submodular. Therefore, executing the k-regret minimization query can be regarded as a monotone nonsubmodular maximization problem with cardinality constraint (i.e., return k data points). By adopting the happiness ratio in RDPGREED, we introduce the PRESGREED algorithm and conduct a rigorous theoretical analysis of the approximation ratio. Moreover, to further speed up the computation, we develop a sampling-based approach, STOCPRESGREED, to achieve a good tradeoff between query processing time and result quality, also with theoretical guarantees."
2103.11630,"data, dataset",254,,,"For end users, identifying the most desired data points from a large dataset to support multi-criteria decision making is an essential functionality in many domains. For instance, when online shopping, it is difﬁcult for a user to view all products. A possible solution is to show the user some representative products based on certain criteria, e.g., the user’s preferences. In the literature, top-k queries [1] and skyline queries [2] are two well-studied and commonly used tools that can effectively reduce the output size. However, both queries suffer from some drawbacks. In the top-k query, we assume that the user can provide a utility function (i.e., user’s preference function) in advance. Then, it can output k data points with the largest scores. In real applications, users usually do not have a speciﬁc function in mind. A skyline query does not ask the user for any utility function. Instead, it retrieves all the data points that are not dominated by others in the dataset. For instance, Table I shows a skyline query toy example. It consists of the 16 skyline NBA players found from the 2009 regular season by considering 3 player attributes, i.e., points, rebounds and steals1. These players do not dominate each other, such as Kevin Durant and LeBron James, where Kevin Durant is"
2103.11630,database,8,,,"database,” in SIGMOD, 2016."
2103.11630,database,10,,,"set in database,” in ICDE, 2019."
2103.11630,database,41,,,"[25] D. Papadias, Y. Tao, G. Fu, and B. Seeger, “Progressive skyline computation in database systems,” TODS, vol. 30, no. 1, pp. 41–82, 2005."
2103.11630,database,45,,,"[1] I. F. Ilyas, G. Beskales, and M. A. Soliman, “A survey of top-k query processing techniques in relational database systems,” CSUR, vol. 40, no. 4, pp. 11:1–58, 2008."
2103.11630,database,124,,,"0 0.2 0.4 0.6 0.8 123456Maximum regret ratiokHD-RRMSPresGreedSPG1SPG4 0 0.2 0.4 0.6 0.8 123456Maximum regret ratiokHD-RRMSPresGreedSPG1SPG4 0 0.2 0.4 0.6 0.8 123456Maximum regret ratiokHD-RRMSPresGreedSPG1SPG4 0 0.2 0.4 0.6 0.8 123456Maximum regret ratiokHD-RRMSPresGreedSPG1SPG4 0 0.2 0.4 0.6 0.8 123456Maximum regret ratiokHD-RRMSPresGreedSPG1SPG4Maximum regret ratioSphere 5 6 7 8 9 10d 10 15 20 25 30 35k 0.4 0.6 0.8 1[27] P. Ciaccia and D. Martinenghi, “Reconciling skyline and ranking queries,” in Proceedings of the VLDB Endowment, 2017, p. 1454–1465. [28] ——, “Flexible skylines: Dominance for arbitrary sets of monotone functions,” ACM Transactions on Database Systems (TODS), vol. 45, no. 4, 2020."
2103.11630,database,306,,,"on the (cid:15)-kernel and [12], [34] via coresets and hitting sets. Faulkner et al. [5] and Qi et al. [7] extended the linear utility functions to CONVEX, CONCAVE, CES utility functions and multiplicative utility functions for k-regret queries. Zeighami and Wong [35] proposed the metric of the average regret ratio to measure user satisfaction. To reduce the bounds of the regret ratio, Nanongkai et al. [23] and Xie et al. [36] combined user interactions into the process of selection. Additionally, based on regret minimization concepts, [13], [37], [38], [14] focus on compact maxima, rank regret representative problems, uniﬁed algorithms for different aggregate norms and min-size versions of the regret minimization query. To reduce the bounds of regret ratio, Nanongkai et al. [23] combine user’s interactions into the process of selection. Moreover, Xie et al. [36] provide a strongly truthful interactive mechanism to leverage the regret ratios using true database tuples instead of artiﬁcial ones and provide provable performance guarantees. Zeighami and Wong [39] propose the metric of average regret ratio to measure user’s satisfaction and further develop efﬁcient algorithms to solve it [35]. Also, based on regret minimization concepts, [13], [37] focus on the compact maxima and rank regret representative problems, respectively. Moreover, the concept of the regret ratio is also adopted to solve the problems in the machine learning area, e.g., multi-objective submodular function maximization [40]."
2103.11630,dataset,1,,,Dataset
2103.11630,dataset,4,,,TABLE VI DATASET STATISTICS
2103.11630,dataset,7,,,the datasets by varying k. We omit
2103.11630,dataset,22,,,"is a 21-dimensional dataset with 100,000 ratings (1-5) from 943 users on 1,682 movies along with 470 skyline ratings."
2103.11630,dataset,23,,,"• Finally, through comprehensive experiments over synthetic and real-world datasets, we demonstrate the efﬁciency and effectiveness of the proposed methods."
2103.11630,dataset,30,,,"Though our STOCPRESGREED algorithms sacriﬁces a little regret ratio, it can work under any circumstances, especially for the large high-dimensional datasets, e.g., the Weather dataset."
2103.11630,dataset,37,,,"Regret ratio. Given a dataset D, a subset S of D and a utility function u, the regret ratio of S, represented as RD(S, u), is deﬁned as"
2103.11630,dataset,39,,,Fig. 5. Performance comparisons. (a) and (b) show the maximum regret ratios and CPU times of the STOCPRESGREED for different values of (cid:15) and λ on the anti-correlated datasets.
2103.11630,dataset,40,,,"• STOCPRESGREED. The sampling-based algorithm pro In this section, we present the results of a comprehensive performance study on both synthetic and real-world datasets to evaluate the efﬁciency and effectiveness of the techniques proposed in this paper."
2103.11630,dataset,47,,,"Fig. 2. Performance comparisons. (a), (c) and (b), (d) show the maximum regret ratios and CPU times of all the algorithms for different values of d and n on the anti-correlated dataset respectively."
2103.11630,dataset,47,,,"hold dataset here, because the regret ratios of the PRESGREED, the STOCPRESGREED and SPHERE algorithms are almost same. Since the algorithms are conducted over the skyline points, the larger the skyline size is, the more effective our STOCPRESGREED algorithm will be."
2103.11630,dataset,54,,,"Problem Deﬁnition. Given a dataset D, a positive integer k and a class of linear utility functions U, we attempt to efﬁciently answer the k-regret minimization query, which ﬁnds a subset S∗ of D containing at most k points such that the minimum happiness ratio is maximized, i.e.,"
2103.11630,dataset,57,,,"(c) NBA Fig. 3. Distributions of regret ratios. (a) shows the regret ratios on 4 datasets of PRESGREED without sampling, and (b)-(e) show the regret ratio distributions of 20 repeated experiments of the STOCPRESGREED algorithm at k = 25 on these datasets."
2103.11630,dataset,68,,,"better in points and rebounds than LeBron James but not in steals. However, in real scenarios, the exact number of skyline results is generally large and uncontrollable and cannot be foreseen before the whole dataset is accessed. In addition, the output size of skyline queries usually increases rapidly with the dimensionality. Consequently, the user will still face many choices."
2103.11630,dataset,69,,,"• The NBA3. NBA dataset is extracted from NBA players’ game statistics from 1946 to 2009 with 21,961 records and 164 skyline records, each of which has 8 dimensions. • Household4. It is a 6-dimensional dataset with 127,932 records and 49 skyline records, each of which represents the percentage of an American family’s annual income on different types of expenditures."
2103.11630,dataset,70,,,"The evaluation of (cid:15) and λ. In STOCPRESGREED, a larger (cid:15) or λ denotes a smaller sample size. Here, we evaluate the performance of STOCPRESGREED by varying (cid:15) and λ on the anti-correlated dataset. The results are shown in Figure 5(a)(b). As we can observe, the maximum regret ratio does not"
2103.11630,dataset,82,,,"Road map. The rest of the paper is organized as follows. We brieﬂy introduce the problem to be studied and the related techniques in Section II. In Section III, we provide the theoretical analysis as well as the sampling-based method. We demonstrate the efﬁciency and effectiveness of the proposed framework in Section IV on both synthetic and real-world datasets and introduce the related works in Section V. Finally, we conclude the paper in Section VI."
2103.11630,dataset,92,,,"For STOCPRESGREED, we only use (cid:15) = 0.01, λ = 1.01 and (cid:15) = 0.1, λ = 1.1 to demonstrate its performance named SP G1 and SP G4, respectively (similar setting as in Example 3). From Lemma 4 and Theorem 2, the adopted λ values are only slightly larger than 1, which can greatly reduce the dataset size, while there is little change in the approximation ratio. This also shows the advantage of our STOCPRESGREED algorithm."
2103.11630,dataset,121,,,"has a side effect of performing many function evaluations via linear programming (LP). Generally, the number of function evaluations conducted by RDPGREED is nk, where n is the cardinality of the whole dataset. In real applications, k is usually much smaller than the size of the skyline set. Thus, the skyline points are considered candidate points, whose setting has also been widely adopted by follow-up studies. Even if n is the cardinality of the skyline, the cost of LPs still signiﬁcantly affects the efﬁciency of the k-regret minimization query. Therefore, due to the time complexity, RDPGREED cannot scale well for larger n and k."
2103.11630,dataset,144,,,"Maximum regret ratio and CPU time by varying k. In Figure 1, we report the maximum regret ratio and CPU time on all the result of the GREEDY algorithm on the Weather dataset because it takes several days to obtain a solution. For the EPS-KERNEL algorithm, the result size of this algorithm is much larger than k, so we randomly return k points as the solution as in [6]. In high-dimensional datasets such as Movie and Weather, the HD-RRMS, HD-GREEDY and EPS-KERNEL algorithms all require too much memory to compute the solutions. Thus, we omit their performances in the Movie and Weather datasets. When k < d, HD-GREEDY and SPHERE do not work [13], [6], so we report their performance for k ≥ d."
2103.11630,dataset,164,,,"As an important operator, the k-regret minimization query is investigated to provide users with high-quality results from a large dataset. The existing solution is efﬁcient, but there is still much space left for improvement, especially for larger datasets. In addition, the approximation ratio of the returned results of the greedy framework is not discussed in the literature. In this paper, we conduct the ﬁrst theoretical analysis and provide an approximation guarantee for the previous RDPGREED greedy framework by utilizing the proposed happiness ratio concept. To reduce the evaluation cost and further speed up the processing, a sampling-based method, STOCPRESis proposed that provides an (1 − e− (1−(cid:15))(k−1)γ ) GREED, approximation ratio. Moreover, careful analysis is presented to demonstrate the sample size required. Experiments over real-world and synthetic datasets were conducted to verify the advantages of the proposed methods."
2103.11630,dataset,207,,,"Abstract—Assisting end users to identify desired results from a large dataset is an important problem for multi-criteria decision making. To address this problem, top-k and skyline queries have been widely adopted, but they both have inherent drawbacks, i.e., the user either has to provide a speciﬁc utility function or faces many results. The k-regret minimization query is proposed, which integrates the merits of top-k and skyline queries. Due to the NP-hardness of the problem, the k-regret minimization query is time consuming and the greedy framework is widely adopted. However, formal theoretical analysis of the greedy approaches for the quality of the returned results is still lacking. In this paper, we ﬁrst ﬁll this gap by conducting a nontrivial theoretical analysis of the approximation ratio of the returned results. To speed up query processing, a sampling-based method, STOCPRESGREED, is developed to reduce the evaluation cost. In addition, a theoretical analysis of the required sample size is conducted to bound the quality of the returned results. Finally, comprehensive experiments are conducted on both real and synthetic datasets to demonstrate the efﬁciency and effectiveness of the proposed methods."
2103.11630,dataset,214,,,"Proof. We prove Lemma 2 by constructing a counterexample. According to [16], a function f is submodular if it satisﬁes f (S1) + f (S2) ≥ f (S1 ∪ S2) + f (S1 ∩ S2) for ∀S1, S2 ⊆ D. Given a dataset D = {(0, 1), (1, 0)} and a function class U with two linear functions (cid:104)1, 0(cid:105) , (cid:104)0, 1(cid:105), let S1 = {(0, 1)} and S2 = {(1, 0)}. Based on the deﬁnition, we have HD(D, U) = 1 and HD(S1, U) = HD(S2, U) = HD(∅, U) = 0. Thus, 1 = HD(S1 ∪ S2, U) + HD(S1 ∩ S2, U) (cid:2) HD(S1, U) + HD(S2, U) = 0. Therefore, the function is not submodular and Lemma 2 is correct."
2103.11630,dataset,239,,,"Unfortunately, submodularity does not hold for HD(S, U), where U represents a class of utility functions. We show that the claim is correct by constructing a counterexample. According to [16], a function f is submodular if it satisﬁes f (S1) + f (S2) ≥ f (S1 ∪ S2) + f (S1 ∩ S2) for ∀S1, S2 ⊆ D. Given a dataset D = {(0, 1), (1, 0)} and a function class U with two linear functions (cid:104)1, 0(cid:105) , (cid:104)0, 1(cid:105), let S1 = {(0, 1)} and S2 = {(1, 0)}. Based on the deﬁnition, we have HD(D, U) = 1 and HD(S1, U) = HD(S2, U) = HD(∅, U) = 0. It means 1 = HD(S1 ∪ S2, U) + HD(S1 ∩ S2, U) (cid:2) HD(S1, U) + HD(S2, U) = 0. Therefore, the function HD(S, U) is not submodular."
2103.11630,dataset,257,,,"Since S is a subset of D, given a utility function u, it is obvious that maxp∈S u(p) ≤ maxp∈D u(p) and the regret ratio ranges from 0 to 1. The user with utility function u will be happy if the regret ratio approaches 0 because the maximum utility of S is close to the maximum utility of D. Linear utility function. Assume there are some nonnegative real values {v[1], v[2], · · · , v[d]}, where v[i] denotes the user’s preference for the ith dimension. Then, a linear utility function can be represented by these nonnegative reals and u(p) = (cid:80)d i=1 v[i] · p[i]. A linear utility function can also be expressed by a vector, i.e., v =< v[1], v[2], ..., v[d] >, so the utility of point p can be expressed by the dot product of v and p, i.e., u(p) = v · p. Maximum regret ratio [3]. Given a dataset D, a subset S of D and a class of utility functions U. The maximum regret ratio of S, represented as RD(S, U), is deﬁned as"
2103.11630,dataset,275,,,"Maximum regret ratio and CPU time by varying d and n. For large d or n, GREEDY and HD-RRMS are too time consuming. Therefore, we omit the evaluation of GREEDY and some results of HD-RRMS here. In Figure 2(a)-(b), we conduct the experiments on the anti-correlated datasets by varying d. When d increases, the maximum regret ratio and CPU time of all algorithms increase since more dimensions need to be processed. With the increase in d, the maximum regret ratios of different algorithms are very close to each other. However, the CPU times of PRESGREED, EPS-KERNEL and RMS-HS are always larger than that of STOCPRESGREED for different values of (cid:15) and λ. This is because PRESGREED requires more function evaluations, EPS-KERNEL needs more time to compute available coreset and RMS-HS consumes plenty of time to solve a large number of hitting-set problems. The CPU time of HD-RRMS increases very quickly and the performance drops quickly, especially in high-dimensional spaces. HD-GREEDY performs better for d < 6, but the query time is longer compared with STOCPRESGREED and SPHERE when d is large. This is because the discretized matrix in HDGREEDY can be large in high-dimensional spaces. The CPU time of STOCPRESGREED is close to SPHERE, but SPHERE has the aforementioned drawback when k < d. We vary n on the anti-correlated dataset and the results are shown in Figure 2(c)-(d). Similar trends can be observed among the algorithms."
2103.11630,dataset,316,,,"Regret ratio distribution. Since STOCPRESGREED is a sampling-based method, we report the regret ratio distribution here to demonstrate the stability of STOCPRESGREED under different (cid:15) and λ settings. We run the algorithms 20 times with k = 25 and record their distribution on all the datasets. The results are shown in Figure 3. Figure 3(a) only shows the regret ratios of the PRESGREED and SPHERE algorithms, which are the algorithms without sampling. The other nonsampling methods are quite time consuming, so we omit them. Figure 3(b)(e) shows the regret ratio distribution for STOCPRESGREED. It records the number of times that the regret ratio has fallen into that interval. The size of each interval is set to approximately 0.05. We can see that most of the regret ratios of STOCPRESGREED are very close to those of PRESGREED. The difference in the regret ratio between our STOCPRESGREED algorithm and the PRESGREED algorithm does not exceed 0.1. For the anti-correlated and Movie datasets, the difference does not exceed 0.05. In addition, the difference of the regret ratio between our STOCPRESGREED algorithm and the SPHERE algorithm is similar to that of PRESGREED on the anti-correlated and NBA datasets. However, due to the high dimensions of the Movie and Weather datasets, SPHERE either does not work or performs poorly when computing the k results. Our STOCPRESGREED algorithm can still provide stable performance. We do not show the results on the House Fig. 1. Performance comparisons. (a)-(e) show the maximum regret ratios of all the algorithms for different values of k, and (f )-(j) show the CPU times for different values of k."
2103.11630,dataset,341,,,"[3] proposed the k-regret minimization problem. The problem is NP-hard, and a greedy strategy is proposed. However, the developed approach suffers from efﬁciency issues, and cannot scale well for large datasets. The following studies attempt to solve the problem from different perspectives. Peng et al. [4] attempted to reduce the candidate points from the whole skyline points to a small candidate set by using geometric properties. Cao et al. [11] and Agarwal et al. [12] proposed the (cid:15)-kernel algorithm independently, which can keep the maximum regret ratio at most (cid:15) with an output size of O((cid:15)− d−1 2 ). However, the output sizes of these algorithms are uncontrollable. They made output size from 2 ) to k. Then the upper bound was O(k− 2 O((cid:15)− d−1 d−1 ) [6]. Agarwal et al. [12] also proposed the hitting-set algorithm, which returns O(k log k) points. Asudeh et al. [13] interpreted the k-regret query in a d-dimensional dataset as a discretized matrix min-max problem and proposed two algorithms with upper bounds. Moreover, Xie et al. [6] designed the state-ofthe-art Sphere algorithm whose upper bound on the maximum regret ratio was asymptotically optimal and restriction-free for datasets of any dimensionality. These bounds or theoretical results focus on the regret ratio instead of approximation guarantees. Qiu et al. [8] and Dong et al. [9] also provided the sampling techniques for 1-RMS and k-RMS queries, respectively. However, the proposed method in [8] is a special case of ours, where the sample factor equals 1, i.e., λ = 1. Additionally, they both do not provide any approximation guarantees."
2103.11630,dataset,345,,,"Due to the inherent limitations of top-k and skyline queries, Nanongkai et al. [3] proposed the k-regret minimization problem. The problem is NP-hard, and a greedy strategy is proposed. However, the developed approach suffers from efﬁciency issues, and cannot scale well for large datasets. The following studies attempt to solve the problem from different perspectives. Peng et al. [4] attempted to reduce the candidate points from the whole skyline points to a small candidate set by using geometric properties. Cao et al. [11] and Agarwal et al. [12] proposed the (cid:15)-kernel algorithm independently, which can keep the maximum regret ratio at most (cid:15) with an output size of O((cid:15)− d−1 2 ). However, the output sizes of these algorithms are uncontrollable. They made output size from 2 ) to k. Then the upper bound was O(k− 2 O((cid:15)− d−1 d−1 ) [6]. Agarwal et al. [12] also proposed the hitting-set algorithm, which returns O(k log k) points. Asudeh et al. [13] interpreted the k-regret query in a d-dimensional dataset as a discretized matrix min-max problem and proposed two algorithms with upper bounds. Moreover, Xie et al. [6] designed the state-ofthe-art Sphere algorithm whose upper bound on the maximum regret ratio was asymptotically optimal and restriction-free for datasets of any dimensionality. These bounds or theoretical results focus on the regret ratio instead of approximation guarantees. Qiu et al. [8] and Dong et al. [9] also provided the sampling techniques for 1-RMS and k-RMS queries, respectively. However, the proposed method in [8] is a special case of ours, where the sample factor equals 1, i.e., λ = 1."
2103.12866,data,15,,,y(f )(ω) depends only on the training data S.
2103.12866,data,25,,,"the sampled training data S = {y(t)(ω)}N probability at least 1 − δ, the inequality"
2103.12866,data,32,,,"In this paper we will derive a PAC-Bayesian bound for learning stochastic LTI systems without inputs. More precisely, we will assume that data generating process y is the output of"
2103.12866,data,46,,,is the normalization term. Note that ˆρ∗(f ) is a function of the empirical loss ˆL(cid:96) y(f )(ω) evaluated for the training data S = {y(t)(ω)}N
2103.12866,data,56,,,"[5] P. E. Caines. Linear Stochastic Systems. John Wiley and Sons, 1988. [6] G. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In UAI. AUAI Press, 2017."
2103.12866,data,88,,,"2) Find probability distribution which minimizes the PACBayesian upper bound for the average generalization error. Since this upper bound involves the empirical loss, we will tend to assign higher probability to models which ﬁt well the training data. However, this will be tempered by the need to take into account the prior distribution and the tuning parameters. This step can be performed using sampled data. The thus obtained probability distribution is analogous to the posterior distribution in the Bayesian setting."
2103.12866,data,88,,,"y,N (f ) : Ω → R is a random variable, and for ω ∈ Ω, ˆL(cid:96) y,N (f )(ω) corresponds to the average prediction error produced by f when applied to the samples y(t)(ω) for t = 0, . . . , N successively, that is, if the training data is S = {y(t)(ω)}N −1"
2103.12866,data,106,,,"measures the mean difference between the actual process y(t) and the predicted value ˆyf (t | s) bases on {y(τ )}t−1 τ =s. However, the most convenient measure of the predictive power of a model, as it depends on the prediction horizon t − s. In practice, the prediction horizon tends to increase with the increase of the number of available data points. For this reason, it is more convenient to consider the prediction error as the beginning of the prediction horizon goes to inﬁnity. Intuitively, we expect"
2103.12866,data,120,,,"The problem of ﬁnding a tight PAC-Bayesian upper bound for LTI systems seems to be a challenging problem. In fact, the ﬁrst PAC-Bayesian upper bound for linear regression [7] with i.i.d. data had the same drawback, namely, the part of the upper bound on the term which corresponds to Ψ(cid:96),π(λ, N ) did not converge to zero as N → ∞. This drawback for eliminated in [17] for linear regression with i.i.d data, but not for linear regression with non i.i.d. data. In the light of this development, the conservativity of our error bound is not surprising."
2103.12866,data,129,,,"By tuning the parameters λ and the prior π the cost function (12) will be different, leading to different choices of ˆρ and model f∗, i.e., to different learning algorithms. The prior π encodes our hypothesis on the model structure. The parameter λ regulates the extent we care about ﬁtting training data. For instance, small λ mean that we would like the posterior ˆρ to be close to the prior (we insist on a certain model structure) and we care less about how well the models ﬁt the training data. In contrast, large λ means that we care less about the model having a certain structure, and more about it ﬁtting the training data."
2103.12866,data,135,,,"The goal of learning is to ﬁnd a model from a set of possible models with the smallest possible prediction error, using a ﬁnite portion of the sample path of y. This can be achieved through minimizing a cost function which involves the so called empirical error. Assume that we would like to learn a model from the time series S = {y(t)(ω)}N t=0, for some ω ∈ Ω. The data S represents a ﬁnite portion of sample path {y(t)(ω)}t∈Z of y, and N + 1 represents the number of data points used for the learning problem. Let us deﬁne ﬁrst the concept of empirical (error) loss."
2103.12866,data,163,,,"Lemma III.2 means that trying to ﬁnd a model F based on sampled data is equivalent to ﬁnding the element of F which corresponds to a realization of y. In fact, even if we cannot ﬁnd the model with the smallest generalization error exactly, this can still be interpreted as ﬁnding a stochastic LTI system output of which is close to y. More precisely, if the parameter set Θ is a topological space and if Σ(θ) is continuous in θ and if we ﬁnd θ ∈ Θ such that θ is close to θ0, then the matrices of Σ(θ) = ( ˆA, ˆK, ˆC) will be close to Σ(θ0) = (A0, K0, C0). It then follows that the output of the stochastic LTI system ( ˆA, ˆK, ˆC, e) will be close to y."
2103.12866,data,177,,,"Lastly u(cid:96),π(λ, N ) can be computed according to Section III-B. Now we have everything necessary to compute the PACbayesian upper bound, the right hand side of (11). For λ = 100, the upper-bound ends up being 4.48, while Ef ∼ ˆρL(cid:96) y ≈ 2.03. Figure 3 showcases the upper bound for various number of training samples N and real values λ > 0. Recall, that in Gibbs distribution (13), λ plays a role of weighting the importance of data versus the importance of the prior distribution, larger λ will minimise Ef ∼ρ ˆL(cid:96) y, however it will increase KL(ρ(cid:107)π). Therefore, it is natural that there exists λ that compromises empirical data ﬁt, with prior assumptions. For this speciﬁc example, realisation and prior, λ = 5 yields the smallest upper bound for all N ."
2103.12866,data,188,,,"Despite an impressive body of literature, PAC-Bayesian error bounds are not available for learning LTI systems. One of the reasons for this is that most of the existing literature dealt with bounded and Lipschitz loss functions [1], [6], [18], which is typical for classiﬁcation problem, but not for regression problems. In [7] PAC-Bayesian bounds for linear regression with a quadratic loss function was developed, later this bound was improved and extended to non i.i.d. data in [17]. In particular, in [17] the derived PAC-Bayesian error bound for non i.i.d linear regression problem was applied to learning ARX models. Note that in [17] control inputs were allowed, while in the current paper we consider only autonomous systems. Since ARX models without inputs correspond to AR models, and the latter class represents a special case of autonomous stochastic LTI state-space representations, the present paper can be viewed as an extension of [17] to stochastic LTI statespace representations."
2103.12866,"data available, data",85,,,"As a rule, algorithms for learning LTI models [14] chose the model parameters in such a manner that the empirical error is small, when evaluated for the data available for learning. However, the fact that a certain model renders the empirical error small does not necessarily mean that the generalization error will be small too. In fact, one of the main theoretical challenge is to prove this implication for certain classes of learning problems and algorithms."
2103.15335,code,51,,,"The code of PPLM can only condition on a single word piece, so we need to remove the rare words that contain multiple word pieces. We ﬁlter out the input prompt in the test set if PPLM cannot condition on any word in the randomly sampled topics."
2103.15335,code,66,,,"GPT2 ﬁne-tuned on English Wikipedia sometimes generate sentences containing special characters (e.g., UTF-8 characters for other languages), which crowdsourcing workers might not understand. Thus, we ﬁlter out the input prompt in the STSb for human evaluation if the input prompt or the continuation generated by any method contains a character that cannot be encoded using the ASCII code."
2103.15335,data,17,,,This work was supported in part by the Center for Data Science and the Center for Intelligent
2103.15335,data,25,,,"Tong Niu and Mohit Bansal. 2018. Polite dialogue generation without parallel data. Transactions of the Association for Computational Linguistics, 6:373–389."
2103.15335,data,44,,,"Arthur P Dempster, Nan M Laird, and Donald B Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1–22."
2103.15335,"data available, data",61,,,"1The framework is ﬂexible. For example, the GPT2 encoders in the two components could be shared. Besides topics, the option generator could be extended to predict likely attributes in the continuation such as positive sentiment and event frames (Tu et al., 2019) if the corresponding label data are available in the training corpus."
2103.15335,dataset,2,,,3.1 Datasets
2103.15335,dataset,60,,,"Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, and Mohit Iyyer. 2020. STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6470–6484, Online. Association for Computational Linguistics."
2103.15335,dataset,178,,,"The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphrases, our interactive writing assistant is especially helpful when users do not know the exact phrases they want to see or when the given keyphrase extractor does not detect the desired topics."
2103.15335,download,65,,,"Microsoft plans to expand the coverage of MSN Messenger in the United States.. nbc.org; November 8, 2008. In its The Windows Messenger 6 web app now has a new web service for mobile devices to download the Windows product. Schools that fail to meet state goals for three years in a row must offer tutoring in addition to transfers."
2103.15335,github,3,,,8https://github.com/uber-research/PPLM
2103.15335,github,64,,,"PPLM uses the default hyperparameters for conditioning on a bag of words in its GitHub repository8. We try several different hyperparameters in PPLM and also try to apply PPLM to the original GPT2 with 117M parameters and to the GPT2 that is ﬁne-tuned on Wikipedia. They produce similar relevancy and perplexity, which are signiﬁcantly worse than ours in automated evaluation."
2103.15335,"github, code available, code",121,,,"In experiments, we demonstrate that our system recommends high-quality topics and often generate sentences that follow the chosen topics. We compare our option generator with global topic models such as LDA (Blei et al., 2001) or local topic models such as clustering the words in the input prompt. The results show that the proposed method generates signiﬁcantly more topics that are plausible and promote the narrative. Moreover, we compare our conditional text generator with PPLM (Plug and Play Language Models) (Dathathri et al., 2020) and demonstrate that our generation is more ﬂuent and relevant to the chosen topics. Our code is available at https://github.com/iesl/interactive_LM."
2103.15335,"github, python, download, code",63,,,We randomly select 8 examples with less than 130 letters from STSb as our input prompts. The topics of different option generators are visualized in Table 8. The continuations of different text generators are visualized in Table 9. You can download our code from https://github.com/iesl/ interactive_LM and test our models using your own prompts via IPython notebook.
2103.16349,"data available, data, dataset",38,,,"All above settings are consistent with the Informer paper. Note that we eliminate the dataset Weather that is also used in the paper, since only raw data is available and the preprocessing operations are unclear."
2103.16349,"data, dataset",122,,,"Long sequence time-series forecasting (LSTF) has become increasingly popular for its wide range of applications. Though superior models have been proposed to enhance the prediction effectiveness and efﬁciency, it is reckless to neglect or underestimate one of the most natural and basic temporal properties of time-series. In this paper, we introduce a new baseline for LSTF, the historical inertia (HI), which refers to the most recent historical data-points in the input time series. We experimentally evaluate the power of historical inertia on four public real-word datasets. The results demonstrate that up to 82% relative improvement over stateof-the-art works can be achieved even by adopting HI directly as output."
2103.16349,"data, dataset",196,,,"However, phase is much more tricky. On the one hand, longer time series provide more evident periodic patterns that can not be reﬂected in short horizons. This increases the chance that the HI be of similar phase as the prediction target, especially in the case that the prediction length is an exact integer multiple of the time series’ period when their is any. On the other hand, HI could also badly hurt the prediction results when 1) there is no periodic pattern; 2) the periodic pattern is not included in historical data; 3) or the historical data is in opposite phase as the prediction target. The multivariate prediction results on ETTm1 dataset serves a good evidence of above statements. Since the data was sampled by 15-minute, a prediction length of 24 or 48 is too short to reﬂect periodic patterns. Therefore, HI performs much worse than SOTA models. However, for predicting length of 96, 288 and 672, where the 1-day (4×24 data-points) period is well covered, the relative improvement surges."
2103.16349,dataset,1,,,Dataset
2103.16349,dataset,4,,,3.1 Datasets and Metrics
2103.16349,dataset,10,,,Dataset Metric MSE MAE MSE MAE MSE MAE MSE MAE
2103.16349,dataset,11,,,Statistics of above datasets can be found in Table 1.
2103.16349,dataset,14,,,Table 2: Summary of univariate long sequence time-series forecasting comparison results. Dataset
2103.16349,dataset,14,,,Table 3: Summary of multivariate long sequence time-series forecasting comparison results. Dataset
2103.16349,dataset,39,,,"the task of predicting 168 and 720 steps ahead on ETTh2 dataset, competitors’ best MSE are 3.242 and 3.467, HI reduces them to 0.572 and 0.635, bringing in up to 82% relative improvement."
2103.16349,dataset,50,,,"In this paper, we ﬁrst address this issue by providing an experimental evaluation of the proposed baseline HI and SOTA models and on a variety of public real-world datasets, and then make a comprehensive discussion on why HI is powerful and how we can beneﬁt from HI."
2103.16349,dataset,58,,,"In this paper we propose a baseline for LSTF, named HI. It directly takes the most recent time steps in the input as output. Extensive experiments in four public real-world datasets validate the strength of HI across different prediction lengths. We hope HI could serve as a basement and spark future LSTF research."
2103.16349,dataset,72,,,"Table 2 shows that in the task of predicting a single variable over time, HI outperforms SOTA models signiﬁcantly on ETTh1 and ETTm1 dataset. Informer and its variant almost dominate the ETTh2 dataset while DeepAR, Informer and HI claims part of the best results on the Electricity dataset. The relative improvement brought by HI can be up to 80% on MSE and 58% on MAE."
2103.16349,dataset,89,,,"3b39isHhy0tM0Vok0guVSfEmnImaNMww2knVRQnIaftcHg79dtPVGkmxaMZp9RPcF+wmBFsrNQdBfl9MDqPgtEkqFTdmjsDWiZeQapQoBFUvnqRJFlChSEca9313NT4OVaGEU4n5V6maYrJEPdp11KBE6r9fHbyBJ1aJUKxVLaEQTP190SOE63HSWg7E2wGetGbiv953czE137ORJoZKsh8UZxxZCSa/o8ipigxfGwJJorZWxEZYIWJsSmVbQje4svLpHVR89ya93BZrd8UcZTgGE7gDDy4gjrcQQOaQEDCM7zCm2OcF+fd+Zi3rjjFzBH8gfP5A06OkUE=</latexit><latexit sha1_base64=""oSaC2NKMvIcKI16EYEUyiy55KNM="">AAAB8nicbVBNS8NAEJ34WetX1aOXxSJ4kJKIoMeiFw8eKtgPaEPYbDbt0s1u2N1IS+jP8OJBEa/+Gm/+G7dtDtr6YODx3gwz88KUM21c99tZWV1b39gsbZW3d3b39isHhy0tM0Vok0guVSfEmnImaNMww2knVRQnIaftcHg79dtPVGkmxaMZp9RPcF+wmBFsrNQdBfl9MDqPgtEkqFTdmjsDWiZeQapQoBFUvnqRJFlChSEca9313NT4OVaGEU4n5V6maYrJEPdp11KBE6r9fHbyBJ1aJUKxVLaEQTP190SOE63HSWg7E2wGetGbiv953czE137ORJoZKsh8UZxxZCSa/o8ipigxfGwJJorZWxEZYIWJsSmVbQje4svLpHVR89ya93BZrd8UcZTgGE7gDDy4gjrcQQOaQEDCM7zCm2OcF+fd+Zi3rjjFzBH8gfP5A06OkUE=</latexit><latexit sha1_base64=""oSaC2NKMvIcKI16EYEUyiy55KNM="">AAAB8nicbVBNS8NAEJ34WetX1aOXxSJ4kJKIoMeiFw8eKtgPaEPYbDbt0s1u2N1IS+jP8OJBEa/+Gm/+G7dtDtr6YODx3gwz88KUM21c99tZWV1b39gsbZW3d3b39isHhy0tM0Vok0guVSfEmnImaNMww2knVRQnIaftcHg79dtPVGkmxaMZp9RPcF+wmBFsrNQdBfl9MDqPgtEkqFTdmjsDWiZeQapQoBFUvnqRJFlChSEca9313NT4OVaGEU4n5V6maYrJEPdp11KBE6r9fHbyBJ1aJUKxVLaEQTP190SOE63HSWg7E2wGetGbiv953czE137ORJoZKsh8UZxxZCSa/o8ipigxfGwJJorZWxEZYIWJsSmVbQje4svLpHVR89ya93BZrd8UcZTgGE7gDDy4gjrcQQOaQEDCM7zCm2OcF+fd+Zi3rjjFzBH8gfP5A06OkUE=</latexit><latexit sha1_base64=""oSaC2NKMvIcKI16EYEUyiy55KNM="">AAAB8nicbVBNS8NAEJ34WetX1aOXxSJ4kJKIoMeiFw8eKtgPaEPYbDbt0s1u2N1IS+jP8OJBEa/+Gm/+G7dtDtr6YODx3gwz88KUM21c99tZWV1b39gsbZW3d3b39isHhy0tM0Vok0guVSfEmnImaNMww2knVRQnIaftcHg79dtPVGkmxaMZp9RPcF+wmBFsrNQdBfl9MDqPgtEkqFTdmjsDWiZeQapQoBFUvnqRJFlChSEca9313NT4OVaGEU4n5V6maYrJEPdp11KBE6r9fHbyBJ1aJUKxVLaEQTP190SOE63HSWg7E2wGetGbiv953czE137ORJoZKsh8UZxxZCSa/o8ipigxfGwJJorZWxEZYIWJsSmVbQje4svLpHVR89ya93BZrd8UcZTgGE7gDDy4gjrcQQOaQEDCM7zCm2OcF+fd+Zi3rjjFzBH8gfP5A06OkUE=</latexit>xLx,1<latexit sha1_base64=""RltCjSlvB2Y4xd17cMVwJm/rECg="">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgIeFOBC2DNhYWEcyHJMext9lLluzuHbt7knDkV9hYKGLrz7Hz37hJrtDEBwOP92aYmRcmnGnjut9OYWV1bX2juFna2t7Z3SvvHzR1nCpCGyTmsWqHWFPOJG0YZjhtJ4piEXLaCoc3U7/1RJVmsXww44T6AvclixjBxkqPoyC7C0Zn3iQoV9yqOwNaJl5OKpCjHpS/ur2YpIJKQzjWuuO5ifEzrAwjnE5K3VTTBJMh7tOOpRILqv1sdvAEnVilh6JY2ZIGzdTfExkWWo9FaDsFNgO96E3F/7xOaqIrP2MySQ2VZL4oSjkyMZp+j3pMUWL42BJMFLO3IjLAChNjMyrZELzFl5dJ87zquVXv/qJSu87jKMIRHMMpeHAJNbiFOjSAgIBneIU3RzkvzrvzMW8tOPnMIfyB8/kDaeuQIw==</latexit><latexit sha1_base64=""RltCjSlvB2Y4xd17cMVwJm/rECg="">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgIeFOBC2DNhYWEcyHJMext9lLluzuHbt7knDkV9hYKGLrz7Hz37hJrtDEBwOP92aYmRcmnGnjut9OYWV1bX2juFna2t7Z3SvvHzR1nCpCGyTmsWqHWFPOJG0YZjhtJ4piEXLaCoc3U7/1RJVmsXww44T6AvclixjBxkqPoyC7C0Zn3iQoV9yqOwNaJl5OKpCjHpS/ur2YpIJKQzjWuuO5ifEzrAwjnE5K3VTTBJMh7tOOpRILqv1sdvAEnVilh6JY2ZIGzdTfExkWWo9FaDsFNgO96E3F/7xOaqIrP2MySQ2VZL4oSjkyMZp+j3pMUWL42BJMFLO3IjLAChNjMyrZELzFl5dJ87zquVXv/qJSu87jKMIRHMMpeHAJNbiFOjSAgIBneIU3RzkvzrvzMW8tOPnMIfyB8/kDaeuQIw==</latexit><latexit sha1_base64=""RltCjSlvB2Y4xd17cMVwJm/rECg="">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgIeFOBC2DNhYWEcyHJMext9lLluzuHbt7knDkV9hYKGLrz7Hz37hJrtDEBwOP92aYmRcmnGnjut9OYWV1bX2juFna2t7Z3SvvHzR1nCpCGyTmsWqHWFPOJG0YZjhtJ4piEXLaCoc3U7/1RJVmsXww44T6AvclixjBxkqPoyC7C0Zn3iQoV9yqOwNaJl5OKpCjHpS/ur2YpIJKQzjWuuO5ifEzrAwjnE5K3VTTBJMh7tOOpRILqv1sdvAEnVilh6JY2ZIGzdTfExkWWo9FaDsFNgO96E3F/7xOaqIrP2MySQ2VZL4oSjkyMZp+j3pMUWL42BJMFLO3IjLAChNjMyrZELzFl5dJ87zquVXv/qJSu87jKMIRHMMpeHAJNbiFOjSAgIBneIU3RzkvzrvzMW8tOPnMIfyB8/kDaeuQIw==</latexit><latexit sha1_base64=""RltCjSlvB2Y4xd17cMVwJm/rECg="">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgIeFOBC2DNhYWEcyHJMext9lLluzuHbt7knDkV9hYKGLrz7Hz37hJrtDEBwOP92aYmRcmnGnjut9OYWV1bX2juFna2t7Z3SvvHzR1nCpCGyTmsWqHWFPOJG0YZjhtJ4piEXLaCoc3U7/1RJVmsXww44T6AvclixjBxkqPoyC7C0Zn3iQoV9yqOwNaJl5OKpCjHpS/ur2YpIJKQzjWuuO5ifEzrAwjnE5K3VTTBJMh7tOOpRILqv1sdvAEnVilh6JY2ZIGzdTfExkWWo9FaDsFNgO96E3F/7xOaqIrP2MySQ2VZL4oSjkyMZp+j3pMUWL42BJMFLO3IjLAChNjMyrZELzFl5dJ87zquVXv/qJSu87jKMIRHMMpeHAJNbiFOjSAgIBneIU3RzkvzrvzMW8tOPnMIfyB8/kDaeuQIw==</latexit>Table 1: Statistics of dataset."
2103.16349,dataset,118,,,"To deal with the challenges of effectively modeling temporal correlations in long sequence and efﬁciently operating on long inputs and outputs, the state-of-the-art (SOTA) work Informer (Zhou et al., 2021) proposes a novel variant of Transformer (Vaswani et al., 2017) to reduce time and space complexity while maintaining prediction accuracy, which is indeed a breakthrough. Despite that the extensive experiments on ﬁve real-world datasets demonstrates Informer’s superiority to its baselines, the enhanced performance can be limited when considering the baseline of taking the most recent values in inputs as outputs, which can be referred to as the historical inertia (HI)."
2103.16349,dataset,121,,,"The modeling capacity of complex architectures is deﬁnitely valuable, but just in some cases the answer to the question can be so simple that might not be answered well when it is complicated by the model. It is desirable that a model’s structure or complexity can be adaptable to the input, which is also known as automated machine learning (AutoML) (e.g. Yao et al. 2018). A simple implementation could be when a speciﬁc dataset is given, the model may ﬁrst analyze its temporal pattens in a pre-processed way, and then score whether the basic model, HI or some median variants should be used for prediction."
2103.16349,dataset,141,,,"Basically, we follow the common practice in the community as described in Zhou et al. (2021). ∆ is ﬁxed as 1. Prediction length is set as [24, 48, 168, 366, 720] for ETTh1 and ETTh2, [24, 48, 96, 288, 672] for ETTm1 and [48, 168, 366, 720, 960] for Electricity. We split the ETT datasets into 12:4:4 and Electricity dataset into 15:3:4 for training, validation and test. Above implementation settings are consistent with the Informer paper. Since the method of HI doesn’t require training, when the dataset split is ﬁxed, the performance is ﬁxed. Thus, only one iteration is sufﬁcient to compute the ﬁnal results."
2103.16349,dataset,148,,,"The very ﬁrst observation is that MLP itself is a also a strong baseline, which outperforms HI and state-of-the-art models across almost all datasets and prediction lengths. Regardless of this point, we take MLP as a basic model, and evaluate the the ensemble of MLP and HI. We operate weighted summation over MLP’s and HI’s outputs to get the ﬁnal prediction. The weights of two models are set as 0.5/0.5. From Table 4 and Table 5, it could be concluded that this hybrid model can obtain better results in many cases, which is especially evidential for the task of univariate forecasting. MLP + HI brings up to 32% relative improvement over HI and 45% relative improvement over MLP on MSE, and 20%, 27% relative improvement on MAE."
2103.16349,dataset,301,,,"Long Sequence Time-series Forecasting: At time t, given a Lx-length time series as input, i.e., X (t) = {X1(t), ..., XLx (t)}, where Xi(t) = [xi,1(t), .., xi,dx (t)] ∈ Rdx, i ∈ [1, ..., Lx], is the observed univariate (dx = 1) or multivariate (dx > 1) variable at the i-th time-stamp, the goal of long sequence time-series forecasting (LSTF) is to predict the corresponds Ly-length sequence ∆ steps ahead, i.e., Y(t) = [Y1(t), .., YLy (t)], where Yi(t) = [yi,1(t), .., yi,dy (t)] ∈ Rdy and dx ≥ dy ≥ 1. When dx = dy, Y(t) = [XLx+∆+1(t), .., XLx+∆+Ly (t)]. Historical Inertia: The historical inertia (HI) baseline takes Ly-length subsequence of X (t) as prediction results, i.e., ˆY(t) = [XLx−Ly+1(t), .., XLx (t)]. Note that the HI requires prediction length to be no longer than the input length, i.e., Lx ≥ Ly, which is not necessary for learning-based LSTF models. Considering that in real application scenarios, the dataset is usually orders of magnitude larger than Ly, this condition can be easily achieved. An illustration of the proposed baseline is shown in Figure 1."
2103.16349,"github, data, dataset",9,,,1https://github.com/zhouhaoyi/ 2https://github.com/laiguokun/multivariate-time-series-data 3https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014
2103.16349,"publicly available, data repository, used dataset, dataset",164,,,"We compare HI with SOTA models on four real-world public datasets. ETT (Electricity Transformer Temperature) 1: The ETT dataset from the Informer paper contains 2-years electric power deployment collected from two Chinese counties. There are 7 features in total. Three sub-datasets are included in our experiments, i.e. ETTh1 and ETTh2 with an 1-hour sampling frequency and ETThm1 with a 15-min sampling frequency. In the univariate forecasting task, the feature ""oil temperature"" is chosen as prediction target. Electricity 2: The raw dataset of Electricity is from the UCI Machine Learning Repository 3, which contains electricity consumption of 370 clients every 15 minutes from 2011 to 2014. We use the pre-processed dataset from Lai et al. (2018), which reﬂects hourly consumption of 321 clients from 2012 to 2014. The last client (column) is used as prediction target in the univariate forecasting task."
2104.00098,data,5,,,Implementation Details and Data Sets
2104.00098,data,47,,,"[45] Jing Zhang, Sepideh Pourazarm, Christos Cassandras, and Ioannis Paschalidis. 2018. The Price of Anarchy in Transportation Networks: Data-Driven Evaluation and Reduction Strategies. Proc. IEEE 106, 4 (2018), 538–553. https://doi.org/10.1109/JPROC.2018.2790405"
2104.00098,data,78,,,Assessment of Theoretical Upper Bounds. We ﬁrst assess the theoretical upper bounds on the ineﬃciency ratio and level of unfairness that were obtained in Section 4.2 with respect to the convex combination parameter α. The latter is obtained using a dense sampling method with increments of 0.01. We present the results for the Prenzlauerberg data-set and note that the results and the following discussion extend to other problem instances in Table 1 as well.
2104.00098,data,89,,,"All results were obtained using a commodity laptop equipped with 2.80GHz 4-core i7-7600U CPU, and 16GB of RAM, running 64bit Ubuntu 20.04 OS. We ran the Frank-Wolfe algorithm for 100 iterations on each data-set, both for I-TAP and the method in [24]. We mention that this number of iterations generally allows the Frank-Wolfe algorithm to achieve a relative error of at most 10−5 when searching for UE and SO solutions over larger scenarios of the traﬃc assignment problem [28]."
2104.00098,data,92,,,Figure 3: Comparison between the ineﬃciency ratio of the solution x(α) of I-TAP on the Prenzlauerberg data-set and the theoretical bound on the ineﬃciency ratio obtained in Theorem 1 (left). The comparison between the theoretical upper bound of the level of unfairness for any convex combination parameter α and the unfairness level of the solution x(α) of I-TAP on the Prenzlauerberg data-set is shown on the right. The values of the convex combination parameter were chosen at increments of 0.01.
2104.00098,"data, dataset",76,,,"We observe from Figure 7 that, modulo numerical errors, the discrete Gini coeﬃcient of the user equilibrium for all the scenarios is very close to zero. As with the earlier explored unfairness measures, we also observe that the I-TAP method outperforms the approach used in Jahn et al. [24] even on the discrete Gini coeﬃcient metric for low levels of desired Gini coeﬃcients for all data-sets but Prenzlauerberg."
2104.00098,"data, dataset",101,,,"We now evaluate the performance of our I-TAP method for β-SO on several real-world transportation networks. The results of our experiments not only characterize the behavior of I-TAP but also highlight that, compared to the algorithm in [24], our approach has much smaller runtimes while achieving lower total travel times for most levels β of unfairness. In the following, we describe the implementation details of the I-TAP method and the unfairness metric as well as the data-sets we use. We further present the corresponding results to evaluate the performance of our approach."
2104.00098,"data, dataset",131,,,"Figure 5 depicts the Pareto frontiers, i.e., the set of all Pareto eﬃcient combinations of system eﬃciency and user fairness, for the six transportation networks in Table 1. In particular, observe that the Pareto frontiers of the I-TAP method are below that of the other two approaches for most values of unfairness. This observation indicates that the I-TAP method outperforms the other two approaches since the ineﬃciency ratio of the I-TAP solution is the lowest for most desired levels of unfairness. Only for the Sioux Falls and Prenzlauerberg data-sets, the algorithm in [24] achieved lower ineﬃciency ratios than both the I-TAP and I-Solution methods for higher values of unfairness, which, in practice, would be undesirable. Furthermore,"
2104.00098,"data, dataset",154,,,"note that, unlike the two convex-combination approaches, the solution of the algorithm in [24] can result in ineﬃciency ratios that are much greater than the PoA for low levels of unfairness. The I-TAP method outperforms the I-Solution method since the set of paths that users can traverse is not restricted to the union of the routes under the UE and SO solutions as is the case for the I-Solution method. In particular, there may be traﬃc assignments with lower total travel times that use paths not encapsulated by the restricted set of paths corresponding to the I-Solution method. Furthermore, while the PoA for each of the data-sets is quite low, some real-world transportation networks may have much higher PoA values (even as high as two) [45], which would make the trade-oﬀ between eﬃciency and fairness even more prominent."
2104.00098,dataset,165,,,"Finally, the Pareto frontiers of the I-TAP method for the 0.01 and 0.05 increments of the convex combination parameter almost overlap each other for all the transportation networks other than Sioux Falls. Since Sioux Falls has a highly discontinuous unfairness function (cf. Figure 4), it is likely that the 0.05 increments of α values may not capture all the low total travel time solutions that keep within a β bound of unfairness that the 0.01 increments of α may be able to capture. For all the other datasets, the near equivalence of the Pareto frontiers for the two α discretizations suggests that a good performance of the I-TAP method can be achieved with coarse discretizations of the convex combination parameter set. Thus, we only need to compute a solution to the convex program I-TAPα for relatively few values of α to characterize the Pareto frontier, implying the computational eﬃciency of the I-TAP method."
2104.00098,github,19,,,[21] Transportation Networks for Research Core Team. 2016. Transportation Networks for Research. github.
2104.00098,"github, publicly available, data",154,,,"We tested our I-TAP method using a single-thread C++ implementation of the Conjugate Frank-Wolfe algorithm [28], which we made publicly available along with our data sets and results (github.com/StanfordASL/ {frank-wolfe-traffic, fair-routing}). Our implementation is based on a previous repository which was developed for [11]. While there is a rich literature on algorithm design to solve the traﬃc assignment problem [6, 7], we decided to use the Frank-Wolfe algorithm which was shown recently to be superior in terms of running time [11]. For shortest-path computation in the all-or-nothing routine, we used the LEMON Graph Library [14]. Within the same framework we implemented the solution method for CSO that was presented in [24], where we used r_c_shortest_paths within the Boost C++ Libraries [1] for constrained shortest-path search."
2104.00098,open-source,21,,,"[14] Balázs Dezső, Alpár Jüttner, and Péter Kovács. 2011. LEMON–an open source C++ graph template"
2104.02307,code,8,,,Code 1 Example of calling PermonSVM API.
2104.02307,"code, package, open-source",128,,,"PermonSVM supports distributed parallel (through MPI) reading from formats like SVMLight, HDF5, PETSc binary ﬁle formats, more than 4 problem formulations of classiﬁcation problem, two types of parallel cross-validation, namely k-fold and stratiﬁed k-fold cross-validation. The resulting QP-SVM problem with implicitly represented Hessian, in which Gram matrix X T X is not assembled, is proceeded by solvers provided by the PermonQP package or the PETSc framework. Unlike standard machine learning libraries, PERMON toolbox provides interface functions to change underlying QP-SVM solver, monitoring and tweaking the algorithms. In Code 1, we present an example of a usage PermonSVM API. Our libraries are developed as an open-source project under the BSD 2-Clause Licence."
2104.02307,data,9,,,error function) on calibration data so that:
2104.02307,data,62,,,"SVM was originally designed by [1] as a supervised binary classiﬁer, i.e. a classiﬁer that decides whether a sample falls into either Class A or Class B by means of a model determined from already categorised samples in the training phase of the classiﬁer. Let us denote the training data as an ordered sample-label pairs such that"
2104.02307,data,103,,,"respectively. K ∈ Rm×m is matrix of inner products called Gramian such that K := X T X, X = [x1, x2, . . . , xm] is data matrix of the training samples, y = [y1, y2, . . . , ym]T is vector of corresponding labels, Y = diag(y), and o ∈ Rm, e ∈ Rm denote a zero-vector and an all-ones vector, respectively. In general, the Gramian K is symmetric positive semi-deﬁnite (SPS) of a rank"
2104.02307,"data, dataset",79,,,"8. Sun, J., Jeliazkova, N., Chupakhin, V., Golib-Dzib, J.F., Engkvist, O., Carlsson, L., Wegner, J., Ceulemans, H., Georgiev, I., Jeliazkov, V., Kochev, N., Ashby, T.J., Chen, H.: ExCAPE-DB: an integrated large scale dataset facilitating big data analysis in chemogenomics. Journal of Cheminformatics 9(1) (mar 2017)"
2104.02307,"data, dataset",97,,,"where l is a number of the calibration samples, fj is estimate of f (xj) for j ∈ {1, 2, . . . , l}. On the other hand, when an optimal model performance is attained in a reasonably small value of the penalty C, e.g., in real-world applications employing linear SVMs, see [7], and data is well-behaved, bias on margin failures usually become small. Therefore, it often possible to simply ﬁt the sigmoid on the training dataset."
2104.02307,"data, dataset",117,,,"However, SVMs have a serious drawback; they are sensitive to imbalanced datasets, outliers and multicollinearies among training samples, which could be a cause of preferencing one group over another. Therefore, we propose to use Platt scalling for an additional model calibration, which is based on transforming the SVM classiﬁcation model output into a posterior probability distribution by ﬁtting logistic regression model to SVM raw prediction scores. This calibrating technique is practically used for reducing impact of overﬁtting to predictor mainly caused by training data. After training calibrated models, we demonstrate balanced predictive relevance of these models by converting them to label prediction using an optimal threshold."
2104.02307,"data, dataset",168,,,"Using PETSc implementation of the Newton method without preconditioning with default setting, the sigmoid-shaped calibration function is computed by minimizing cross-entropy (13) on calibration data. Since the Newton method converges quickly to optimal solution x∗ when vector x is close enough to x∗, [7] proposed initial guesses for parameters of sigmoid such that A0 = 0 and B0 = log l++1 l−+1 , where l+ and l− denote numbers of active and inactive samples associated with the calibration dataset TCA. To avoid numerical difﬁculties or catastrophic cancellations that could arise from evaluation 1 − pi, where pi is close to 1, we evaluate (10) by using exp(−Af (x)−B) 1+exp(−Af (x)−B) when Af (x) + B ≥ 0 else we use (10). This numerical improvements were proposed by [4]. Other numerical obstacles could arise from evaluating Hessian"
2104.02307,"data, package",127,,,"The PermonSVM package is a part of the PERMON toolbox designed for usage in a massively parallel distributed environment containing hundreds or thousands computational cores. Technically, it is an extension of the core PERMON package called PermonQP, from which it inherits basic data structures, initialization routines, build system, and utilizes computational and QP transformation routines, e.g. normalization of an objective function, dualization, etc. Programmatically, core functionality of PERMON toolbox is written on the top of the PETSc framework, follows the same design and coding style, making it easy-touse for anyone familiar with PETSc. It is usable on all main operating systems and architectures consisting of smartphones through laptops to high-end supercomputers."
2104.02307,"database, dataset",139,,,"Abstract In this paper, we present a technique for balancing predictive relevance models related to supervised modelling ligand biochemical activities to biological targets. We train uncalibrated models employing conventional supervised machine learning technique, namely Support Vector Machines. Unfortunately, SVMs have a serious drawback. They are sensitive to imbalanced datasets, outliers and high multicollinearity among training samples, which could be a cause of preferencing one group over another. Thus, an additional calibration could be required for balancing a predictive relevance of models. As a technique for this balancing, we propose the Platt’s scaling. The achieved results were demonstrated on single-target models trained on datasets exported from the ExCAPE database. Unlike traditional used machine techniques, we focus on decreasing uncertainty employing deterministic solvers."
2104.02307,"database, dataset",244,,,"In this section, we analyze numerical experiments related to balancing predictive relevance of the singletarget relaxed-bias classiﬁcation model using the Platt’s Calibration technique, which we introduced in Section 2 and Section 3, respectively. We benchmark this approach on datasets associated with biochemical activities of ligands on 4 biological targets, namely abl1 (Abelson murine leukemia viral oncogene homolog 1 protein), adora2a (Adenosine A2A receptor), cnr1 (cannabinoid receptor type 1), and cnr2 (cannabinoid receptor type 2). These datasets were exported from the ExCAPE database, which was developed by [8]. While it is possible to calibrate a model on the same dataset, on which the model was trained, see Section 3, it could be problematic to decide if a bias of an uncalibrated model is small enough. Therefore, we split training samples into the training and calibration datasets. After training models, we evaluate their performance on the test dataset using precision, sensitivity, and area under the curve receiver operating characteristic (AUC) performance scores. The input datasets were divided into training, calibration and test datasets such that they consist of 640, 200, 160 ligands, respectively, and a ratio of active and inactive ones is sufﬁciently preserved. Characteristics associated with these datasets are summarized in Table 1."
2104.02307,dataset,4,,,Target (dataset)
2104.02307,dataset,23,,,"Table 1 The characteristics of training, calibration and test dataset related to abl1, adora2a, cnr1, cnr2 biological targets."
2104.02307,dataset,38,,,"score (Pre.) and sensitivity (Sen.) on the test dataset is minimized, and F1 score must be greater than 0.50, i.e. predictive ability of model must be better than random."
2104.02307,dataset,42,,,"In this paper, we focused on a problem dealing with balancing predictive relevance of single-target models trained using SVMs. This calibration could be required since SVMs is sensitive to imbalanced datasets, outliers and high multicorrelation among training samples."
2104.02307,dataset,62,,,"Analysing predictive relevance of uncalibrated models, we can see that active ligands are preferred in cases of models related to abl1, cnr1, and cnr2, on the other hand, inactive ligands are prefered for adora2a dataset. To balance predictive relevance, we perform model calibration. After this, we can see in Table 3, the"
2104.02307,dataset,62,,,"In the original paper, Platt suggested to use an additional training set, i.e. a calibration set, for training calibration curve on output of general instance-based SVM to avoid incorporating bias failures, i.e. cases when 1 − yifi > 0, on functional-margin γ = |1|. Let us denote such dataset as an ordered set:"
2104.02307,dataset,66,,,"Table 3 abl1, adora2a, cnr1, cnr2 calibrated single-target models: quality of models in probabilistic sense (Brier score) and performance scores in sense of binary classiﬁcation on test datasets related to models, which are converted from target probabilities to labels, using the optimal threshold (Thr.). Results are presented for the both l1-loss and l2-loss SVM."
2104.02307,dataset,68,,,"Because of all datasets are too small to utilize more than one processor core, all experiments were run on 1 MPI process pinned to a processor core. In all presented experiments, we utilized the same node of the ANSELM supercomputer at IT4Innovations. Evaluations of performance scores are summarized in Table 2 and Table 3 for uncalibrated models and models after calibration, respectively."
2104.02307,dataset,68,,,"From achieved results, it seems that it is better to train models using the l2-loss SVM that are not such robust as in the case of the l1-loss SVM and, then, perform their calibration. Moreover, we can obtain a better convergence rate by employing this approach. We observe speedup up to 1.62 in case of training model on the cnr2 dataset."
2104.02307,dataset,106,,,"Looking at performance scores presented in Table 2, we can see that l1-loss SVM outperforms l2loss in overall performance scores (F1 and AUC) in order to abl1 and cnr1 datasets, while l2-loss SVM provides slightly better models for adora2a and cnr2 datasets. As we mentioned in Section 2, l1-loss SVM commonly produces better quality models. However, we have to take into an account that we relax bias b in our approaches. Therefore the models are relaxed as well, which could be a cause of these unexpected results in the sense of performance score of models."
2104.02307,dataset,167,,,"However, calibrating models could cause a deterioration of overall model performance determined by means of AUC as we can see in Table 4. Speciﬁcally, the over performance scores of models decrease by 1% to 4% in the cases of the abl1 (both loss-type models), cnr1 (l1-loss model) and cnr2 (l2-loss model). In order to models related to adora2a target, l1-loss and l2-loss models associated with cnr1 and cnr2, respectively, AUC scores are same. Since models were trained, calibrated and tested on different datasets, we can consider the calibrated models have the same overall performance score in the sense of AUC as their related uncalibrated models. Comparing the quality of the remaining calibrated and uncalibrated models is application-speciﬁc. In some application, they could be considered as models of same quality. For more strict quality merits, the models can be considered that differ signiﬁcantly."
2104.02307,dataset,192,,,"1 1+exp(−Af (x)−B) 1+exp(Af (x)+B) , see [4]. Since the Hessian (14) is SPS in general [4], we regu Instead of stochastic optimization, which commonly used in the machine learning community, our used solvers are deterministic in these experiments. They pass all training samples in one iteration. Therefore we consider terms epoch and iteration as identical in this text. In other words, we do not take into account a batch of a training dataset during the training phase of the classiﬁer. By this, we obtain strictly settled and reproducible training pipelines, unlike employing DNNs or other traditionally used techniques. On the other hand, deterministic solvers suffer on their cost in the sense of computational resources. Thus, training predictors can take longer than in the case of stochastic optimization. At this expense, we reduce uncertainty during the training process, which could be crucial for some scientiﬁc application, e.g. ones related to the pharmaceutical industry."
2104.09994,"code available, data, code",62,,,"The framework architecture, depicted in Figure 1, consists of 𝐾 clients that own the data from a single device each and a server that coordinates the FL process. The following sections provide the design details about each component making up the proposed architecture. The code used to implement the whole pipeline is available at [37]."
2104.09994,data,2,,,Need data
2104.09994,data,4,,,4.1.1. Data Acquisition
2104.09994,data,4,,,4.1.3. Data Preprocessing
2104.09994,data,4,,,Data Data Data Model
2104.09994,data,27,,,"of model poisoning attacks because training from wrong data produces a wrong model. Next, some of the most basic model poisoning attacks are described:"
2104.09994,data,29,,,• Centralized approach. All training data is shared with a server in charge of training and testing a model with it. It does not preserve privacy.
2104.09994,data,30,,,"In summary, this section has shown the lack of solutions dealing with FL approaches considering data generated by decentralized sources to detect malware aﬀecting IoT devices and scenarios."
2104.09994,data,34,,,environmental security and privacy in case of falling into malicious hands. A similar situation occurs in scenarios where the monitored data sources are related to human beings and private actions are involved.
2104.09994,data,39,,,"• A pool of experiments measuring the performance of the proposed framework when detecting malware in IoT devices. To that end, the next scenarios have been compared: i) a centralized approach where all the data"
2104.09994,data,42,,,"this particular scenario of malware detection in IoT devices, using more data to train the model presents a signiﬁcant improvement, especially on previously unseen devices. Besides, FL-based training successfully reaches the centralized performance in a privacy-preserving manner."
2104.09994,data,42,,,"• A use case presenting a B5G scenario where there is a necessity of detecting cyberattacks aﬀecting IoT devices, managing sensitive data, having Non-IID (Independent and Identically Distributed) data, and with non trusted stakeholders or clients."
2104.09994,data,49,,,"data is generally easy and does not need manual labelling, this situation is often termed as unsupervised in the literature [22, 33, 36]. However, in the strict sense of the term, it refers to single-class supervised learning [33]."
2104.09994,data,53,,,"[2] C. L. Stergiou, K. E. Psannis, and B. B. Gupta, “Iot-based big data secure management in the fog over a 6g wireless network,” IEEE Internet of Things Journal, vol. 8, no. 7, pp. 5164–5171, 2020."
2104.09994,data,55,,,"Two situations are considered here. The supervised situation assumes a setup in which each client has access to labeled data from its own device. In the second situation, we assume that each client only has access to the benign traﬃc of its device. Since getting a large quantity of benign traﬃc"
2104.09994,data,62,,,"In the experiments implementing data poisoning attacks, the All labels ﬂipping attack is selected for testing, as it combines both benign and attack label ﬂipping. Since the focus is placed on intentional data poisoning, 𝑝𝑝𝑜𝑖𝑠𝑜𝑛 = 1 is always used. This approach enables the veriﬁcation of the maximum impact of the attack in the generated model."
2104.09994,data,64,,,Client kModelevaluationModel trainingDatapreprocessingServerAggregatedglobalmodelUpdatedindividualmodelDataset fromdevice k12345Legend       Behavioral       data       Models       Evaluation       resultsData from device kUnused (1%)Train (79%)Known devicetest (20%)BenignAttackBenignAttackData from device kKnowndevicetest (20%)Knowndevicetest (100%)Unused (1%)Thresholdselection(39.5%)Train(39.5%)Federated Learning for Malware Detection in IoT Devices
2104.09994,data,66,,,"is shared, ii) a distributed approach where each entity trains an independent model with its local data, and iii) a federated approach where a joint model is generated sharing the local model updates. Two different federated learning algorithms that diﬀer in the number of communications (model sharing updates) with the server have been considered in the previous comparison."
2104.09994,data,74,,,"is set based on statistics about the reconstruction error of benign data. During testing, if a sample has a mean squared reconstruction error higher than the speciﬁed threshold, it is considered as anomalous (positive), otherwise it is considered as benign (negative). The threshold formula used in this work comes from [22], and selects for client 𝑘 ∈ [𝐾] the threshold"
2104.09994,data,77,,,"Model poisoning attacks are conducted through corrupted model updates sent to the server. They are a very big issue when using FL because the clients can send arbitrarily bad models to the server, and, due to the privacy that FL gives, it becomes hard to check whether the models received actually correspond to the local training data or not. In a sense, data poisoning attacks could be considered as a subset"
2104.09994,data,77,,,"Such malicious clients are often referred to as Byzantine workers [40]. The server and the honest participants could be considered as honest-but-curious as well (trying to infer as much information as possible without deviating from the protocol), but privacy issues are out of the scope of this work. Next, several data poisoning and model poisoning attacks are described, to be later implemented and evaluated in Section 6."
2104.09994,data,77,,,"the same distributions. However, the intuition behind the use of these functions is still the same. These countermeasures have been selected for their great simplicity, as they only require a modiﬁcation of the step of model aggregation, which is easy to implement. They also do not require any previous knowledge of the distribution of the client’s data, which is hard to obtain in a realistic federated setting."
2104.09994,data,85,,,"Here centralizing the data presents overall a high performance improvement over the naive method. Furthermore, the FL algorithms also very successfully deal with the unsupervised ﬁngerprinting task. Speciﬁcally, the centralized performance is almost reached by both the MULTI-EPOCH AVG and the MINI-BATCH AVG methods. Once again, MULTIEPOCH AVG seems to help the model to generalize better, as it demonstrates on the new device a marginally better TNR than MINI-BATCH AVG. Interestingly, the threshold, as displayed"
2104.09994,data,87,,,"To that end, the collaborative grid search is deﬁned as a grid search in which the federated clients share their validation results for each considered set of hyper-parameters, so that the selected hyper-parameters are those that give the best results on average. Note that for the unsupervised solution, the model is validated only with benign data, so the selected hyper-parameters are those that minimize the loss. In the supervised solution, however, the selection is based on validation accuracy."
2104.09994,data,97,,,"Note that in reality, a supervised situation with several clients able to generate a decent amount of labeled data is plausible but uncommon. Therefore, the supervised solution has two main motivations. The ﬁrst one is to have a comparison point for the unsupervised solution. As the supervised situation is easier to tackle and is more controllable than the unsupervised one, the second motivation is to be able to go as in-depth as possible in our experiments, to potentially reveal vulnerabilities or other concerns about using FL for malware detection."
2104.09994,data,106,,,"• Gradient factor attack. In this case, the malicious clients multiply their gradients by a negative factor 𝛼𝑔𝑟𝑎𝑑 before updating their local model and sharing it with the server. This attack is inspired by the omniscient attack in [18], but instead of being aware of the estimate of the gradient, the malicious clients simply have access to the data from one device. Therefore, they are only able to compute the estimate of the gradient on their own data distribution. With 𝐾 total clients among which 𝑓 are malicious, the factor 𝛼𝑔𝑟𝑎𝑑 is chosen to verify"
2104.09994,data,112,,,"supervised and the unsupervised situations are considered, and the attention is placed on single-sample analysis. In the supervised situation, as N-BaIoT contains 10 diﬀerent attacks performed using Mirai and BASHLITE, we use all the available attacks labeled using the same class (attack) in order to detect as many attacks as possible. Note that in [35], a collaborative learning approach is proposed. However, the assumed scenario and the goal are diﬀerent from ours, as they focus on building one model per device with the assumption that the data of a single device comes from 2 or 3 diﬀerent sources."
2104.09994,data,116,,,"In MINI-BATCH AGGREGATION, the Model Training component trains the model with a single mini-batch of data before sending it to the server for aggregation. The Model Training component then receives the new aggregated global model, with which the training can continue. This process is repeated until a number 𝐸 of epochs over the full train set are completed. In MULTI-EPOCH AGGREGATION, the model is trained for all 𝐸 epochs at once before being sent to the server for aggregation. A potential drawback is that, as explained in [7], averaging models could have arbitrarily bad results because of the non-convexity of the objective. This problem"
2104.09994,data,125,,,"In the proposed framework architecture, the server is in charge of coordinating the training eﬀorts of the federated clients. Speciﬁcally, it initializes the model at the very beginning, and it aggregates the models sent by the clients into a so-called global model. It also has to coordinate the additional steps described in Section 4.3, i.e. the collaborative normalization, the collaborative grid searches, and the collaborative threshold selection (for the anomaly-detection approach). In the B5G architecture context [38], the server component would be placed in the CLOUD SLICING layer, either on the Fog Nodes or in the Cloud Data Centres, depending on the scope of the clients covered."
2104.09994,data,130,,,"[6] Y. Qu, C. Dong, J. Zheng, Q. Wu, Y. Shen, F. Wu, and A. Anpalagan, “Empowering the edge intelligence by air-ground integrated federated learning in 6g networks,” arXiv preprint arXiv:2007.13054, 2020. [7] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-eﬃcient learning of deep networks from decentralized data,” in Artiﬁcial Intelligence and Statistics, ser. Proceedings of Machine Learning Research, A. Singh and Fort Lauderdale, FL, USA: PMLR, J. Zhu, Eds., vol. 54. http: 20–22 Apr 2017, pp. 1273–1282. [Online]. Available: //proceedings.mlr.press/v54/mcmahan17a.html"
2104.09994,data,132,,,"privacy-sensitive scenarios enabled by 5G and B5G networks. However, FL also suﬀers from inherent problems of dealing with unknown and, therefore, untrusted parties. Malicious clients executing poisoning attacks over data and models is one of the best examples in this direction. Following the previous characteristics, this work considers the following key aspects for the deﬁned scenario: i) data is non identically distributed across the IoT devices (owned by the clients), ii) it is needed to detect anomalies provoked by unseen or zero-day malware aﬀecting IoT devices, iii) it is required to classify well-known malware aﬀecting diﬀerent IoT devices, iv) adversaries can be present among the federated clients, so some countermeasures should be applied."
2104.09994,data,135,,,"After the model has been trained for a satisfying number of iterations through the FL process, it is ready to be evaluated. In order to assess the robustness of the trained models, we evaluate them on diﬀerent test sets. The known devices performance is given by the evaluation of the model on the test part of the data from the devices owned by the clients. The new device performance is computed on the data from a device that is totally new to all clients in the federation (it has not been seen during the selection of the normalization values, the hyper-parameter selection nor the training). Note that the new device’s test set thus has a diﬀerent distribution than the training sets in general."
2104.09994,data,145,,,"Once behavior sources are selected and monitored, the next step to achieve successful malware detection is to process the data and generate device behavior ﬁngerprints. In the 5G and B5G context, Artiﬁcial Intelligence (AI) techniques, mainly Machine Learning (ML) and Deep Learning (DL), have gained enormous relevance in recent years [6]. Nowadays, most of the existing solutions that use ML/DL to detect malware rely on a central entity in charge of collecting data from diﬀerent devices and training global models. Later, these models are distributed between individual clients, or these clients send their live test data to the server for behavior evaluation and malware detection. However, this approach is not suitable for scenarios where device behaviors contain sensitive or conﬁdential data that would signiﬁcantly aﬀect"
2104.09994,data,148,,,"The purpose of the component is to train the federated ML model that will be used for malware detection. To that end, we ﬁrst present the architectures used for classiﬁcation and anomaly-detection. Later, for two diﬀerent FL algorithms, we explain how this component interacts with the server. Throughout the rest of this work, the model parameters of client 𝑘 are referred as 𝑤𝑘 and the global model parameters as 𝑤. Further, with 𝑑 the number of dimensions of 𝑤 and with 𝑖 ∈ [𝑑], 𝑤(𝑖) speciﬁes the 𝑖th dimension of 𝑤. Note that most well-known ML models are compatible with our framework, as long as the trained models do not vary in structure among clients and do not store training data explicitly (otherwise sharing them would compromise privacy)."
2104.09994,data,150,,,"Once again, ELU is used after each hidden layer. All considered architectures have 29 coding dimensions. A low number of dimensions is a way to constrain the autoencoder to learn a representation that is more speciﬁc to benign data. Indeed, using 115 coding dimensions would make the autoencoder able to learn the identity function for any input vector in ℝ115, making it produce low reconstruction errors for very unusual data, even if it is trained only with benign data. The choice of 29 coding dimensions corresponds to what is used in [22]. Without looking at labeled data, it is hard to make a better selection of this hyper-parameter. The numbers 86, 58, 38 and 29, correspond respectively to 75%, 50%, 33% and 25% of the input dimension."
2104.09994,data,157,,,"Because of its decentralized nature, FL shares the threat among multiple entities, namely the clients and the server. The work of [16] reviews many of the problems that can arise when considering an adversarial setup, as well as most of the well-known defenses to protect the system against that. In [17], several data poisoning attacks against Support Vector Machines were deﬁned. Their baseline experiment used the idea of label ﬂipping, in which the binary label of some datapoints in the training set is inverted to hinder the training of the model. In [18], the authors studied the resilience of a distributed implementation of Stochastic Gradient Descent against arbitrarily behaving (Byzantine) adversaries. To that end, a model poisoning attack from the standpoint of a malicious client, that is capable of estimating the gradi Page 3 of 17"
2104.09994,data,161,,,"Unsupervised situation. In this setup, autoencoders are used for anomaly detection, following a similar methodology as the authors of N-BaIoT [22]. An autoencoder is a special form of feed-forward neural network made of two parts, the encoder and the decoder. The encoder transforms the input by reducing its number of dimensions to a value deﬁned as the coding dimension, and the decoder tries to map the encoded input back to the original input. It is trained by minimizing the Mean Squared Error (MSE) between the reconstructed features and the input. In order to use this principle for anomaly detection, an autoencoder is trained with benign data, learning how to reconstruct it, such that it has a low reconstruction error on future benign data and a high reconstruction error on anything that deviates from benign data. Once the training process is completed, a threshold"
2104.09994,data,166,,,"In such a context where data privacy and integrity are critical, Federated Learning (FL) [7] and Blockchain are gaining huge relevance in the last years as a collaborative ML paradigm. In FL, the algorithm training is performed in a decentralized manner by diﬀerent nodes, or clients, that use local data. In this scenario, each decentralized node trains an individual model using its own data and shares the model parameters (instead of the data) with the rest. The exchange of the model parameters and their aggregation to create a unique and global model can be performed through a central entity, called server, or following a peer-to-peer approach [8]. After several iterations, each client has a global model obtained as an aggregation of the individual model of each client. This approach enables data privacy by design, as data is not shared with any external identity."
2104.09994,data,202,,,"and individual model. This comparison has shown that the use of more diverse and larger data, as done in the federated and centralized methods, has a considerable positive impact on the model performance both in a supervised and in an unsupervised scenario. Besides, it has been demonstrated that the privacy of the data can be preserved without losing model performance by following the federated approach. The resilience of the federated models against malicious clients has been tested through the following adversarial attacks: i) a data poisoning attack ﬂipping all labels, ii) a model poisoning attack multiplying gradients by a negative factor, and iii) a model cancelling attack. The results showed that without using a robust technique to aggregate the models, a single malicious client in the federation can ruin the model. Several robust aggregation functions, acting as countermeasures against adversarial attacks, have been applied to solve this problem, with median aggregation showing promising yet insuﬃcient improvements. This ﬁrst step in the direction of making the system robust against attacks shows that a lot of eﬀort is still required to reach satisfying outcomes."
2104.09994,"data available, data",17,,,"more representative and aligned with the reality, where usually much more benign data is available."
2104.09994,"data available, data https, dataset",48,,,"[28] H. Kang, D. H. Ahn, G. M. Lee, J. D. Yoo, K. H. Park, and H. K. Kim, “Iot network intrusion dataset,” IEEE Dataport, 2019. [Online]. Available: https://dx.doi.org/10.21227/q70p-q449"
2104.09994,"data available, data, dataset",54,,,"[26] A. Alsaedi, N. Moustafa, Z. Tari, A. Mahmood, and A. Anwar, “Ton_iot telemetry dataset: a new generation dataset of iot and iiot for data-driven intrusion detection systems,” IEEE Access, vol. 8, pp. 165 130–165 150, 2020."
2104.09994,"data available, data, dataset",121,,,"All experiments performed in this section have followed a similar methodology. In this sense, the federation consists of 𝐾 = 8 clients, each owning data of one of the 9 devices available in the N-BaIoT dataset. The data of one device was not used during training, keeping it as an unseen device for testing purposes. In this context, nine diﬀerent combinations of devices (with an unseen one) were used in all experiments. Moreover, experiments were repeated 5 times to improve the consistency of results. Finally, the results of each experiment show the average over 45 runs in total (9 possible unseen devices and 5 executions)."
2104.09994,"data available, data, dataset",135,,,"Figure 3 shows for both situations how we have split the dataset of a single device for training and testing purposes. In the supervised situation, each dataset is split chronologically between 3 parts: the train set (79%), the aforementioned unused set (1%) and the test set (20%). In the unsupervised situation, only the benign data is available for training, so this part is split between 4 diﬀerent sets: the train set (39.5%), a so-called threshold-selection set (39.5%), the unused set (1%) and the benign part of the test set (20%). All attack data is available for the ﬁnal testing of our experiments."
2104.09994,"data, dataset",30,,,We also decided to re-balance the dataset in three diﬀerent ways in order to cover several possible data scenarios. We selected the following class proportions for every device:
2104.09994,"data, dataset",41,,,Table 4 Supervised results comparing both FL approaches (Multiepoch avg and Mini-batch avg) with the naive approach and the centralized approach. The percentages of benign data of the datasets used are indicated in color on the left.
2104.09994,"data, dataset",45,,,"these results could probably be slightly improved by using a higher number 𝑇 of federation rounds. Figure 4 shows that for the datasets with 50% and 95% benign data, the accuracy seems to have not exactly converged after 30 rounds."
2104.09994,"data, dataset",59,,,"[4] P. M. S. Sánchez, J. M. J. Valero, A. H. Celdrán, G. Bovet, , M. G. Pérez, and G. M. Pérez, “A Survey on Device Behavior Fingerprinting: Data Sources, Techniques, Application Scenarios, and Datasets,” IEEE Communications Surveys & Tutorials, In press."
2104.09994,"data, dataset",62,,,"Supervised situation. First, the supervised solution is veriﬁed. Here, the three diﬀerent dataset splitting options explained in Section 4.1.2 (7.87%, 50% and 95% benign data) are used in repeated tests, also checking how the diﬀerent class balances aﬀect the results. Table 4 shows the results achieved in these experiments."
2104.09994,"data, dataset",103,,,"Data poisoning attacks operate through the medium of the client dataset. The client could be malicious and intentionally modify its own data with the goal of making it misleading. Even if the client is honest, the attack could come from any part in the client data pipeline on which an external malicious entity has control. Therefore this attack category is the one that assumes the less from the clients and that is the most likely to happen. Three data poisoning attacks, all based on label ﬂipping [17], are described for the supervised situation."
2104.09994,"data, dataset",115,,,"Several public datasets aligned with B5G application scenarios and IoT malware exist in the literature. Among them, N-BaIoT is the most suitable to evaluate privacy-preserving collaborative training. Speciﬁcally, this dataset already separates the IoT devices’ traﬃc data into diﬀerent ﬁles, making it easy to split it into several non identically distributed parts for a realistic federated setting. For that reason, we selected N-BaIoT to evaluate our approach. Note that a drawback of this dataset is that it only contains the data from 9 IoT devices, which is a limitation for the experiments as it limits the maximum number of clients that can be considered."
2104.09994,"data, dataset",116,,,"After analyzing the most relevant characteristics of the dataset, we also reviewed some of the most notable existing works on N-BaIoT [22, 32, 33, 34, 35, 36]. Most of those focus on unsupervised anomaly-detection solutions, using only the benign part of the dataset to train. Still, in [36], the hyper-parameters are tuned using also some attack data, and in [34], a supervised classiﬁcation is considered instead. Some works use multiple samples in order to detect potential malware, and others focus on the more granular task of single-sample classiﬁcation. In our methodology, both the"
2104.09994,"data, dataset",156,,,"The remainder of this paper is organized as follows. Section 2 describes related work on AI for IoT cybersecurity, FL algorithms, vulnerabilities and countermeasures, and datasets containing IoT cyberattack data. Section 3 depicts an IoT scenario with privacy requirements that are accomplished by the N-BaIoT dataset, which serves as use case for this work. Section 4 details the design and implementation of the proposed framework, which uses FL to detect malware affecting IoT devices. Section 5 deﬁnes the adversarial attacks and countermeasures tested against the proposed framework. Section 6 shows the results of the experiments done in this work, comparing the federated approaches against traditional ones and detailing the results of the adversarial settings. Section 7 analyzes the lessons learned as well as the possible drawbacks of the architecture. Finally, Section 8 shows the conclusions of this research and future directions."
2104.09994,"data, dataset",170,,,"sources (network data but also sensor readings, operating system logs and telemetry data) about a network containing several IoT/IIoT devices. In [27], the authors propose a dataset collecting benign and volumetric attacks traﬃc traces for 27 IoT devices. The main purpose of this dataset is to evaluate volumetric attacks perpetrated against a network containing real commercial IoT devices. The dataset proposed in [28] was generated with the traﬃc of 2 home IoT devices under multiple attack scenarios. It also includes simulated Mirai traﬃc appearing to come from the IoT devices. Finally, IoT23 [29] is a dataset consisting of 20 captures that include malware activity as well as 3 captures of benign IoT traﬃc. To conclude, it is worthy to mention that there is a lack of dataset suitable for FL approaches detecting malware in IoT devices. Existing FL-based solutions must consider split centralized datasets in order to apply federated techniques."
2104.09994,"data, dataset",172,,,"In [22], a dataset called N-BaIoT was produced by preprocessing the traﬃc generated by 9 commercial IoT devices of various types, either infected by Mirai or BASHLITE (two botnet malware attacks), or uncorrupted. In [23], a medium-sized network of 83 real or emulated IoT devices is considered to produce the MedBIoT dataset. It uses the same packet preprocessing as in N-BaIoT, but here other stages of malware traﬃc are considered (infection, propagation and communication with the command and control server). In [24], the evaluation dataset consists of a network made of 8 security cameras suﬀering from several attacks. Additionally, they included another network consisting of 9 commercial IoT devices, among which one was infected by Mirai. [25] proposes a dataset called Bot_IoT, that contains legitimate and simulated IoT network traﬃc, including diﬀerent attacks. The dataset TON_IoT [26] consists of heterogeneous data"
2104.09994,"data, dataset",182,,,"This work proposes a privacy-preserving framework for IoT malware detection that leverages FL to train and evaluate both supervised and unsupervised models without sharing sensitive data. This framework is designed to be deployed on the network nodes providing access to the IoT devices in Wiﬁ, 5G or B5G networks, oﬄoading the computation from the IoT device itself. In this sense, the client side is designed to be deployed on the RAN while the server side is intended for Fog/Cloud deployment. To demonstrate its feasibility in a realistic IoT scenario, the N-BaIoT dataset has been used due to its heterogeneity and divisibility in terms of IoT devices and malware samples. Using N-BaIoT, we compared the performance of: i) a federated approach, where all device owners train their own model, which are periodically aggregated in a server, ii) a non privacy-preserving setup, in which the whole dataset is centralized and trained by the server, and iii) a local setup where each device owner trains one isolated"
2104.09994,"data, dataset",189,,,"Each sample in the dataset corresponds to a network packet sniﬀed by Wireshark. For each, 115 numerical features characterizing the context of the packet were extracted. The available features are statistics about the size, count and jitter of aggregated network packets, in the last 100 ms, 500 ms, 1.5 sec, 10 sec and 1 min. For example, one feature is the mean packet size over the last 10 seconds in the traﬃc between the current packet source IP and destination IP. Noticeably, the features of packets captured in a very short time interval are highly correlated. This means that this dataset needs to be handled with care in order to reduce as much as possible the data leak between the train and the test sets when separating a given ﬁle into those two parts. To that end, we always used chronological splitting to make the train and test parts, and we left a small set of samples unused between the train part and the test part for each ﬁle in the dataset."
2104.09994,"data, dataset",190,,,"Considering that IoT devices generally have limited resources and modest reliability, the clients in charge of training the models are not the devices to be protected, but other entities capable of collecting the traﬃc of the IoT devices present in the same network, such as B5G base stations or other access points. In this sense, in the B5G architecture [38], the present system would be incorporated in the RAN SLICING Edge Nodes or in the CLOUD SLICING Fog Nodes. This system falls into the category of cross-silo FL, as deﬁned in [15], where the federated clients are few but powerful and reliable. Note that each client can own several IoT devices, but for the sake of simplicity, the architecture and the experiments are described with a single one per client. Figure 2 details the architecture of a client after data acquisition, as well as its interactions with the server. The dataset and the components depicted in the ﬁgure mentioned above are explained in detail in the remainder of this section."
2104.09994,"data, dataset",204,,,"In the last years, FL is gaining importance in the ﬁeld of cybersecurity, with several works already using this paradigm for IoT security. In this context, the research proposed in [14] clearly stated the data privacy problem of traditional AI-based solutions, but the evaluation took place on a private dataset. Also, the data was randomly split among clients, which can be improbable in realistic scenarios, as the one considered in this work, in which each client data comes from a diﬀerent distribution in general. The works presented in [10, 11] also have very similar objectives, but these researches were conducted speciﬁcally for industrial IoT devices, and they analyzed respectively application samples and sensor readings rather than network data, as we do in this work. In [12], FL was studied through the use case of intrusion detection systems. This work also includes blockchain technology to mitigate the problems faced in adversarial FL. However, it concentrates on the early steps of intrusion detection rather than detecting already running malware, and it does not focus speciﬁcally on IoT devices."
2104.09994,"data, dataset",209,,,"Despite the novelty and beneﬁts of FL approaches, their application in real-world scenarios still presents several open questions that must be analyzed and solved (or at least improved) [9]. Previous works dealing with FL for intrusion detection [10, 11, 12] lack the use of realistic datasets in the FL context, the analysis on adversarial impact, or the discussion of their deployment in B5G scenarios, among others. In this sense, some of the most relevant open challenges can be summarized as: 1) how can FL be used in the IoT context to build joint models without sharing sensitive data?; 2) how do FL approaches aﬀect the performance of traditional anomaly detectors and classiﬁers in IoT scenarios?; 3) what is the impact of diﬀerent adversarial attacks aﬀecting federated models designed to detect cyberattacks on IoT scenarios?; and 4) are existing countermeasure mechanisms able to mitigate the eﬀects of adversarial attacks?; and if so, 5) what are the most suitable countermeasures for IoT scenarios?; 6) how these solutions could be incorporated in future networks such as B5G?."
2104.09994,"data, dataset",210,,,"Malware This section presents the characteristics of the scenario deﬁned in this work and explains the details of the dataset used to evaluate the performance of the proposed framework. Our cities have millions of IoT devices connected to the Internet and sensing heterogeneous pieces of data. The number and heterogeneity of devices will increase exponentially with the advent of B5G networks, as they enable new verticals and scenarios based on the enhanced network performance in terms of latency and throughput [30]. Some examples are Unmanned Aerial Vehicle Services, Holographic Teleportation, or Extended Reality. In such a context, privacy issues frequently appear when pieces of sensed data belong to sensitive aspects of our daily lives or organizations [31]. As demonstrated, IoT devices are constrained in terms of resources and have not been designed with security in mind, making them vulnerable to a wide variety of malware. In these scenarios, traditional AI-based detection approaches are not suitable due to the impossibility of training centralized models with sensitive data belonging to diﬀerent organizations or subjects. Because of that, FL is raising as a key mechanism to detect anomalous behaviors and trigger mitigation mechanisms in"
2104.09994,"data, dataset",233,,,"The number of samples per device is also ﬁxed to a constant in order to make the results less dependent on the number of training instances and to keep the dataset size ﬁxed no matter what the proportions of classes are. 100 000 samples per device are used for the supervised solution and only 10 000 for the unsupervised one. This is because the unsupervised training takes more time to converge, and it only needs benign data (which rarely reaches 100 000 samples anyway) in its train set. The procedure followed to reach at the same time these numbers of samples and the desired class proportions is to use upsampling (duplicating the original samples) when more samples than available are needed, and downsampling (keeping only a subset of the original samples) otherwise. Either way, it takes place after splitting the data between the train and test sets, so no data leak is created. After setting the proportions of each class and the desired number of samples per device, we obtain a dataset where the number of samples and the proportions of classes are the same for all devices. After this balancing process, the train set of client 𝑘 is and its threshold selection set (in the unsu. The number of training . |"
2104.09994,"data, dataset",332,,,"Billions of IoT devices lacking proper security mechanisms have been manufactured and deployed for the last years, and more will come with the development of Beyond 5G technologies. Their vulnerability to malware has motivated the need for eﬃcient techniques to detect infected IoT devices inside networks. With data privacy and integrity becoming a major concern in recent years, increasing with the arrival of 5G and Beyond networks, new technologies such as federated learning and blockchain emerged. They allow training machine learning models with decentralized data while preserving its privacy by design. This work investigates the possibilities enabled by federated learning concerning IoT malware detection and studies security issues inherent to this new learning paradigm. In this context, a framework that uses federated learning to detect malware aﬀecting IoT devices is presented. N-BaIoT, a dataset modeling network traﬃc of several real IoT devices while aﬀected by malware, has been used to evaluate the proposed framework. Both supervised and unsupervised federated models (multi-layer perceptron and autoencoder) able to detect malware aﬀecting seen and unseen IoT devices of N-BaIoT have been trained and evaluated. Furthermore, their performance has been compared to two traditional approaches. The ﬁrst one lets each participant locally train a model using only its own data, while the second consists of making the participants share their data with a central entity in charge of training a global model. This comparison has shown that the use of more diverse and large data, as done in the federated and centralized methods, has a considerable positive impact on the model performance. Besides, the federated models, while preserving the participant’s privacy, show similar results as the centralized ones. As an additional contribution and to measure the robustness of the federated approach, an adversarial setup with several malicious participants poisoning the federated model has been considered."
2104.09994,"data, dataset",349,,,"With data privacy and integrity becoming a major concern in recent years, increasing with the arrival of 5G and Beyond networks, new technologies such as federated learning and blockchain emerged. They allow training machine learning models with decentralized data while preserving its privacy by design. This work investigates the possibilities enabled by federated learning concerning IoT malware detection and studies security issues inherent to this new learning paradigm. In this context, a framework that uses federated learning to detect malware aﬀecting IoT devices is presented. N-BaIoT, a dataset modeling network traﬃc of several real IoT devices while aﬀected by malware, has been used to evaluate the proposed framework. Both supervised and unsupervised federated models (multi-layer perceptron and autoencoder) able to detect malware aﬀecting seen and unseen IoT devices of N-BaIoT have been trained and evaluated. Furthermore, their performance has been compared to two traditional approaches. The ﬁrst one lets each participant locally train a model using only its own data, while the second consists of making the participants share their data with a central entity in charge of training a global model. This comparison has shown that the use of more diverse and large data, as done in the federated and centralized methods, has a considerable positive impact on the model performance. Besides, the federated models, while preserving the participant’s privacy, show similar results as the centralized ones. As an additional contribution and to measure the robustness of the federated approach, an adversarial setup with several malicious participants poisoning the federated model has been considered. The baseline model aggregation averaging step used in most federated learning algorithms appears highly vulnerable to diﬀerent attacks, even with a single adversary. The performance of other model aggregation functions acting as countermeasures is thus evaluated under the same attack scenarios. These functions provide a signiﬁcant improvement against malicious participants, but more eﬀorts are still needed to make federated approaches robust."
2104.09994,"data, used dataset, dataset",62,,,"The client is in charge of gathering the traﬃc data from the device under observation. This can be done, for example, by using port mirroring on the switch that connects the IoT device (as described in [22]). In our solution, since we use an existing dataset, this component was not developed."
2104.09994,"data, used dataset, dataset",123,,,"Countermeasures when Detecting Malware Once the performance of the federated approach has been veriﬁed, the next step is to evaluate how the diﬀerent adversarial attacks proposed in Section 5 aﬀect the federated approach. Besides, diﬀerent aggregation functions are applied to test how they improve the model resilience against the diﬀerent attacks. For conciseness, these experiments focus on the supervised situation and use only the dataset balance with 95% of benign data. Moreover, they are conducted with the MINI-BATCH AGGREGATION federated algorithm. A batch size of 𝐵 = 64 (instead of 𝐵 = 8) is used for all the adversarial experiments, as it allows smoother updates for the robust aggregation functions."
2104.09994,database,42,,,"be used as a decentralized database where each client would share its local model and retrieve the models of other clients when performing the aggregation. Thus, the framework would be totally decentralized without an entity coordinating the generated models."
2104.09994,dataset,3,,,4.1.2. Dataset
2104.09994,dataset,6,,,2.3. Datasets Modeling Cyberattacks Aﬀecting
2104.09994,dataset,20,,,"• 50% benign traﬃc and 50% attack traﬃc, making the dataset perfectly balanced for binary classiﬁcation."
2104.09994,dataset,30,,,"[20] L. He, S. P. Karimireddy, and M. Jaggi, “Byzantine-robust learning on heterogeneous datasets via resampling,” arXiv preprint arXiv:2006.09365, 2020."
2104.09994,dataset,35,,,"• 7.87% benign traﬃc and 92.13% attack traﬃc. This is the original dataset balance, with the diﬀerence that the proportion of each class now does not vary across the devices."
2104.09994,dataset,36,,,"• Naive decentralized approach. Each client uses its local dataset for training and testing. Since each client produces its own model, the results are compared by averaging the performance of each client."
2104.09994,dataset,39,,,Figure 3: Initial splitting of the dataset owned by client k for the supervised and the unsupervised situations. The relative size of the benign part with respect to the attack part is not respected for readability.
2104.09994,dataset,40,,,"[23] A. Guerra-Manzanares, J. Medina-Galindo, H. Bahsi, and S. Nõmm, “Medbiot: Generation of an iot botnet dataset in a medium-sized iot network.” in ICISSP, 2020, pp. 207–218."
2104.09994,dataset,43,,,"[29] A. Parmisano, S. Garcia, and M. J. Erquiaga, “A labeled dataset with malicious and benign iot network traﬃc,” Stratosphere Laboratory, 2020. [Online]. Available: https://www.stratosphereips. org/datasets-iot23"
2104.09994,dataset,50,,,"This experiment seeks to measure the performance of our solution when detecting IoT malware using N-BaIoT dataset. To verify that the federated learning approach ﬁts our IoT malware scenario properly, it is necessary to compare it with traditional solutions. More speciﬁcally, the compared alternatives are:"
2104.09994,dataset,53,,,"Deployment This section details the architectural design of the proposed FL-based framework, describing its components and how they interact with each other during the model training and evaluation processes. Besides, it also depicts how the framework is deployed for our validation use case, which leverages the N-BaIoT dataset."
2104.09994,dataset,53,,,"[25] N. Koroniotis, N. Moustafa, E. Sitnikova, and B. Turnbull, “Towards the development of realistic botnet dataset in the internet of things for network forensic analytics: Bot-iot dataset,” Future Generation Computer Systems, vol. 100, pp. 779–796, 2019."
2104.09994,dataset,65,,,"This section details the current state-of-the-art in diﬀerent topics covered in the present work. First, it reviews the usage of AI for IoT cybersecurity, with special consideration of FL. Then, it describes the main literature on adversarial attacks against the FL process and their possible mitigations. Finally, it reviews the available datasets modeling cyberattacks on IoT devices."
2104.09994,dataset,67,,,"It is important to note that these three re-balancings lead to three diﬀerent problems. Both train and test sets are indeed aﬀected by each change, and the goal is not to compare the impact on the model performance when re-balancing the classes. This is an operation to make the results as broad as possible rather than a way to handle the dataset imbalance."
2104.09994,dataset,76,,,"Scalability in real B5G scenarios is also a matter that could not be studied with any of the available datasets, raising a need for generating a much larger and much more diverse one. The deployment of the architecture in a fully distributed manner using Blockchain for the exchange of the federated models is also considered. Besides, Blockchain incorporation into the framework could improve possible security and privacy concerns of the clients."
2104.09994,dataset,102,,,"There are many diﬀerent ways to make the system secure against attacks. One of the most extended ideas is to use model aggregation and update processing solutions that take into account the possibility of malicious clients trying to hijack the model [16]. Next, two diﬀerent aggregation functions, in addition to averaging (AVG), are deﬁned as well as a prior step to be applied to the models sent by the clients. Most of the convergence proofs of these aggregation functions do not hold in this work because the clients datasets are not from"
2104.09994,dataset,104,,,"The ﬁrst noticeable result is that the centralized method’s performance is higher than the distributed naive one, especially when evaluated on an unseen device. Moreover, on all three dataset settings, the MINI-BATCH AVG results are very close to the centralized ones, even sometimes exceeding them. Although obtaining better results than in the centralized method could be surprising, this can be explained by several factors, such as the randomness of the experiments or the fact that the hyper-parameters are computed diﬀerently. Figure 4 shows how fast the models converge near the centralized performance."
2104.09994,dataset,138,,,"Focusing on FL algorithms and their particularities, the work of [7] deﬁnes the term federated learning by characterizing the decentralized non-IID optimization problem. They propose the Federated Averaging (FedAVG) algorithm that now serves as a powerful baseline for many researches using FL. In this algorithm, several clients use their individual datasets to collaboratively train a global model, thanks to the coordination provided by a central server. The role of the server is to average the parameters of the models sent by the clients and return the resulting global model to them. This process is iterated until a terminating condition is met. FL has matured a lot since then and several surveys ([8, 15]) review the latest advances in that domain."
2104.09994,dataset,139,,,"Unsupervised situation. Once the supervised performance is veriﬁed, the next step is to evaluate the unsupervised one. Here, only the benign traﬃc is used for training, so the ﬁnal model does not depend on the class balance in the dataset. In order to make our results independent of the class balance used, we only show the TPR and the TNR for this solution (and not the accuracy). The equation used to deﬁne the threshold is described in Section 4.1.4. Among the possible architectures, the ﬁrst one (Autoencoder A) is always the one giving the best validation loss during all hyper-parameter selections. All results from the unsupervised situation are further produced with Autoencoder A. Table 5 shows the unsupervised results of the system."
2104.09994,dataset,153,,,"One of the limitations in the experimentation has been the low number of clients used, 8 for training, due to the availability of datasets suitable for federated learning. In a real B5G scenario, device deployments will reach up to 10M devices per km2 according to ITU (International Telecommunication Union) requirements [42]. However, we consider that the experiments are valid since, although the number of adversaries is low, namely 1, 2 and 3, the percentage they represent over the total number of clients performing the training is relatively high, 12.5%, 25% and 37.5%, respectively (see Figure 6). Thus, the results can be extrapolated to environments with a much larger number of clients but where the adversaries represent a small percentage of the total, no more than 50%."
2104.09994,dataset,157,,,"ent, was experimented. First, it demonstrated that the usual model averaging step executed by the server in most FL algorithms does not handle even a single malicious client in the federation. More generally, they proved that no model aggregation function, linear in the models sent by the clients, is robust against Byzantine adversaries. In [19], two additional robust model aggregation functions were proposed. In particular, they are based on the coordinate-wise median and the coordinate-wise trimmed mean of the models sent by the clients to the server. The authors of [20] proposed resampling to reduce heterogeneity in the distribution of the models sent by the clients. It is meant to be applied before using a robust aggregation function, and it aims at reducing the side-eﬀects that such a function has when applied to models trained with non-IID datasets."
2104.09994,dataset,200,,,"Datasets are key for AI in general and FL in particular. In this sense, several public network datasets about IoT security can be found in the literature. Table 1 reviews some of the most interesting ones. All of those datasets are generated at a central location, but for some of them, a realistic splitting strategy is doable to let them be used in FL approaches. In this context, the Splitting column presents our proposal in terms of possible strategies to split the dataset among diﬀerent entities. In the Device splitting strategy, the dataset already has the traﬃc from each device placed into a diﬀerent ﬁle. The IP strategy would consist of grouping the dataset samples by IP address to manually isolate the traﬃc of each device. The Scenario splitting strategy would take advantage of the fact that the dataset was generated in several diﬀerent scenarios, and it might be possible to consider each scenario as coming from a diﬀerent client. Finally, the Unrealistic label means that no realistic (non-IID) strategy was found to make the dataset appear to come from several sources."
2104.09994,dataset,231,,,"s-Resampling. Rather than being an aggregation function, s-Resampling [20] is an additional step that can be done prior to the aggregation. In a scenario where each client’s dataset has its own distribution, it aims at reducing the heterogeneity of the models sent by each client. Thus, s-Resampling is meant to be combined with a robust aggregation function to reduce the side-eﬀects of using such a function on nonIID models. Note that combining s-Resampling with AVG is useless, as the result is always exactly the same as when only using AVG. It operates by replacing each model by the average between 𝑠 models randomly sampled from the 𝐾 clients models. Each model can be sampled a maximum of 𝑠 times in total. Algorithm 2 is a slightly adapted version of the second algorithm from [20]. Indeed, s-Resampling may also cause the malicious models to be diluted into several of the models that it outputs, increasing the reach of the malicious clients. For that reason, it is only expected to work satisfyingly with a small number of malicious clients, a small value of 𝑠, and with an aggregation function that gets rid of a high number of extreme values, such as MED or TM(2)."
2104.09994,dataset,234,,,"The following steps are followed both for the supervised and the unsupervised solutions. First, two important hyperparameters (the architecture of the model and the L2- regularization value 𝜆) are selected for each setup using grid searches. The MLP and autoencoder architectures considered are those described in Section 4.1.4. The values considered for 𝜆 are 0, 10−5 and 10−4. Note that for the naive method, the hyper-parameters are selected per client because the clients do not collaborate on hyper-parameter selection. For the FL approaches, each federation used collaborative grid searches to select the hyper-parameters, as deﬁned in Section 4.3.2. Finally, in the centralized method, the grid search is performed directly by the server that receives the whole dataset. For all experiments, a batch size of 𝐵 = 64 was used when training, except with MINI-BATCH AVG where the batch size was divided by the number of clients (𝐵 = 8), so that each model update is made with a total of 64 samples as well. In all of the experiments, the model updates are computed with Stochastic Gradient Descent (SGD). For the supervised solution, the training is conducted for 𝐸 = 4 epochs; for the unsupervised solution, it is made with 𝐸 = 120 epochs."
2104.09994,download,58,,,"Table 6 Computation and communication costs per client. The communication cost is from the client’s perspective and has to be considered in both directions (download and upload). The assumed model sizes correspond to the largest architectures that were used in our experiments, for both the supervised and the unsupervised approaches."
2104.09994,github,5,,,Available: https://github.com/ValerianRey/fed_iot_guard
2104.09994,"publicly available, dataset",7,,,Table 1 Public IoT network datasets.
2104.13049,data,4,,,B. Data Privacy
2104.13049,data,15,,,Fig. 3: BEoT-based secure authentication in surveillance data sharing for industrial applications.
2104.13049,data,29,,,Current trendBlockchain forsecured data sharingComputationalcomplexity of resourceconstraint IoT devicesRegulatory compliancerequirementLoadbalancingPrivacy preservingdata transactionlow latencyresponseFuturedirectionsCognitive EdgeReliable inter devicecommunicationPowerful mining ofbigdata accumulationsSub secondLatency responseHigher dataprocessing rateNetwork slicing forpeer interactionamong multi user8
2104.13049,data,53,,,"Although EoT has been widely adopted in various customer applications such as smart transportation and smart healthcare for improving user quality-of-experience, some critical challenges concerned with security and data distribution at edge needed to be explored. For example, healthcare systems with mobile devices generate sensitive user data such as personal"
2104.13049,data,58,,,Genesis BlockBlockchain nodeGenerate a block in a blockchain networkDownload the required content from edge nodeTrusted authenticationStakeholder's can view the data blocks Public KeyIPFS HashData HashPrevious HashBlock 1Public KeyIPFS HashData HashPrevious HashBlock 2Public KeyIPFS HashData HashPrevious HashBlock NCommercial building surveillanceObservation SurveillanceResidential SurveillancePhone SurveillancePersonal SurveillanceRoad SurveillanceHealth Gadget surveillanceDiversified Video Surveillance SystemsLive StreamingVideo analysisImage analysisSurveillance ForecastingOperations at Edge Server5
2104.13049,data,58,,,"This section presents BEoT’s interventions to address security and privacy in smart cities. Due to exponential growth in technology, cybersecurity and efﬁcient data sharing are primary concerns for organizations and individuals.BEoT can provide effective and reliable information exchange among the individuals as the blockchain offers secure transparent transactions that can be monitored continuously."
2104.13049,data,64,,,"N Deepa is currently working as Assistant Professor (Senior) in School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu, India. She completed her Doctoral degree in Vellore Institute of Technology, Vellore. Her areas of interest are Machine Learning, Soft computing, Data mining, Artiﬁcial Intelligence, Predictive Analytics."
2104.13049,data,67,,,"visualization. In the smart city environment, CE in BEoT will provide an effective resource utilization, fault prediction with minimal computational overhead, security attack classiﬁcation, prediction and detection. Also, CE assists in load management with increased data processing rate. Making the CE perform better in BEoT training the AI models in this diversiﬁed environment is still a challenging issue."
2104.13049,data,70,,,"data aggregation facility can be applied in the smart grid for privacy-preserving transactions. MEC nodes are responsible for decisions on energy trading in order to minimize the processing time, which leads to the reduction of latency; whereas also blockchain provides security for transactions in energy trading. Blockchain has been successfully deployed in many applications such as energy Internet, solar electricity exchanges and energy auctions."
2104.13049,data,73,,,"In any surveillance application, resource utilization, maintaining low latency, and privacy-protection are challenging tasks. Blockchain, in combination with the Nudge Theory, breaks down the data disclosure schemes of new customers with the collaborative customer. Filtering framework plays a key role in ensuring the privacy of task scheduling and access management for edge devices. BEoT’s privacy mechanism is accomplished in industrial IoT applications through"
2104.13049,data,79,,,"The proposed BEoT framework can alleviate the security issues concerned through its security services and ensures full trust in providing privacy-preserving data transactions among the IoT devices and MEC servers. The BEoT framework expedites all the services without relying on any third party, thereby leading to more secured and privacy-preserving data transactions. Though blockchain at MEC and IoT may consume more energy, BEoT can effectively distribute and manage its resources through faster data processing."
2104.13049,data,84,,,"• IoT: IoT devices are responsible for generating or gathering data from the physical environments and then transmitting them to the nearby edge servers via access points or base stations. IoT devices with certain resources (e.g., smart phones and laptops) can act as a mobile blockchain entity to make transactions to communicate directly with the MEC servers, while lightweight IoT devices can participate in the blockchain network via their representative gateways (e.g., mobile phones)."
2104.13049,data,97,,,"cloud environment in a remote location for processing. It is challenging to provide scalability for the centralized cloud environment where a large amount of data are generated by ubiquitous devices, and IoT devices and cloud are located far away from each other. In this context, MEC would be a highly effective technique to enhance the networking, computing, intelligent and storage facilities for IoT devices in smart home settings, with the ability to provide faster response and reduce network trafﬁc at the network edge as speciﬁed in the BEoT architecture.."
2104.13049,data,100,,,"• Industrial applications: BEoT can enable new industrial applications. For example, in a BEoT-based smart transportation system, MEC servers can support lowlatency trafﬁc analytics, while blockchain ensures secure vehicular communications among distributed vehicles [5]. • Security services: Enabled by the inherent security properties such as decentralization, immutability, and traceability, blockchain can provide a number of important security services for BEoT, including access authentication, data privacy, attack detection, and trust management. The analysis of such security services will subsequently be presented in detail."
2104.13049,data,109,,,"On the other side, multi-access edge computing (MEC) has emerged as a promising technology to support IoT systems by allocating computation and storage resources at the network edge for low-latency and real-time services [1]. As a result, the integration of MEC with IoT creates a new model as Edge-of-Things (EoT) [2]. In EoT, MEC nodes are placed in near proximity of the IoT devices for storage and lowlatency computation, making it well suited for performing realtime data tasks in IoT applicationssuch as self-driving cars, automatic manufacturing, and object detection [1]."
2104.13049,data,110,,,"”things” which are connected to the Internet. Due to the advancements in information and communication technology and the affordability of the Internet, a number of IoT-based applications has been on the rise in recent years. The rapid developments of smart cities and human-centric services across the globe have contributed to the increase in the IoT-based applications. To handle the voluminous data generated from the IoT devices at regular intervals, most IoT applications have relied on cloud computing for data processing and storage. However, this cloud-based model suffers from high communication overhead and thus is not suitable for timesensitive IoT applications."
2104.13049,data,117,,,"techniques are used to ensure trust management. Hyperledger pays special attention to perform decryption and secure data transactions in a blockchain. Blockchain combines with a Reinforcement learning (RL) algorithm to overcome computational latency, streamline power usage, and load balancing of edge nodes. The RL algorithm speciﬁes the number of edge processors to complete the computational tasks, depending on the size of the task and the blockchain messages. An Ethereum system is also integrated to provide a federated, secure edge computing system in which blockchain tokens are generated for reliable transactions. From the above discussion, it is clear that blockchain provides trust management for EoT."
2104.13049,data,141,,,"V. CONCLUSION In this article, we have introduced BEoT, a novel solution that facilitates low-latency computation of IoT applications while providing high security degrees at the network edge, empowered by the cooperation of MEC and blockchain. Speciﬁcally, we have explored the opportunities brought by BEoT in some key applied domains, namely smart home, smart healthcare, smart grid, and smart transportation. The roles of BEoT in providing security services have been also analyzed, ranging from access authentication, data privacy preservation to attack detection and trust management. Finally, we have pointed out some key research challenges and future directions. We believe that the signiﬁcant improvements attainable over current BEoT approaches will pave the way for new innovative researches and solutions for enabling the next generation of BEoT."
2104.13049,data,150,,,"data and personal preferences which may possess an elevated risk of compromised security. The blockchain technology with its unique features such as traceability, immutability, and decentralization makes it an ideal solution to provide high security for EoT applications and networks. This is enabled by its network architecture, where each block in the blockchain network stores data transactions and the hash of the previous is maintained by a node, creating a chain of blocks that consensus mechanism such as proof-of-work (PoW). These properties of the blockchain ensure that the records are secure and tamper-proof. Particularly, the convergence of blockchain and EoT form a new paradigm called blockchain-enabled EoT (BEoT) which supports low-latency data services with degrees of security and privacy for IoT applications such as smart grids, smart healthcare, and smart homes [3]."
2104.13049,data,151,,,"Due to the population growth, the electricity demand also increases. The smart grid offers effective trading of electrical energy in a smart city environment. The decentralization and distributed nature of smart grids facilitate the efﬁcient utilization of power resources. Energy trading through the smart grid is prone to numerous security attacks through which the attackers try to deceive the consumers’ usage proﬁles and their electricity. Thus, the blockchain framework with assisted cryptographic techniques can be utilized for secured energy transactions without data leakage, and smart contracts enabled MEC services can be used for attaining low-latency response [10]. Moreover, the usage of various cryptographic techniques with BEoT should not incur a high computational overhead; therefore, a lightweight consortium, blockchain was recommended for better scalability and effective energy trading. In the BEoT architecture, blockchain integrated with"
2104.13049,data,157,,,"[7] K. Gai, Y. Wu, L. Zhu, Z. Zhang, and M. Qiu, “Differential privacybased blockchain for Industrial Internet-of-Things,” IEEE Transactions on Industrial Informatics, vol. 16, no. 6, pp. 4156–4165, Oct. 2019. [8] Y. Fu, F. R. Yu, C. Li, T. H. Luan, and Y. Zhang, “Vehicular blockchainbased collective learning for connected and autonomous vehicles,” IEEE Wireless Communications, vol. 27, no. 2, pp. 197–203, Feb. 2020. [9] M. Zhaofeng, W. Xiaochang, D. K. Jain, H. Khan, G. Hongmin, and W. Zhen, “A blockchain-based trusted data management scheme in edge computing,” IEEE Transactions on Industrial Informatics, vol. 16, no. 3, pp. 2013–2021, Aug. 2019."
2104.13049,data,159,,,"Abstract—Blockchain is gaining momentum as a promising technology for many application domains, one of them being the Edge-of-Things (EoT) that is enabled by the integration of edge computing and the Internet-of-Things (IoT). Particularly, the amalgamation of blockchain and EoT leads to a new paradigm, called blockchain enabled EoT (BEoT) that is crucial for enabling future low-latency and high-security services and applications. This article envisions a novel BEoT architecture for supporting industrial applications under the management of blockchain at the network edge in a wide range of IoT use cases such as smart home, smart healthcare, smart grid, and smart transportation. The potentials of BEoT in providing security services are also explored, including access authentication, data privacy preservation, attack detection, and trust management. Finally, we point out some key research challenges and future directions in this emerging area."
2104.13049,data,164,,,"The advancement of artiﬁcial intelligence (AI), IoT, mobile computing, cloud computing and big data, the conventional healthcare system is converted to smart health services. People can monitor their health conditions using wearable devices and obtain medical assistance from online systems. Several issues such as privacy, security, interoperability, transparency, data storage and decentralization are being faced during the implementation of the centralized smart healthcare service. Blockchain with MEC helps transform the current centralized health architecture to a more decentralized and secure system. It enables to synchronize the medical data collected, provides data security using encryption, authenticates the users and allows data storage in the server. Blockchain integrated with MEC includes security, data sharing and decentralization for medical data where the patient is allowed to share the data with trusted members. The MEC processes the medical data and overcomes high bandwidth problems. In some other smart"
2104.13049,data,215,,,"smart contracts where each node’s task information can be monitored. All the available edge nodes are interconnected to a distributed platform and the data is scattered using the alias feature of blockchain [7]. Edge devices need to carry out the required task and determine time and energy utilization. Power depletion and electrical grid security ﬂaws are key risks in smart grid applications. BEoT overcomes inappropriate users by monitoring the illegal consumption of electricity. Network nodes with high computational capability (e.g., MEC servers), are conﬁgured in the blockchain, taking responsibility for secure resource allocation, using an authorization scheme. Edge nodes supply power to a valid end-user that saves costs on a centralized server and aims to improve the computing process. The information obtained and produced by vehicles mostly consists of sensitive data. Limited memory size and data privacy are challenges for vehicle networks. BEoT seeks to overcome these challenges by providing a blockchain consortium that focuses on delivering data privacy and effective information sharing across vehicular networks [8]. Blockchain provides a reputation-based mechanism to edge nodes that maintain vehicle reputation details when interacting between one vehicle and another, thus enhancing data reliability."
2104.13049,data,227,,,"1) Cognitive Edge in BEoT: AI uses the intelligent agents that can mimic the intelligence of the human brain without human input. It is used to solve complex decision problems by learning through previous experience. The application of these intelligent agents has bought remarkable advancements in all domains. Machine learning and deep learning, the subset of AI have signiﬁcant contributions to these technological advancements. AI’s sophistication can be utilized for combating the computational challenges in the BEoT environment. Cognitive Edge (CE) computing refers to the integration of AI’s cognition with edge computing for effective processing at the edge [14]. CE in BEoT environment allows the end devices to mine their data to be stored at blocks, thus alleviating the data processing at blocks and minimizing the bandwidth consumption for processing at blocks. CE will assist in forecasting the load utilization through knowledge aggregation and ensure minimal data caching at the edge. Furthermore, for secured knowledge trading among edge nodes, blockchainbased credit coins can be used. As the smart city environment comprises a large number of heterogeneous devices, deep learning algorithms can be used for extracting meaningful information from these enormous data accumulations. Machine learning algorithms can be used for data representation and"
2104.13049,data,259,,,"2) Integration of BEoT with 5G and Beyond: All the cellular networks are moving towards the adoption of faster ﬁfth-generation (5G) and beyond systems which can carry a larger payload in a shorter duration with optimal resource utilization. 5G and beyond technologies can best host the application in BEoT environment, as the BEoT environment uses peer-to-peer communication for the transactions with a higher data processing rate. Due to the higher computational requirement for mining, blockchain utilizes the edge server for task ofﬂoading and low-latency response. BEoT in 5G provides faster data processing with the low-latency response, global infrastructure sharing and network slicing to accommodate multiple users with peers interaction, as indicated in Fig. 5. The conjugal of 5G and CE with BEoT will embark the powerful mining of big data accumulations with a low-latency response. Beyond 5G, the sixth generation (6G) communications with the full support of AI and blockchain will be a better host for BEoT applications with CE. Consequently, while moving beyond 5G, the issues pertaining to 5G as well as underlying issues such as high data rate, upgraded quality-of-services, secured data transactions and low-latency response must be considered before adoption [15]. 6G will offer sub-second latency with higher processing rates than 5G. Therefore, the resource management constraint of BEoT can be effectively managed by the intelligence of cognitive edge and higher data processing of 5G beyond communication systems."
2104.13049,data,263,,,"recent advancements in satellite communication, vehicular technologies and IoT, intelligent transport system (ITS) has become an emerging research area during recent years. The conventional centralized system faces many issues like security, data storage, and server failure. The implementation of blockchain in the Internet of Vehicles(IoV) platform has been explored to support the information exchange necessity of ITS [11]. The application of blockchain in ITS provides immutabiltransparency, security, automation and decentralization ity, facilities. Decentralized IoV network of blockchain includes distributed entities such as vehicles, humans, and roadside nodes. These entities are allowed to operate autonomously. Blockchain helps IoV to remove the entities such as the central control, trusted intermediate parties, administrators, and central service manager. The blockchain network allows the participants to manage the transactions and vehicular services independently which results in reduced operational cost. Blockchain uses advanced cryptographic algorithms to provide security and privacy for IoV networks [8]. In the BEoT architecture, data alteration and tampering in IoV networks can be prevented by the immutability property of blockchain. AI-based connected autonomous vehicle framework trains the learning model in MEC systems and provides the obtained knowledge to the blockchain. This enables collective intelligence for connected vehicles and avoids large data transmission. Also, blockchain aids in securing the distributed trained models. Thus, connected vehicles have several advantages such as low latency response thereby increasing the network performance."
2104.13049,data,338,,,"It is, therefore, vital to ensure efﬁcient authentication and authorization between the various key enabling technologies. Fig. 2 depicts the sequence diagram for Blockchain-based authentication model. However, the convergence of BEoT can meet the standards for the better control of these types of authentication and authorization challenges. Byzantine tolerance consensus approaches are used to build a blockchain to ensure secure communication, authentication and end-to-end quality control. Resolution edge nodes and cache nodes are deployed at the edge, offering edge authentication, and boosting the hit ratio [6]. BEoT pays a lot of attention to facilitate mutual authentication for smart grid applications. The key agreement protocol is used at the edges where the edge server information is added to the blockchain, thus avoiding the leakage of the smart meter information. Using a smart contract allows only registered users to be connected to the public key, which helps to maintain the security and reliability of the smart grid network. Smart contract processing of key inputs can be used to reinforce the key revocation thus, preventing the need to prefer a trusted centre and preventing a design ﬂaw. In VEC, authenticating and authorizing information exchange is a challenge, as vehicle networks are dynamic. BEoT promises secure, efﬁcient transmission, trackable map paths that use a dynamic decentralized route hash chain and ensure a reliable system with low communication overhead. Blockchain technology integrated with the 5G RFID supply chain system has facilitated reliable computing and storage costs for EoT. The authentication mechanism uses the cryptographic hash function and bitwise XOR rotation, which consists of N blocks, and each block has a reader tag. The reader tag conﬁrm its authenticity followed by an acknowledgement of the requested data. Moreover, BEoT provides efﬁcient authentication for surveillance data sharing, an important service in industrial applications, as indicated in Fig. 3."
2104.13049,data,343,,,"The data generated by IoT devices has become progressively complex and nuanced, having a signiﬁcant impact on the reliable and efﬁcient transmission of data to the end user. It is, therefore, vital to ensure efﬁcient authentication and authorization between the various key enabling technologies. Fig. 2 depicts the sequence diagram for Blockchain-based authentication model. However, the convergence of BEoT can meet the standards for the better control of these types of authentication and authorization challenges. Byzantine tolerance consensus approaches are used to build a blockchain to ensure secure communication, authentication and end-to-end quality control. Resolution edge nodes and cache nodes are deployed at the edge, offering edge authentication, and boosting the hit ratio [6]. BEoT pays a lot of attention to facilitate mutual authentication for smart grid applications. The key agreement protocol is used at the edges where the edge server information is added to the blockchain, thus avoiding the leakage of the smart meter information. Using a smart contract allows only registered users to be connected to the public key, which helps to maintain the security and reliability of the smart grid network. Smart contract processing of key inputs can be used to reinforce the key revocation thus, preventing the need to prefer a trusted centre and preventing a design ﬂaw. In VEC, authenticating and authorizing information exchange is a challenge, as vehicle networks are dynamic. BEoT promises secure, efﬁcient transmission, trackable map paths that use a dynamic decentralized route hash chain and ensure a reliable system with low communication overhead. Blockchain technology integrated with the 5G RFID supply chain system has facilitated reliable computing and storage costs for EoT. The authentication mechanism uses the cryptographic hash function and bitwise XOR rotation, which consists of N blocks, and each block has a reader tag. The reader tag conﬁrm its authenticity followed by an acknowledgement of the requested data."
2104.13049,"data available, data",87,,,"The integration of blockchain with EoT poses a plethora of research questions such as secured data sharing, authorized access to diversiﬁed IoT devices, regulatory compliance requirements, standardized models for interoperability, load balancing, and resource management. This section provides the potential research problem of optimal resource utilization in the BEoT paradigm. Also, the future scope in the integration of blockchain with EoT is presented. The current trends and future directions of BEoT are depicted in Fig. 5."
2104.13049,"data available, data",110,,,"Nowadays, it is mandatory to provide a secure and safe environment for elderly and disabled people. Blockchain can provide a viable solution to protect all IoT devices in smart home and data acquired from these devices for establishing secure communication using immutable ledgers among entities such as data servers, smart home devices, and homeowner [3]. Further, blockchain can provide reliable authentication to the devices and prevent data theft by using smart contracts that can perform user access veriﬁcation at the smart home gateway. Additionally, IoT devices are exceptionally delicate, and the maximum amount of data is transmitted to the"
2104.13049,"data available, data",111,,,"healthcare systems, blockchain with MEC provides access control and protects the access events. A mutual agreement facility is applied to all the access events for veriﬁcation and storage. The medical data is saved in edge nodes, and access control policies are applied by the edge nodes to obtain attribute-based access using blockchain. The authorized users with attribute-based access control are allowed to access the medical data. Thus, in the proposed BEoT architecture, the healthcare data is collected through IoT devices and shared using blockchain within the MEC framework in order to provide security, transparency and privacy for the smart healthcare system."
2104.13049,"data, code",236,,,"2) Security in BEoT: Although the BEoT paradigm can bring signiﬁcant improvements in the performance of various smart systems, security still remains a considerable concern in this integration. While blockchain ensures privacy-preserving data transaction, it is prone to various security threats such as crypto key exchange, data leakage, regulatory needs, double spending of currency, linking transactions and handling data in the chain. The immutability of blockchains in the BEoT paradigm may open a loophole for third-party service providers to loot the data leading to various cyber-attacks. Also, existing security services such as information coding and digital signatures, impose a higher computational overhead. Further, those services shall introduce additional overhead in mining nodes which perform both mining and authorization. The smart contracts or the chain code are written using different languages and are vulnerable to many security risks such as different interpretations for programming constructs and concurrency issues. Therefore, appropriate validation and veriﬁcation must be carried for smart contracts to avoid vulnerable security risks. The open research challenges include privacy-preserving data transactions with lower computational overhead, the privacy of chain code, authorized third-party services, and segregating activities from the mining node to reduce the overhead. Above all, the new enforced techniques should be well-suited with the scalable nature of the BEoT paradigm."
2104.13049,"data, database",204,,,"• Blockchain: Blockchain is to form the BEoT system running on top of the EoT network, aiming to interconnect IoT devices, MEC servers and end users together in a decentralized fashion. Particularly, blockchain ensures reliable operations of BEoT systems without requiring any third party by using its inherited services such as data consensus, smart contracts, and shared ledgers [4]. More speciﬁcally, data consensus provides veriﬁcation services on user transactions by using mechanisms such as PoW managed by a network of miners. This service is highly necessary for BEoT in improving blockchain consistency and ensuring high network security. Smart contracts can provide self-executing and independent features to build business logic and trust in the BEoT system. They also provide security services on user access authentication or data sharing veriﬁcation once the IoT peer nodes perform transactions, which also supports maintaining security over the edge blockchain. Moreover, shared ledger represents the database that is shared and distributed among BEoT members (e.g., IoT devices, MEC servers and end users). The shared data ledger records transactions, such as information exchange or data sharing among IoT"
2104.13049,"data, database",303,,,"computational overhead while participating in the blockchain network as encrypting the blocks is mandatory. In contrast, the decentralized blockchain consumes more bandwidth and more storage. Real-time video streaming which consumes larger bandwidth is a mandated need for most of the applications in the smart city environment. Usually, the processing of data at blocks will consume more computational resources when the ledger is accumulated with more information. As such, side chains can be used with the blocks segregating the secondary data from the central ledger and storing them in the side chains [12]. Though the mining process in PoW consensus blockchains consumes more energy [13] as it is energyintensive by design and has no signiﬁcant impact when the blockchain scales up. Therefore, a lightweight blockchain with less energy consuming cryptocurrencies should be designed. Intel’s recent work proves proof-of-the-elapsed-time (PoET) to leverage the trusted computing by enforcing random waiting times in block creation. The computational resource utilization in data processing between the cloud and the end devices can be accompanied by edge servers to guarantee the lowlatency response. This, in turn, makes the processing faster and ensures the availability of computational resources. The limited storage capacity at the edge cannot withstand the dynamic user demand, so the transactions at the edge can be managed using blockchains. Furthermore, skyline queries that search only the related data instead of the whole database can be used for retrieving data information from massive data accumulation. Thus, the effective resource utilization in the scalable BEoT environments alleviates the storage overheads involved in processing enormous data accumulation. Also, it requires an authorized load provisioning system for sustainable services."
2104.13049,"data, dataset provided",41,,,"• MEC: In BEoT networks, MEC servers can offer computing and storage resources to handle data tasks ofﬂoaded from IoT devices and provide data services for end users, ranging from data analytics, to data mining, to"
2104.13049,"data, dataset provided",94,,,"MEC offers a variety of services to increase the efﬁciency of data transfer and communication overhead and, therefore, to ensure end-to-end trust, while the blockchain technology has been used to build trust at the edge of the network. A blockchain-based, trusted data management system (BlockTDM) has been proposed in [9] to provide edge node data protection in IoT networks. BlockTDM offers data security and privacy through the use of a multi-channel data segment. Before the data is processed inside the blockchain, encryption"
2105.04903,data,23,,,"To ensure high data quality, the annotation tasks were performed by top-rated MTurk workers, i.e., Mechanical Turk Masters. Since"
2105.04903,data,77,,,"• To the best of our knowledge, ours is the first study on EL in conversational systems. We subdivide entities into three categories (named entities, concepts, and personal entities), and analyze the importance of each for conversational data. We further investigate different aspects of EL for three categories of conversational tasks: QA, task-oriented, and social chat. • We investigate effective designs for collecting large-scale EL"
2105.04903,data,103,,,"[41] James Mayfield, Dawn Lawrie, Paul McNamee, and Douglas W. Oard. 2011. Building a Cross-Language Entity Linking Collection in Twenty-One Languages. In Multilingual and Multimodal Information Access Evaluation, Pamela Forner, Julio Gonzalo, Jaana Kekäläinen, Mounia Lalmas, and Marteen de Rijke (Eds.). 3–13. [42] Edgar Meij, Wouter Weerkamp, and Maarten De Rijke. 2012. Adding Semantics to Microblog Posts. In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining (WSDM ’12). 563–572."
2105.04903,data,139,,,"[28] Paolo Ferragina and Ugo Scaiella. 2010. TAGME: On-the-fly Annotation of Short Text Fragments (by Wikipedia Entities)). In Proceedings of the 19th ACM international conference on Information and knowledge management. 1625–1628. [29] Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating Named Entities in Twitter Data with Crowdsourcing. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk (CSLDAMT ’10). 80–88. [30] Jianfeng Gao, Michel Galley, and Lihong Li. 2019. Neural Approaches to Conversational AI. Foundations and Trends® in Information Retrieval 13, 2–3 (2019), 127–298."
2105.04903,data,142,,,"conversations. Their approach consists of three steps: (1) identifying user utterances that are related to personal entities, (2) predicting entity mentions by classifying those utterances, and (3) finding the personal entities. Tigunova et al. [53] address the problem of identifying personal entities from implicit textual clues. They proposed a zero-shot learning method to overcome the lack of sufficient labelled training data [54]. All these studies focus on identifying predefined classes of predicates. Extracting RDF triples without predefined relation classes has been studied in the context of open information extraction [3, 18, 27, 61], but not in relation to personal entities. In this study, we annotate conversations with personal entity mentions and their corresponding entities."
2105.04903,data,217,,,"4.2 Personal Entities Annotating conversations with personal entities requires identifying personal entity mentions and mapping them to the corresponding explicit entity mentions in the conversation history (if exists); e.g., mapping the personal entity mention “my guitar” to the explicit entity mention “Gibson Les Paul.” Once this mapping was done, mention-entity pairs can be identified as described in Stage 1 of Section 4.1. We note that in some cases, explicit entity mentions are not present in the conversation history and the system needs to detect them from other information sources (e.g., previous conversations or user profile data). In this study, we confined ourselves to the cases where explicit entity mentions can be found in conversation history; i.e., personal entity mention without explicit entity mention in the conversation history were not mapped to any entity. We designed a crowdsourcing experiment, where workers were given a conversation history along with the personal entity mention, and their task was to select the text span in the conversation history that the given personal entity mention refers to. Figure 2 shows an example of this task. “None of the above” answers were resolved by an expert annotator."
2105.04903,"data available, data",5,,,annotations for conversational data.
2105.04903,"data available, publicly available, dataset",194,,,"6 CONCLUSION In this paper, we studied entity linking in a broad setting of conversational systems: QA, task-oriented, and social chat. Using crowdsourcing, we analyzed existing conversational datasets and annotated them with concepts, named entities, and personal entities. We found that while both concepts and named entities are useful for understanding the intent of user utterances, personal entities are mainly important in social chats. Further, we compared the performance of different established EL methods in a conversational setting and concluded that none of the examined methods can handle this problem effectively, falling short in providing both high recall and precision, as well as annotating concepts, named entities, and personal entity mentions. Our annotated conversational dataset (ConEL) and interface designs are made publicly available. These resources come with detailed instructions on the procedure of collecting the annotations, which can be used for further extension of the collection. Following the insights from this study, developing conversational entity linking methods and employing them in various types of conversational systems are obvious future directions."
2105.04903,"data, dataset",47,,,"In the final step, each dataset in our shortlist was closely examined, and at least one data set was selected for each conversational problem; see Table 1 for an overview of the selected datasets. The reasoning behind our selections is detailed below."
2105.04903,"data, dataset",291,,,"question, we set out to analyze entity linking annotations for existing conversational datasets. We perform a thorough analysis of existing conversational datasets and select four of these for annotation. These cover the three main categories of conversational problems [30]: question answering (QA), task-oriented systems, and social chat (cf. Sect. 3). We aim to annotate “natural” conversations, and therefore bias our selection of datasets towards those that are obtained using a Wizard-of-Oz setup. Annotating dialogues, however, is an inherently complex task, where it is a challenge to keep the cognitive load sufficiently low for crowd workers. This leads us to a secondary research question: What are effective designs for collecting large-scale annotations for conversational data? Although a large body of research exists on effective designs for collecting large-scale entity annotations [1, 6, 29, 41], to the best of our knowledge, there is no work on conversational data. We run a number of pilot experiments using Amazon Mechanical Turk (MTurk) to select the best design and instruction. Based on these experiments, we develop the Conversational Entity Linking (ConEL) dataset, consisting of of 100 annotated dialogues (708 user utterances) sampled from the QuAC [16], MultiWOZ [62], WoW [24], and TREC CAsT 2020 [21] datasets. To enable further study in conversational EL, we also annotate a separate sample of 25 WoW dialogues (containing references to personal entities) and all 25 manually rewritten dialogues of TREC CAsT 2020."
2105.04903,dataset,8,,,Conversational Entity Linking: Problem Definition and Datasets
2105.04903,dataset,8,,,KEYWORDS Entity Linking; Conversational System; Datasets
2105.04903,dataset,11,,,Table 2: Entity linking results on the ConEL dataset.
2105.04903,dataset,20,,,Dataset QuAC [16] MultiWOZ [62] WoW [24] TREC CAsT 2020 [21] QA
2105.04903,dataset,21,,,"and is focused on question rewriting. Because of the overlapping questions with other datasets, it was also ignored."
2105.04903,dataset,32,,,"• As an additional (online) resource, we provide a comprehensive list of around 130 conversational datasets released by different research communities with a detailed comparison of their characteristics."
2105.04903,dataset,33,,,"[44] Gustavo Penha, Alexandru Balan, and Claudia Hauff. 2019. Introducing MANtIS: a novel multi-domain information seeking dialogues dataset. arXiv preprint arXiv:1912.04639 (2019)."
2105.04903,dataset,35,,,"The resources provided in this paper allow for further investigation of entity linking in conversational settings, can be used for evaluation or training of conversational EL systems, and complement existing conversational datasets."
2105.04903,dataset,47,,,"[49] Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019. Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 5370–5381."
2105.04903,dataset,55,,,"[10] Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašić. 2018. MultiWOZ - A LargeScale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 5016–5026."
2105.04903,dataset,60,,,"Comparing concepts and named entities, we found that 43% of linked entities in the ConEL dataset are marked as named entities by crowd workers, which implies that the remaining 57% entities are concepts. This indicates that in addition to named entities, concepts are also found useful for understanding the intents of user utterances."
2105.04903,dataset,65,,,"[25] Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj Goyal, Peter Ku, and Dilek Hakkani-Tur. 2020. MultiWOZ 2.1: A Consolidated Multi-Domain Dialogue Dataset with State Corrections and State Tracking Baselines. In Proceedings of the 12th Language Resources and Evaluation Conference. 422–428."
2105.04903,dataset,67,,,"3 DATASET SELECTION There exists a large number of conversational datasets released by the natural language processing, machine learning, dialogue systems, and information retrieval communities. We made an extensive list of around 130 datasets,2 extracted from ParlAI [43] and other dataset comparison lists [16, 36, 44]. These datasets target three conversational problems [30]:"
2105.04903,dataset,75,,,"[11] Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Ben Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young Kim, and Andy Cedilnik. 2019. Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 4516–4525."
2105.04903,dataset,78,,,"ACM Reference Format: Hideaki Joko, Faegheh Hasibi, Krisztian Balog, and Arjen P. de Vries. 2021. Conversational Entity Linking: Problem Definition and Datasets. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’21), July 11–15, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/ 3404835.3463258"
2105.04903,dataset,84,,,"4 ENTITY ANNOTATION PROCESS This section describes the process of annotating dialogues from the selected conversational datasets. Our aim is to identify entities that can aid machine understanding of user utterances; this includes named entities, concepts, and mentions of personal entities. Note that our focus is on user utterances, since the system is supposedly aware of the text it generates during a conversation. The knowledge graph we use for annotations is Wikipedia (2019-07 dump)."
2105.04903,dataset,92,,,"To understand the frequency of personal entities in conversational datasets, we applied the method described in Section 4.2 to identify all personal entity mentions in all the datasets. We found that WoW contains more dialogues with personal entity mentions compared to other datasets; i.e., 33% of dialogues in WoW vs. 0.3%, 11%, and 12% of dialogues in QuAC, MultiWOZ, and TREC CAsT, respectively. These results indicate that personal entity mentions are mainly present in social chat conversations."
2105.04903,dataset,100,,,"5 ANNOTATION RESULTS In this section, we describe our findings based on the analysis of the entity annotations obtained for the selected datasets. We also present baseline results for the entity-annotated conversations. The results are shown in Tables 2–5. In these tables, the last character of each method, “t” or “h,” stands for “turn” or “history,” respectively (cf. Section 4.1). Precision, recall, and F1 scores are micro-averaged and computed using the strong matching approach [55]."
2105.04903,dataset,109,,,"Task-oriented. The MultiWOZ [62] and KVRET [26] datasets were examined for task-oriented dialogues. MultiWOZ covers seven various goal-oriented domains: Attraction, Hospital, Police, Restaurant, Hotel, Taxi, and Train. KVRET, on the other hand, deals with only three domains, all of which are in-car situations. We, therefore, selected the MultiWOZ dataset, which also has more dialogues than KVRET (8.4K vs. 3K). Note that MultiWOZ has several versions [10, 25, 62]; we used the latest version, MultiWOZ 2.2 [62]."
2105.04903,dataset,112,,,"Table 2 shows the results of different EL methods on the ConEL dataset. While TagMe achieves the highest F1 scores on WoW and CAsT, WAT and REL are the best performing tools (with respect to F1) on the MultiWOZ and QuAC datasets, respectively. Comparing the “turn” and “history” methods, we observe that conversation history improves EL results for most datasets and tools. We also find that REL has higher precision but lower recall compared to TagMe and WAT. One might argue that high precision EL is preferred in a conversational setting, as incorrect results can lead to high"
2105.04903,dataset,114,,,"Social chat. The Wizard of Wikipedia (WoW) [24], Empathetic Dialogues [49], Persona-Chat [63], and TaskMaster-1 [11] datasets were shortlisted for social chat dialogues. We excluded TaskMaster1, as the majority of dialogues (7.7K) were collected by crowd workers who were instructed to write full conversations, i.e., played both the user and the system roles on their own. Persona-Chat and Empathetic Dialogues are more focused on emotional and personal topics, while WoW is knowledge grounded and makes use of knowledge retrieved from Wikipedia. We therefore chose WoW as a social chat dataset."
2105.04903,dataset,118,,,"[62] Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, and Jindong Chen. 2020. MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI. 109–117. [63] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing Dialogue Agents: I have a dog, do you have pets too?. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2204–2213."
2105.04903,dataset,140,,,"To obtain a comprehensive view of entity linking in conversational systems, we set out to analyze at least one dataset for each of the three main categories of conversational problems. To this end, we shortlisted datasets that resemble real conversations. That is, multi-domain and multi-turn datasets, collected based on actual interactions between two humans. Datasets that are extracted from web services (e.g., Reddit and Stack Exchange) or created based on templates (e.g., bAbI [52]) were thus ignored. To ensure that the selected datasets are sizable, they were required to contain at least 100 dialogues. This list was further narrowed down by selecting relatively popular datasets based on citation counts and publication year.3 By applying these criteria, nine datasets were shortlisted."
2105.04903,dataset,142,,,"Additionally, we also included the TREC 2020 Conversational Assistance Track (CAsT) [21] dataset in our study. TREC CAsT [20] is an important initiative by the IR community, and is focused on the information seeking aspect of conversations. Unlike other datasets, which represent dialogues as a sequence of user-system exchanges, TREC CAsT 2019 provides relevant passages that a system may return in response to a user utterance—therefore, a unique conversation cannot be made for a given conversational trajectory. This has been changed in TREC CAsT 2020 [21], where a canonical response is given for each user utterance. We generated conversations for our crowdsourcing experiments using these canonical responses. In the remainder of this paper we refer to TREC CAsT 2020 as CAsT."
2105.04903,dataset,145,,,"REFERENCES [1] Omar Adjali, Romaric Besançon, Olivier Ferret, Hervé Le Borgne, and Brigitte Grau. 2020. Building a Multimodal Entity Linking Dataset From Tweets. In Proceedings of the 12th Language Resources and Evaluation Conference. 4285–4292. [2] Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2020. Open-Domain Question Answering Goes Conversational via Question Rewriting. arXiv preprint arXiv:2010.04898 (2020). [3] Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. 2015. Leveraging Linguistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 344–354."
2105.04903,dataset,151,,,"The CAsT dataset comes with manually rewritten user queries, where each rewritten query can be answered independently of the conversation history. We also annotated the manually rewritten CAsT queries to allow for a comparison between raw and rewritten queries. To extend our analysis on personal entity linking, we annotated another sample of dialogues from the WoW dataset. To generate this sample, we randomly selected 500 dialogues that contain personal entity mentions and presented them to crowd workers to find their entity references in the dialogues (cf. Section 4.2). Workers agreed that, in 180 dialogues of this sample, the references to the personal entity mentions are present in the dialogue. We then randomly selected 25 dialogues (containing 216 user utterances) out of these 180 dialogues and annotated their concepts, named entities, and personal entities."
2105.04903,dataset,177,,,"Generating annotation candidates. We employ a pooling approach to generate an extended set of candidate mentions and entities. Three EL tools were used to annotate the dialogues: TagMe [28], WAT [46], and REL [57]. Each tool was employed in two ways: (i) the turn method, which annotates a single turn, irrespective of the conversation history, and (ii) the history method, which annotates each turn given the conversation history up to that turn. For the CAsT dataset, only user utterances were given to the EL tool, while for other datasets both system and user utterances were considered as conversation history. This is due to relatively long system utterances in the CAsT dataset, which makes infeasible for the EL tools to annotate the whole conversation history. To further improve the recall of our pool, we included the top-10 Wikipedia search results, using mentions as queries sent to the MediaWiki API.4"
2105.04903,dataset,191,,,"4.3 Dialogue Selection and Annotation Annotating all dialogues in the selected datasets was infeasible for us, due to its high costs. We therefore selected a random sample of dialogues from a pool of presumably difficult dialogues from the QuAC, MultiWOZ, and WoW datasets. This pool contains dialogues with at least one complex mention, a personal entity mention, or a clarification question in user utterances. By complex mention we refer to cases where the same mention is linked to different entities by the EL tools (i.e., REL, WAT, and TagMe). The clarification questions were identified based on the patterns stated in [8], and personal entity mentions were extracted as described in Section 4.2. A total of 100 samples were selected (25 for each dataset), amounting to 708 user utterances. Note that unlike the other datasets, CAsT contains only 25 dialogues, therefore all its dialogues were annotated. Based on this selection, we are able to analyze the differences between the three main categories of conversational tasks."
2105.04903,dataset,213,,,"ABSTRACT Machine understanding of user utterances in conversational systems is of utmost importance for enabling engaging and meaningful conversations with users. Entity Linking (EL) is one of the means of text understanding, with proven efficacy for various downstream tasks in information retrieval. In this paper, we study entity linking for conversational systems. To develop a better understanding of what EL in a conversational setting entails, we analyze a large number of dialogues from existing conversational datasets and annotate references to concepts, named entities, and personal entities using crowdsourcing. Based on the annotated dialogues, we identify the main characteristics of conversational entity linking. Further, we report on the performance of traditional EL systems on our Conversational Entity Linking dataset, ConEL, and present an extension to these methods to better fit the conversational setting. The resources released with this paper include annotated datasets, detailed descriptions of crowdsourcing setups, as well as the annotations produced by various EL systems. These new resources allow for an investigation of how the role of entities in conversations is different from that in documents or isolated short text utterances like queries and tweets, and complement existing conversational datasets."
2105.04903,"dataset provided, used dataset, dataset",100,,,"QA. Among the QuAC [16], CoQA [50], and QReCC [2] datasets, we selected QuAC for QA dialogues. QuAC is a widely used dataset for conversational QA and contains 13.6K dialogues between two crowd workers. CoQA, on the other hand, is a machine reading comprehension dataset with provided source texts for every dialogue. Since these source texts are not necessarily available in real conversations, CoQA was left out. QReCC is built based on questions from other datasets, including QuAC and TREC CAsT,"
2105.04903,github,3,,,1https://github.com/informagi/conversational-entity-linking
2105.04903,github,3,,,https://github.com/chauff/
2105.04903,github,10,,,Assistance Track (CAsT). https://github.com/daltonj/treccastweb.
2105.04903,"github, publicly available, dataset",32,,,"2This list is publicly available at: https://github.com/informagi/conversational-entitylinking 3While admittedly this is a loose measure, it helps to identify datasets that became widely accepted by the research community."
2105.04903,open-source,133,,,"2.1 Entity Linking Entity linking in documents. Entity linking plays an important role in understanding what a document is about [4]. TagMe [28] is one of the most popular EL tools, redesigned and improved by Piccinno and Ferragina [46] and renamed to WAT. Van Hulst et al. [57] presented REL, which is an open source EL tool based on state-of-the-art NLP research. Other state-of-the-art EL methods include DeepType [48], Blink [58], and GENRE [12]. Although these approaches are effective for documents, it is known that EL algorithms with high performance on general documents are less effective when applied to short informal texts like queries [17]."
2105.04903,"publicly available, data, dataset",37,,,"• We make the annotated conversational datasets publicly available.1 This data comes with detailed account of the procedure that was followed for collecting the annotations, which can be used for further extension of the collection."
2105.04903,"used dataset, dataset",40,,,"Table 1: Overview of the selected conversational datasets for the entity annotation process. A sample of QuAC, MultiWOZ, and WoW, and all dialogues in TREC CAsT 2020 were used for generating the ConEL dataset."
2105.05076,data,18,,,"Symposium “From Data to Models and Back (DataMod), Toulouse, France, 2018."
2105.05076,data,21,,,"Data,” in Advances in Information Retrieval, vol. 7224, Berlin, Heidelberg: Springer, 2012."
2105.05076,data,24,,,"[4] J. Bu Daher, A. Brun, and A. Boyer, “A Review on Heterogeneous, Multi-source and Multi-dimensional data"
2105.05076,data,27,,,"[3] J. Bu Daher, A. Brun, and A. Boyer, “Multi-source Data Mining for e-Learning,” presented at the 7th International"
2105.05076,data,28,,,"[11] H. Saif, M. Fernandez, Y. He, and H. Alani, “On Stopwords, Filtering and Data Sparsity for Sentiment Analysis"
2105.05076,data,51,,,"[25] A. Tiwari, K. Rajesh, and P. Srujana, “Semantically Enriched Knowledge Extraction with Data Mining,” International Journal of Computer Applications Technology and Research, vol. 4, no. 1, pp. 7–10, Jan. 2015, doi: 10.7753/IJCATR0401.1002."
2105.05076,data,53,,,"[1] C.-F. Chien and L.-F. Chen, “Data mining to improve personnel selection and enhance human capital: A case study in high-technology industry,” Expert Systems with Applications, vol. 34, no. 1, pp. 280–290, Jan. 2008, doi: 10.1016/j.eswa.2006.09.003."
2105.05076,data,54,,,"[8] F. Shi, L. Chen, J. Han, and P. Childs, “A Data-Driven Text Mining and Semantic Network Analysis for Design Information Retrieval,” Journal of Mechanical Design, vol. 139, no. 11, p. 111402, Nov. 2017, doi: 10.1115/1.4037649."
2105.05076,data,66,,,"[23] D. Graus, M. Tsagkias, W. Weerkamp, E. Meij, and M. de Rijke, “Dynamic Collective Entity Representations for Entity Ranking,” in Proceedings of the Ninth ACM International Conference on Web Search and Data Mining - WSDM ’16, San Francisco, California, USA, 2016, pp. 595–604, doi: 10.1145/2835776.2835819."
2105.05076,data,108,,,"Text mining is a collection of methods and algorithms intended for information extraction and knowledge discovery from structured, semi-structured and unstructured textual data [8], [9]. In the mining process, several techniques work individually and collaboratively to process raw data and utilize it in the knowledge extraction. Such techniques and methods include pre-processing, document retrieval, classification, topic discovery and modelling, and word association and information extraction. Furthermore, the combination of outputs of individual methods can be used to further inform future models as seen in [10], in an industrial context."
2105.05076,data,168,,,"Although obtaining data from multiple sources poses the potential of supporting the textual data with other data modalities, it also reveals a main challenge concerning the fusion of those modalities in one representation, as well as the extraction of relations between them. In this paper, a text mining-based algorithm provides the overall system with the ability to reduce the imprecision in textual data, represent the documents in meaningful token vectors and extract possible patterns in the text represented by the extracted N-grams. Document vectors hold the potential to extract relations between the documents. However, searching for a certain document through those relations is not very different from a key-word search approach, which is a limited approach in this particular use case. Therefore, to enhance the connectivity, searchability and retrieval of design documentation, a knowledge graph is utilized for its ability to represent relations between those documents based on several criteria. 4.1 Knowledge Graph Construction"
2105.05076,data,180,,,"Abstract. The production of microchips is a complex and thus well documented process. Therefore, available textual data about the production can be overwhelming in terms of quantity. This affects the visibility and retrieval of a certain piece of information when it is most needed. In this paper, we propose a dynamic approach to interlink the information extracted from multisource production-relevant documents through the creation of a knowledge graph. This graph is constructed in order to support searchability and enhance user’s access to large-scale production information. Text mining methods are firstly utilized to extract data from multiple documentation sources. Document relations are then mined and extracted for the composition of the knowledge graph. Graph search functionality is then supported with a recommendation use-case to enhance users’ access to information that is related to the initial documents. The proposed approach is tailored to and tested on microchip design-relevant documents. It enhances the visibility and findability of previous design-failure-cases during the process of a new chip design."
2105.05076,data,224,,,"Document VectorsText ExtractionMultiple sourced documentsTextual DataPre-processingMeta Data ExtractionSpecial Strings RemovalStop-word RemovalTokenization Stemming and LemmatizationSymbolic Tokens FilteringExpert-knowledge AcquisitionSpecial Character MeaningDomain Vocabulary DefinitionDomain-related AbbreviationsN-Gram ExtractionTF-IDF CalculationsTF ScoresIDF Scores  the largest useful number of combinations that reveal possible patterns in the failure cases’ description, which is three tokens or named as trigram. Then we utilize all N-grams equal or below this threshold, i.e. unigrams, bigrams and trigrams, in order to enhance the relation extraction between documents that contain those N-grams. However, unlike the TF-IDF based vectors, the N-grams approach does not represent the document’s topic based on the frequent and unique terms in it. Therefore, instead of including those N-grams in the document vectors, we utilize the structure of the constructed knowledge graph itself and introduce a new type of nodes to the graph that is mainly based on the N-gram approach. This maintains the document-specific vectors and allows the addition of new relations to the graph through the N-grams. We call those nodes the linking nodes; and they serve the goal of creating new connections between graph nodes, based on the mined possible patterns in the textual data. The following section presents a detailed description of this concept, along with the graph construction procedure."
2105.05076,data,337,,,"Knowledge graphs, or knowledge maps, are graphical representations of a knowledge base. Whether ontologies or other semantic representations that are based on relations between graph entities, those networks have been a robust approach utilized for knowledge representation, information extraction and decision support. Semantic knowledge representations can enhance or be enhanced by other technologies related to data preparation, such as text mining, information extraction, deep learning and natural language processing [21]. In their summary on the utilization of knowledge graphs with text mining, authors in [21] highlight the rising role of knowledge graphs in text-centric retrieval, especially for search system applications. Their review focuses on linking graph entities, the retrieval of those entities and the role of text mining in the retrieval process. In this context, an entity of a knowledge graph is one entry in this graph, or a node, which has its attributes and relations to other nodes/entities in the graph. Since each node in the graph has normally a textual form of information attached to it, such as annotations, properties or names, linking graph nodes can then be defined as the process of identifying the relevance of this textual content between two nodes in the graph. This can then be expressed explicitly as an additional relation between them. Retrieving graph nodes, on the other hand, refers to the process of identifying and extracting nodes in the graph, which are related to a certain query of the user. In other words, it is determining the relevance of a node, based on a relevance measure between the text of a search term and the textual content of the nodes in the graph. Since, in a text extraction-based graph generation, graph nodes are representing extracted text from text documents, retrieval models are required, which are called retrieval models."
2105.05076,data,342,,,"In this context, an entity of a knowledge graph is one entry in this graph, or a node, which has its attributes and relations to other nodes/entities in the graph. Since each node in the graph has normally a textual form of information attached to it, such as annotations, properties or names, linking graph nodes can then be defined as the process of identifying the relevance of this textual content between two nodes in the graph. This can then be expressed explicitly as an additional relation between them. Retrieving graph nodes, on the other hand, refers to the process of identifying and extracting nodes in the graph, which are related to a certain query of the user. In other words, it is determining the relevance of a node, based on a relevance measure between the text of a search term and the textual content of the nodes in the graph. Since, in a text extraction-based graph generation, graph nodes are representing extracted text from text documents, retrieval models are required, which are called retrieval models. A standard retrieval model combines heterogeneous and semi-structured information about the node, e.g. its attributes, into one representation that can be static, as in [22], or dynamic such as in [23]. Textual data can enhance the node retrieval through supporting the matching process between the node attributes, on one side, and the textual query on the other side. This can be accomplished on a level that uses keywords or on a complex features level, where machine learning algorithms can play an effective role in learning the matching functions [24]. Knowledge graphs can also utilize other data mining methods to enhance the search and matching tasks. In [25], authors develop a concept to integrate the Resource Description Framework (RDF), as a machine understandable"
2105.05076,data,342,,,"In this paper, a new approach for a document tailored construction of knowledge graphs has been proposed, based on an extended text mining pipeline and the introduction of a linking node type, in order to enhance the graph’s utilization through connecting and weighting similar concepts. The objective of the research is to improve the microchip design quality, through increasing document visibility and findability for design engineers, based on the constructed graph. Multisource, multimodal, data sets have been utilized in the definition of document models and the extraction of interrelations amongst them. An extended text mining algorithm was utilized and tailored to define and construct the different types of extraction-based nodes in the knowledge graph. Nodes and relations were built upon the information extracted from multiple data types. An approach to enhance the graph’s construction and inter-linking has been proposed and tested in order to highlight the increased visibility of relevant queried design documentations to the user, through a search and recommendation scenario. Results of this approach show that the introduction of linking nodes positively influenced the document visibility and access by up to 43%.  The presented approach is specifically tailored to the individual types of documents present in the given use-case, which are utilized in a way to maximize the extraction and interlinking of relevant information, e.g. through further exploit their explicit, known structure. However, the exploited structural and contextual information is equally present in other failure, best-practice and design related documentations of products in other domains of manufacturing and can thus be applied and tailored to more domains as a novel integrated approach of knowledge graph generation and utilization. The next steps of this research are to include the development of extended recommendation algorithms that build upon the proposed relation-weight method, in order to improve the user feedback from the system, as well as the relevance of the search results."
2105.05076,"data available, data",73,,,"To handle the multiple sourced data fusion methods are needed. Among several fusion approaches reviewed within the state-of-the-art, such as [5]–[7], intelligent semantic methods provide an important solution to integrate multiple data modalities, such as text mining results and available structural information, in one framework that is queryable and can be a base for a decision support system. 2.1 Text Mining"
2105.05076,"data available, data",317,,,"In Equ.1, t is the term in question, d is the document in which the term t is looked up; and N is the total number of documents. Scores for the all tokens are calculated from the TF and the IDF parts. Then, a certain threshold determines the choice of tokens that will represent each document in a vector format. Document vectors provide a unified approach for describing the content of documents from the multiple sources through the most representative terms in those documents. This approach has been chosen for its benefits in the construction of the knowledge graph, as well as the fusion of information extracted from different documentation process. This, in turn, supports the search functions to perform the same query on all data sources, since they have a coherent representation in the graph. However, document vectors are not the only type of information extracted from the available data sources. Each of the available data sources contains extra information based on the source nature itself. For example, a project description, as a documentation source, inherits the information about the structure of the project, which is utilized to create links between certain documents. Moreover, failure documentations that contain free textual descriptions of maintenance procedures, also contain patterns that appear when the same person writes the case description, or when the workers in the design department use certain terminology amongst each other. Those patterns can also be mined and utilized to extract relations between maintenance documents. In order to achieve this goal, we exploit an N-gram approach in the text mining algorithm. N-grams are combinations of N number of tokens that appear in the same pattern over the textual data. In our proposed approach, we identify"
2105.05076,"data available, data, database, dataset",325,,,"In this work, textual data from multiple documentation sources has been collected for analysis. Multisource data includes maintenance reports of previous failure cases, written with a lessons-learned perspective, product descriptions and specifications, as well as design project structures and details from the semiconductor design. Since the obtained data was originally generated in a very specific context, which is the microchip design, it contains a considerable amount of symbolic descriptions, free textual inputs and remarks for the lessons-learned context, as well as semi-structured texts that include a considerable amount of misspellings, non-standard abbreviations, incomplete sentences and, in some cases, multiple languages. Handling those cases is an essential step to create a valid descriptive model of each document, which in turn will be the enabler for connecting those documents and mining the relations among them. To accomplish this goal, we utilize the previously described text mining pipeline as the first step to reduce undesired or error- containing tokens in the textual data. However, since standard text mining methods are not sufficient to create meaningful representations of the domain specific documents in hand, an extended text mining algorithm was built as a pipeline (Fig.1) that integrates an expert- and data-driven, rule-based, approach to handle domain specific differences in the textual data. After defining the multiple formats of documents from the available dataset, all texts have been extracted from those individual documents. Then, non-symbolic special characters are removed, along with specific patterns that are automatically generated from the database’s software. The remaining text is then filtered form punctuation and tokenized. The resulting tokens are lemmatized and stemmed in different phases of the procedure. Resulting tokens are also the input for the expert-supported part, where firstly, standard abbreviations are identified and isolated."
2105.05076,"data available, data, database, dataset",344,,,"However, since standard text mining methods are not sufficient to create meaningful representations of the domain specific documents in hand, an extended text mining algorithm was built as a pipeline (Fig.1) that integrates an expert- and data-driven, rule-based, approach to handle domain specific differences in the textual data. After defining the multiple formats of documents from the available dataset, all texts have been extracted from those individual documents. Then, non-symbolic special characters are removed, along with specific patterns that are automatically generated from the database’s software. The remaining text is then filtered form punctuation and tokenized. The resulting tokens are lemmatized and stemmed in different phases of the procedure. Resulting tokens are also the input for the expert-supported part, where firstly, standard abbreviations are identified and isolated. Then, special symbols that are domain-specific, such as multiple underscores, are also reconstructed so that they are handled similarly between the multiple document sources. With the reduction of non-meaningful tokens, a representative model of each document is constructed. Document models serve the goal of representing a full document with a vector of features, which is reliable to describe the extended content of the original document. In this paper, we utilize the TF-IDF algorithm (Equ.1) to extract most descriptive tokens from a document, as well as from the overall sum of available documents. In the microchip design use case, term frequency 𝑡𝑓 is capable of detecting meaningful repeating words in a design document, such as the term “oscillator” or “processor”. However, this part of the TF-IDF also detects non-relevant frequent words, such as “volt”, which do not hold a unique meaning in the document, although it is considerably frequent. This is why the IDF part enters the process, in order to balance the discovered terms identified in the TF part."
2105.05076,"data, database, dataset",307,,,"For example, failures in previous production processes are usually documented along the production line. However, it is not likely that a new worker will search for all possible failures that can take place when adjusting a certain parameter in a machine. In this case, a system that automatically highlights a previous failure case, which is related to this particular parameter or this machine, has the potential to save time or prevent possible damage. In this paper, we handle this challenge in a limited environment, which is the microchip design process. In the design phase of a new semiconductor product, the ability of the design engineer to have an overview on the database of previous design failure cases and tailored lessons learned, enables an improvement in the design process. It leads to time and material savings and quality enhancements, e.g. for the choice of combinations of electronic elements for the chip. This, in turn, is reflected as an overall quality improvement, not only of the product, but also of the design procedure itself. Therefore, our proposed system is meant to support the design process through enhancing the visibility of designrelated textual data from manufacturing-relevant datasets. It allows the design engineer to search for specific failure cases and provides automatic highlights of the previous failures that are related to the current design, which the engineer is working on. As shown in [2], especially in the field of semiconductor manufacturing, tailored instead of off-the-shelf solutions are continuously needed to enable smart solutions, despite growing amounts of relevant, tracked data. The textual data, utilized for this approach, reveals three challenges facing the preparation and searchability:"
2105.05076,"data, database, dataset",335,,,"Manufacturing processes produce a considerable amount of data, which contains, but is not limited to: measurements, parameters, reports and documentations [1]. With time, accessing especially the accumulated textual data becomes a long and rather inefficient search procedure, if it is approached through traditional searching methods, such as a key-word-based search. In this context, searching the available data is a mean of enhancing its visibility, aligned to the content of the respective search. However, it is not the only method of accessing data. Data visibility, to the users, can be handled through searching, hints or recommendations, among other methods. Depending on the context, one or more methods can be utilized to enable not only accessing textual data, but also extracting the specific intention of documents, such as previous experiences in the production process, in the form of lessons-learned documents from the past. For example, failures in previous production processes are usually documented along the production line. However, it is not likely that a new worker will search for all possible failures that can take place when adjusting a certain parameter in a machine. In this case, a system that automatically highlights a previous failure case, which is related to this particular parameter or this machine, has the potential to save time or prevent possible damage. In this paper, we handle this challenge in a limited environment, which is the microchip design process. In the design phase of a new semiconductor product, the ability of the design engineer to have an overview on the database of previous design failure cases and tailored lessons learned, enables an improvement in the design process. It leads to time and material savings and quality enhancements, e.g. for the choice of combinations of electronic elements for the chip."
2105.05076,"data, database, dataset",340,,,"In the pre-processing phase, raw textual data is cleaned from undesired strings, or strings that are without meaning for the respective case, such as special characters and email addresses. After cleaning, the text is divided into smaller pieces that can be analyzed. This is referred to as tokenization, in which the large corpus is cut into sentences, phrases or words, each called a token. In tokenization, some identifiers such as punctuations can be used to separate the individual tokens. Tokens are then also scanned to eliminate less meaningful tokens that are referred to as stop words. Stop words can be [11]: punctuations or pronouns, frequent words or, in some cases, non-frequent vocabulary. Extra steps can be performed such as stemming and lemmatization, in which different forms of the same name or verb are turned back to their basic dictionary form. In the context of semantic representation, information extraction methods that can discover the relations between textual data points are of considerable importance. This is due to the role they play in mapping the extracted knowledge and linking its information in a queryable manner, where the user can navigate from a textual entry or a search term, reaching relevant information. Information extraction has traditional methods that are manually crafted in many cases [12]; as well as intelligent methods based on machine and especially deep learning [13], [14]. In [14], authors consider relation extraction as a classification problem, in which different predefined relation types can be used to recognize the belonging of a relation in the corpus to a certain category. In this context, probabilistic methods accompany deep learning methods to perform the classification. Such approaches have been utilized by the authors in [15] to extract relations from free textual data and an existing knowledge base."
2105.05076,"data, database, dataset",342,,,"Their approach builds a domain specific knowledge graph, which can be queried in applications such as question answering and chatting. The graph is built on two levels, a static level from the predefined database, based on a domain specific terminology, and a dynamic graph that is built automatically from the free text. Relation extraction is conducted by the deep neural network through a matching task on a large training dataset. The resulting static and dynamic graphs are merged into one, allowing an effective knowledge representation and querying. Similar approaches can also be observed in [16]–[19]. With the development of search engines and the popularity they witnessed, retrieving the most relevant results for a search query got more focus in the literature. With textual data arranged in documents, several methods have been implemented for document matching, in order to find the result that best corresponds to a user input. A document in this context can be defined as a full or partial textual corpus, such as a paragraph or a sentence. Document matching is essential in applications such as search engines, question answering and recommender systems. In such fields, the matched documents are also ranked and ordered according to their relevance to the user query [20]. Ranking can be based on a weighting criterion such as the well-known Term Frequency-Inverse Document Frequency (TF-IDF) approach, which measures the importance of frequent words in a document, in contrast to rarely existing words throughout a set of documents. The assumption in this algorithm is that a frequent word in one document can be a feature that describes the document content, which can also be the case for words that exist in only few documents within a larger dataset. This allows a more differentiated retrieval of documents when such feature words are included in the user query. 2.2 Knowledge Graphs"
2105.05076,"data, database, dataset",347,,,"In the context of semantic representation, information extraction methods that can discover the relations between textual data points are of considerable importance. This is due to the role they play in mapping the extracted knowledge and linking its information in a queryable manner, where the user can navigate from a textual entry or a search term, reaching relevant information. Information extraction has traditional methods that are manually crafted in many cases [12]; as well as intelligent methods based on machine and especially deep learning [13], [14]. In [14], authors consider relation extraction as a classification problem, in which different predefined relation types can be used to recognize the belonging of a relation in the corpus to a certain category. In this context, probabilistic methods accompany deep learning methods to perform the classification. Such approaches have been utilized by the authors in [15] to extract relations from free textual data and an existing knowledge base. Their approach builds a domain specific knowledge graph, which can be queried in applications such as question answering and chatting. The graph is built on two levels, a static level from the predefined database, based on a domain specific terminology, and a dynamic graph that is built automatically from the free text. Relation extraction is conducted by the deep neural network through a matching task on a large training dataset. The resulting static and dynamic graphs are merged into one, allowing an effective knowledge representation and querying. Similar approaches can also be observed in [16]–[19]. With the development of search engines and the popularity they witnessed, retrieving the most relevant results for a search query got more focus in the literature. With textual data arranged in documents, several methods have been implemented for document matching, in order to find the result that best corresponds to a user input."
2105.05076,"data, dataset",336,,,"Search depth is defined as the number of intermediate relations that separate a base node from the furthest node connected to it. Within this definition, if the search depth is equal to 1, search results will contain base nodes, that are directly matching the query, and those that are connected to them. The deeper the search is, i.e. the more relation steps are followed between the nodes, the higher the number of results is. In fact, the number of results increases exponentially with the increase of search depth. Therefore, other parameters or search strategies are needed to govern the number of research results provided to the user. To handle this issue, relation weights are utilized in this work, alongside the search depth, to order search results by relevance and provide the user with a reasonable number of connected documents. In the passive search scenario, triggered by adding components to the design process, this method forms the base of a simple recommendation process, which offers the user hints about relevant maintenance documents that are related to an inserted element and, in general, the current design. Table 1 provides an overview of the relations extracted and utilized in the construction of the knowledge graph. Both inter-node-type and intra-node-type relations are shown in the table, and they highlight the condensed interlinking between failure case node in the graph. This is due to the fact that retrieving previous failure cases, as lessons learned, is the main objective of the graph search. This is also clear from Table 2, where the percentages of node numbers, relative to the total node count in the generated graph, is reported. It is noticeable that within the existing dataset, the number of maintenance documents, represented be FC nodes, is relatively low compared to other types of documents."
2105.05076,"data, dataset",350,,,"the more relation steps are followed between the nodes, the higher the number of results is. In fact, the number of results increases exponentially with the increase of search depth. Therefore, other parameters or search strategies are needed to govern the number of research results provided to the user. To handle this issue, relation weights are utilized in this work, alongside the search depth, to order search results by relevance and provide the user with a reasonable number of connected documents. In the passive search scenario, triggered by adding components to the design process, this method forms the base of a simple recommendation process, which offers the user hints about relevant maintenance documents that are related to an inserted element and, in general, the current design. Table 1 provides an overview of the relations extracted and utilized in the construction of the knowledge graph. Both inter-node-type and intra-node-type relations are shown in the table, and they highlight the condensed interlinking between failure case node in the graph. This is due to the fact that retrieving previous failure cases, as lessons learned, is the main objective of the graph search. This is also clear from Table 2, where the percentages of node numbers, relative to the total node count in the generated graph, is reported. It is noticeable that within the existing dataset, the number of maintenance documents, represented be FC nodes, is relatively low compared to other types of documents. This is a usual limitation facing industrial datasets, due to the level of digitalization in the factory and the confidentiality of available data. However, through the text mining of maintenance reports’ textual content and the thorough relation extraction between them, they still possess the highest number of links in the graph, which positively influenced their retrieval rates, reaching between 28% and 43% more visibility, depending on the search configurations."
2105.05076,"data, dataset provided",67,,,"In order to handle textual data without an explicit pre-known structure, text mining techniques provide solutions for extracting useful information from plain unstructured texts. However, textual data can be differently structured, depending on the type of document, which may enable to extract information that is more contextualized and therefore more specific. This work will consider three types of specialized documents:"
2105.05076,"data, dataset provided",310,,,"Since the number of connections in a knowledge graph can be overwhelming, the third challenge in this work is related to the ordering and recommendation of most relevant failure cases to a certain current design. This is handled with a simple recommendation approach that utilizes both, the information gained from the text mining algorithm and the construction of the graph nodes. In this paper, we introduce a text mining pipeline to construct knowledge graphs from product-design-focused documents with regard to the specific type of a document. Furthermore, we propose a text mining-based concept for enhancing the construction of the created knowledge graphs through the introduction of a special type of connection-focused nodes in the graph. We call this node type the linking nodes, which are proposed to enhance the connectivity between multisource and multimodal data nodes within the graph, in order to improve its searchability. The proposed approach also highlights main limitations in the application environment, which are mainly related to the data quality. The amount of data, as well as the amount of non-meaningful tokens, authoring misconceptions and irregular misspellings can influence a mining algorithm, since it can limit the amount of useful information extracted from it. We follow mitigation procedures to counteract those limitations and we address their effect on the results in the result section. The first findings of this approach are reported in this paper as follows: in the second section of the paper, the background of the utilized algorithms is presented. In the third and fourth sections, the proposed methodology is described regarding text mining and knowledge graphs respectively. The experimental results are presented in the fifth section, and the sixth section concludes this work along with its future perspectives."
2105.05076,"data, dataset provided",337,,,"Different sources of data provide important information [3], [4], regarding design and production processes. Those sources provide different data types, which accompany the main textual one, such as numerical values or structure-related data from the design project. In order to enable the user of accessing those different sources and their corresponding data types, this work solves the challenge with the utilization of knowledge graphs. As a network of interconnected data, a knowledge graph holds the potential to represents multiple data sources in one network. Therefore, different data sources and modalities can be intelligently fused together, in form of nodes, in the graph. This allows the search engine to access multiple modalities in one search command. Moreover, information that lies within the different sources also influences the intelligent construction of the graph itself, allowing more hidden links and relations between failure cases to be discovered, and thus, enhances the overall visibility of related lessons from previous design processes. Since the number of connections in a knowledge graph can be overwhelming, the third challenge in this work is related to the ordering and recommendation of most relevant failure cases to a certain current design. This is handled with a simple recommendation approach that utilizes both, the information gained from the text mining algorithm and the construction of the graph nodes. In this paper, we introduce a text mining pipeline to construct knowledge graphs from product-design-focused documents with regard to the specific type of a document. Furthermore, we propose a text mining-based concept for enhancing the construction of the created knowledge graphs through the introduction of a special type of connection-focused nodes in the graph. We call this node type the linking nodes, which are proposed to enhance the connectivity between multisource and multimodal data nodes within the graph, in order to improve its searchability."
2105.05076,"data, dataset provided",350,,,"The first challenge is the free nature of the textual input. Design failures are reported by engineers with semistructured and unstructured formats. Free textual input may also include symbolic descriptions of elements, standard abbreviations, internal domain-specific abbreviations and, in some cases, two languages in the failure description. Those issues have to be handled, in order to prepare the available data for a search engine. Therefore, an extended text mining pipeline was developed and utilized for the preprocessing of this data. The second challenge comes from the multisource nature of the available data. Different sources of data provide important information [3], [4], regarding design and production processes. Those sources provide different data types, which accompany the main textual one, such as numerical values or structure-related data from the design project. In order to enable the user of accessing those different sources and their corresponding data types, this work solves the challenge with the utilization of knowledge graphs. As a network of interconnected data, a knowledge graph holds the potential to represents multiple data sources in one network. Therefore, different data sources and modalities can be intelligently fused together, in form of nodes, in the graph. This allows the search engine to access multiple modalities in one search command. Moreover, information that lies within the different sources also influences the intelligent construction of the graph itself, allowing more hidden links and relations between failure cases to be discovered, and thus, enhances the overall visibility of related lessons from previous design processes. Since the number of connections in a knowledge graph can be overwhelming, the third challenge in this work is related to the ordering and recommendation of most relevant failure cases to a certain current design. This is handled with a simple recommendation approach that utilizes both, the information gained from the text mining algorithm and the construction of the graph nodes."
2105.05076,dataset,55,,,"The inverse document frequency 𝑖𝑑𝑓 in our use case identifies the tokens that appear in a limited number of documents among the total document dataset. Based on the assumption that those terms are most likely to represent the topic of the document, they are given a high score by the IDF algorithm."
2105.05076,"dataset provided, data, dataset",229,,,"The construction of a knowledge graph consists mainly of two steps: defining the graph’s nodes and extracting the relations between them. In this paper, a graph node is defined as a representation of an individual document. This representation is mainly accomplished with the document vector. However, since available datasets contain multiple document types that require different processing, graph nodes do yield a different meaning, depending on the specific document type they represent. In other words, although all document-representing nodes in the graph contain the feature vector of the document, each type of the documents attaches different information to the node that is representing it. In this way, a graph of multiple node types, has been constructed, in order to include all useful information extracted from multiple data sources. Examples of this complexity is the structural information that accompanies the textual data extracted from project descriptions. In this case, project documents provide not only the elements used in a certain microchip design, but also the hierarchical structure of the final chip. This information is added to those nodes in the graph which represent project description nodes.  In the proposed graph, conceptually illustrated in Fig. 2, we define three types of document representing nodes:"
2105.05076,"dataset provided, data, dataset",327,,,"format, with human understandable formats using natural language. Their approach is meant to support the representation of large heterogeneous and distributed datasets, which in turn helps in building decision support systems. Sinoara, Antunes and Rezende in [26] review the most utilized text mining approaches in semantic descriptions of data. In their study, the authors point out the majority of algorithms that exploit text classification and clustering in support of the graph construction or the matching and search within the constructed graph. They also highlight the role of domain experts in the utilization of textual data in knowledge graphs, since they can provide valuable information about the potential relations between the graph’s nodes. However, they find that domain experts are seldom integrated in the mining process, and they assign that to the complexity that is added to the system when considering interactive interfaces, which engage the expert user in the graph construction process. Moreover, the authors in [26] describe the visualization process in the context of knowledge graphs and text mining. Several approaches and tools are mentioned, which consider recommended methods of visualizing the relations within mined textual data, such as the Pinda tool [27], which adopts a hierarchical representation of texts using the k-means clustering algorithm for grouping the mining results. In [28], approaches are introduced to extract sub-knowledge-graphs as procedural representations, to partially exploit hierarchical information in semistructured texts. In this paper, we utilize the results from the state of the art and propose a new concept for a document aligned knowledge graph construction, in which text mining supports introducing a new type of nodes in the graph, called the linking nodes, in order to enhance the graph’s connectivity, multiple source data fusion and searchability."
2105.05076,python,82,,,"[12] S. Bird, E. Klein, and E. Loper, “Natural Language Processing with Python,” p. 504, 2009. [13] Y. S. Chan and D. Roth, “Exploiting background knowledge for relation extraction,” p. 9, 2010. [14] S. Zheng, J. Xu, P. Zhog, H. Bao, Z. Qi, and B. Xu, “A neural network framework for relation extraction: Learning"
2105.10148,data,33,,,"Philip Thomas and Emma Brunskill. Data-eﬃcient oﬀ-policy policy evaluation for reinforcement learning. In Proceedings of The 33rd International Conference on Machine Learning, volume 48, pages 2139–2148, 2016."
2105.10148,data,42,,,Table 1: BSuite tasks. Every Catch episode has 9 transitions. The average length of an episode in the Mountain Car/Cartpole increases/decreases as the level of environment randomness p increases. The training and validation data ratio is 9:1.
2105.10148,data,92,,,"In this paper, we focus on the problem of Oﬄine Policy Evaluation (OPE), also known in the literature as Oﬀ-policy Policy Evaluation. OPE involves estimating the value of a new policy using logged data produced by possibly many diﬀerent policies. This is a problem of great signiﬁcance because individuals and organizations often need to choose a single policy among a wide set of proposed policies for deployment. Choosing which policy to deploy well can result in improved user satisfaction, or better medical treatments."
2105.10148,data,102,,,"We follow the hyper-parameter selection method in Bennett et al. (2019b). For every candidate of hyper-parameter setting and every checkpoint during the training i, we evaluate the value function Qθi and adversarial function gτi on a ﬁxed set of validation data points. The metric for choosing the hyper-parameters is the objective in Equation (28) except that it is evaluated on the validation set (both Ψn and the , and Q˜θ in the regularization expectation in Rg), the function g is in the ﬁnite set Rg is averaged over all Qθi."
2105.10148,data,137,,,"Our main ﬁndings are that when doing OPE for a policy near to that which generated the available data, the confounding eﬀect can be very pronounced, and ignoring it — as in Deterministic Bellman Residual Minimization (DBRM) (Saleh and Jiang, 2019) — is problematic. In this scenario, we ﬁnd that the best IV method - AGMM - performs on par with FQE, and is only outperformed by distributional FQE. On more diﬃcult scenarios where the evaluation policy is far from the behavioral policy, additional eﬀects due to a combination of distribution shift and model mismatch come into play. In this context, while AGMM performs on par FQE and DFQE, DBRM is also competitive, while being more stable than competing methods."
2105.10148,data,147,,,"We note that the derivation by Bradtke and Barto (1996) requires that the value function lives in the linear subspace of the features, Equation (12). Then they show that as the number of data increases, the solution converges under regularity conditions to the least-square ﬁxed-point approximation (without mentioning it). The instrumental variable interpretation also requires the structural function in Equation (14) to hold, which is derived from Equation (12). Convergence of the LSTD algorithm does not require Equation (12) to be valid, however Lagoudakis and Parr (2003) show that there is indeed no need to make such an assumption, and obtain a direct derivation by leastsquare ﬁxed-point approximation. We refer the readers to Lagoudakis and Parr (2003) for this alternative interpretation."
2105.10148,data,155,,,"Deep neural networks have made it possible for reinforcement learning (RL) to attain superhuman performance in challenging domains such as ATARI from raw sensory data (Mnih et al., 2015) and Go (Silver et al., 2016). While RL is starting to be used for realworld applications (Bellemare et al., 2020), its adoption remains fairly limited. Standard RL techniques require repeated interaction with the environment. This is often too costly to implement practically, and running a poor policy could lead to disastrous outcomes (e.g., in power plants or healthcare decision-making systems). While controlling a large and/or complex real-world system can be costly and risky, data acquisition is often comparatively cheap. The goal of oﬄine RL is to evaluate and learn new policies based only on logged data, without any interaction with the environment."
2105.10148,data,167,,,"very useful. On more realistic examples from BSuite and DM Control, we have investigated scenarios where the evaluation policy is close to the behavioral one and where it is far from it. When doing OPE for a policy near to the one having generated data, we ﬁnd that the confounding eﬀect could be very pronounced and that techniques like DBRM are performing poorly. We also ﬁnd that the best IV method - AGMM - displays performance on par with FQE and is only outperformed by distributional FQE. However, when evaluating a policy far from the one(s) having generated the observations, the combination of distribution shift and model mismatch has a non-negligible impact on performance. While we observe that AGMM also performed on par with FQE and DFQE, DBRM surprisingly also performs very well, and in general appears more stable (suﬀers from fewer outliers with poor performance) than FQE and DFQE."
2105.10148,data,186,,,"Let us consider a simple MDP with 100 discrete states allocated uniformly along the interval [ . The agent always starts at the ﬁrst 0, 1, . . . , 99 } state s0 in every episode and terminates at the last state s99. There is only a single action, a = right, in every state to move to right, and therefore the policy is always ﬁxed. The state is transitioned to the right neighboring state with a probability of p > 0 or stays in the same location. It is easy to show that the resulting state distribution µ(s) pooled from diﬀerent steps across the trajectory, i.e. the oﬄine data distribution, , whatever the value is uniform among all the non-terminating states i of p and µ(s99) = pµ(s98). The reward function is deﬁned with a Gaussian kernel as s2 R = exp( 0.22 ), and is illustrated in Figure 4 together with Q(s, a = right)."
2105.10148,data,253,,,"We show that the popular reinforcement learning (RL) strategy of estimating the state-action value (Q-function) by minimizing the mean squared Bellman error leads to a regression problem with confounding, the inputs and output noise being correlated. Hence, direct minimization of the Bellman error can result in signiﬁcantly biased Qfunction estimates. We explain why ﬁxing the target Q-network in Deep Q-Networks and Fitted Q Evaluation provides a way of overcoming this confounding, thus shedding new light on this popular but not well understood trick in the deep RL literature. An alternative approach to address confounding is to leverage techniques developed in the causality literature, notably instrumental variables (IV). We bring together here the literature on IV and RL by investigating whether IV approaches can lead to improved Q-function estimates. This paper analyzes and compares a wide range of recent IV methods in the context of oﬄine policy evaluation (OPE), where the goal is to estimate the value of a policy using logged data only. By applying diﬀerent IV techniques to OPE, we are not only able to recover previously proposed OPE methods such as modelbased techniques but also to obtain competitive new techniques. We ﬁnd empirically that state-of-the-art OPE methods are closely matched in performance by some IV methods such as AGMM, which were not developed for OPE1. Keywords: forcement learning; Two Stage Least Squares; Oﬄine policy evaluation"
2105.10148,data,339,,,"A plethora of methods have been proposed to address this problem; see e.g. Precup (2000); Precup et al. (2001); Dud´ık et al. (2011); Thomas and Brunskill (2016); Jiang and Li (2016); Liu et al. (2018); Farajtabar et al. (2018); see also Levine et al. (2020); Fu et al. (2021) for recent reviews. We focus here on methods that are relying on an estimate of the state-action value function, known as the Q-function. It is well-known that the Q-function can be estimated by minimizing the mean squared Bellman error. However, the resulting regression problem is not standard as the inputs and the output noise are correlated, leading to some confounding. We show here that ﬁxing the target Q-network in the popular Deep Q-Networks (DQN) (Mnih et al., 2013) and Fitted Q Evaluation (FQE) (Le et al., 2019) can be re-interpreted as a strategy addressing this confounding. We then investigate a diﬀerent class of approaches to address the same problem. In causal inference, Instrumental Variables (IV) regression is a standard strategy for learning causal relationships between confounded treatment and outcome variables from observational data by utilizing an instrumental variable, which aﬀects the outcome only through the treatment (Stock and Trebbi, 2003). The connection between RL and IV ideas was made early on by Bradtke and Barto (1996) when introducing Least Square Temporal Diﬀerences (LSTD), a method to estimate on-policy linearly parameterized value functions. Their derivation made use of the two-stage least squares (2SLS) algorithm, the most standard IV regression technique. However, the connection between RL and IV seemed to have been largely ignored ever since in the literature."
2105.10148,"data, dataset",47,,,Table 2: DM Control Suite tasks. Every episode has 1000 transitions. The training and validation data ratio is 9:1. The oﬄine dataset of the Humanoid Run task is subsampled with by 10% from the 100K episodes generated from the training process.
2105.10148,"data, dataset",99,,,"We ﬁrst study the performance of all the algorithms on the easy oﬄine dataset with a near-policy data distribution and suﬃciently larget data size. Figure 8 shows the scatter plot of the estimated policy value versus the ground-truth value. Each dot represents the mean and 1-standard deviation of the estimate from 5 random runs for every environment and every random level. Additionally, we show the absolute error of the estimates for each task in Figure 9 and a box-plot of the distribution of errors pooled from all tasks as a summary in Figure 10."
2105.10148,"data, dataset",170,,,"We display the absolute error of the estimated state value at the initial state Q(s0, a = right) in Figure 5. We ﬁnd the error of LSTD-Q increases and eventually becomes on par with DRBM when we reduce the size of the dataset, or increase the data distribution shift, which also decreases the eﬀective dataset size. The error of LSTD-Q also increases when we reduce the number of features, which will lead to model misspeciﬁcation, and could violate the IV requirement that the residual needs to have zero mean. Lastly, we see that DBRM is unbiased when the dynamics are deterministic, p = 1, but the bias increases quickly when p decreases. In contrast, the error of LSTD-Q increases but much more slowly, which we suspect is due to an increasingly diverse distribution of transitions (s, s(cid:48)) that requires more data to learn the estimate accurately."
2105.10148,"data, dataset",192,,,"Nonetheless, this observation is indeed understandable. The estimation error of the policy value comes from multiple sources including: (1) the approximation error due to lack of model capacity (2) generalization error due to the limited dataset size (3) the oﬀ-policyness, or the divergence between the oﬄine data distribution and the target policy distribution, which aﬀects the eﬀective size of the dataset (4) optimization error, aﬀected by the optimization algorithm, and (5) the bias of ignoring the stochasticity of the dynamics. As demonstrated in Section 5.1, the beneﬁts of IV methods depend on multiple conditions. Compared to the large near-policy dataset on BSuite, the pure oﬄine dataset, including the continuous control tasks has higher modeling challenges, less data, and a larger divergence between the behavior and target distributions. All the other sources of error may dominate the bias present in DBRM, and due to the simplicity of DBRM as a single minimization problem, it is simpler to optimize than the loss of the other algorithms."
2105.10148,"data, used dataset, dataset",226,,,"For the easy dataset, we consider the three BSuite tasks only. For every task we deﬁne the behavior policy in the same way as the target policy except with a slightly larger exploration probability of ε = 0.3 instead of 0.1. Therefore the behavior policy is close to the target policy. We then play the behavior policy in the corresponding environment repeatedly and collect a suﬃciently large oﬀ-policy dataset for each task. For the hard dataset, we restart the agent training process with a diﬀerent random seed and collect the episodes along the training. The resulting dataset consists of episodes generated from various partially trained policies, some of which are close to the initial random policies while others are close to a well-optimized policy. The dataset is then split randomly into training and validation subsets with a ratio of 9:1. This is akin to the data generation protocol in the RL Unplugged dataset (Gulcehre et al., 2020) with two diﬀerences: (1) we modify environments with random dynamics from the original deterministic environment, (2) the dataset is collected from the training process of a diﬀerent random seed, therefore the policies used to generate the dataset could be substantially diﬀerent from the target policy to be evaluated."
2105.10148,dataset,2,,,Near-policy Dataset
2105.10148,dataset,3,,,5.4.1 Near-policy dataset
2105.10148,dataset,3,,,Pure Oﬄine Dataset
2105.10148,dataset,4,,,5.4.3 Pure offline dataset
2105.10148,dataset,8,,,5.2.2 Target policies for evaluation and offline datasets
2105.10148,dataset,12,,,Figure 13: Estimated policy value vs groundtruth with the oﬄine dataset
2105.10148,dataset,12,,,Figure 8: Estimated policy value vs groundtruth with the near-policy dataset
2105.10148,dataset,12,,,"valid dataset with a ratio of 9:1, train each algorithm on"
2105.10148,dataset,13,,,Figure 14: Absolute error of policy value estimation with the oﬄine dataset
2105.10148,dataset,13,,,Figure 9: Absolute error of policy value estimation with the near-policy dataset
2105.10148,dataset,15,,,Figure 10: Distribution of the absolute error across all tasks with the near-policy dataset
2105.10148,dataset,15,,,Figure 15: Distribution of the absolute error across all tasks with the oﬄine dataset
2105.10148,dataset,34,,,"where ˆf is the optimal solution of Equation (27) on a dataset, and f0 is the optimal solution of arg inf f ∈F supg∈G Ψ(f, g)."
2105.10148,dataset,37,,,"where the expectation in Ψ is the empirical estimate from a dataset of size n. Rf (f ) and Rg(g) are regularization terms for f and g, respectively, for identiﬁability."
2105.10148,dataset,40,,,"Figure 10 shows that DFQE is the most accurate OPE method on this dataset. Among the IV methods, AGMM, ASEM and DFIV all give fairly small estimation errors across all tasks while AGMM performs the best."
2105.10148,dataset,43,,,"Nonetheless, by inspecting the estimation error in Figure 14 and Figure 15 we notice that the relative performance of IV methods is roughly in agreement with the results observed in the near-policy dataset. AGMM is the most robust IV method implemented"
2105.10148,dataset,47,,,"As suggested in Hartford et al. (2017b) we choose the hyper-parameters associated with training the treatment network according to the log-likelihood on the validation dataset, and those associated with training the value network according to the regression loss on the validation dataset."
2105.10148,dataset,50,,,"We choose the hyper-parameters of each IV algorithm using the recommended method in their original work. To be self-contained, we give a brief description for each method in train and validation this section. We split the oﬄine dataset randomly into a training train and choose the best"
2105.10148,dataset,53,,,"where Φ, Φ(cid:48), and R are matrices where every row corresponds respectively to the transpose of φ(s, a), φ(s(cid:48), a(cid:48)) and r from the oﬄine dataset, except for a(cid:48)"
2105.10148,dataset,56,,,"We then evaluate all the algorithms on the hard dataset in both BSuite and DM Control environments. The episodes are generated from a mixture of partially trained policies from a diﬀerent run, and the distribution of states is likely to have a quite diﬀerent coverage from the distribution generated by the target distribution."
2105.10148,dataset,64,,,"Figure 5: Simple MDP ablation study. Each plot shows the absolute error of Q(s0, a = right) as a function of dataset size, number of features, stochasticity of the dynamics, and the distribution shift between the dataset and that generated by the target policy. The dots represent the default setting in Figure 4."
2105.10148,dataset,74,,,"When an agent is prohibited to interact with the environment directly, one has to rely on an existing dataset of trajectories or transition tuples (s, a, r, s(cid:48)), to estimate the policy value or learn the optimal policy. The dataset could have been collected by one or a mixture of potentially unknown policies of potentially unknown analytical form, denoted by πb("
2105.10148,dataset,75,,,"A simple algorithm to approximate the objective of Equation (5) using transition samples (s, a, r, s(cid:48), a(cid:48)) from the dataset is known as Deterministic Bellman Residual Minimization (DBRM) (Saleh and Jiang, 2019). We consider here a simple variant of DBRM as a baseline with two independent action samples from the given target policy,"
2105.10148,dataset,76,,,"The goal of oﬄine policy evaluation (OPE) is to evaluate the value of a target policy, π, based on the oﬄine behavior dataset. This problem has been extensively studied in the literature. The readers are referred to Levine et al. (2020) for a review and Voloshin et al. (2019b); Fu et al. (2021) for benchmarks of recent OPE algorithms."
2105.10148,dataset,91,,,"Note that due to the state distribution shift between the behavior policy and target policy, the best hyper-parameter setting on the validation dataset from the behavior distribution does not guarantee a good performance when evaluating the target policy value. It remains an open research problem how to select the hyper-parameter for OPE given one does not have access to the ground truth value (Paine et al., 2020). We explain the metric adopted for selecting the hyper-parameters of each algorithm in details in Appendix B."
2105.10148,dataset,102,,,"As there is only a single policy in this environment, the target policy is the same as the behavior policy. We sample a dataset of N = 105 transitions with p = 0.5, and estimate the state value using a ﬁxed set of D = 90 Gaussian kernel features . For this linear instrumental vari0, 1, . . . , 89 φj(s) = exp } { able regression problem, we compare the LSTD-Q method in Section 3 with DBRM, which reduces to a naive least square minimization algorithm in this case."
2105.10148,dataset,113,,,"Next, we conduct an ablation study to investigate how the advantage of IV method depends on the following four variables: dataset size N , feature dimensions D, transition p), and the extent of oﬀ-policyness. While the target policy always randomness (1 matches the behavior policy in this problem, we create an oﬄine dataset with a shifted distribution by sampling the states with the following distribution µ(s) exp(αs), where α = 0 corresponds to the original uniform distribution, and a larger value of α leads to a shifted distribution towards the right end of the state space."
2105.10148,dataset,138,,,"Some of the algorithms implemented are sensitive to the choice of hyper-parameters. In order to ensure a fair comparison, we run a thorough hyper-parameter search for every algorithm in every environment. We randomly sample up to 100 hyper-parameter settings for every algorithm and choose the setting with the best metric on a heldout validation dataset. Due to the large number of tasks (environment and dataset combinations), we search for the best hyper-parameter at one environment random level in every dataset (p = 0.2 for BSuite and σ = 0.4 for DM Control tasks) and apply the same setting to other levels. Once the hyper-parameter is selected, we run each algorithm with 5 random seeds for every task to measure the mean and variance of the estimate."
2105.10148,dataset,188,,,"We run the default DQN agent for every random level of the three BSuite tasks and the default D4PG agent for the four DM Control tasks from the ACME library (Hoﬀman et al., 2020) until the episodic return does not increase noticeably any more. The number of training episodes is provided in Table 1 and 2. We use the learned policy with a small amount of action noise as the target policy for evaluation. For the DQN agent, we use an ε-greedy policy, that is, taking the greedy action of arg maxa Q(s, a) with a probability 1 ε and a random action otherwise where ε = 0.1. For the D4PG agent, we use the learned policy network with an additive Gaussian noise with a standard deviation of 0.2. We consider two types of oﬄine datasets with a diﬀerent level of diﬃculty for OPE: an easy near-policy dataset and a hard pure oﬄine dataset. The size of the dataset for each environment is given in Table 1 and 2."
2105.10148,dataset,230,,,"Most algorithms provide an accurate estimate of the policy value on this dataset, except Deep GMM, Deep IV, and KIV. We observe experimentally that the training process of Deep GMM is very unstable compared the other two adversarial approaches (AGMM and ASEM). Figure 11 shows a typical trajectory of the training loss and the estimated policy value along the training process. We suspect it is due to the ˜θ in the regularization term. Deep IV fails with both a use of the optimal weighting large mean absolute error and variance. In particular, in the Catch environment, the generative model completely fails to predict the next state. This illustrates the challenge of modeling a moderately-high dimensional state space (50-dimensions) using a simple feed-forward network to predict the parameters of a mixture of Gaussian generative model as proposed in Hartford et al. (2017b). We expect the performance to improve with a more sophisticated generative model as evidenced by recent model-based OPE work in Zhang et al. (2021). KIV fails in the Cartpole and Mountain Car environments with a large error too. This is in agreement with the observation by Xu et al. (2021) that shallow features are not capable of modeling complex structural functions."
2105.10148,github,4,,,4. https://github.com/microsoft/AdversarialGMM/tree/main/mliv/neuralnet
2105.10148,"github, dataset, open-source",82,,,"• We evaluate the performance of these techniques empirically on a variety of tasks and environments, including Behaviour Suite (BSuite) (Osband et al., 2019) and DeepMind Control Suite (DM Control) (Tassa et al., 2020). We found experimentally that some of the recent IV techniques such as AGMM display performance on par with state-of-the-art FQE methods. We open-source the implementation of all methods and datasets at https://github.com/liyuan9988/IVOPEwithACME."
2105.10148,"open-source, code, open-source code, github, dataset",23,,,Instrumental variable regression; Generalized method of moments; Rein 1. We open-source all our code and datasets at https://github.com/liyuan9988/IVOPEwithACME
2106.03536,"download, code available, code",26,,,[9] National Grid. Draft Grid Code – Grid Forming Converter Speciﬁcation. [Online]. Available: https://www.nationalgrideso. com/document/159296/download
2106.05260,data,3,,,imal spider-biologist-denies-suspicions-widespread-data-fraud-his-animal-personality.
2106.05260,data,5,,,4.1 Visualizing High Dimensional Data
2106.05260,data,5,,,5.4.1 Discrete/Discrete Data Type Pairs
2106.05260,data,5,,,5.4.2 Continuous/Continuous Data Type Pairs
2106.05260,data,5,,,5.4.3 Discrete/Continuous Data Type Pairs
2106.05260,data,7,,,5.3 Setting Parameter Arguments and Loading Data
2106.05260,data,10,,,Figure 3: permutations of possible data type pairings.
2106.05260,data,12,,,• Generate charts for each feature pair based on data type;
2106.05260,data,13,,,SIRIUS: A MUTUAL INFORMATION NETWORK TOOL FOR EXPLORATORY VISUALIZATION OF MIXED DATA
2106.05260,data,15,,,Sirius: A Mutual Information Network Tool for Exploratory Visualization of Mixed Data A PREPRINT
2106.05260,data,15,,,• Compute mutual information between each pair of features based on detected data type;
2106.05260,data,16,,,large-scale heterogenous data sets are common in a wide range of real-world data analysis tasks.
2106.05260,data,18,,,High MutualInformationLow MutualInformationSirius: A Mutual Information Network Tool for Exploratory Visualization of Mixed Data A PREPRINT
2106.05260,data,21,,,"Keywords mutual information, exploratory analysis, feature selection, feature extraction, graph mining, network graphs, data visualization"
2106.05260,data,21,,,"• Support EDA of high numbers of features of mixed continuous, discrete, and binary data types, as these"
2106.05260,data,23,,,Converting this method from probability mass functions to probability density functions for a pair of continuous data type features yields the formula:
2106.05260,data,23,,,"John T. Behrens. Principles and procedures of exploratory data analysis. Psychological Methods, 2(2):131–160, 1997."
2106.05260,data,26,,,"Brian C Ross. Mutual information between discrete and continuous data sets. PLoS One, 9(2):e87357, 2014. ISSN"
2106.05260,data,28,,,Generate User Interface with FeatureNetwork Graph; Show Pair Charts on Edge ClickDDHHQK0.840.921.032.12      Sirius: A Mutual Information Network Tool for Exploratory Visualization of Mixed Data A PREPRINT
2106.05260,data,28,,,"We present prior contributions to scientiﬁc literature on the topics of visualization for high-dimensional data, using network graphs to visualize feature relationships, and mutual information."
2106.05260,data,29,,,AA - Liver failure clusterC - Sodium clusterB - Cirrhosis/Bilirubinpairwise feature comparisonCBcirrhosisd1_bilirubin_minwith cirrhosiswithout cirrhosisSirius: A Mutual Information Network Tool for Exploratory Visualization of Mixed Data A PREPRINT
2106.05260,data,32,,,(A) Small Multiples(B) Matrix(C) Dendrogram(D) NetworkSirius: A Mutual Information Network Tool for Exploratory Visualization of Mixed Data A PREPRINT
2106.05260,data,36,,,"David I. Warton. Regularized Sandwich Estimators for Analysis of High-Dimensional Data Using Generalized Estimating Equations. Biometrics, 67(1):116–123, 2011. ISSN 1541-0420. doi:10.1111/j.1541-0420.2010.01438.x. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2010.01438.x."
2106.05260,data,38,,,"David Gotz and Harry Stavropoulos. DecisionFlow: Visual Analytics for High-Dimensional Temporal Event Sequence Data. IEEE Transactions on Visualization and Computer Graphics, 20(12):1783–1792, December 2014. ISSN 1941-0506. doi:10.1109/TVCG.2014.2346682."
2106.05260,data,39,,,"For a pair of discrete data type features, mutual information is calculated using probability mass functions across all non-null i ∈ U and j ∈ V . The mutual information formula for discrete-discrete pairs is given as:"
2106.05260,data,40,,,"Xiaoming Gao, Emilio Ferrara, and Judy Qiu. Parallel Clustering of High-Dimensional Social Media Data Streams. In 2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, pages 323–332, May 2015. doi:10.1109/CCGrid.2015.19."
2106.05260,data,41,,,"Min Chen, Shiwen Mao, and Yunhao Liu. Big Data: A Survey. Mobile Networks and Applications, 19(2): 171–209, April 2014. ISSN 1572-8153. doi:10.1007/s11036-013-0489-0. URL https://doi.org/10.1007/ s11036-013-0489-0."
2106.05260,data,42,,,"Jinwook Seo and Ben Shneiderman. A Rank-by-Feature Framework for Interactive Exploration of Multidimensional Data. Information Visualization, 4(2):96–113, June 2005. ISSN 1473-8716. doi:10.1057/palgrave.ivs.9500091. URL https://doi.org/10.1057/palgrave.ivs.9500091. Publisher: SAGE Publications."
2106.05260,data,43,,,"E. Bertini, A. Tatu, and D. Keim. Quality metrics in high-dimensional data visualization: An overview and systematization. IEEE Transactions on Visualization and Computer Graphics, 17(12):2203–2212, Dec 2011. ISSN 2160-9306. doi:10.1109/TVCG.2011.229."
2106.05260,data,45,,,"Eun Ju Nam, Yiping Han, Klaus Mueller, Alla Zelenyuk, and Dan Imre. ClusterSculptor: A Visual Analytics Tool for High-Dimensional Data. In 2007 IEEE Symposium on Visual Analytics Science and Technology, pages 75–82, October 2007. doi:10.1109/VAST.2007.4388999."
2106.05260,data,45,,,"• Association networks are only for comparing features of a binary data type. There is no opportunity to integrate into the “Lift"" algorithm an item for which there are multiple responses, e.g. “Blueberry"" or “Raspberry"" or"
2106.05260,data,46,,,Consider thisnetwork:Static thresholding:Edges below a certain weight are cutEDGESStaticThresholdDynamicThresholdsEDGESDense clusters aregiven prioritySparse clustersare preserved &dense clusters arebroken apartBackbone sparsification:Edge importance determined by local significanceSirius: A Mutual Information Network Tool for Exploratory Visualization of Mixed Data A PREPRINT
2106.05260,data,46,,,"We provide three domain-speciﬁc example data sets with Sirius, and discuss two such data sets as case studies here. We encourage readers to test this program on other high-dimensional data sets, and to share results, obstacles, or extensible functionality requests."
2106.05260,data,46,,,"• In all of these contexts, provide a dual view of data, allowing rapid task switching between macro-scale, ’big picture’ feature relationships; and micro-scale, record-level exploratory analysis, wherein speciﬁc subpopulation and individual data can be accessed visually."
2106.05260,data,52,,,α1: C = 0Number ofConnectedComponents(C)Edge Weight Significance Threshold (α)α2: C = 2α3: C = 4α3: C = 5Cmaxα4: C = 4α5: C = 1Sirius: A Mutual Information Network Tool for Exploratory Visualization of Mixed Data A PREPRINT
2106.05260,data,54,,,Initial observations of the mutual information feature network show promise for the method as a new technique for exploratory analysis. Parameterization preserves customizability for speciﬁc data structures and domain applications. Further statistical measures of resultant network structures offer potential avenues for insight into more complex feature relationships in high-dimensional data analysis.
2106.05260,data,61,,,"Adil Fahad, Najlaa Alshatri, Zahir Tari, Abdullah Alamri, Ibrahim Khalil, Albert Y. Zomaya, Sebti Foufou, and Abdelaziz Bouras. A Survey of Clustering Algorithms for Big Data: Taxonomy and Empirical Analysis. IEEE Transactions on Emerging Topics in Computing, 2(3):267–279, September 2014. ISSN 2168-6750. doi:10.1109/TETC.2014.2330519."
2106.05260,data,61,,,"One approach for EDA of a limited number of features is small multiples [MacEachren et al., 2003]. By tiling features across rows and columns, all permutations of paired feature charts can be viewed simultaneously. This method is advantageous in that it supports visualization of heterogenous data types, by selecting appropriate charts for each feature"
2106.05260,data,62,,,"Meredith Lee, Jesse Raffa, Marzyeh Ghassemi, Tom Pollard, Sharada Kalanidhi, Omar Badawi, Karen Matthys, and Leo Anthony Celi Celi. Wids (women in data science) datathon 2020: Icu mortality prediction. PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals, 2020. doi:10.13026/vc0e-th79."
2106.05260,data,65,,,"However, there are growing ethical concerns among researchers and the general public about ‘black box’ models, which can obscure inadvertent statistical biases in the machine learning pipeline and lead to disparate real-world impacts [Zafar et al., 2017, Feldman et al., 2015]. Simply excluding protected class data, such as race or income, from model"
2106.05260,data,65,,,"rendered is determined by the data types of the two features being compared. These charts include ridge line plots, heat maps, and 2D kernel density plots with scatter plot overlays. This information theoretic approach enables researchers to quickly view individual record data through statistically identiﬁed feature relationships of potential interest, with many tuning parameters for iterative customization in exploration."
2106.05260,data,68,,,"SciKit Learn’s feature_selection library is used to compute mutual information for all data type pairs using the mutual_info_regression function. This is described in Section 5.6[Pedregosa et al., 2011]. Further research could warrant additional parameterization or comparative evaluation of various other mutual information estimator methods, but this is generally accepted as a benchmark standard method of computing mutual information."
2106.05260,data,71,,,"Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and Removing Disparate Impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’15, pages 259–268, Sydney, NSW, Australia, August 2015. Association for Computing Machinery. ISBN 978-1-4503-3664-2. doi:10.1145/2783258.2783311. URL https://doi.org/10. 1145/2783258.2783311."
2106.05260,data,75,,,"The purpose of this technique is to inform subsequent hypothesis generation, clustering, feature selection, imputation and predictive modeling in a data science pipeline. In this paper, we: review existing high dimensional exploratory visualization approaches; describe the technique and mechanisms; discuss examples of domain-speciﬁc applications; and note areas for potential future research in the visualization space of high dimensional feature exploration through mutual information network graphs."
2106.05260,data,76,,,"pairing (e.g. scatter plots for continuous-continuous pairings, heatmaps or conditional bar charts for discrete-discrete pairings, and violin plots or ridgeline plots for discrete-continuous pairings) as illustrated in Fig. 2. This is the approach of INFUSE: Interactive Feature Selection for Predictive Modeling of High Dimensional Data, which uses small multiples of pie charts showing feature importance scores across quantitative features [Bertini et al., 2011]."
2106.05260,data,77,,,"Correlation networks, popularized by Weighted Gene Correlation Network Analysis (WGCNA), are commonly applied in genomics research [Langfelder and Horvath, 2008]. Large network graphs are generated in which nodes represent genes, and edges are drawn according to co-expression correlation across cell lines. Data types for this kind of visualization are classically all continuous values, such as gene expression probabilities [Niemira et al., 2019]."
2106.05260,data,79,,,"Saumyadipta Pyne, Xinli Hu, Kui Wang, Elizabeth Rossin, Tsung-I. Lin, Lisa M. Maier, Clare Baecher-Allan, Geoffrey J. McLachlan, Pablo Tamayo, David A. Haﬂer, Philip L. De Jager, and Jill P. Mesirov. Automated high-dimensional ﬂow cytometric data analysis. Proceedings of the National Academy of Sciences, 106(21):8519–8524, May 2009. ISSN 0027-8424, 1091-6490. doi:10.1073/pnas.0903028106. URL https://www.pnas.org/content/106/21/8519."
2106.05260,data,85,,,"• Suggest related features which could be grouped together in a lower-dimensional visualization, as through Principle Component Analysis (PCA). For example, in a healthcare setting, there may be many features related to body systems, such as blood sugar, cardiovascular status, or liver health. Quickly identifying all such groupings can allow data scientists to extract a single feature to represent all features in a given system and more easily generate meta-analyses of interacting body systems."
2106.05260,data,94,,,"These kinds of feature-driven networks, in high-dimensional settings, can often become extremely dense and difﬁcult to navigate during exploratory analysis. Both association networks and correlation networks also do not support comparisons across heterogeneous data types. The key value propositions of the Sirius mutual information feature network method described in Section 5.5 are 1) backbone sparsiﬁcation, which acts as a form of dimensionality reduction to simplify the exploratory space, and 2) comparison among and between continuous and discrete data types, as described in section 5.4."
2106.05260,data,98,,,"This work was supported in part by funding from Massachusetts Mutual Life Insurance Company, through the establishment of the MassMutual Center of Excellence in Complex Systems and Data Science at the University of Vermont. Our deepest thanks to Mark Maier, Adam Fox, and Sears Merritt at MassMutual Data Science; and Juniper Lovato, Melissa Rubinchuk, and Alexa Woodward at the University of Vermont; for coordinating and supporting this institutional research partnership. Thanks also to David Rushing Dewhurst for aiding in our understanding of mutual information and kernel density estimation."
2106.05260,data,100,,,"The feature networks generated by Sirius from the ICU mortality data also highlight the importance of graphical representation. Consider the feature group highlighted in Figure 7. Referring to the table, summary statistics about the subgroups of patients with and without metastatic solid tumors are extremely close and difﬁcult to differentiate. However, when the mutual information feature network is computed, the slight differences in pCO2 readings among tumor-positive and tumor-negative patients are highlighted in the exploratory analysis space, and researchers are given readily available access to interact with these speciﬁc feature comparison charts."
2106.05260,data,102,,,"Sindhu P Menon and Nagaratna P Hegde. A survey of tools and applications in big data. In 2015 IEEE 9th International Conference on Intelligent Systems and Control (ISCO), pages 1–7, January 2015a. doi:10.1109/ISCO.2015.7282364. Rakesh Agrawal, Tomasz Imieliundeﬁnedski, and Arun Swami. Mining association rules between sets of items In Proceedings of the 1993 ACM SIGMOD International Conference on Management of in large databases. Data, pages 207–216, New York, NY, USA, 1993. Association for Computing Machinery. ISBN 0897915925. doi:10.1145/170035.170072. URL https://doi.org/10.1145/170035.170072."
2106.05260,data,111,,,"Figure 2: Some different approaches for graphically representing feature relationships. From left to right: Small multiples (A), which are advantageous for small numbers of features of homogenous or heterogenous data types; (B) Matrices, i.e. correlation or conditional probability matrices, and (C) dendrograms, both advantageous for comparing small or medium numbers of features of a homogenous data type using summary statistics or similarity/distance measures; and (D) Networks which support a high number of features, usually of a homogenous data type, but supporting heterogenenous data types through the Sirius mutual information implementation."
2106.05260,data,113,,,"The mutual information feature network, as with other feature networks used for feature selection and data mining, is concerned only with edges between pairs of features, not multidimensional interactions between more than two features. Feature networks fall short of addressing statistical concerns such as Simpson’s Paradox, in which unidentiﬁed tertiary conditions render summary conclusions invalid [Berman et al., 2012]. Future research could involve topological analysis of feature networks to identify confounding variables and generate multivariate visualizations upon interaction with network graph faces (contained by 3 or more vertices) through the use of simplicial complexes [Jonsson, 2008]."
2106.05260,data,114,,,"We discern between scales in that ‘record level’ refers to pairwise graphs showing data for individual records in a data set (e.g. “540 East Street"", “Patient 001754"", or “Grocery Order #AQB54"") whereas ‘feature level’ observations refer to relationships between features generally (e.g. “Number of Bedrooms"" and “Lot Size"", “Blood Oxygen"" and “Asthma"", or “Whole Milk"" and “Cookies""). It may help to think of ‘features’ as the columns in a spreadsheet, and ‘records’ as rows."
2106.05260,data,117,,,"• Highlight potential ’proxy variables’ or features with implications for privacy and equity. Unexpected feature relationships can sometimes indicate disparate impacts for certain populations, or deanonymization vulnerabilities in sensitive data. For example, low appraisal values of homes in certain zip codes may warrant investigation of potential redlining effects. An innocuous ""ID number"" feature intended to anonymize protected patient health data, when shown to be associated with a hospital admittance source, can drastically narrow a search space for nefarious actors seeking to deanonymize data. Bringing these unforeseen associations to the attention of decision-makers can provide vital insight for policy-making and have signiﬁcant consequences for stakeholders."
2106.05260,data,120,,,"Svante Wold, C. Albano, W. J. Dunn, U. Edlund, K. Esbensen, P. Geladi, S. Hellberg, E. Johansson, W. Lindberg, and M. Sjöström. Multivariate Data Analysis in Chemistry. In Bruce R. Kowalski, editor, Chemometrics: Mathematics and Statistics in Chemistry, NATO ASI Series, pages 17–95. Springer Netherlands, Dordrecht, 1984. ISBN 978-94017-1026-8. doi:10.1007/978-94-017-1026-8_2. URL https://doi.org/10.1007/978-94-017-1026-8_2. Yapeng Su, Qihui Shi, and Wei Wei. Single cell proteomics in biomedicine: High-dimensional data acquisition, visualization, and analysis. PROTEOMICS, 17(3-4):1600267, 2017. ISSN 1615-9861. doi:10.1002/pmic.201600267. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/pmic.201600267."
2106.05260,data,123,,,"Sirius is designed to take place after the extract, transform, load (ETL) step in a data processing pipeline. It is recommended that data be cleaned to remove outliers, and that temporal and geospatial columns be removed or encoded (e.g. map string birth dates to numeric ages, or map numeric postal codes to string county names). This step is omitted from the scope of this paper due to the unique edge cases that often arise in real-world data science applications. However, the intended generalizability of this technique means that it is designed not to break in these edge cases, but rather force-classify features as continuous or discrete data types."
2106.05260,data,126,,,"A heterogenous pairing of data types employs a nearest neighbor mutual information regression method[Ross, 2014, Pedregosa et al., 2011]. Discrete features with r responses are converted from a 1D array of length m to a sparse matrix of shape (r, m), with columns for each unique response and values of 0 or 1 for all records k. Nearest neighbor approximations of mutual information in a sample population are less susceptible to variance due to parameterization than traditional binning methods; e.g. adjusting the neighbor N count for each point x does not inﬂuence the variance in mutual information as signiﬁcantly as adjusting the bandwidth of a Gaussian kernel[Ross, 2014]."
2106.05260,data,130,,,"David I Shuman, Sunil K. Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging ﬁeld of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83–98, May 2013. ISSN 1558-0792. doi:10.1109/MSP.2012.2235192. Sindhu P Menon and Nagaratna P Hegde. A survey of tools and applications in big data. In 2015 IEEE 9th International Conference on Intelligent Systems and Control (ISCO), pages 1–7, January 2015b. doi:10.1109/ISCO.2015.7282364. Efﬁcient Machine Learning for Big Data: A Review. Big Data Research, 2(3):87–93, September 2015. ISSN 2214-5796. doi:10.1016/j.bdr.2015.04.001. URL http://www.sciencedirect.com/science/article/pii/ S2214579615000271."
2106.05260,data,133,,,"Association networks are commonly employed in market basket analysis, in which edges are drawn between items if purchasing one item increases the likelihood of purchasing a different item, a weight referred to as the ‘lift’, derived from the conditional probabilities of purchasing two items in the same transaction [Agrawal et al., 1993]. Thus, these association networks are directed weighted networks, and are classically deﬁned for boolean data types only. The resultant network visualizations show feature groupings among products which are frequently purchased together, and might provide actionable insight for marketers looking to target potential consumers of a certain product. Association networks are discussed in more detail in Section 6.2 in reference to the ‘groceries’ case study."
2106.05260,data,157,,,"Mutual information is a way of evaluating dependence of two features by measuring entropy reduction. If knowing the response to one feature for a given record increases one’s accuracy of predicting that record’s value for another feature, it is likely that these two features share high mutual information. In the mutual information feature network technique we present here, features with high mutual information would be connected in the resulting network graph. Conversely, if knowing the response to one feature does not signiﬁcantly improve prediction of a second feature, those features are likely relatively independent and will have a low mutual information. In the Sirius application, an edge between these two unrelated features’ nodes will likely not be displayed in the graph. An abstracted matrix of data type pairings and mutual information ranges to aid understanding is presented in Fig. 3."
2106.05260,data,193,,,"We are careful here not to compare feature networks to real-world networks (i.e. social networks, ecological systems, power grids). The network visualization is used simply as a relational graph. It remains to be seen whether the mutual information feature network (or association networks or gene correlation networks) are subject to the same properties as real-world networks, e.g. the ‘friendship paradox’ (in which the average node is connected to nodes with a higher average number of connections), or eligible for traditional network study such as percolation theory or community detection [Feld, 1991]. Meta-analysis of the properties of various mutual information feature networks could yield interesting insight into organic feature relationship behavior, as compared to null models of synthetically generated data. For example, the connectivity or graph spectra of real mutual information networks could be fundamentally different from feature networks of synthesized high-dimensional data, which may help data scientists to more easily identify cases of data fraud or imputation through examination of anomalous feature relationships [Pennisi, 2020]."
2106.05260,data,213,,,"Statistical analysis provides robust mathematical insight for a wide range of domain applications, including ﬁnance [Griebel and Holtz, 2010], marketing [Russell and Petersen, 2000, Raeder and Chawla, 2011], genomics [Langfelder and Horvath, 2008, Nam et al., 2007, Pyne et al., 2009], chemistry [Wold et al., 1984, Su et al., 2017, McCarthy et al., 2004], environmental science [Warton, 2011], and social interaction [Gao et al., 2015, Gotz and Stavropoulos, 2014, Shuman et al., 2013]. Advances in computational power and storage, coupled with a rapid uptick in data collection and exchange, have caused massive increases in the volume of data collected: by some estimates, 90% of all current data was created in the past few years [Menon and Hegde, 2015b, Al-Jarrah et al., 2015]. This has engendered the development of powerful machine learning tools which support imputation, community detection, clustering, and prediction [Fahad et al., 2014, Ward and Barker, 2013, Chen et al., 2014]."
2106.05260,"data available, data",2,,,6.0.1 Data
2106.05260,"data available, data",85,,,"“Did not purchase"", nor is there a prescribed way to include quantitative information, (e.g. “Milk is bought commonly in transactions exceeding $30 or visits in excess of 15 minutes"".) If this data was available, the continuous features “transaction cost"" or “visit duration"" could be included as nodes in the mutual information network graph, but would have to be converted to a binary encoding in an association network."
2106.05260,"data available, data",107,,,"inputs is not sufﬁcient to offset disparate impacts when other features act as proxy variables [Wan, 2018, Skeem and Lowenkamp, 2016]. This problem is in part due to the obfuscation of raw data (e.g. through summary statistics, feature projection, or complex neural network architectures) from data scientists during the exploration and development phases of the statistical modeling pipeline. Exploratory Data Analysis (EDA) preserves data scientists’ access to record level data, while addressing the growing need to holistically explore a large number of complexly related features [Behrens, 1997]."
2106.05260,"data https, dataset",32,,,"Irfan Nasrullah. Groceries Market Basket Dataset, 2019. URL https://kaggle.com/irfanasrullah/groceries. Dean De Cock. House Prices: Advanced Regression Techniques, 2019. URL https://kaggle.com/c/"
2106.05260,"data, data https",23,,,"Jonathan Stuart Ward and Adam Barker. Undeﬁned By Data: A Survey of Big Data Deﬁnitions. arXiv:1309.5821 [cs],"
2106.05260,"data, data https, open-source, data available, package, github, python",284,,,"Data scientists across disciplines are increasingly in need of exploratory analysis tools for data sets with a high volume of features [Menon and Hegde, 2015a]. We expand upon graph mining approaches for exploratory analysis of high-dimensional data to introduce Sirius, a visualization package for researchers to explore feature relationships among mixed data types using mutual information and network backbone sparsiﬁcation. Visualizations of feature relationships aid data scientists in ﬁnding meaningful dependence among features, which can engender further analysis for feature selection, feature extraction, projection, identiﬁcation of proxy variables, or insight into temporal variation at the macro scale. Graph mining approaches for feature analysis exist, such as association networks of binary features, or correlation networks of quantitative features, but mixed data types present a unique challenge for developing comprehensive feature networks for exploratory analysis [Agrawal et al., 1993, Raeder and Chawla, 2011]. Using an information theoretic approach, Sirius supports heterogeneous data sets consisting of binary, continuous quantitative, and discrete categorical data types, and provides a user interface exploring feature pairs with high mutual information scores [Kraskov et al., 2004, Ross, 2014]. We leverage a backbone sparsiﬁcation approach from network theory as a dimensionality reduction technique, which probabilistically trims edges according to the local network context [Serrano et al., 2009]. Sirius is an open source Python package and Django web application for exploratory visualization, which can be deployed in data analysis pipelines. The Sirius codebase and exemplary data sets can be found at: https://github.com/compstorylab/sirius."
2106.05260,"data, dataset",23,,,A comparison of the Association Network and the mutual information feature network of the groceries data set are shown in Figure 9.
2106.05260,"data, dataset",38,,,"In the healthcare data set, the mutual information feature network gives valuable insight into feature relationships within body systems. As shown in Figure 6, features are often connected that have strong clinical importance to one"
2106.05260,"data, dataset",47,,,"• A healthcare data set related to intensive care unit (ICU) mortality, comprised of 186 features: a mixture of demographic information, lab results, and risk assessments (e.g “Blood Glucose"", “Weight"", and “Apache 2"
2106.05260,"data, dataset",51,,,"• The Ames housing data set is comprised of 81 features of mixed data types for 1,460 records of homes in Ames, Iowa [Cock, 2019]. It includes physical characteristics of homes such as dimensions and materials, as well as qualitative assessments and sale information."
2106.05260,"data, dataset",62,,,"The healthcare data set contains 91,713 records of admitted ICU patients, comprised of 186 features. Features include demographic information, lab results, and risk assessments [Lee et al., 2020]. There are a number of features directly related to mortality prediction. Expectedly, many of the risk assessment features scored high in mutual information."
2106.05260,"data, dataset",63,,,"• A groceries data set comprised of 169 features: unique items in a grocery store (e.g. “Vegetables"", “Cleaning Supplies"", and “Specialty Meats""), with boolean values (corresponding to ‘purchased’ or ‘not purchased’) from 9,835 records of market basket transactions [Nasrullah, 2019]."
2106.05260,"data, dataset",76,,,"The resultant network graphs for these data sets group features with statistically signiﬁcant dependence, with potential applications for: clinicians using data similar to the ICU mortality data set; marketers with data resembling the groceries data set; or real estate agents or tax assessors examining data paralleling the housing data set. This indicates promise for other real-world applications which require comparisons across a large number of features with differing data types."
2106.05260,"data, dataset",91,,,"Figure 1: Screenshots of the Sirius web application: (A) The mutual information feature network generated from a provided example data set, along with a graph ﬁlter side panel which allows a user to select a feature of interest. (B) A scatter plot showing record-level data for two continuous features of interest, selected by clicking on an edge between feature nodes. Both the network graph and the pairwise charts are interactive, allowing users to pan and zoom for in-depth analysis."
2106.05260,"data, dataset",92,,,"Figure 9: A side-by-side comparison of (left) an association network of the groceries data set, thresheld at a static edge weight such that 50% of edges remain; and (right) the Sirius mutual information network of the same data set, with dynamic thresholding using the alpha values from the network backbone sparsiﬁcation step of the data processing. The right panel menu allows rapid user search to ﬁnd features of interest, with selected node(s) connections highlighted in the graph."
2106.05260,"data, dataset",123,,,"Figure 6: We show an annotated screenshot from Sirius of a portion of the healthcare data set, with groups of feature names related to body systems and measurements. In particular, feature group (A) contains features related to liver function, including quantitative bilirubin readings and boolean ﬂags for cirrhosis and liver failure. This comports with clinical studies which suggest that elevated bilirubin levels may indicate poor liver function [Chen and Lin, 2017]. Paired feature chart (B) shows a ridgeline plot of patient record-level data comparing one such bilirubin value with a diagnostic ﬂag for cirrhosis. Feature group (C) includes features related to sodium levels in patients."
2106.05260,"data, dataset",163,,,"The groceries data set contains 169 unique items in a grocery store (e.g. “Ham"", “Flower Soil / Fertilizer"", “Grapes"", “Butter""), which we consider our ‘features’, with a “1"" or “0"" for each record corresponding to whether that item was purchased or not in each of 9,835 transactions [Nasrullah, 2019]. This data set is unique in our examples, in that it only contains a binary response: all features only have 1 of 2 possible answers, and this data set is included primarily to demonstrate that the mutual information feature network technique can also be employed for data sets of homogenous data types, e.g. all discrete or all continuous features, and the resultant networks can be compared to other feature networks such as the association network, as shown in Figure 9."
2106.05260,"data, dataset",181,,,"Traditional visualization methods are generally vertically scalable: that is, as the number of records grows, the challenge for visual representation is primarily in computational power, e.g. browsers’ memory struggling to support rendering of thousands of circle glyphs in a scatter plot on a web page. However, horizontal scalability (increasing the number of features) when visualizing data for exploratory analysis presents a challenge for traditional aesthetic encoding methods: while a data set with 4 features f might visually encode f1: x-position, f2: y-position, f3: point size and f4: color for each record in a scatterplot, increasing the number of features to 100 is not simply a question of increasing visual complexity to add z-position, shape, texture, time, transparency, soniﬁcation, etc. encodings up to f100. Instead of continuing this line of ever-growing encoding complexity, it becomes helpful to consider applications of visual or statistical summary methods for analysis of feature dependence in a visual context."
2106.05260,"data, dataset",182,,,"Figure 8: Annotated screenshots from Sirius of portions of the groceries data set show feature groupings related to product types. In (A) we see ‘bottled beer’ connected to other alcohol-related products: prosecco, red blush wine, liquor (appetizer) and liquor. In (B) we see the makings of a ham sandwich: processed cheese, ham, and white bread. Feature groups in (C) include (C1): Frozen meals and frozen dessert; (C2): cling ﬁlm bags and housekeeping products; and (C3) zwieback (a kind of cracker that originated in Eastern Europe) connected to ‘chocolate marshmallow’ which may correspond to Krembo–two items that might be found in the Kosher foods section of a grocery store. Screenshot (D) shows ‘chocolate’ connected to ﬁnished products (i.e. snacks), candy, and wafﬂes. These kinds of feature groupings could inform an analyst about product placement or marketing campaigns."
2106.05260,"data, dataset",227,,,"Each feature is classiﬁed as either discrete or continuous for the purposes of mutual information method selection and visualization chart type. These do not necessarily map to numeric or string data types. The unique response count threshold between discrete and continuous features is a parameter that can be adjusted in the parameter ﬁle, or in the command line argument ﬂags. If the number of unique responses for a feature is less than the speciﬁed threshold, the feature is treated as discrete. Thus, a numeric feature with unique numeric responses {1,2,3} would be treated as a discrete (categorical) value by default, and its corresponding charts would either be heatmaps or ridgeline plots, depending on whether it was paired with a discrete or continuous feature, respectively. Features are also classiﬁed as discrete when the number of unique responses exceeds the threshold in the event that the unique feature responses cannot be converted to numeric values (e.g. {‘AU’,‘EU’,‘UK’...} when all country codes are present in the data set). This may produce heatmaps with many cells, or ridgeline plots with many rows, in charts comparing this feature to another discrete or continuous feature, respectively."
2106.05260,"data, dataset provided",103,,,"We have selected three chart types (heatmap, ridgeline density, and 2D kernel density) for each of the three permutations of data type pairing supported by Sirius. However, other visual encodings could be investigated. For example, grouped bar charts may be a more intuitive alternative to heatmaps for some users when comparing two discrete features; boxplots, violin plots, or bean plots are valid alternative visual encodings for pairings of discrete and continuous data types. These alternatives could be provided as parameter settings or toggles in future iterations of user interface design."
2106.05260,"data, open-source, data open-source , package, python",140,,,"We introduce the mutual information feature network as a technique compiling established information theory and network science algorithms into a tool for exploratory analysis. We provide Sirius3, a Python package and web application to demonstrate this technique [Adams et al.]. The result is an interactive network graph of feature dependence weighted by mutual information, with charts of 2-dimensional feature relationships shown upon interaction. The mutual information feature network aids exploratory analysis of feature level relationships by narrowing the scope of visual comparison among features through statistical methods, while preserving access to record level data among feature pairs of potential interest. Sirius is an open source repository for implementation of the mutual information feature network, supported with several example data sets of the sort commonly used in real-world data analysis."
2106.05260,"data, package",69,,,"After installing the Sirius package according to the instructions in the package README, all data processing can be run from the command line or through the provided walkthrough notebook. There are a number of customizable parameters in the Sirius script, which can be changed using ﬂags when running the script from the command line, or directly through the declared variables in the notebook ﬁle."
2106.05260,"data, package",126,,,"In this way, there is not a speciﬁc mutual information threshold set for the entire network, but rather each node is evaluated individually, and its edges are preserved or thinned according to how relatively important an edge is to that node speciﬁcally. An alpha threshold is chosen for which the number of graph components C is maximal, which provides for unique ’constellations’ in the visualization, as shown in Fig 5. This is where we have derived the name Sirius for our exploratory analysis package: Sirius, the brightest star in the night sky, provided navigational support for sailors, just as we hope to provide directional insight for wayward data scientists [Gregersen]."
2106.05260,"data, package, dataset",51,,,"A synthetic data set is also included in the package, with 34 features of mixed data types for 2,400 artiﬁcial records. Feature names correspond to statistical properties of synthesized data. This data is not discussed here, as it is primarily for interface testing and demonstration purposes."
2106.05260,"data, package, python",73,,,"The Sirius Python package includes a README with directions on how to run the data processing pipeline and web application. After running the Sirius data processing pipeline, either in-notebook or via the command line, users can specify the graph and chart json ﬁles generated by the Python package to be used by the application, and interact with the visualizations in a web browser through the provided Django application."
2106.05260,"data, python",22,,,"Wes McKinney. Data structures for statistical computing in python. In Stéfan van der Walt and Jarrod Millman, editors,"
2106.05260,"data, python, package, dataset",61,,,"Diagnosis"") from 91,713 records of admitted ICU patients [Lee et al., 2020]. Note that due to the large size of this data set, the raw data s not included in the package itself, but can be pulled from the Kaggle API using the Python script included in the corresponding ICU data folder."
2106.05260,"dataset provided, data, dataset",64,,,"As Sirius is an exploratory analysis tool, many insights and conclusions can be drawn from the data sets provided. We have chosen to describe here a small handful of these insights: feature grouping patterns seen in the healthcare data set; and a comparison of the Sirius mutual information feature network to an association network of the groceries data set."
2106.05260,github,7,,,Exploratory analysis tool. URL github.com/compstorylab/sirius.
2106.05260,github,66,,,"Erik Gregersen. Sirius | Facts & Location. URL https://www.britannica.com/place/Sirius-star. NetworkX developer team. Networkx, 2014. URL https://networkx.github.io/. Thomas M. J. Fruchterman and Edward M. Reingold. Graph drawing by force-directed placement. Software: Practice and Experience, 21(11):1129–1164, 1991. ISSN 1097-024X. doi:10.1002/spe.4380211102. URL https: //onlinelibrary.wiley.com/doi/abs/10.1002/spe.4380211102."
2106.05260,"github, data available, data, data https",15,,,3The Sirius codebase and exemplary data sets can be found at: https://github.com/compstorylab/sirius
2106.05260,package,34,,,"Peter Langfelder and Steve Horvath. Wgcna: an r package for weighted correlation network analysis. BMC Bioinfor matics, 9(1):559, Dec 2008. ISSN 1471-2105. doi:10.1186/1471-2105-9-559."
2106.05260,"publicly available, data",18,,,Example publicly available data sets used with this tool include (in order of record volume):
2106.05260,python,16,,,"Proceedings of the 9th Python in Science Conference, pages 51 – 56, 2010."
2106.05260,python,66,,,"F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011."
2106.05260,"python, data, open-source data, open-source",37,,,"The open source implementation of this method, Sirius, performs data processing in Python using Pandas data structures, and renders a network visualization in-browser using a Django web application[McKinney, 2010]."
2106.07229,data,3,,,3.2.2 Data packing
2106.07229,data,5,,,3.2.3 Data range and precision
2106.07229,data,24,,,[2] Qian Lou and Lei Jiang. SHE: A fast and accurate deep neural network for encrypted data. Advances in
2106.07229,data,46,,,"[22] Fabian Boemer, Yixing Lao, Rosario Cammarota, and Casimir Wierzynski. nGraph-HE: A graph compiler for deep learning on homomorphically encrypted data. In Proceedings of the 16th ACM International Conference on Computing Frontiers, pages 3–13, 2019."
2106.07229,data,49,,,"[23] Fabian Boemer, Anamaria Costache, Rosario Cammarota, and Casimir Wierzynski. nGraph-HE2: A high-throughput framework for neural network inference on encrypted data. In Proceedings of the 7th ACM Workshop on Encrypted Computing & Applied Homomorphic Cryptography, pages 45–56, 2019."
2106.07229,data,53,,,"[16] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In Proceedings of International Conference on Machine Learning (ICML), pages 201–210, 2016."
2106.07229,data,68,,,"[19] Ahmad Al Badawi, Jin Chao, Jie Lin, Chan Fook Mun, Sim Jun Jie, Benjamin Hong Meng Tan, Xiao Nan, Aung Mi Mi Khin, and Vijay Chandrasekhar. Towards the Alexnet moment for homomorphic encryption: HCNN, the ﬁrst homomorphic CNN on encrypted data with GPUs. IEEE Transactions on Emerging Topics in Computing, 2020."
2106.07229,data,83,,,"Most of the operations in the ResNet-20 are convolutions with zero-padded input to maintain their size. We use the packed single input single output (SISO) convolution with stride 1 used in Gazelle [20], which has low complexity for the encrypted data. Convolution with stride 2 is also required to perform down-sampling. The striding convolution was also proposed by Gazelle [20] to decompose an input and a ﬁlter and perform convolutions and adding."
2106.07229,data,121,,,"We modify the striding convolution of Gazelle to reduce the required number of rotation operations. In the conventional striding convolution, we need to rearrange the encrypted data when decomposing the data, but the rearrangement increases the additional rotation operations. Since the rotation operation requires time-consuming key-switching procedures, it is desirable to reduce the number of rotation operations as much as possible. Instead of rearranging the slots to a well-structured channel, we perform the stride-2 convolution by extracting the valid values by multiplying the window kernel consisting of 0 and 1 from the stride-1 convolution result as shown in Figure 2:(b). It does not require any additional rotation operations."
2106.07229,data,134,,,"Classiﬁcation accuracy Even if ML models are trained with the same hyper-parameters, the ML models have different performances because weights are initialized to random values for each training. Thus, the ML model performance, such as accuracy, is shown as the average values obtained by training several times. However, since we focus on implementing ResNet-20 for homomorphically encrypted data, we train this model only once, not many times. Nevertheless, we have shown that the encrypted ResNet-20 operation is possible with almost the same accuracy as the original ResNet-20 paper [Reference]. Furthermore, since it is implemented in the FHE with bootstrapping, it can be expected that the same result will be obtained for a deeper network than the Resnet-20."
2106.07229,data,158,,,"Running time The running time for the proposed model, which is about 4 hours, is somewhat large for practical use. This work ﬁrstly shows the possibility of applying the FHE to standard deep learning models with high accuracy, but it has to be optimized and improved in various ways to reduce the running time. Therefore, the essential future work is the advanced implementation of the ResNet-20 with the RNS-CKKS scheme with various accelerators realized using GPU, FPGA, or ASIC. Since research on implementing the state-of-the-art FHE scheme is advancing rapidly, the ResNet-20 over the encrypted data will be made more practically soon. Also, we implement the PPML model for only one image, and the running time per image can be much improved if we properly use the packing method of the RNS-CKKS scheme. We leave this optimization for many images as future work."
2106.07229,data,159,,,"Table 4 shows the agreement ratio between the classiﬁcation results of the implemented privacypreserving ResNet-20 and that of the original ResNet-20, which shows almost the same results. We test the inference on 75 encrypted images, and the 95% conﬁdence interval is suggested for each result. Only one result of our proposed model for the encrypted data shows a different result from that of the ResNet-20 model for plaintext data. In other words, the agreement ratio is 98.67%± 2.59%, which is a sufﬁciently high agreement result. The classiﬁcation accuracy of the ResNet-20 for the encrypted data is 90.67%± 6.58%, while that of the original ResNet-20 for the corresponding plaintext image is 89.33%± 6.99%. Thus, we verify that the ResNet-20 can be successfully carried out using the RNS-CKKS scheme with sufﬁcient accuracy for classiﬁcation and the proper bootstrapping operation."
2106.07229,data,168,,,"Since we have to consume many depths to implement the ResNet-20 on the RNS-CKKS scheme, many bootstrapping procedures are required to ensure enough homomorphic multiplications. For the ﬁrst time, we apply the bootstrapping technique to perform the deep neural network such as the ResNet-20 on the encrypted data and prove that the FHE scheme with the state-of-the-art bootstrapping can be successfully applied for privacy-preserving deep neural networks. Since the SEAL library does not support any bootstrapping technique, we implement the most advanced bootstrapping with the SEAL library [10, 11, 12]. The COEFFTOSLOT and the SLOTTOCOEFF are implemented using collapsed FFT structure [8] with depth 2. The MODREDUCTION is implemented using the composition of the cosine function, two double angle formulas, and the inverse sine function [9, 11], where the cosine function and the inverse sine function are approximated with the multi-interval Remez algorithm as in [11]."
2106.07229,data,171,,,"The most successful PPML model on the homomorphically encrypted data until now was constructed with the TFHE homomorphic encryption scheme by Lou and Jiang [2], but it used the leveled version of the TFHE scheme without bootstrapping, which is not an FHE scheme. In other words, they chose in advance the parameters that can be used to perform the desired network without bootstrapping. If we want to design a deeper neural network with the leveled homomorphic encryption scheme, much impractically larger parameters have to be used, and it causes heavy run-time or memory overhead. Further, since the packing technique cannot be applied easily in the TFHE scheme, it can cause additional inefﬁciency with regard to the running time and the memory overhead if we want to process many data at once. Thus, it is desirable to use the FHE with moderate parameters and bootstrapping, which naturally supports the packing technique in the PPML model."
2106.07229,data,180,,,"The most crucial issue when using the bootstrapping of the RNS-CKKS scheme is the bootstrapping failure. 1,149 bootstrapping procedures are required in our model, and the result of the whole neural network can be largely distorted if even one of the bootstrapping procedures fails. The bootstrapping failure occurs when one of the slots in the input ciphertext of the MODREDUCTION procedure is not on the approximation region. The approximation interval can be controlled by the bootstrapping parameters (K, (cid:15)), where the approximation region is ∪K−1 i=−(K−1)[i−(cid:15), i+(cid:15)] [7]. While the parameter (cid:15) is related to the range and the precision of the input message data, the parameter K is related to the values composing the ciphertext and not related to the input data. Since the values contained in the ciphertext are not predictable, we have to investigate the relation between the bootstrapping failure probability and the parameter K."
2106.07229,"data, data https",38,,,"[18] Tim van Elsloo, Giorgio Patrini, and Hamish Ivey-Law. SEALion: A framework for neural network inference on encrypted data. arXiv preprint, abs/1904.12840, 2019. http://arxiv.org/abs/1904. 12840."
2106.07229,"data, data https",43,,,"[1] Junghyun Lee, Eunsang Lee, Joon-Woo Lee, Yongjune Kim, Young-Sik Kim, and Jong-Seon No. Precise approximation of convolutional neural networks for homomorphically encrypted data. arXiv preprint, abs/2105.10879, 2021. http://arxiv.org/abs/2105.10879."
2106.07229,"data, dataset",117,,,"The model parameters are prepared by the following training method. We use 32 x 32 color images, subtract the mean of the pixels in the training dataset, and adopt a data argumentation method such as shifting and mirroring horizontally for training. We adopt the He initialization [30] as the weight initialization and no dropout. We train the model with 32 × 32 mini-batches and cross-entropy loss function. The learning rate starts with a 0.001 learning rate divided by 10 after 80 epochs and 100 after 120 epochs during training. The classiﬁcation accuracy with the trained model parameters is 91.89%, which is tested with 10,000 images."
2106.07229,"data, dataset",130,,,"We prepare the pre-trained model parameters by training the original ResNet-20 model with the CIFAR-10 plaintext dataset and perform the privacy-preserving ResNet-20 with these plaintext pre-trained model parameters and encrypted input images. We ﬁnd that the inference result of the proposed implemented privacy-preserving ResNet-20 is 98.67% identical to that of the original ResNet-20. It achieves 90.67% classiﬁcation accuracy, which is quite close to the original accuracy of 91.89%. Thus, we verify that the proposed implemented PPML model successfully performs the ResNet-20 on the encrypted data. Further, the proposed implementation result shows the highest accuracy among the CNN implementation with the FHE scheme, while the previous highest result with the word-wise FHE scheme is only 77% classiﬁcation accuracy."
2106.07229,"data, dataset",201,,,"HE-friendly network Some previous works re-design the machine learning model compatible with the HE scheme by replacing the standard activation functions with the simple non-linear polynomials [16, 17, 18, 19], called the HE-friendly network. However, these machine learning models are usually successful only for the simple MNIST dataset and cannot reach sufﬁciently high accuracy for the CIFAR-10 dataset. The highest classiﬁcation accuracy of the HE-friendly CNN with the simple polynomial activation function implemented by word-wise HE is only 77% for the CIFAR-10 dataset [19]. Since the choice of the activation functions is sensitive in the advanced machine learning model, it may not be desirable to replace the standard and famous activation functions with simple arithmetic functions. Moreover, the additional pre-training process has to be followed before the PPML service is given. Since the training process is somewhat expensive in that it is pretty time-consuming and needs quite a lot of data, it is preferable to use the standard model such as ResNet and VGGNet for the plaintext data with its pre-trained model parameters when the data privacy has to be preserved."
2106.07229,"data, dataset",230,,,"In this paper, we ﬁrstly implement the ResNet-20 model for the CIFAR-10 dataset [13] using the residue number system CKKS (RNS-CKKS) [5] scheme, which is a variant of the CKKS scheme using the SEAL library 3.6.1 version [14], one of the most reliable libraries implementing the RNSCKKS scheme. ResNet is one of the historic convolutional neural network (CNN) models which enables a very deep neural network with high accuracy for complex datasets such as CIFAR-10 and ImageNet. Many high-performance works for image classiﬁcation are based on the ResNet model since these models can reach sufﬁciently high classiﬁcation accuracy by stacking more layers. We ﬁrstly apply the ReLU function based on the composition of minimax approximate polynomials [1] to the encrypted data. Using the results, we ﬁrstly show the possibility of applying the FHE with the bootstrapping to the standard deep machine learning model by implementing ResNet-20 over the RNS-CKKS scheme. We use the RNS-CKKS bootstrapping with the SEAL library [14]. The SEAL library is one of the most practical RNS-CKKS libraries, but it does not support the bootstrapping technique. The used bootstrapping can support sufﬁciently high precision to successfully use the bootstrapping in the ResNet-20 with the RNS-CKKS scheme for the CIFAR-10 dataset."
2106.07229,"data, dataset",350,,,"Fully homomorphic encryption (FHE) is one of the prospective tools for privacypreserving machine learning (PPML), and several PPML models have been proposed based on various FHE schemes and approaches. Although the FHE schemes are known as suitable tools to implement PPML models, previous PPML models on FHE encrypted data are limited to only simple and non-standard types of machine learning models. These non-standard machine learning models are not proven efﬁcient and accurate with more practical and advanced datasets. Previous PPML schemes replace non-arithmetic activation functions with simple arithmetic functions instead of adopting approximation methods and do not use bootstrapping, which enables continuous homomorphic evaluations. Thus, they could not use standard activation functions and could not employ a large number of layers. The maximum classiﬁcation accuracy of the existing PPML model with the FHE for the CIFAR-10 dataset was only 77% until now. In this work, we ﬁrstly implement the standard ResNet-20 model with the RNS-CKKS FHE with bootstrapping and verify the implemented model with the CIFAR-10 dataset and the plaintext model parameters. Instead of replacing the non-arithmetic functions with the simple arithmetic function, we use state-of-the-art approximation methods to evaluate these non-arithmetic functions, such as the ReLU, with sufﬁcient precision [1]. Further, for the ﬁrst time, we use the bootstrapping technique of the RNS-CKKS scheme in the proposed model, which enables us to evaluate a deep learning model on the encrypted data. We numerically verify that the proposed model with the CIFAR-10 dataset shows 98.67% identical results to the original ResNet-20 model with non-encrypted data. The classiﬁcation accuracy of the proposed model is 90.67%, which is pretty close to that of the original ResNet-20 CNN model. It takes about 4 hours for inference on a dual Intel Xeon Platinum 8280 CPU (112 cores) with 512 GB memory. We think that it opens the possibility of applying the FHE to the advanced deep PPML model."
2106.07229,dataset,27,,,Note that the highest classiﬁcation accuracy of the previous model for the CIFAR-10 dataset over the word-wise FHE scheme is only 77% [19].
2106.07229,dataset,157,,,"The applicable FHE schemes with this property are word-wise FHE schemes, such as Brakerski-FanVercauteren (BFV) scheme [3] or Cheon-Kim-Kim-Song (CKKS) scheme [4, 5]. Especially, the CKKS scheme has gained lots of interest for a suitable tool of the PPML implementation since it can deal with the encrypted real number naturally. However, these schemes support only homomorphic arithmetic operations such as the homomorphic addition and the homomorphic multiplication. Unfortunately, the popular activation functions are usually non-arithmetic functions, such as ReLU, sigmoid, leaky ReLU, and ELU. Thus, these activation functions cannot be evaluated directly using the word-wise FHE scheme. When the previous machine learning models using FHE replaced the non-arithmetic activation function with the simple polynomials, these models were not proven to show high accuracy for advanced classiﬁcation tasks beyond the MNIST dataset."
2106.07229,github,14,,,"[14] Microsoft. Microsoft SEAL. https://github.com/microsoft/SEAL, 2021."
2106.07229,"publicly available, data",154,,,"The privacy-preserving issue is one of the most practical problems for machine learning recently. Fully homomorphic encryption (FHE) is the most appropriate tool for privacy-preserving machine learning (PPML) to ensure strong security in the cryptographic sense and satisfy the communication’s succinctness. FHE is an encryption scheme whose ciphertexts can be processed with any deep Boolean circuits or arithmetic circuits without access to the data. The security of FHE has been usually deﬁned with indistinguishability under chosen-plaintext attack (IND-CPA) security, which is a standard cryptographic security deﬁnition. If the client sends the public keys and the encrypted data with an FHE scheme to the PPML server, the server can perform all computation needed in the desired service before sending the encrypted output to the client. Therefore, the application of FHE to PPML has been researched much until now."
2106.07229,"publicly available, data",201,,,"The CKKS scheme [4] is an FHE scheme supporting the arithmetic operations on encrypted data over real or complex numbers. Any users with the public key can process the encrypted real or complex data with the CKKS scheme without knowing any private information. The security of the CKKS scheme is based on the Ring-LWE hardness assumption. The supported homomorphic operations are the addition, the multiplication, the rotation, and the complex conjugation operation, and each operation except the homomorphic rotation operation is applied component-wisely. The rotation operation homomorphically performs a cyclic shift of the vector by some step. The multiplication, rotation, and complex conjugation operations in the CKKS scheme need additional corresponding evaluation keys and the key-switching procedures. Each real number data is scaled with some big integer, called the scaling factor, and then rounded to the integer before encrypting the data. When the two data encrypted with the CKKS scheme are multiplied homomorphically, the scaling factors of the two data are also multiplied. This scaling factor should be reduced to the original value using the rescaling operation for the following operations."
2106.07691,data,33,,,"Jordan J. Bird, Anik´o Ek´art, and Diego R. Faria. 2020. Chatbot interaction with artiﬁcial intelligence: Human data augmentation with t5 and language transformer ensemble for text classiﬁcation."
2106.07691,data,38,,,"Sabri Boughorbel, Fethi Jarray, and Mohammed El-Anbari. 2017. Optimal classiﬁer for imbalanced data using matthews correlation coefﬁcient PloS one, 12(6):e0177678–e0177678. metric. 28574989[pmid]."
2106.07691,data,57,,,"Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, page 1156–1165, New York, NY, USA. Association for Computing Machinery."
2106.07691,"data available, code, publicly available, data repository, dataset",143,,,"This paper introduced APT (Adversarial Paraphrasing Task), a task that uses the adversarial paradigm to generate paraphrases consisting of sentences with equivalent (sentence-level) meanings, but differing lexical (word-level) and syntactical similarity. We used APT to create a human-generated dataset / benchmark (APH ) and two machinegenerated datasets (AP M T 5 ). Our goal was to effectively augment how paraphrase detectors are trained, in order to make them less reliant on word-level similarity. In this respect, the present work succeeded: we showed that RoBERTabase trained on TwitterPPDB performed poorly on APT benchmarks, but this performance was increased signiﬁcantly when further trained on either our human- or machine-generated datasets. The code used in this paper along with the dataset has been released in a publicly-available repository.4"
2106.07691,"data, dataset",81,,,"To quantify our datasets’ contributions, we designed experiment setups wherein we trained RoBERTabase (Liu et al., 2019) for paraphrase detection on a combination of TwitterPPDB and our datasets as training data. RoBERTa was chosen for its generality, as it is a commonly used model in current NLP work and benchmarking, and currently achieves SOTA or near-SOTA results on a majority of NLP benchmark tasks (Wang et al., 2019, 2020;"
2106.07691,"data, dataset",105,,,"on APH ? Does RoBERTabase RoBERTabase was trained on each training dataset (90% training data, 10% validation data) for ﬁve epochs with a batch size of 32 with the training and validation data shufﬂed, and the trained model was tested on APH and APH -test. The results of this are shown in Table 6. Note that since the number of M I and non-M I sentences in all the datasets is imbalanced, Matthew’s Correlation Coefﬁcient (MCC) is a more appropriate performance measure than accuracy (Boughorbel et al., 2017)."
2106.07691,"data, dataset",107,,,"only 6.09% of T5base’s attempts were AP T . This does not mean that the remaining 94% of attempts can be discarded, since they amounted to the negative examples in the dataset. Since we trained it on TwitterPPDB itself, we expected that T5base would generate better paraphrases, as measured by a higher chance of passing AP T on TwitterPPDB, than any other dataset we tested. This is supported by the data in Table 2, which shows that T5base was able to generate an AP T passing paraphrase for 84.8% of the sentences in TwitterPPDB."
2106.07691,"data, dataset",130,,,"For each source sentence, multiple paraphrases may have been generated. Hence, to avoid data leakage, we created a train-test split on APH such that all paraphrases generated using a given source sentence will be either in APH -train or in APH test, but never in both. Note that APH is not balanced as seen in Table 2. Table 4 shows the distribution of M I and non-M I pairs in APH -train and APH -test and ‘M I attempts’ and ‘non-M I attempts’ columns of Table 2 show the same for other adversarial datasets. The test sets used were APH wherever APH -train was not a part of the training data and APH -test in every case."
2106.07691,"data, dataset",189,,,"to ﬁlter the dataset, which reduces its size, making model training more difﬁcult. Our present work tries instead to generate adversarial examples to increase dataset size. Other examples of adversarial datasets in NLP include work done by Jia and Liang (2017a); Zellers et al. (2018, 2019). Perhaps the closest to our work is PAWS (Zhang et al., 2019), short for Paraphrase Adversaries from Word Scrambling. The idea behind PAWS is to create a dataset that has a high lexical overlap between sentence pairs without them being ‘paraphrases.’ It has 108k paraphrase and non-paraphrase pairs with high lexical overlap pairs generated by controlled word swapping and back-translation, and human raters have judged whether or not they are paraphrases. Including PAWS in the training data has shown the state-of-the-art models’ performance to jump from 40% to 85% on PAWS’s test split. In comparison to the present work, PAWS does not explicitly incorporate inferential properties, and we seek paraphrases minimizing lexical overlap."
2106.07691,database,44,,,"Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. Ppdb: The paraphrase database. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 758–764."
2106.07691,dataset,1,,,Dataset
2106.07691,dataset,2,,,Source Dataset
2106.07691,dataset,3,,,3.3 Dataset Properties
2106.07691,dataset,3,,,T5base-generated APT dataset
2106.07691,dataset,6,,,Dataset APH -train APH -test MSRP-train
2106.07691,dataset,14,,,Property of passing the adversarial paraphrase test (see §3) Human-generated APT dataset
2106.07691,dataset,19,,,Table 5: Performance of RoBERTabase trained on just TwitterPPDB (no adversarial datasets) vs. random prediction.
2106.07691,dataset,26,,,"Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference."
2106.07691,dataset,26,,,Table 6: Performance of RoBERTabase trained on adversarial datasets. Size is the number of training examples in the dataset rounded to nearest 1000.
2106.07691,dataset,29,,,• We propose a way to create a machinegenerated adversarial dataset and discuss ways to ensure it does not suffer from the plateauing that other datasets suffer from.
2106.07691,dataset,32,,,"Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew E. Peters, Ashish Sabharwal, and Yejin Choi. 2020. Adversarial ﬁlters of dataset biases."
2106.07691,dataset,32,,,"• We create an additional dataset by training a paraphrase generation model to perform our adversarial task, creating another large dataset that further improves the paraphrase detection models’ performance."
2106.07691,dataset,37,,,Training Dataset TwitterPPDB + APH -train AP M T 5 APH -train + AP M T 5 AP T w T 5 APH -train + AP T w T 5 APT 5 APH -train + APT 5
2106.07691,dataset,37,,,"• We show that a SOTA language model trained on paraphrase datasets perform poorly on our benchmark. However, when further trained on our adversarially-generated datasets, their MCC scores improve by up to 0.307."
2106.07691,dataset,43,,,"Figure 2: BLEURT distributions on adversarial datasets. All ﬁgures divide the range of observed scores into 100 bins. Note that AP T sentence pairs are also M I, whereas those labeled ‘MI’ are not AP T ."
2106.07691,dataset,49,,,"The resulting dataset of sentence pairs, which we call APH (Adversarial Paraphrase by Humans), consists of 5007 human-generated sentence pairs, both M I and non-M I (see Table 2). Humans were able to generate AP T paraphrases for 75.48% of"
2106.07691,dataset,49,,,"Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017. A continuously growing dataset of sentential paraphrases. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1224–1234, Copenhagen, Denmark. Association for Computational Linguistics."
2106.07691,dataset,56,,,Table 3: Examples from adversarial datasets. The source dataset (TP short for TwitterPPDB) tells which dataset the sentence pair comes from (and whether it is in AP M T 5 for APT 5). All datasets have AP T passing and failing M I and non-M I sentence pairs.
2106.07691,dataset,67,,,"Combining these results, we can conclude that although machine-generated datasets like APT 5 can help paraphrase detectors improve themselves, a smaller dataset of human-generated adversarial paraphrases improved performance more. Overall, however, the highest MCC (0.525 in Table 6) is obtained when TwitterPPDB is combined with all three adversarial datasets, suggesting that the two approaches nicely complement each other."
2106.07691,dataset,73,,,"Do human-generated adversarial paraphrases improve paraphrase detection? We introduce APH -train to the training dataset along with TwitterPPDB. This improves the MCC by 0.222 even though APH -train constituted just 8.15% of the entire training dataset, the rest of which was TwitterPPDB (Table 6). This shows the effectiveness of human-generated paraphrases, as is especially impressive given the size of APH -train compared to TwitterPPDB."
2106.07691,dataset,81,,,"paraphrases that were low on BLEURT, they tended to become non-M I (e.g., line 12 in Table 3). However, T5base did generate more AP T -passing sentences with a lower BLEURT on Twitter-PPDB than on MSRP, which may be a result of overﬁtting T5base on TwitterPPDB. Furthermore, all three adversarial datasets have a distribution of M I and non-M I sentence pairs balanced enough to train a model to identify paraphrases."
2106.07691,dataset,85,,,"To visualize the syntactic and lexical disparity of paraphrases in the three adversarial datasets, we present their BLEURT distributions in Figure 2. As might be expected, the likelihood of a sentence pair being M I increases as BLEURT score increases (recall that AP T -passing sentence pairs are simply M I pairs with BLEURT scores <= 0.5), but Figure 2 shows that the shape of this increase is not straightforward, and differs among the three datasets."
2106.07691,dataset,90,,,"Our motivation behind creating an adversarial dataset was to improve the performance of paraphrase detectors by ensuring they recognize paraphrases with low lexical overlap. To demonstrate the extent of their inability to do so, we ﬁrst compare the performance of RoBERTabase trained only on TwitterPPDB on speciﬁc datasets as shown Table 5. Although the model performs slightly well on MSRP, it does barely better than a random prediction on APH , thus showing that identifying adversarial paraphrases created using APT is nontrivial for paraphrase identiﬁers."
2106.07691,dataset,98,,,"The composition of the three adversarial datasets can be found in Table 2. These metrics are useful to understand the capabilities of T5base as a paraphrase generator and the “paraphrasability” of sentences in MSRP and TwitterPPDB. For instance, T5base’s attempts on TwitterPPDB tend to be M I much less frequently than those on MSRP and human’s attempts on MSRP + PPNMT. This might be because in an attempt to generate syntactically dissimilar sentences, the T5base paraphraser also ended up generating many semantically dissimilar ones as well."
2106.07691,dataset,102,,,"Table 2: Proportion of sentences generated by humans (APH ) and T5base (APT 5). “Attempts” shows the number of attempts the participant made and “Uniques” shows the number of source sentences from the dataset that the performer’s attempts fall in that category on. For instance, 1631 unique sentences were presented to humans, who made a total of 5007 attempts to pass AP T and were able to do so for 2659 attempts which amounted to 1231 unique source sentences that could be paraphrased to pass AP T ."
2106.07691,dataset,119,,,"ANLI (Nie et al., 2020), a dataset designed for Natural Language Inference (NLI) (Bowman et al., 2015), was collected via an adversarial human-andmodel-in-the-loop procedure where humans are given the task of duping the model into making a wrong prediction. The model then tries to learn how not to make the same mistakes. AFLite (Bras et al., 2020) adversarially ﬁlters dataset biases making sure that the models are not learning those biases. They show that model performance on SNLI (Bowman et al., 2015) drops from 92% to 62% when biases were ﬁltered out. However, their approach is"
2106.07691,dataset,124,,,"The adversarial paradigm can be used to dive deeper into comparing how humans and SOTA language models understand sentence meaning, as we did with APT. Furthermore, automatic generation of adversarial datasets has much unrealized potential; e.g., different datasets, paraphrase generators, and training approaches can be used to generate future versions of APT 5 in order to produce AP T passing sentence pairs with lower lexical and syntactic similarities (as measured not only by BLEURT, but also by future state-of-the-art STS metrics). The idea of more efﬁcient automated adversarial task performance is particularly exciting, as it points to a way language models can improve themselves while avoiding prohibitively expensive human participant fees."
2106.07691,dataset,139,,,"Finally, the most signiﬁcant contribution of this paper, APT, presents a dataset creation method for paraphrases that will not saturate because as the models get better at identifying paraphrases, we will improve paraphrase generation. As models get better at generating paraphrases, we can make APT harder (e.g., by reducing the BLEURT threshold of < 0.5). One might think of this as students in a class who come up with new ways of copying their assignments from sources as plagiarism detectors improved. That brings us to one of the many applications of paraphrases: plagiarism generation and detection, which inherently is an adversarial activity. Until plagiarism detectors are trained on adversarial datasets themselves, we cannot expect them to capture human levels of adversarial paraphrasing."
2106.07691,dataset,141,,,"ParaNMT (Wieting and Gimpel, 2018) was created by using neural machine translation to translate the English side of a Czech-English parallel corpus (CzEng 1.6 (Bojar et al., 2016)), generating more than 50M English-English paraphrases. However, ParaNMT’s use of machine translation models that are a few years old harms its utility (Nighojkar and Licato, 2021), considering the rapid improvement in machine translation in the past few years. To rectify this, we use the google-translate library to translate the Czech side of roughly 300k CzEng2.0 (Kocmi et al., 2020) sentence pairs ourselves. We call this dataset ParaParaNMT (PPNMT for short, where the extra para- preﬁx reﬂects its similarity to, and conceptual derivation from, ParaNMT)."
2106.07691,dataset,146,,,"Owing to expeditious progress in NLP research, performance of models on benchmark datasets is ‘plateauing’ — with near-human performance often achieved within a year or two of their release — and newer versions, using a different approach, are constantly having to be created, for instance, GLUE (Wang et al., 2019) and SuperGLUE (Wang et al., 2020). The adversarial paradigm of dataset creation (Jia and Liang, 2017a,b; Bras et al., 2020; Nie et al., 2020) has been widely used to address this ‘plateauing,’ and the ideas presented in this paper draw inspiration from it. In the remainder of this paper, we apply the adversarial paradigm to the problem of paraphrase detection, and demonstrate the following novel contributions:"
2106.07691,dataset,166,,,"Some work has been done in improving the quality of paraphrase detectors by training them on a dataset with more lexical and syntactic diversity. Thompson and Post (2020) propose a paraphrase generation algorithm that penalizes the production of n-grams present in the source sentence. Our approach to doing this is with the APT, but this is something worth exploring. Sokolov and Filimonov (2020) use a machine translation model to generate paraphrases much like ParaNMT. An interesting application of paraphrasing has been discussed by Mayhew et al. (2020) who, given a sentence in one language, generate a diverse set of correct translations (paraphrases) that humans are likely to produce. In comparison, our work is focused on generating adversarial paraphrases that are likely to deceive a paraphrase detector, and models trained on the adversarial datasets we produce can be applied to Mayhew et al.’s work too."
2106.07691,dataset,199,,,"To test the effectiveness of APT in guiding the generation of mutually implicative but lexically and syntactically disparate paraphrases for a given sentence, we designed an Amazon Mechanical Turk (mTurk) study (Figure 1). Given a starting sentence, we instructed participants to “[w]rite a sentence that is the same in meaning as the given sentence but as structurally different as possible. Your sentence should be such that you can infer the given sentence from it AND vice-versa. It should be sufﬁciently different from the given sentence to get any reward for the submission. For example, a simple synonym substitution will most likely not work.” The sentences given to the participants came from MSRP and PPNMT (Section 1). Both of these datasets have pairs of sentences in each row, and we took only the ﬁrst one to present to the par ticipants. Neither of these datasets has duplicate sentences by design. Every time a sentence was selected, a random choice was made between MSRP and PPNMT, thus ensuring an even distribution of sentences from both datasets."
2106.07691,dataset,214,,,"If two sentences have the same meaning, it should follow that they are equivalent in i.e., each sentheir inferential properties, tence should textually entail the other. However, many paraphrase datasets currently in widespread use rely on a sense of paraphrase based on word overlap and syntax. Can we teach them instead to identify paraphrases in a way that draws on the inferential properties of the sentences, and is not over-reliant on lexical and syntactic similarities of a sentence pair? We apply the adversarial paradigm to this question, and introduce a new adversarial method of dataset creation for paraphrase identiﬁcation: the Adversarial Paraphrasing Task (APT), which asks participants to generate semantically equivalent (in the sense of mutually implicative) but lexically and syntactically disparate paraphrases. These sentence pairs can then be used both to test paraphrase identiﬁcation models (which get barely random accuracy) and then improve their performance. To accelerate dataset generation, we explore automation of APT using T5, and show that the resulting dataset also improves accuracy. We discuss implications for paraphrase detection and release our dataset in the hope of making paraphrase detection models better able to detect sentence-level meaning equivalence."
2106.07691,dataset,214,,,"Paraphrase generation (given a sentence, generate its paraphrase) (Gupta et al., 2018) is an area of research beneﬁting paraphrase detection as well. Lately, many paraphrasing datasets have been introduced to be used for training and testing ML models for both paraphrase detection and generation. MSRP (Dolan and Brockett, 2005) contains 5801 sentence pairs, each labeled with a binary human judgment of paraphrase, created using heuristic extraction techniques along with an SVM-based classiﬁer. These pairs were annotated by humans, who found 67% of them to be semantically equivalent. The English portion of PPDB (Ganitkevitch et al., 2013) contains over 220M paraphrase pairs generated by meaning-preserving syntactic transformations. Paraphrase pairs in PPDB 2.0 (Pavlick et al., 2015) include ﬁne-grained entailment relations, word embedding similarities, and style annotations. TwitterPPDB (Lan et al., 2017) consists of 51,524 sentence pairs captured from Twitter by linking tweets through shared URLs. This ap proach’s merit is its simplicity as it involves neither a classiﬁer nor a human-in-the-loop to generate paraphrases. Humans annotate the pairs, giving them a similarity score ranging from 1 to 6."
2106.07691,dataset,286,,,"next word according to its conditional probability distribution, introduces non-determinism in language generation. Fan et al. (2018) introduce top-k sampling, which ﬁlters k most likely next words, and the probability mass is redistributed among only those k words. Nucleus sampling (or top-p sampling) (Holtzman et al., 2020) reduces the options to the smallest possible set of words whose cumulative probability exceeds p, and the probability mass is redistributed among this set of words. Thus, the set of words changes dynamically according to the next word’s probability distribution. We use a combination of top-k and top-p sampling with k = 120 and p = 0.95 in the interest of lexical and syntactic diversity in the paraphrases. For each sentence in the source dataset (MSRP3 and TwitterPPDB for AP M T 5 respectively), we perform ﬁve iterations, in each of which, we generate ten sentences. If at least one of these ten sentences passes AP T , we continue to the next source sentence after recording all attempts and classifying them as M I or non-M I. If no sentence in a maximum of 50 attempts passes AP T , we record all attempts nonetheless, and move on to the next source sentence. For each increasing iteration for a particular source sentence, we increase k by 20, but we also reduce p by 0.05 to avoid vague guesses. Note the distribution of M I and non-M I in the source datasets does not matter because we use only the ﬁrst sentence from the sentence pair."
2106.07691,dataset,290,,,"Since human studies can be time-consuming and costly, we trained a paraphrase generator to perform APT. We used T5base (Raffel et al., 2020), as it achieves SOTA on paraphrase generation (Niu et al., 2020; Bird et al., 2020; Li et al., 2020) and trained it on TwitterPPDB (Section 2). Our hypothesis was that if T5base is trained to maximize the APT reward (Equation 1), its generated sentences will be more likely to be AP T . We generated paraphrases for sentences in MSRP and those in TwitterPPDB itself, hoping that since T5base is trained on TwitterPPDB, it would generate better paraphrases (M I with lower BLEURT) for sentences coming from there. The proportion of sentences generated by T5base is shown in Table 2. We call this dataset APT 5, the generation of which involved two phases: Training: To adapt T5base for APT, we implemented a custom loss function obtained from dividing the cross-entropy loss per batch by the total reward (again from Equation 1) earned from the model’s paraphrase generations for that batch, provided the model was able to reach a reward of at least 1. If not, the loss was equal to just the crossentropy loss. We trained T5base on TwitterPPDB for three epochs; each epoch took about 30 hours on one NVIDIA Tesla V100 GPU due to the CPU bound BLEURT component. More epochs may help get better results, but our experiments showed that loss plateaus after three epochs. Generation: Sampling, or randomly picking a"
2106.07691,github,3,,,4https://github.com/
2106.08752,data,5,,,4.1 Data and experimental setup
2106.08752,data,9,,,"of Data Science, Fudan University (e-mail:"
2106.08752,data,9,,,"on mini-batches of M data points, i.e.,"
2106.08752,data,15,,,"and School of Data Science, Fudan University, 200433, Shanghai, China."
2106.08752,data,19,,,"The proposed VAE maximizes the joint log-likelihood of the complete data. For the source data, we have"
2106.08752,data,19,,,"[51] L. van der Maaten and G. Hinton, “Visualizing data using t-sne,” Journal"
2106.08752,data,21,,,"j=1← Random mini-batch of M data points; in source and target data, respectively ; T , Σnj T ;"
2106.08752,data,24,,,"Similarly, we introduce another VAE for the target data, and qθT (z) can be estimated in the same way."
2106.08752,data,30,,,"As all data points can be taken as samples from i.i.d. random variables, the joint log-likelihood becomes the sum over that of individual data points, i.e.,"
2106.08752,data,32,,,"deal with this missing data using the pseudo labels, i.e., the predicted segmentation (cid:98)yT of xT . The variational lower bound for the target domain becomes,"
2106.08752,data,35,,,"For unpaired data, extracting modality-invariant features is particularly challenging. As deep learning methods have been widely used in domain adaptation, here we mainly focus on DNN-based approaches. Researchers have pro 4"
2106.08752,data,35,,,We denote the proposed VAE-based method with explicit regularization for domain adaptation as VarDA. We used two data sets for experiments. One is the Multi-Modality Whole Heart Segmentation (MM-WHS) Challenge dataset1
2106.08752,data,35,,,"[32] M. Pilanci and E. Vural, “Domain adaptation on graphs by learning aligned graph bases,” IEEE Transactions on Knowledge and Data Engineering, pp. 1–1, 2020."
2106.08752,data,39,,,"For the target data, we have a lower bound similar to Eq.(4), denoted as LBV AE(θT , φT ), where we however do not have ground truth segmentation. We"
2106.08752,data,45,,,"of the model also improved with the increasing number of the target samples, though not as stably as the improvement with diﬀerent source data. The results demonstrated that both the diversities of source and target data were important for the model training."
2106.08752,data,47,,,"In this work, we adopt the minibatch strategy to optimize (1). We randomly sample M data points independently from the source domain and M samples from the target domain, then D(φS, φT ) can be approximately calculated by,"
2106.08752,data,51,,,"[7] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng, “Self-taught learning: transfer learning from unlabeled data,” in Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 759–766."
2106.08752,data,56,,,"We compared the proposed VarDA with three state-of-the-art methods, i.e., PnP-AdaNet (also denoted as PnP-Ada for short) [10], SIFA [11] and CFDnet [24]. We also included the method solely trained on the source data without adaptation, which is referred to as NoAdapt."
2106.08752,data,60,,,"To see how the distribution discrepancy was reduced by VarDA, we further visualized the features by T-SNE [51]. As shown in Fig.8, the data points in two domains showed strong clustering before adaptation. After training with VarDA, the representations learned by the model became more indistinguishable between the source and target domains."
2106.08752,data,62,,,"[28] L. Antelmi, N. Ayache, P. Robert, and M. Lorenzi, “Sparse multi-channel variational autoencoder for the joint analysis of heterogeneous data,” in Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, 2019, pp. 302– 311."
2106.08752,data,66,,,"[35] C. Ouyang, K. Kamnitsas, C. Biﬃ, J. Duan, and D. Rueckert, “Data eﬃcient unsupervised domain adaptation for cross-modality image segmentation,” in Medical Image Computing and Computer Assisted Intervention MICCAI 2019 - 22nd International Conference, Shenzhen, China, October 13-17, 2019, Proceedings, Part II, 2019, pp. 669–677."
2106.08752,data,73,,,"Figure 1: Illustration of the diﬀerence between the previous works and the proposed method for domain adaptation in a latent feature space. xS and xT denote the input source and target image, respectively. z is the latent feature variable. pθS and pθT are probability functions on the source and target data, parameterized with θS and θT , respectively. qφ is the variational approximation of pθS/T ."
2106.08752,data,82,,,"𝑥𝑥𝑆𝑆𝑥𝑥𝑇𝑇𝑧𝑧𝑆𝑆𝑧𝑧𝑇𝑇𝑧𝑧discriminatordiscrepancy metricsadversarial training, e.g, DANNminimization of CF distance, MMD, etc.reduce discrepancy of 𝑝𝑝𝜃𝜃𝑆𝑆(𝑧𝑧)and 𝑝𝑝𝜃𝜃𝑇𝑇(𝑧𝑧)𝑝𝑝𝜃𝜃𝑆𝑆𝑝𝑝𝜃𝜃𝑇𝑇𝑧𝑧𝑆𝑆𝑧𝑧𝑇𝑇𝑧𝑧𝑥𝑥𝑆𝑆𝑥𝑥𝑇𝑇𝑝𝑝𝜃𝜃𝑆𝑆𝑝𝑝𝜃𝜃𝑇𝑇𝑞𝑞𝜙𝜙Regularization termfor 𝑞𝑞𝜙𝜙𝑆𝑆and 𝑞𝑞𝜙𝜙𝑇𝑇regularized by via Variational BayesVariationalauto-encoderdata. Recently, researchers have proposed a number of new methodologies [7,8], among which domain adaptation, a special technique of transfer learning, has became increasingly popular, thanks to its needlessness of additional labeled images from the target modalities [9]."
2106.08752,data,96,,,"The VAE is adapted to work for both the labeled source data and unlabeled target data. For segmentation, the VAE incorporates a prediction model, which takes only latent features as input, and thus can be shared by both domains. We further deduce a new form of the variational lower bound for the log-likelihood of the labeled data. This lower bound can be taken as an objective function to train the VAE models, and thus is used to form the total loss for the domain adaptation and segmentation task."
2106.08752,data,103,,,"For the domain discrepancy, one can estimate the two variational approximations for zS and zT , i.e., qφS (z) and qφT (z), from which the input data of the segmentor are sampled, based on the two VAE modules mentioned before. To force the two approximations to be the same distribution, we therefore propose a regularization for this discrepancy by computing the distance between qφS (z) and qφT (z). The loss function is denoted as Lossdiscrepancy(qφS , qφT ) and will be discussed in Section 3.3."
2106.08752,data,114,,,"For the computation of Eq.(9), we can obtain the Monte Carlo estimate of LBV AE(θT , φT ) from the complete target data, similar to Eq.(6) and Eq.(8) for the source domain. We denote this computation as (cid:101)LT (θT , φT ; XT , (cid:98)YT ), where (cid:98)YT = {(cid:98)yi i=1. Also, we minimize the MSELoss between the input image and the reconstructed image from the decoder, to maximize the reconstruction term in Eq.(9)."
2106.08752,data,123,,,"In the proposed framework, the VAE models need to incorporate the function of segmentation, as our task is cardiac image segmentation. This is in contrast to the widely used VAE in [36], where the decoding is only for the reconstruction of images. Therefore, we propose to maximize the joint log-likelihood (JLL) of the complete data as the objective function of VAE, i.e., JLL = log pθ(X, Y ). The complete data consist of both the images and their segmentation labels. This is the major diﬀerence to the VAE in [36], whose objective function is based on the log-likelihood of the images only."
2106.08752,data,128,,,"In this work, we aim to align the two domains in a common latent feature space. We ﬁrst transform each domain into a latent feature variable. The two variables are then driven and approximated by a common and parameterized variational form via VAEs. As this approximation can be estimated with either the source or target data, we obtain two forms of estimation for the distribution of the common variational form. The distance of the two estimations is then used as an eﬀective regularization for domain adaptation. Then, we train the segmentation model with the latent feature and the ground truth of source data. The resulting model is expected to be applicable to the segmentation of the target images."
2106.08752,data,141,,,"To validate the model performance with diﬀerent sample complexity, we conducted two set of ablation experiments. We ﬁrst used all the 45 LGE CMR images as the target data, and set the number of source data, i.e., the bSSFP CMR images, to be 5, 10, 15, 20, 25, 30 and 35, respectively. As Fig. 7 showed, the performance of the model improved and even did not show convergence with the increasing number of the source samples. We further did the experiments with all the source images, and set the number of target data to be 5, 10, 15, 20, 25, 30, 35, 40 and 45, respectively. As shown in Fig. 7, the performance"
2106.08752,data,144,,,"metric, i.e., (cid:82) [qφS (z) − qφT (z)]2dz. Hereafter, we denote this distance metric as D(qφS (z), qφT (z)) or D(φS, φT ). Note that for the MMD method, the sampling operation for the latent features could lead to large variation of MMD estimator, and thus lead to ineﬀectiveness of the metric for discrepancy minimization [24]. The distributions of the latent feature can be estimated via the modiﬁed VAE. For the source data, as the VAE model can approximate the posterior probability pθS (z|x) by a parameterized model qφS (z|x) [36,42], the distribution of the approximation qφS (z) can be estimated by,"
2106.08752,data,147,,,"Domain adaptation transforms the images from diﬀerent modalities into a modality-invariant common space [10], or directly translates the images from one modality to another with the same anatomic structures [11]. Formally, the imaging modality with ground truth segmentation is referred to as the source domain, and the modality without ground truth, which is our target for automated segmentation, is denoted as the target domain. Correspondingly, the data from these two domains are known as the source data and target data, respectively. In this work, we follow the idea of aligning the distributions of these domains in a latent feature space, which is equivalent to extract modality-invariant features in this speciﬁc task [12–14]. Hence, domain adaptation becomes a problem of feature learning and minimization of domain discrepancy."
2106.08752,data,152,,,"and MR. Therefore, the domain adaptation between bSSFP and LGE is easier. Compared to PnP-AdaNet, VarDA obtained signiﬁcantly better Dice scores and ASSD values on all structures (p < 0.01). Compared to SIFA, VarDA achieved better accuracies on MYO and LV segmentation but worse on RV. Compared to CFDnet, VarDA obtained comparable average Dice score (p = 0.05042), and signiﬁcantly better average ASSD value (p = 0.001487). In addition, for reference we provide the results of MS-CMRSeg challenge here. The Dice scores of the top ten methods on the leaderboard, which tackled paired data, ranged in (0.854, 0.922) for LV, (0.713, 0.827) for Myo and (0.792, 0.874) for RV. We further provided the best Dice scores reported from the MS-CMRSeg Challenge"
2106.08752,data,194,,,"ImagesVarDASIFAPnP-AdaNetGround TruthsSubject1Subject2Subject365.8/5.6835.6/10.130.4/19.363.3/15.151.4/17.265.8/1.8466.4/4.6337.7/11.252.0/14.9LARARVLVMYOperformed worse compared to the proposed VarDA. Particularly, on the right atrium and right ventricle, SIFA had over 20% Dice score deﬁcit. SIFA largely depends on the quality of the fake target image translated from the source data [11]. Since the textures of MR images are more complex than that of the CT images, translating CT to MR with the same semantic contents is more diﬃcult than the other way around. Hence, in their work [11] SIFA obtained much better accuracies on CT images when MR images were used as the source domain. For CFDnet, as it failed in the segmentation of all structures, we did not report the results in the table. This indicates that the CF distance could be challenged in domain adaptation, when the structures to be segmented are over complex. In addition, the average Dice score of the eleven supervised methods reported in [44] ranged in (0.674, 0.874) with mean value as 0.824. We provide this result here solely for reference."
2106.08752,data,214,,,"Regarding to segmentation of domain adaptation, one of the most related works to ours is from Dou et al. [10]. They proposed a plug-and-play domain adaptation module, based on a dilated fully convolutional network, to map two domains into a common space. Their framework was a modiﬁed version of the domain adversarial neural networks (DANN) [16]. It was validated on MRI-CT cross-modality segmentation of 2D cardiac images. Another related state-of-the-art work is from Chen et al. [11], whose method is referred to as SIFA. The authors implemented adaptation at both of the image and feature levels via adversarial training, and used the cycle-consistency loss. This work was reported to outperform peer methods for natural images by a signiﬁcant margin. More recently, Ouyang et al. [35] proposed a data eﬃcient method for multi-domain medical image segmentation. They combined a VAE-based feature prior matching and domain adversarial training to learn a shared latent space which is expected to be domain-invariant. These state-of-the-art methods designed for medical image segmentation are all based on adversarial training, which aligns the distributions of two domains by minimizing their discrepancy implicitly."
2106.08752,data,222,,,"VAE is a popular deep generative model. Using VAE, one can approximate the posterior distribution of a latent variable conditioned on data using a normal distribution [36]. This property is particularly useful, and enables us to drive two domains towards a common and parameterized variable in a latent space. Kingma et al. [37] developed a new model based on VAE for semi-supervised learning with small labeled data sets. The model allowed for eﬀective generalization to large unlabeled ones, which were assumed to be from the same distributions with the labeled data. Furthermore, based on conditional variational autoencoder with Gaussian latent variables, Sohn et al. [38] proposed a stochastic neural network to make inference for structured output variables. They performed the deterministic inference by maximizing a generation network, which uses both the data and its corresponding latent features for prediction. Another important VAE work was from Walker et al. [39], where the authors constructed a VAE model to predict dense trajectories from pixels. The method employs an encoder to estimate the posterior distributions of the latent variables conditioned on the data and labels, and a decoder to predict the conditional distributions of trajectories given images."
2106.08752,data,237,,,"For domain adaptation, a particular scenario is when we have the paired data, namely each image from one modality (domain) has the corresponding image of the same subject from the other modality (domain). Chartsias et al. [25] studied a multi-input multi-output fully convolutional neural network for MR synthesis. They combined latent features from all modalities into a single fused representation, which can be transformed into any target modality with a decoder. Tulder et al. [26] proposed a shared autoencoder-like convolutional network, which learns a common representation from multi-modal data. They utilized feature normalization and modality dropout techniques to improve the crossmodality classiﬁcation accuracy. The core idea in these two works was to force the latent features from paired data to be similar. To learn the models, they processed the training data pairwisely, which does not need to consider the difference of data distributions from modalities. Moreover, Shi et al. [27] proposed the mixture-of-experts of VAE, which implicitly decomposed multi-modal data into shared and private subspaces, and could be applied for complex heterogeneous modalities, such as images and the language data. Antelmi et al. [28] adopted multi-channel VAE to tackle paired heterogeneous data, and extracted parsimonious and interpretable representations by variational dropout."
2106.08752,data,329,,,"Although CFD was validated to be eﬀective, it requires to be combined with other techniques to achieve a comparable performance to the adversarial training methods. These techniques include the mean value matching and image reconstruction. Moreover, it was only tested on simple segmentation tasks of two or three structures. While for the whole heart segmentation which has much more complex structures, CFD could be challenged to achieve successful unsupervised domain adaptation due to the complexity of the task. Besides these statistic estimators, graph matching based metrics were also studied. For example, Das and Lee [30] ﬁrst constructed the representation graphs for both source and target domains, and then minimized the matching loss between them, which consisted of node-tonode matching and edge-to-edge matching. Yang and Yuen [31] constrained the edges in both graphs for two domains, and mapped the data into features with uniﬁed structure criteria via adversarial training. Pilanci and Vural [32] matched the Fourier bases of their graph instead of the nodes, such that the label functions on the two graphs can be reconstructed with similar coeﬃcients. GAN has recently been shown to have great potential for domain adaptation, particularly for medical image generation and segmentation. Zhang et al. [33] utilized cycle-consistent generative adversarial networks (CycleGANs) to achieve cross-modality synthesis between MRI and CT images. To reduce the geometric distortion, they introduced a shape-consistency loss for the generators. Combining the idea of style transfer, Huang et al. [34] proposed to decompose image features from both domains into disentangled representation of domain-invariant content and domain-speciﬁc style, which were then used to simulate target images. They employed GANs to align the distributions of these translated target images with the real ones, and the alignment was implemented on image levels."
2106.08752,data,344,,,"posed explicit metrics to measure domain discrepancy. Tzeng et al. [23] and Long et al. [20] employed the Maximum Mean Discrepancy (MMD) as a domain confusion metric, and minimized the MMD loss together with a task-speciﬁc loss to learn the modality-invariant representations. Sun et al. [22] proposed a new method, referred to as Deep CORAL in their paper, to align the second-order statistics of the source and target distributions. Instead of using the Euclidean distance in Deep CORAL, Wang et al. [29] proposed to adopt the Riemannian distance of covariance matrices between the source and target domains, which can be approximated by the Log-Euclidean distance. While these methods were solely developed for classiﬁcation tasks, recently, Wu and Zhuang [24] proposed the CFD method for medical image segmentation, which explicitly calculated the domain discrepancy with the distance between the characteristic functions of the distributions in a latent feature space. Although CFD was validated to be eﬀective, it requires to be combined with other techniques to achieve a comparable performance to the adversarial training methods. These techniques include the mean value matching and image reconstruction. Moreover, it was only tested on simple segmentation tasks of two or three structures. While for the whole heart segmentation which has much more complex structures, CFD could be challenged to achieve successful unsupervised domain adaptation due to the complexity of the task. Besides these statistic estimators, graph matching based metrics were also studied. For example, Das and Lee [30] ﬁrst constructed the representation graphs for both source and target domains, and then minimized the matching loss between them, which consisted of node-tonode matching and edge-to-edge matching. Yang and Yuen [31] constrained the edges in both graphs for two domains, and mapped the data into features with uniﬁed structure criteria via adversarial training."
2106.08752,"data available, data",98,,,"In this work, the distributions of the source data (with labels) and target data (without labels) are diﬀerent, since the domain shift exists. Also, the VAE model, such as the one proposed by Walker et al. [39], generally requires labels for all data, which however are not available for the target data in the unsupervised domain adaptation task. Therefore, in this work we propose a new form of VAE for domain adaptation, and deduced an explicit regularization for domain discrepancy."
2106.08752,"data, code",81,,,"For classiﬁcation or segmentation tasks, domain adaptation was mainly conducted on feature levels. Without disentangling features, data from diﬀerent domains were mapped into a common latent code, which was then forced to be domain-invariant. Kamnitsas et al. [18] proposed a so-called multi-connected architecture for the domain discriminator, where the multi-layer features were connected before being input into the discriminator. They obtained evident improvement in MRI-CT cross-modality segmentation of brain lesions."
2106.08752,"data, dataset",137,,,"Accurate cardiac segmentation is an essential prerequisite for many medical applications, such as 3D modeling and functional analysis, which are important for diagnosis of cardiac diseases [1, 2]. In clinics, multi-modality medical images are widely used to assist diagnosis. However, obtaining the automated segmentation for all modality images can be label intensive and expensive. To alleviate this, an eﬀective learning-based approach is to train an automatic segmentation model using existing labeled data of one modality and adapt this model for the automatic segmentation of the other modalities. However, the performance of such adapted segmentation models usually degrades signiﬁcantly when they are tested on images from new modalities, due to dataset bias, which is also known as domain shift [3]."
2106.08752,dataset,13,,,Figure 3: The losses of VarDA during training on MS-CMR dataset.
2106.08752,dataset,15,,,"For complete source dataset, we have the estimator of the lower bound, based"
2106.08752,dataset,47,,,"Table 3: Quantitative evaluation results of the segmentation of 2D MR images from MM-WHS dataset, where the CT images as the source domain. Note: N/A means that the ASSD value cannot be calculated due to no prediction for that cardiac structure."
2106.08752,dataset,55,,,"Table 4: Performance comparison of methods on LGE CMR images from MSCMRSeg dataset with bSSFP images as the source domain. Here, in row Best Result, we present the best Dice scores reported from the MS-CMRSeg Challenge, which did not use same setting of unsupervised domain adaptation as this work."
2106.08752,dataset,63,,,"Table 2: Quantitative evaluation results of the segmentation of 3D MR images from MM-WHS dataset, where the CT images were used as the source domain. Note: N/A means that the ASSD value cannot be calculated due to no prediction for that cardiac structure. The best result in each column of the unsupervised methods is in boldface."
2106.08752,dataset,72,,,"[47] T. Heimann, B. Van Ginneken, M. A. Styner, Y. Arzhaeva, V. Aurich, C. Bauer, A. Beck, C. Becker, R. Beichel, G. Bekes et al., “Comparison and evaluation of methods for liver segmentation from CT datasets,” IEEE transactions on medical imaging, vol. 28, no. 8, pp. 1251–1265, 2009."
2106.08752,dataset,153,,,"Particularly, the generative adversarial networks (GANs) have demonstrated great potential [15–17]. By introducing discriminators, the discrepancy between domains can be minimized implicitly. This adversarial training has also been adopted in medical image analysis [10, 11, 18]. However, this technique is still challenging, due to the diﬃculty of obtaining the nash equilibrium point in GANs when they are applied to domain adaptation [19]. Also, training the generator and discriminator networks for the implicit minimization of domain discrepancy could be complex [15]. For cardiac segmentation, PnP-AdaNet [10] and SIFA [11] are two most related works. PnP-AdaNet utilized features from multi-layer for adaptation, and SIFA minimized the domain discrepancy on both feature and image levels. They were both validated to be eﬀective on cardiac dataset."
2106.08752,dataset,266,,,"For experiments, we designed a U-net [48] based network for multi-scale segmentation. The details of the network structure are presented in the Supplementary Material. We adopted Adam optimizer [49] for the training with batch size of 10. The initial learning rate was set to be 10−4, and was decreased with a stepped decay rate of 0.9 every 150 iterations. We empirically valued the tradeoﬀ parameters in (14). To achieve this, we validated VarDA on MS-CMRSeg dataset with α1 = 1, α2 = 10−1, 100, 101, α3 = 100, 10−1, 10−2, 10−3, 10−4, 10−5 , 10−6. We found that the setting of α2 = 1 and α3 = 10−2 was a suitable choice, and we kept this parameter setting in the following experiments. We further plotted the loss terms of VarDA during training in Fig.3. As the segmentation loss from source domain contributed largely to the model optimization, we separated the loss of the source VAE, i.e., (cid:101)LS, into two parts, including the segmentation loss (denoted by (cid:101)Lseg) and the remaining loss (denoted by (cid:101)LS/seg). As shown in Fig.3, the regularization term (cid:101)D dropped fast and converged quickly. Moreover, (cid:101)LS/seg and its counterpart in target domain, i.e., (cid:101)LT , even converged in the ﬁrst 100 iterations."
2106.08752,"dataset provided, data, dataset",256,,,"MS-CMRSeg dataset: The challenge provided three CMR sequences, i.e., the LGE , bSSFP and T2 images, and the target of this challenge is to segment RV, LV and MYO of the LGE CMR images. Hence, we used the LGE images as the target domain. Also, the bSSFP CMR covers more similar part of the heart with the LGE images and contains more slices than T2, we therefore chose bSSFP images as the source domain. The organizers provided 45 bSSFP CMR images with 35 of them having gold standard segmentation of RV, LV and MYO. The target data consisted of 5 annotated LGE CMR images for validation and 40 images without ground truth for test. The bSSFP images consist of 8 to 12 contiguous slices with in-plane resolution of 1.25 × 1.25 mm, covering the full ventricles from the apex to the basal plane of the mitral valve. The LGE CMR images are composed of 10 to 18 slices with in-plane resolution of 0.75 × 0.75 mm, covering the main body of the ventricles. As the proposed method was for unsupervised domain adaptation problem, we shuﬄed the bSSFP CMR and LGE CMR images, which were collected from the same subjects, to make them unpaired. For experiments, all images were intensity normalized, resized to be the same resolution of 1.0 × 1.0 mm and cropped with an ROI of 192 × 192 pixel."
2106.08752,"dataset provided, dataset",196,,,"MM-WHS dataset: The organizers provided 20 MR and 20 CT 3D images with gold standard segmentation, which are not paired. These images were collected from diﬀerent patients and diﬀerent clinical sites. For evaluation, we included the following ﬁve structures for segmentation: the right atrium blood cavity (RA), the right ventricle blood cavity (RV), the left atrium blood cavity (LA), the left ventricle blood cavity (LV), and the myocardium of the left ventricle (MYO). We employed the CT images as the source domain, and the MR images as the target. For comparison, we followed the experiment protocols in [10, 11], and randomly split the images from both domains into the training set consisting of 16 subjects and the test set of the remaining 4 subjects. For convenience, all images were rigidly aligned and resampled into 1 × 1 × 1 mm. For 2D experiments, we used the axial slices, which were intensity normalized and cropped with an ROI of 192 × 192 pixel."
2106.08752,github,3,,,1https://zmiclab.github.io/mmwhs
2106.08752,github,3,,,2https://zmiclab.github.io/mscmrseg19
2106.08752,"github, data, code, data https",267,,,"Unsupervised domain adaptation is useful in medical image segmentation. Particularly, when ground truths of the target images are not available, domain adaptation can train a target-speciﬁc model by utilizing the existing labeled images from other modalities. Most of the reported works mapped images of both the source and target domains into a common latent feature space, and then reduced their discrepancy either implicitly with adversarial training or explicitly by directly minimizing a discrepancy metric. In this work, we propose a new framework, where the latent features of both domains are driven towards a common and parameterized variational form, whose conditional distribution given the image is Gaussian. This is achieved by two networks based on variational autoencoders (VAEs) and a regularization for this variational approximation. Both of the VAEs, each for one domain, contain a segmentation module, where the source segmentation is trained in a supervised manner, while the target one is trained unsupervisedly. We validated the proposed domain adaptation method using two cardiac segmentation tasks, i.e., the cross-modality (CT and MR) whole heart segmentation and the cross-sequence cardiac MR segmentation. Results show that the proposed method achieved better accuracies compared to two state-of-the-art approaches and demonstrated good potential for cardiac segmentation. Furthermore, the proposed explicit regularization was shown to be eﬀective and eﬃcient in narrowing down the distribution gap between domains, which is useful for unsupervised domain adaptation. Our code and data has been released via https://zmiclab.github.io/projects.html."
2106.09922,code,13,,,Algorithm 1 A pseudo-code of island-based distributed steady-state GA in the i-th island
2106.09922,code,119,,,"Algorithm 1 shows a pseudo-code of the distributed island-based steady-state GA (dssGA) implemented in this work. A dssGA consists of many subpopulations (islands) that evolve governed by a local steady-state GA in parallel. First, the initial population is generated, and the required ﬁtness values are evaluated. Then, a parent selection method (e.g., binary tournament selection) selects two parents, and crossover and mutation operations create a new oﬀspring. Finally, a newly generated oﬀspring is evaluated and compared with a randomly selected individual in the population. If the oﬀspring is better than the random member, the oﬀspring replaces it in the population."
2106.09922,data,7,,,6.4. Initial Data on Fitness Evaluation
2106.09922,data,38,,,"In this subsection, we show the result of model ﬁtting for the speed-up. As the data for model ﬁtting, we use the speed-up when using the migration gap of 64 shown in Section 9."
2106.09922,data,46,,,"Appendix A.2. Model Selection for the Wall-clock Time In this subsection, we show the result of model ﬁtting for the wall-clock time. As the data for model ﬁtting, we use the median wall-clock time between migration gaps shown in Section 7."
2106.09922,data,55,,,"[19] H. Khalloof, P. Ostheimer, W. Jakob, S. Shahoud, C. Duepmeier, V. Hagenmeyer, Superlinear speedup of parallel population-based metaheuristics: A microservices and container virtualization approach, in: International Conference on Intelligent Data Engineering and Automated Learning, Springer, 2019, pp. 386–393."
2106.09922,data,120,,,"As to the results of this basic ﬁrst study, the average evaluation time for P-PEAKS is 30.90 µs for the small problem and 3060.97 µs for the large one. The large problem is then 100 times slower in evaluating solutions, although it is only ten times larger. This behavior was expected since the computational complexity of its ﬁtness function (O(n2)). In addition to shedding some light on the relationship between problem running times with actual data, it allowed us to conﬁrm that our implementation runs as expected, and that theory matches practice. Even if simple, these conclusions are hardly found in previous works on PGAs."
2106.09922,data,243,,,"To answer these research questions, we conduct experiments using a cluster of multiprocessors, a commodity MIMD (Multiple Instruction, Multiple Data) architecture. As a PGA method, this paper uses a distributed islandbased steady-state GA (dssGA). Our experiments focus on the two NP-hard problems: the P-PEAKS problem, using a binary encoding, and the vehicle routing problem (VRP), which uses permutations. This choice is easy to justify since they represent two very important families of problems in combinatorial optimization. To answer RQ1, we solve these two problems with diﬀerent sizes (dimensions) while also studying the migration gap and the diﬀerent cores used (a high number of combinations and independent runs needed). Although a migration strategy involves making many decisions (gap, selection policy, and distributed topology), this paper focuses on the migration gap as a ﬁrst approach to test our fresh understanding of their performance under a wide set of conﬁgurations. Indeed, the migration gap highly impacts the diversity of the island-based PGA. For RQ2, we devise a mathematical model that enables a higher level and quantitative analysis beyond the usual comparison of ﬁgures in tables/graphs found in the literature. We then used all that for a ﬁnal study of PGAs to better understand their computational and numerical eﬀorts (RQ3)."
2106.09922,open-source,106,,,"RAM, Linux Ubuntu 18.04.5 LTS operating system, connected through a 1 Gbps Ethernet. We compare a wealth of dssGA algorithms over the diﬀerent number of cores, in particular, 1, 2, 4, 8, 16, 32, and 64 cores are used, and their behavior is compared and mathematically modeled. In order to utilize six computers uniformly, our experiments conﬁgure the number of utilized cores as shown in Table 2. Our algorithms are implemented in C++/MPI. We choose Open MPI [32], an open-source MPI implementation that is very popular."
2106.13764,code,27,,,"elements without the need to execute their code. Additionally, SlimWeb maintains the core visual, interactive and functional components of the mobile web pages."
2106.13764,code,36,,,"[66] Yao Wang, Wan-dong Cai, and Peng-cheng Wei. A deep learning approach for detecting malicious javascript code. security and communication networks, 9(11):1520–1534, 2016."
2106.13764,code,46,,,"[44] Xincheng He, Lei Xu, and Chunliu Cha. Malicious javascript code detection based on hybrid analysis. In 2018 25th Asia-Paciﬁc Software Engineering Conference (APSEC), pages 365–374, Nara, Japan, 2018. IEEE, IEEE."
2106.13764,code,116,,,"4.1 SlimWeb employs a novel ML classiﬁer to categorize JS elements embedded in a web page, and then label each of these elements as critical or non-critical according to the user preference. To overcome the challenging task of understanding the non-deterministic behavior of JS [67], the main design goal of SlimWeb is to give an insight into each JS element embedded in a web page, without evaluating the exact behavior of these elements, nor executing its code. The classiﬁer categorizes a given JS element into one of the known JS categories using supervised neural network model, which shows a better and more robust performance over alternatives."
2106.13764,code,194,,,"JavaScript classiﬁcation is a challenging problem due to the dynamic non-deterministic behavior of the code [67], especially when attempted on-the-ﬂy using low-end mobile phones, since this leaves no room for executing the JS code– which tends to consume numerous resources and time. Existing research on JS ﬁltering primarily focuses on the identiﬁcation of advertising and/or tracking JS [42, 47, 49], in addition to malicious JS code snippets [32, 33, 44, 66, 68, 69]. Existing client-side solutions to reduce the cost of JS rely on the integration of a JS blocker as a browser extension [35, 58]. However, these blockers aim to prohibit a speciﬁc category of JS (such as Ads) [6, 18, 24], by relying on predeﬁned blocking lists. Although these lists can be periodically updated, existing blockers fail to automate the classiﬁcation of previously-unseen JS elements that can be created from existing known libraries by changing the serving domain or obfuscating code and metadata of web pages [17]."
2106.13764,"code, dataset",141,,,"With SlimWeb, we aim to ﬁll this gap by classifying JS elements via a supervised Machine Learning (ML) model. Instead of examining the JS code to infer its semantics and behaviour, SlimWeb extracts highly predictive features from the code, and then uses them to classify JS. A labeled dataset is required to train such a JS classiﬁer, which unfortunately does not exist today to the best of our knowledge. Thus, using categories identiﬁed by experts of the web community [22], we built a labeled dataset with 127,000 JS elements by scraping more than 20,000 popular web pages. Using this labeled dataset, we show that SlimWeb’s JS classiﬁer achieves a 90% classiﬁcation accuracy across eight different JS categories using 508 carefully selected features."
2106.13764,"code, dataset",194,,,"identiﬁed and commonly used by experts in the web community [22]. Our dataset consists of 127,000 JS element that were labeled with categories out of 500,000 JS elements crawled from 20,000 popular web pages. The category of a given JS element is used to determine the criticality of that element and decide to block or not to block it accordingly. Each JS element is represented as a vector of features according to [27] with a category label. These features cover the comprehensive set of APIs [53, 54] to interact with the Document Object Model (which deﬁnes the structure of HTML documents and the way in which they can be accessed and manipulated). While other representations can take the structure of the code into account, it is shown that the text representation is sufﬁcient [59]. To learn effectively over a small training dataset, we apply Recursive Feature Elimination (RFE), which is a feature selection method that removes the less informative features, resulting in a list of 508 features instead of 1262."
2106.13764,data,33,,,"[50] Conor Kelton, Matteo Varvello, Andrius Aucinas, and Benjamin Livshits. Browselite: A private data saving solution for the web. arXiv preprint arXiv:2102.07864, 2021."
2106.13764,data,38,,,"[29] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785–794, 2016."
2106.13764,data,77,,,"[19] Victor Agababov, Michael Buettner, Victor Chudnovsky, Mark Cogan, Ben Greenstein, Shane McDaniel, Michael Piatek, Colin Scott, Matt Welsh, and Bolian Yin. Flywheel: Google’s data compression proxy for Symposium on Networked Systems the mobile web. In 12th } NSDI Design and Implementation ( { [20] Abien Fred Agarap. Deep learning using rectiﬁed linear units (relu)."
2106.13764,data,109,,,"In Flywheel [19], a proxy service is proposed to extend the life of mobile data plans by compressing responses between the servers and the browsers. However, improving the page load time through Flywheel is not always possible. BrowseLite [50] is another recent tool that aims to achieve data savings by applying different image compression techniques. However, it assumes the existence of server-side tools to perform the desired compression. Other platform-speciﬁc solutions have been proposed to improve the browsing experience, such as Apple News [21], and Instant Articles [38] for Facebook users."
2106.13764,data,123,,,"6.4.1 SlimWeb User Study. We evaluated SlimWeb by requesting 20 users to install the plugin on their smartphones and browse the web from these devices for a period of two weeks. Participants were then requested to respond to a set of survey questions by the end of the evaluation period. Out of 20 participants, 19 respond to the survey questions. All participants believe that SlimWeb is useful in providing lighter versions of web pages. Results show that the plugin helps 63% of the users to browse more pages with their data plan. Around 68% of the users agree that the feedback on broken pages helps in improving the performance of the plugin. The"
2106.13764,data,148,,,"via a proxy server (see Section 3). Privacy Badger [39] is a browser extension that blocks both advertising and tracking, whereas ADBlock [18] is one of the most popular ad-blocking browser extension that is used by tens of millions of users worldwide. Our comparison is both quantitative, i.e., focusing on speed and data usage, and qualitative, i.e., focusing on similarity of the lightened pages with respect to the original pages. We leverage Firefox browser using the HUAWEI phone equipped with the SlimWeb plugin. Browsertime is used for automation and no proxy is used given that Firefox mobile supports addons – with the exception of JSCleaner which is a proxy-based solution. We consider real 3G and 4G network conditions to evaluate the top 500 most popular pages from [23]."
2106.13764,data,149,,,"A third deployment option is to integrate the classiﬁcation engine directly into the browser. Indeed, some of our early testing have demonstrated that it is possible to run the JS classiﬁcation on a mobile phone browser using TensorFlow [16]. However, such an approach comes with its own limitations. For example, since the classiﬁcation engine has to operate at the client side, the content of a given JS ﬁle has to be downloaded ﬁrst by the browser, thus defying the data saving goals for unknown JS. Nevertheless, once the JS element is classiﬁed, the class label will be internally stored within the plugin cache to be used later. This means that a JS element would only need to be classiﬁed once, and the label will be known in subsequent requests to the same JS."
2106.13764,data,190,,,"Figure 4 shows the Cumulative Distribution Functions (CDFs) for page load time, and number of network requests. Figure 4a shows that blocking ads only (green curve) achieves the highest PLT reductions compared to the other two JS categories, which is almost comparable to blocking all JS categories (orange curve). The ﬁgure also shows that social has the least impact on the PLT compared to the other categories, followed by blocking analytics. Although blocking ads achieves the closest reductions to blocking all categories, it does not come close in terms of reducing the number of network requests (Figure 4b). This suggests that it is beneﬁcial for users in developing regions who care about their data consumption to block all three categories to achieve the most data savings, with more than 50% reduction in page size at the median (from 1300 KB to 600 KB). Additionally, reducing the number of network requests indirectly improves the phone energy consumption, given that it reduces the usage of the cellular interface."
2106.13764,data,287,,,"6 EVALUATIONS This section evaluates SlimWeb with respect to objective metrics like how fast it can load webpages, data savings, and web compatibility. We use two low-end mobile devices: a HUAWEI Y511-U30 (which costs about $89, and is equipped with a dual-core 1.3 GHz Cortex-A7 CPU and 512 MB RAM) and a Xiaomi Remdi Go (which costs about $70, and is equipped with a quad-core 1.4 GHz Cortex-A53 CPU and 1 GB RAM). The Xiaomi phone connects to the Internet over a fast WiFi where network throttling was used to emulate both 3G (downlink/uplink set to 1.6 Mbps/768 Kbps, RTT set to 150ms) and 4G (downlink/uplink set to 12 Mbps, RTT set to 70ms) networks, whereas the HUAWEI phone connects to the Internet using real 3G/4G networks. The two phones are located in two different labs, whose locations were anonymized to respect the double-blind review policy. Each mobile device connects via USB to a Linux machine which is used to run browser automation tools: Browsertime [51] with the Firefox browser and Lighthouse [11] with both Chrome and Brave. Such browser automation tools are used to automate both web page loads and telemetry collection, e.g. performance metrics and network requests. Our testbed further consists of a second Linux machine which acts as SlimWeb’s server and runs the JS labeling service (see Figure 1). This machine was located in the same lab as the Xiaomi Remdi Go and about 100 ms ⇠ (median latency) away from the HUAWEI Y511-U30."
2106.13764,"data, data https",51,,,"[35] Sybu Data. Sybu javascript blocker – google chrome extension. https: //sybu.co.za/wp/projects/js-blocker/, 2016. Accessed: 2020-05-02. [36] Houssein Djirdeh. Javascript | 2019 | the web almanac by http archive. https://almanac.httparchive.org/en/2019/javascript, 2019. Accessed: 2020-01-2."
2106.13764,database,138,,,"SlimWeb’s JS classiﬁcation service crawls popular web pages to identify JS elements used in these pages, and then employs the classiﬁer to label these elements and store their categories in a database. It periodically updates and shares the labels with the users’ browser plugins. On the other hand, SlimWeb’s browser plugin is responsible for blocking noncritical JS elements. These elements are identiﬁed based on the labels received from the service. When a web page is requested by the user, the plugin ﬁrst checks if a label is locally available for each JS element, such that non-critical elements are immediately blocked. In the case of a label absence, the plugin considers the corresponding JS element as critical and requests it from the web."
2106.13764,database,207,,,"5 IMPLEMENTATION & DEPLOYMENT On the user’s browser, SlimWeb uses a custom plugin that assigns a class to each JS element (either critical or non-critical) according to the user preference. Motivated by the results of the user preference survey presented in Section 2.2, the plugin considers ads and analytics as non-critical categories by default, while other categories are considered critical. Users are allowed to alter the default settings according to their preferences, and to have different conﬁgurations on a per-page basis according to their browsing experience. The plugin’s local storage is implemented using IndexedDB API to store the labels of the JS resources with a capacity limited to 50 MB. The database consists of a sequence of JS entries, where each entry is associated with a URL and an assigned label. The SlimWeb plugin is implemented using JavaScript for Firefox browser, and is accepted by Mozilla as a Firefox extension. By default, SlimWeb is deployed as a server-based solution as described in Section 4, where JS elements are classiﬁed by a server-side service that regularly updates and shares the classiﬁcation results with the users’ browsers."
2106.13764,dataset,81,,,"4.2 Dataset We created a dataset to train the supervised learning models, by crawling 20,000 popular pages from [1] and caching JS elements used by these pages. For each JS element, we extract the domain that corresponds to it for a potential match with an entity in a recent HTTP Archive repository [45], which provides a set of entities representing existing JS libraries with their associated domains and categories. These categories are"
2106.13764,dataset,149,,,"Table 4 presents a summary of the evaluation results of the six models. Each value in the table represents a weightedaverage of the corresponding metric, which is computed across all the 8 different categories. In general, tree-based classiﬁcation models provide better performance in comparison to the distance-based models (KNN, SVM, and LSVC), while the neural network model achieves the best performance (with a slight improvement in precision and recall compared to RFC and XG Boost). Consequently, we select it for JS classiﬁer in SlimWeb. For a multi-class classiﬁcation problem with 8 classes, a 90% precision-recall achieved by a simple neural network is quite signiﬁcant; we attribute this accuracy due to the size of the dataset of 127, 000 JS elements and believe that this can improve with a larger dataset."
2106.13764,dataset,156,,,"The question here might be: why not matching the URLs of the JS elements in web pages to know their categories instead of utilizing an ML classiﬁer. The answer is that the model would serve the objective of predicting the categories of unknown JS elements embedded in web pages. Moreover, developers might select to locally host some of the known JS, where the local URLs would not match with the elements in the repository. Although the created dataset is limited to well-known JS libraries, it serves a large portion of JS in today’s web due to the following reasons: 1) it combines the features (not the URLs) of the elements with their associated categories. 2) The utilized features are associated with the JS access to web pages, which makes them appropriate for category prediction of previously unseen web-based JS."
2106.13764,download,119,,,"With SpeedReader [40] tool added to Brave, pages suitable for the reader mode can be signiﬁcantly enhanced, but SpeedReader is not yet available for mobile phones, and does not consider pages that utilize JS for content generation. Wprof [64] attracts the attention to JS a key bottleneck in the page load due to its role in blocking HTML parsing. To speedup PLT, Shandian [65] restructures the page loading process, whereas Polaris [56] detects additional edges for more accurate fetch schedules. Unlike SlimWeb, these solutions require the browser to download, process and execute the entire JS brought by a given page."
2106.13764,download,228,,,"Table 1 shows the performance results with four metrics. The number of JS requests and the size of resources show the difference in JavaScript utilization, and its impact on the total resources’ download size, respectively. The effect of JavaScript utilization on the page performance is shown by the Speed Index [7] and the Time to Interactive [9]. As the table shows, removing analytics, advertising and social elements reduces the number of requests by around 78% (from 23 requests in the original page to 5 requests in the optimized page). This reduction causes a 50% decrease in the size of resources transferred. A considerable impact was also observed in the Time to Interactive, which decreased from 15 seconds in the original version to 6.1 seconds in the optimized version. The optimized page was visually evaluated in terms of content completeness and functionality. The main content of the page and the functional components were fully preserved in the optimized page. The impact of JS removal has resulted in the removal of an ad element from the top of the page. This example shows potential of identifying and removing non-critical JS from web pages for the beneﬁt of mobile users without affecting the quality of these pages."
2106.13764,"download, code",61,,,"[32] Marco Cova, Christopher Kruegel, and Giovanni Vigna. Detection and analysis of drive-by-download attacks and malicious javascript code. In Proceedings of the 19th international conference on World wide web, pages 281–290, Raleigh North Carolina USA, 2010. ACM, Association for Computing Machinery, New York, NY, United States."
2106.13764,"github, data",17,,,"at 8afa2d8cadddec8f0db39e7d715c07e85fb0f8ec · patrickhulce/thirdparty-web. https://github.com/patrickhulce/third-party-web/blob/ 8afa2d8cadddec8f0db39e7d715c07e85fb0f8ec/data/entities.json5, September 2019. Accessed: 2020-01-2."
2106.13764,open-source,16,,,[12] Mitmproxy – a free and open source interactive https proxy. https:
2107.07983,data,5,,,data reordering is required.
2107.07983,data,9,,,synthesis data with typical weight and activation sparsity.
2107.07983,data,22,,,"To keep the MAC utilization high, this load imbalance is usually evened out using a data staging buffer, which collects"
2107.07983,data,40,,,"S2TA is the ﬁrst architecture exploiting both W-DBB and A-DBB, and is the ﬁrst to incorporate DBB into a systolic array with the novel time-unrolled technique exploiting new dimensions of data reuse for up to 8× peak speedup."
2107.07983,data,42,,,"Data Reuse In addition to naturally supporting DBB sparsity, the TPE organization exposes two new dimensions of data reuse compared to the 1 × 1 × 1 PE organization in classic systolic arrays. First, moving from a scalar MAC"
2107.07983,data,50,,,"6. S2TA-AW on typical CNN microbenchmarks with 50% (75%) weight and activation sparsity show 8 (16) TOPS, 14.3 (26.5) TOPS/W, and 2.16 (4.21) TOPS/mm2, in 16nm, based on full-accelerator synthesis data in Table 5."
2107.07983,data,53,,,"Key Insight The energy consumption of the actual INT8 MAC computation in Fig. 1 is signiﬁcantly overshadowed by the buffers used for operands and accumulators. Therefore, any sparsity-exploiting scheme must not introduce signiﬁcant overheads that exacerbate the data buffering cost, as that is already the dominant energy consumer."
2107.07983,data,55,,,"A common strategy to improve CNN accelerator efﬁciency is to exploit sparsity, as zeros in the data tensors (both weights and activations) reduce the theoretical compute and storage requirement signiﬁcantly. Zeros in CNNs are statistically distributed in a random pattern. Exploiting the random sparsity, † Denotes equal contribution."
2107.07983,data,57,,,"• Structured Sparse Tensor Accelerator (S2TA) We show that joint DBB weight and activation sparsity can be efﬁciently incorporated into CNN accelerators. As a case study, we target the systolic array template, by extending the traditional scalar PE into a tensor PE (TPE) that consumes compressed DBB data blocks."
2107.07983,data,63,,,"Figure 2: Hardware structures for sparse GEMM require explicit data re-ordering, which introduces overheads (blocks in red), in the form of either (a) an operand gather stage before the MAC compute (SMT-SA [38]), or (b) a result scatter with a distributed accumulator (SCNN [30])."
2107.07983,data,69,,,"Today’s sparsity-exploiting mechanisms, unfortunately, introduce signiﬁcant area and energy overhead for data buffering due to non-trivial data re-ordering, which can be classiﬁed into two fundamental categories, as illustrated in Fig. 2. The ﬁrst category (Fig. 2a) performs a gather operation in the front-end to collect matching pairs of non-zero operands before buffering and ﬁnally performing MAC computations in"
2107.07983,data,72,,,"6.3 Other Design and Implementation Details On-chip SRAM As is commonplace for accelerators, we heavily leverage local software managed SRAM [25] to provide a low-cost operand supply. The 0.5MB weight buffer (WB) and the 2MB activation buffer (AB) are separate. Both are double buffered to overlap computation in the TPE array and DMA data transfer. The SRAM is grouped, rather than"
2107.07983,data,81,,,"DBB Weight Sparsity Kang [19] implements accelerator exploiting a ﬁxed 2/8 W-DBB sparsity. The design is based on a dot product microarchitecture with limited data reuse. Similar work [45] also exploited W-DBB in the GPU context. The proprietary Nvidia A100 GPU implements ﬁxed 2/4 WDBB, which achieves 1.5× speedup and 3.12 TOPS/W (peak) [8], 4× lower than the S2TA-W baseline at 12.4 TOPS/W (Table 4)."
2107.07983,data,81,,,"The weight (W ) and activation (A) tensors have independent random sparsity patterns. Thus, during execution, we must walk the indexes to ﬁnd matching pairs of non-zero positions to pass to the MAC units. The number of matches varies wildly depending on the position and the input data, which gives rise to an unpredictable number of MACs in any single cycle, leading to variable, unbalanced PE utilization at run time."
2107.07983,data,83,,,"Activation DBB compression (A-DBB) can be used together with W-DBB, allowing us to exploit the full potential of sparse data, without signiﬁcant overheads. However, ADBB is more challenging than W-DBB because: (1) activation sparsity is unbounded, and (2) optimal activation DBB sparsity varies signiﬁcantly across the layers of a network. In this section, we describe the co-designed Dynamic Activation Pruning (DAP) and time-unrolled variable DBB architecture."
2107.07983,data,87,,,"DBB provides two beneﬁts. First, processing DBB blocks in order ameliorates the load imbalance problem and removes the distributed accumulator problem encountered with unstructured sparsity, avoiding the energy- and area-hungry buffers. Second, DBB lends itself to a simple hardware design based around the movement of small DBB data blocks, as we will illustrate with our S2TA accelerator (Sec. 6). This compact and efﬁcient hardware architecture is possible because results are naturally generated in sequential blocks; no"
2107.07983,data,88,,,"Building on W-DBB and A-DBB, we describe how to integrate the two building blocks into a fully-sparse CNN accelerator that efﬁciently exploits sparsity in both weights and activations. In particular, we focus on the well-known systolic array architecture, popular in mobile/embedded inference scenarios due to its extreme efﬁciency arising from high data reuse. We show that both W-DBB and A-DBB can be easily integrated into a classic SA architecture by introducing a tensor PE (TPE) design enabling further efﬁciency gains."
2107.07983,data,94,,,"However, implementing fully sparse GEMM on energy and area-constrained INT8 mobile/IoT accelerators is challenging. Basically, removing MACs with zero operands breaks the regular compute pattern and requires complex on-chip buffering and data re-ordering to maximize the hardware utilization. The additional buffers signiﬁcantly increase the energy and area overhead. Fig. 1 shows the energy breakdown of an INT8 dense systolic array accelerator for a typical CNN layer. The data is obtained from the extracted post-layout power estimation in a 16nm technology node with fully annotated switching activity."
2107.07983,data,101,,,"These two new forms of data reuse result in much smaller on-chip buffer sizes, as the ﬂip-ﬂops required in the TPE are increasingly shared amongst a larger number of MAC units. Table 1 shows that S2TA-W with a 4×4×4_4×8 TPE array, and time-unrolled 8×4×4_8×8 S2TA-AW for BZ=8 have ∼7–1,886× less total buffers per MAC than previous architectures. As a result, a larger TPE would also increase the energy efﬁciency, albeit at a marginally reduced clock frequency. Note that the outer-product TPE is more efﬁcient than the dot-product counterpart due to increased data reuse."
2107.07983,data,106,,,"S2TA-W On average, S2TA-AW consumes 1.84× lower energy and achieves 1.26× speedup over S2TA-W (Fig. 11). SA-SMT SA-SMT [38] exploits unstructured sparsity in a systolic array. SA-SMT suffers from the overhead of distributing matching pairs, which can stall the data ﬂow in the systolic array. SA-SMT resolves it using expensive operand buffer FIFOs (Sec. 2.2). We reimplemented SA-SMT, which achieves 8.01 TOPS/W compared to 14.3 TOPS/W for S2TA-AW at the same sparsity, as shown in Tbl. 4. This is due to the high energy cost of the FIFOs"
2107.07983,data,119,,,"Local MCU with SIMD We implement non-GEMM operations such as activation functions, pooling, scaling, normalization and data type casting using Arm Cortex-M33 [1] microcontrollers (MCUs), which have 32-bit SIMD instructions [2]. M33 is very small (0.008mm2 [1]) and low power (3.9µW/MHz [1]) in 16 nm. Control and data movement (DMA) tasks are also performed by the MCUs, e.g. loading the input image into AB. We use a cluster of 4 MCUs each with a small 64KB control store SRAM, which is sufﬁcient to ensure that the MCUs are never the performance bottleneck."
2107.07983,data,121,,,"Exploiting unstructured sparsity introduces hardware overheads due to additional buffers used in data manipulation. There are two fundamental approaches to supporting random sparsity. The ﬁrst is the inner-product style [6, 38] which requires an operand gather stage to evenly distribute the unpredictable workload to the PEs to perform the MAC operations. The second, is the outer-product style which has more conventional operand distribution and MAC operations, but then requires a result scatter stage to spread the non-contiguous results of the individual MACs over the output feature maps. Both of these approaches introduce signiﬁcant hardware overheads in the form of large buffers that drastically degrade the energy and area-efﬁciency of the accelerator."
2107.07983,data,125,,,"Sparsity in Systolic Arrays SAs (e.g. Google TPU [18]) are efﬁcient because they have high data reuse and local communication. SMT-SA [38] is an SA that exploits unstructured FP32 sparsity using data staging FIFOs, which are energy inefﬁcient for INT8 datapath, although acceptable for FP32. Kung et al. [23] showed a preprocessing step of column combining of sparse weight matrices, before processing on a dense SA architecture. Liu et al. [26] exploited W-DBB sparsity for INT8 datapath in systolic architecture. NB-SMT [37] is a sparse SA with the ability to momentarily halve the MAC precision during to aid load balancing pipeline hazards."
2107.07983,data,150,,,"We argue that the sparsity-exploiting microarchitecture structures must be lightweight for it to be beneﬁcial to exploit sparsity at all in mobile/embedded CNN accelerators. Otherwise, the energy/area overheads can easily eclipse the speedup gains. To that end, we propose to exploit structured sparsity, which has regular sparsity patterns that allow the hardware additions required to be very lean. In particular, we focus on Density Bound Block (DBB) [26] format, which tiles data tensors into blocks, and then introduces a bound on the maximum number of non-zero elements per block. DBB overcomes both of the challenges with random sparsity. Firstly, the maximum number of MAC operations is ﬁxed, which signiﬁcantly achieves high PE utilization without operand buffers. Secondly, the blocked data limits the location of output results, eliding distributed accumulators."
2107.07983,data,165,,,"The rest of the paper discusses how to build an efﬁcient accelerator to exploit weight and activation DBB sparsity, without signiﬁcant accuracy drop. Weight sparsity is statically known, and thus the hardware design is relatively straightforward. Activation sparsity, however, is dynamic. We propose a dynamic pruning scheme co-designed with a novel timeunrolled architecture to exploit DBB sparsity in activations. Building on top of DBB compression for both weight and activation, we describe how to integrate them into a complete DNN accelerator (Sec. 6). In particular, we focus on the popular systolic architecture. We show that both weight and activation DBB support can be easily integrated into a classic SA architecture by grouping PEs, leading to a tensor PE (TPE) design. Critically, the TPE design naturally exposes additional dimensions of data reuse that is unobtainable in the traditional SA design, enabling further efﬁciency gains."
2107.07983,data,178,,,"Indexed Unstructured Sparsity EIE [15] implements a ﬁne-grained sparse CSR-encoded INT16 matrix-vector accelerator, and ESE [16] extends this to LSTMs. Doping [39] and MASR [14] also exploit unstructured sparsity for LSTMs and RNNs, but uses a bitmask encoding. A number of papers target unstructured sparse matrix multiplication for very sparse data, such as Outer Space [29], which uses an outer product scheme, and SpArch [44], which further optimizes for locality. Cnvlutin [4] skips compute for zero activations, without explicit indexes. SCNN [30] implements a fully CSRindexed sparse CNN accelerator using an outer product to exploit sparse weights and activations. FixyNN [43] demonstrates a ﬁxed-weight accelerator, that can very efﬁciently exploit random sparsity. SparTen [13] and Eyeriss v2 [6] both support fully-sparse inference. We focus on DBB sparsity, but compare with SparTen, and Eyeriss v2 (Table 4)."
2107.07983,data,181,,,"We propose to exploit structured sparsity, more speciﬁcally, Density Bound Block (DBB) sparsity for both weights and activations. DBB block tensors bound the maximum number of non-zeros per block. DBB thus exposes statically predictable sparsity patterns that enable lean sparsity-exploiting hardware and efﬁcient memory access. We propose new hardware primitives to implement DBB sparsity for (static) weights and (dynamic) activations, respectively, with very low overheads. Building on top of the primitives, we describe S2TA, a systolic array-based CNN accelerator that exploits joint weight and activation DBB sparsity and new dimensions of data reuse unavailable on the traditional systolic array. S2TA in 16nm achieves more than 2× speedup and energy reduction compared to a strong baseline of a systolic array with zero-value clock gating, over ﬁve popular CNN benchmarks. Compared to two recent non-systolic sparse accelerators, Eyeriss v2 (65nm) and SparTen (45nm), S2TA in 65nm uses about 2.2× and 3.1× less energy per inference, respectively."
2107.07983,data,215,,,"the matched operand pairs and packs them into groups of a ﬁxed size that matches the datapath width [27, 32]. The hardware supporting this approach is shown in Fig. 2a. While the non-zero operand matching itself may be of a reasonable cost [13], the data staging buffer (typically FIFO) introduces high energy and area overheads, which are especially signiﬁcant for low energy/area INT8 mobile/embedded accelerators. Fig. 3 quantiﬁes the energy and area overhead of the FIFOs used for distributing matching pairs for INT8 operands. We compare the energy and area across four designs: a dense systolic array (SA), a systolic array with ZVCG optimization (SA-ZVCG), and two variants of SA-SMT, a recent systolic array that exploits random sparsity using operand FIFOs [38]. The two SA-SMT variants differ by their FIFO depths; one uses 2-entry FIFOs (SMT-T2Q2) and the other uses 4-entry FIFOs (SMT-T2Q4). The PPA is obtained for a typical convolution layer with 50% weight and activation sparsity. The energy and area are broken down into the two key SA components: MACs (compute) and the on-chip buffers."
2107.07983,dataset,1,,,Dataset
2107.07983,dataset,24,,,"Finally, we demonstrate A/W-DBB pruning of Transform ers, by training I-BERT [20] on the GLUE dataset [40]."
2107.07983,download,16,,,[Online]. Available: https://developer.download.nvidia.com/video/ gputechconf/gtc/2020/presentations/s22085-accelerating-sparsity-inthe-nvidia-ampere-architecture%E2%80%8B.pdf
2107.07983,python,107,,,"Automatic RTL Generation The S2TA accelerator is highly modular and can be conﬁgured to make a calculated trade-off between area, performance, and power consumption. Instead of evaluating an arbitrary design point, we implement a parameterized Python RTL generator to explore the full design space, deﬁned by ﬁve main parameters: the three TPE dimensions (A, B, C in Sec. 6.1) and the dimension of the entire SA (M, N); altogether denoted as A×B×C_M×N. Each design can be further conﬁgured with any combination of W-DBB, A-DBB, ZVCG, and time-unrolling."
2107.08795,data,32,,,model’s updates to the central aggregator. The federated learning framework promises to enhance the model’s performance by multipartite data aggregation under clients’ absolute privacy conditions.
2107.08795,data,36,,,TextDecoderPre-netEncoderPre-netDynamic EncoderDynamic DecoderPost BlockStop TokenScaled Positional EncodingDynamic EncoderDynamic DecoderDynamic EncoderDynamic DecoderDynamic EncoderDynamic Decoder1cL1cL111ncL1ncL1t2t1)(ncTt communication round Model AggregationModel Update...Data Owner NNtW1tWServerServerUpdatetWClient 1Client 1Client NClient NDT-TTSDT-TTSData Owner 1Algorithm 1 Federated Dynamic Transformer
2107.08795,data,88,,,"[21] A. Hard, K. Partridge, C. Nguyen, N. Subrahmanya, A. Shah, P. Zhu, I. Lopez-Moreno, and R. Mathews, “Training keyword spotting models on non-iid data with federated learning,” in Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 2529 October 2020, H. Meng, B. Xu, and T. F. Zheng, Eds. ISCA, 2020, pp. 4343–4347."
2107.08795,data,89,,,"[11] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efﬁcient learning of deep networks from decentralized data,” in Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, ser. Proceedings of Machine Learning Research, A. Singh and X. J. Zhu, Eds., vol. 54. PMLR, 2017, pp. 1273–1282."
2107.08795,data,89,,,"[28] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efﬁcient learning of deep networks from decentralized data,” in Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, ser. Proceedings of Machine Learning Research, A. Singh and X. J. Zhu, Eds., vol. 54. PMLR, 2017, pp. 1273–1282."
2107.08795,data,95,,,"To tackle this problem, federated learning [11–14] has presented as a paradigm that allows clients to train a shared global model collaboratively without data breaches. When training models in the federated learning setting, participating clients do not upload their local data to the central server; instead, a central aggregator coordinates the optimization procedure among the clients [15, 16]. At each iteration of this procedure, clients compute gradient-based optimization using their local data to update the current model parameters and then upload only these"
2107.08795,data,154,,,"To ensure the security and privacy of original data, we introduce federated learning to the Text-to-Speech synthesis. As the communication overhead is the bottleneck of the federated learning, we deploy our Dynamic Transformer for TTS model training in a federated setting, as shown in Figure 1(d). Following the classical federated learning optimization method FedAvg [28], in each communication round, ﬁrstly, the server randomly choose M clients composing St, then send the encrypted current global model’s weights to every client in St. Each client decrypts it and updates the weights using its local data. Secondly, clients upload their updated weights to the server through the same encrypt-decrypt procedure. After collecting updates, the server would do model aggregation by averaging all clients’ updates together, then update the global model with the aggregation value."
2107.08795,data,175,,,"Text to speech (TTS) [1–3] synthesis is an active research area. Despite decades of research, generating natural speech from text is still a challenging task. With the development of voice technology in recent years, various voice services and applications have new advances, affecting people’s daily lives. Over time, different technologies have taken over the ﬁeld. There are now feasible ways to produce natural prosody with high audio ﬁdelity using a much-simpliﬁed voice building pipeline [4–7]. Recently, many variants of transformer models such as FastSpeech [8], Transformer-TTS [9], AlignTTS [1], MultiSpeech [10] have shown great promises in the TTS task. However, such transformer models typically require sizable high-quality training data to achieve a good performance. Nevertheless, limitations like data privacy, liability, and regulatory concerns may make it difﬁcult for clients to collaborate with other data owners for centralized aggregation training."
2107.08795,data,191,,,"when the number of clients increases. Obviously, federated learning greatly helps by collaborating more data from other parties. In Table 2, we detail the MOS comparison between FedT-TTS and FedDT-TTS. We can see that when increasing the client’s number, MOS for both models is ascending, our FedDT-TTS is approaching baseline’s performance when all clients participating (3.96 to 4.03), especially. Besides, ours outperforms FedT-TTS in terms of client’s numbers. On the other hand, ours hugely reduces the total training time by around 20% in different clients number with the superior performance. Especially in a 5-clients situation, FedDT-TTS’s time cost is 49.26h, almost 30% reduction compared with FedT-TTS, 66.42h. We can see an almost linear reduction of FedDT-TTS according to the increasing number of clients. This majorly owes to the fewer parameters brought by the dynamic process, reducing training time and communication time. Above all, these signiﬁcantly demonstrate that ours is more stably trained and time-efﬁcient than the FedT-TTS."
2107.08795,"data, dataset",116,,,"3.1. Dataset All experiments are trained on the LJSpeech data [29]. This dataset consists of 13,100 short audio clips of a single speaker reading passages from 7 non-ﬁction books. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours. The texts are normalized and inserted with the beginning character (a space) and the end character (a period), e.g. “I am 10 years old” is converted to “ i am ten years old.”. The LJSpeech dataset is randomly divided into two sets: 12600 samples for training and 500 samples for testing."
2107.08795,"data, dataset",155,,,"4. Conclusion In this paper, we propose the Dynamic Transformer for TTS synthesis. Comparing with T-TTS, ours shows a signiﬁcant reduction in training time with miniature MOS deducted. Combining with the federated learning framework, we improve the model’s performance by collaborating with more data owners under absolute data privacy protection. The MOS of our model keeps increasing while adding clients, approaching the baseline method gradually. Besides, experiments on the LJSpeech datasets show that our FedDT-TTS surpasses FedT-TTS in time efﬁciency and model performance. On the one hand, the dynamic adding layers process signiﬁcantly saves client’s training time and communication costs. On the other hand, it contributes to improving global convergence robustly. Moreover, experiments on an unbalanced dataset and Mel-spectrograms analysis also prove that our DT-TTS is superior to Transformer-TTS in the federated learning framework."
2107.08795,"data, dataset",224,,,"Text to speech (TTS) is a crucial task for user interaction, but TTS model training relies on a sizable set of high-quality original datasets. Due to privacy and security issues, the original datasets are usually unavailable directly. Recently, federated learning proposes a popular distributed machine learning paradigm with an enhanced privacy protection mechanism. It offers a practical and secure framework for data owners to collaborate with others, thus obtaining a better global model trained on the larger dataset. However, due to the high complexity of transformer models, the convergence process becomes slow and unstable in the federated learning setting. Besides, the transformer model trained in federated learning is costly communication and limited computational speed on clients, impeding its popularity. To deal with these challenges, we propose the federated dynamic transformer. On the one hand, the performance is greatly improved comparing with the federated transformer, approaching centralize-trained Transformer-TTS when increasing clients number. On the other hand, it achieves faster and more stable convergence in the training phase and signiﬁcantly reduces communication time. Experiments on the LJSpeech dataset also strongly prove our method’s advantage. Index Terms: Text-to-Speech, Speech Synthesis, Transformer models, Dynamic Growing, Federated Learning"
2107.08795,dataset,17,,,"[29] K. Ito et al., “The lj speech dataset,” 2017."
2107.08795,dataset,41,,,"Separate the training dataset into ﬁve subsets, each having 2520 samples. Each client owns one subset, which has a length of 4.6 hours roughly. Additionally, the test set has a length of almost 1 hour."
2107.08795,dataset,130,,,"On Unbalanced Dataset: We also compare FedT-TTS and FedDT-TTS on an unbalanced dataset, which commonly happens in real federated learning applications. We investigate in a 3-clients setting, resplit the whole trainset into three subsets according to the split ratios, then assign each subset to each client. It shows that on the unbalanced dataset, ours still outperforms the FedT-TTS. The average improvement is about +0.09 for all split ratios. These results suggest that the FedDT-TTS is more suitable in federated learning, even on the unbalanced dataset. Mel-Spectrograms Visualization: We also analyze Melspectrograms generated by DT-TTS and T-TTS, respectively, inputting the same text, and compare them together with ground truth, as shown in Figure 3."
2107.09869,data,24,,,SMOTE is a data augmented technique which is used to reduce overﬁtting during training and is helpful to reduce the biasness of classiﬁer.
2107.09869,data,34,,,Bins are the quantiles where the probability distribution is same. Any number of bins can be selected for ECG to MTF images. We decided to take 10 bins as the data is
2107.09869,data,42,,,"[48] A. Uyar and F. Gurgen, “Arrhythmia classiﬁcation using serial fusion of support vector machines and logistic regression,” in 2007 4th IEEE Workshop on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications."
2107.09869,data,43,,,ANIKA TABASSUM is a recent graduate of the MSc Data Science and Analytics program at Ryerson University. She received her BA degree in Computer Science from McGill University in 2013. She has previously worked 4+ years as a software engineer/developer.
2107.09869,data,49,,,"IEEE, 2007, pp. 1–8. [73] Z. Ahmad and N. Khan, “Towards improved human action recognition using convolutional neural networks and multimodal fusion of depth and inertial sensor data,” in 2018 IEEE International Symposium on Multimedia (ISM)."
2107.09869,data,78,,,"[31] Y. Chen, H. Chen, Z. He, C. Yang, and Y. Cao, “Multi-channel lightweight convolution neural network for anterior myocardial infarction detection,” in 2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI). IEEE, 2018, pp. 572–578."
2107.09869,data,79,,,"2) We transform heartbeats of ECG signal to images using Gramian Angular Field (GAF), Recurrence Plot (RP) and Markov Transition Field (MTF) to conserve the spatial domain correlated information among the data samples. These transformations result in an improvement in classiﬁcation performance in contrast to the existing approaches of transforming ECG to images using spectrograms or methods involving timefrequency analysis (Short time Fourier transform or wavelet transform)."
2107.09869,data,98,,,"1) Two multimodal fusion frameworks for ECG heartbeat classiﬁcation called Multimodal Image Fusion (MIF) and Multimodal Feature Fusion (MFF), are proposed. At the input of these frameworks, we convert the heartbeats of raw ECG data into three types of twodimensional (2D) images using Gramian Angular Field (GAF), Recurrence Plot (RP) and Markov Transition Field (MTF). Proposed fusion frameworks are computationally efﬁcient as they keep the size of the combined features similar to the size of individual input modality features."
2107.09869,data,126,,,"ZEESHAN AHMAD1, (Graduate Student Member, IEEE), ANIKA TABASSUM2,LING GUAN3, (Fellow, IEEE), NAIMUL MEFRAZ KHAN4, (Senior Member, IEEE) 1 Department of Electrical, Computer and Biomedical Engineering, Ryerson University, Toronto, Canada.(e-mail: z1ahmad@ryerson.ca) 2 Master of Data Science program, Ryerson University, Toronto, Canada. (e-mail: anika.tabassum@ryerson.ca) 3 Department of Electrical, Computer and Biomedical Engineering, Ryerson University, Toronto, Canada.(e-mail: lguan@ee.ryerson.ca) 4 Department of Electrical, Computer and Biomedical Engineering, Ryerson University, Toronto, Canada.(e-mail: n77khan@ryerson.ca)"
2107.09869,data,167,,,"and shown in Fig. 5. The motivation of choosing GAF, MTF and RP is that they are three different statistical methods of transforming ECG to images. During transformation they preserve the temporal information and hence they are lossless transformations. We combine these three gray scale images to form a triple channel image (GAF-RP-MTF). A triple channel image is a colored image in which GAF, RP and MTF images are considered as three orthogonal channels like three different colors in RGB image space. However, this three-channel image is not conventional way of converting a gray scale image to RGB, rather in this paper all three gray scale images are formed from raw ECG data with different statistical methods. Thus, a three-channel image in the presented work carries statistical dynamics of the ECG and therefore, is more informative. Furthermore, three-channel image can be easily utilized with with off-the-shelf CNNs like AlexNet."
2107.09869,data,171,,,"layers. In [41], authors generated dual beat coupling matrix from the sections of heartbeats. This dual beat coupling matrix was then as 2D input to a CNN classiﬁer. Gray-level co-occurrence matrix (GLCM), obtained from ECG data is employed for features vector description due to its exceptional statistical feature extraction ability in [42]. In [43], ECG signals were segmented into heartbeats and each of the heartbeats were transformed to 2D grayscale images which were input to CNN. In [44], two second segments of ECG signal are transformed to recurrence plot images to classify arrhythmia in two steps using deep learning model. In the ﬁrst step the noise and ventricular ﬁbrillation (VF) categories were recognized and in the second step, the atrial ﬁbrillation (AF), normal, premature AF, and premature VF labels were classiﬁed. Experimental results show the promising performance of the proposed method."
2107.09869,data,172,,,"Conventional methods for heartbeat classiﬁcation using ECG signal rely mostly on hand-crafted or manually extracted features using signal processing techniques such as digital ﬁlter-based methods [6], mixture of experts methods [7], threshold-based methods [8], Principal Component Analysis (PCA) [9], Fourier Transform [10] and wavelet transform [11]. Some of the classiﬁers used with these extracted features are Support Vector Machines (SVM) [12], Hidden Markov Models (HMM) [13] and Neural Networks [14]. The ﬁrst disadvantage with these conventional methods is the separation of feature extraction part and pattern classiﬁcation part. Furthermore, these methods need expert knowledge about the input data and selected features [15]. Moreover, extracting features using subject experts is a time consuming process and features may not invariant to noise, scaling and translations and thus can fail to generalize well on unseen data."
2107.09869,data,243,,,"Exemplary performance of deep neural networks (DNNs) on ECG [16] and especially the performance of CNN using ID convolution [17] and 2D convolution [18] has recently attracted attention of many researchers. Deep learning models are capable of automatically learning invariant and hierarchical features directly from the data and employ end-to-end learning mechanism that takes data as input and class prediction as output. Recent deep learning models use 1D ECG signal or 2D representation of ECG by transforming ECG signal to images or some matrix form. For 1D ECG classiﬁcation, commonly used deep learning models are deep belief networks, restricted Boltzmann machines, auto encoders, CNN [19] and recurrent neural network (RNN) [20]. For 2D ECG classiﬁcation, CNNs are used and the input ECG data is transformed to images or some other 2D representation. It is experimentally proved in [21] that 2D representation of ECG provides more accurate heartbeat classiﬁcation compared to 1D. In our previous work [22], univariate ECG signal is transformed to images by segmenting ECG signal between successive R-R intervals and then stacking these R-R intervals row wise to form images. Finally, multidomain multimodal fusion is performed to improve the stress assessment. Experimental results proved that multidomain multimodal fusion achieved highest performance as compared to single ECG modality."
2107.09869,data,269,,,"B. TWO-DIMENSIONAL CNN APPROACHES The knock out performance of CNN on 2D data such as images convinced the researchers to convert raw ECG data to images for improved results. In [21], short-time Fourier transform is used to convert ECG signal into timefrequency spectrograms that were used as input to CNN for arrhythmia classiﬁcation. Experimental results show that 2D-CNN achieved higher classiﬁcation accuracy than 1DCNN. In [36], ECG signal is converted into spectro-temporal images that were sent as an input to multiple dense convolutional neural network to capture both beat-to-beat and singlebeat information for analysis. Authors in [37] transformed heartbeat time intervals of ECG signals to images using wavelet transform. These images are used to train a six layer CNN for heartbeat classiﬁcation. In [38], Generative neural network is used to convert the raw 1D ECG signal data into a 2D image. These images are input to DenseNet which produces highly accurate classiﬁcation, with high sensitivity and speciﬁcity using 4 classes of heart beat detection. To distinguish abnormal ECG samples from normal, authors in [39] used pretrained CNNs such as AlexNet, VGG-16 and ResNet-18 on spectrograms obtained from ECG. Using a transfer learning approach, the highest accuracy of 83.82% is achieved by AlexNet. In [40], multi-lead ECG are treated as 2D matrices for input to a novel model called multileadCNN (ML-CNN) which employs sub two-dimensional (2D) convolutional layers and lead asymmetric pooling (LAP)"
2107.09869,data,322,,,"In [45], a Multi-scale Fusion convolutional neural network (MS-CNN) is proposed for heartbeat classiﬁcation using ECG signal. The Multi-scale Fusion convolutional neural network is a two stream network consisting of 13 layers. The features obtained from the last convolutional layer are concatenated before classiﬁcation. Another Deep Multi-scale Fusion CNN (DMSFNet) is proposed in [46] for arrhythmia detection. Proposed model consists of backbone network and two different scale-speciﬁc networks. Features obtained from two scale speciﬁc networks are fused using a spatial attention module. Patient-speciﬁc heartbeat classiﬁcation network based on a customized CNN is proposed in [47]. CNN contains an important module called multi-receptive ﬁeld spatial feature extraction (MRF-SFE). The MRF-SFE module is designed for extracting multispatial deep features of the heartbeats using ﬁve parallel convolution layers with different receptive ﬁelds. These features are concatenated before being sent to the third convolutional layer for further processing. Two stage serial fusion classiﬁer system based on SVM’s rejection option is proposed in [48]. SVM’s distance outputs are related with conﬁdence measure and then ambiguous samples are rejected with ﬁrst level SVM classiﬁer. The rejected samples are then forwarded to a second stage Logistic Regression classiﬁer and then late fusion is performed for arrhythmia classiﬁcation. Authors in [49] presented a unique feature fusion method called parallel graphical feature fusion where all the focus is given to geometric features of data. Original signal was ﬁrst split into subspaces, then multidimensional features are extracted from these subspaces and then mapped to the points in high-dimensional space. Multi-stage feature fusion framework based on CNN and attention module was proposed in [50] for multiclass arrhythmia detection. Classiﬁcation is performed by extracting features from different layers of CNN. Combination of CNN"
2107.09869,data,342,,,"C. FUSION BASED APPROACHES Fusing different modalities mitigates the weaknesses of individual modalities both in 1D and 2D forms by integrating complementary information from the modalities to perform the analysis and classiﬁcation tasks accurately. In [45], a Multi-scale Fusion convolutional neural network (MS-CNN) is proposed for heartbeat classiﬁcation using ECG signal. The Multi-scale Fusion convolutional neural network is a two stream network consisting of 13 layers. The features obtained from the last convolutional layer are concatenated before classiﬁcation. Another Deep Multi-scale Fusion CNN (DMSFNet) is proposed in [46] for arrhythmia detection. Proposed model consists of backbone network and two different scale-speciﬁc networks. Features obtained from two scale speciﬁc networks are fused using a spatial attention module. Patient-speciﬁc heartbeat classiﬁcation network based on a customized CNN is proposed in [47]. CNN contains an important module called multi-receptive ﬁeld spatial feature extraction (MRF-SFE). The MRF-SFE module is designed for extracting multispatial deep features of the heartbeats using ﬁve parallel convolution layers with different receptive ﬁelds. These features are concatenated before being sent to the third convolutional layer for further processing. Two stage serial fusion classiﬁer system based on SVM’s rejection option is proposed in [48]. SVM’s distance outputs are related with conﬁdence measure and then ambiguous samples are rejected with ﬁrst level SVM classiﬁer. The rejected samples are then forwarded to a second stage Logistic Regression classiﬁer and then late fusion is performed for arrhythmia classiﬁcation. Authors in [49] presented a unique feature fusion method called parallel graphical feature fusion where all the focus is given to geometric features of data. Original signal was ﬁrst split into subspaces, then multidimensional features are extracted from these subspaces and then mapped to the points in high-dimensional space. Multi-stage feature fusion framework based on CNN and attention module was proposed in [50] for multiclass arrhythmia detection."
2107.09869,"data available, data https, dataset",61,,,"[62] R. Bousseljot, D. Kreiseler, and A. Schnabel, “Nutzung der ekgsignaldatenbank cardiodat der ptb über das internet,” Biomedizinische Technik/Biomedical Engineering, vol. 40, no. s1, pp. 317–318, 1995. [63] Ecg heartbeat categorization dataset. [Online]. Available: https://www."
2107.09869,"data, dataset",34,,,"PTB Diagnostic ECG dataset [62] for MI classiﬁcation using both proposed fusion frameworks. For experiments, ECG lead-II re-sampled data at sampling frequency of 125Hz is used as the input."
2107.09869,"data, dataset",76,,,"1) PhysioNet MIT-BIH Arrhythmia Dataset Forty seven subjects were involved during the collection of ECG signals for the dataset. The data was collected at the sampling rate of 360Hz and each beat is annotated by at least two experts. Using these annotations, ﬁve different beat categories are created in accordance with Association for the Advancement of Medical Instrumentation (AAMI) EC57 standard [64] as shown in Table 2."
2107.09869,database,43,,,"[61] G. B. Moody and R. G. Mark, “The impact of the mit-bih arrhythmia database,” IEEE Engineering in Medicine and Biology Magazine, vol. 20, no. 3, pp. 45–50, 2001."
2107.09869,dataset,1,,,Dataset
2107.09869,dataset,3,,,Dataset MIT-BIH PTB
2107.09869,dataset,7,,,dataset are shown in Fig 5.
2107.09869,dataset,9,,,TABLE 4. Training and Testing Samples of datasets
2107.09869,dataset,11,,,TABLE 5. Experimental results of MIT-BIH Dataset using AlexNet.
2107.09869,dataset,11,,,TABLE 7. Experimental results of PTB Dataset using AlexNet.
2107.09869,dataset,14,,,TABLE 10. Comparison of MI Classiﬁcation results of PTB Dataset with Previous Methods
2107.09869,dataset,15,,,TABLE 6. Experimental results of MIT-BIH Dataset using simpler CNN of Fig. 4
2107.09869,dataset,15,,,TABLE 8. Experimental results of PTB Dataset using simpler CNN of Fig. 4
2107.09869,dataset,15,,,TABLE 9. Comparison of heart beat Classiﬁcation results of MITBIH Dataset with Previous Methods
2107.09869,dataset,18,,,TABLE 3. Information about Number of Heartbeats before and after SMOTE for training component of MIT-BIH Dataset
2107.09869,dataset,21,,,TABLE 11. Comparison of Computational Cost of AlexNet and CNN of Fig. 4 using MFF Framework on MIT-BIH Dataset
2107.09869,dataset,22,,,"FIGURE 5. GAF, RP and MTF Images of MIT-BIH dataset according to the ﬁve different heartbeats deﬁned in Table 2"
2107.09869,dataset,25,,,IV. EXPERIMENTAL RESULTS A. ECG DATABASES Experiments are performed with PhysioNet MIT-BIH Arrhythmia dataset [60] [61] for heartbeat classiﬁcation and
2107.09869,dataset,27,,,2) PTB Diagnostic ECG dataset Two hundred and ninety (290) subjects took part during collection of ECG records for PTB Diagnostics dataset. 148
2107.09869,dataset,42,,,We perform experiments using both proposed fusion frameworks on MIT-BIH dataset with the training and testing samples shown in Table 4 and with the training parameters shown in Tables 1. The experimental results are shown in Tables 5 and 6.
2107.09869,dataset,44,,,"The comparison provided in Tables 9 and 10 is on the basis of datasets and the performance metrics. There are slight changes in the conditions for testing in few of the comparisons, However, it is appropriate to compare the results."
2107.09869,dataset,54,,,"[71] M. A. Ahamed, K. A. Hasan, K. F. Monowar, N. Mashnoor, and M. A. Hossain, “Ecg heartbeat classiﬁcation using ensemble of efﬁcient machine learning approaches on imbalanced datasets,” in 2020 2nd International Conference on Advanced Information and Communication Technology (ICAICT)."
2107.09869,dataset,66,,,"We can see from Fig. 5, that for each kind of image (GAF, RP and MTF), the gray scale images are more interpretable. These images show different patterns for each of the ﬁve categories of MIT-BIH dataset. The x-y values of the 2D images are just pixel values of the GAF, RP, and MTF images."
2107.09869,dataset,76,,,"We also provide the comparison of both proposed fusion frameworks in terms of inference speed as shown in Table 12. Inference speed is the time consumed by classiﬁer to recognize one test sample. It is expressed in microseconds (µs). It is observed that MFF yields high accuracy, precision and recall for both datasets as compared to MIF, however, MIF is computationally efﬁcient in terms of inference speed."
2107.09869,dataset,80,,,"For training on CNN, we need large number of samples. We use the same testing and training segments provided in [63] to train on CNNs. Since there is a class-imbalanced in the training part of the dataset as apparent from the numbers, we applied SMOTE [65] to upsample the minority classes (classes other than N) and ﬁnally settled on the numbers shown in the right column of Table 3."
2107.09869,dataset,112,,,"We used the standardized form of both datasets provided in [63]. These datasets are already denoised and the training and testing parts are provided in the form of standard ECG heartbeats. Furthermore, ﬁve classes of arrythmia and MI localization has already been done and provided in terms of standard ECG heart-beats. Our study focused on ECG to image transformation and to the design of proposed multimodal fusion frameworks. The main focus is increasing the overall performance of classiﬁcation of heartbeats. We did not attempt at modeling or solving for a speciﬁc type of noise. We conduct our experiments on Matlab R2020a on a"
2107.09869,dataset,167,,,"To justify the importance of the proposed fusion frameworks, we assess the performance of different components of the proposed framework with both datasets by concatenation and average fusion methods. We performed average fusion by accrediting the unity value to all the weights i.e w1 = 1, w2 = 1 and w3 = 1 in the gated fusion network. Since we have three modalities, therefore, by taking simple average, we get the equal value of 0.333 for each weight. We also experiment with 0.333 and get the same results. Since weights are equal in average fusion, therefore, to make things simpler, we assign a unity value to every weight. It is possible that better weight can be acquired through trainable weight coefﬁcients. This is something we plan to investigate in future. Tables 5, 6, 7 and 8 reports the results of assessing different fusion methods along with proposed fusion frameworks."
2107.09869,dataset,199,,,"E. TRAINING AND OPTIMIZATION We resize images to 227 x 227 to perform experiments with AlexNet. We also perform experiments with smaller but computationally efﬁcient CNN, whose architecture is shown in Fig. 4, to show that proposed frameworks can achieve comparable performance even with the smaller CNN. The comparison in terms of computational cost between both CNN models is provided in Table 11. We ﬁne tune Alexnet by reducing the size of second last fully connected layer ’fc7’ from 4096 to 512 and the size of last fully connected layer ’fc8’ from 1000 to size equal to the number of classes in our datasets. The size of “fc7” layer of AlexNet is 4096 which is according to size of classiﬁcation layer which is 1000. For our MIT-BIH dataset and PTB dataset, we need the size of classiﬁcation layer equal to 5 and 2 respectively due to number of classes in these datasets. Thus to make ‘fc7’ compatible with classiﬁcation layer, we reduce its size to 512. The training parameters for AlexNet and CCN are shown in Table 1."
2107.09869,dataset,201,,,"VI. CONCLUSION We proposed two computationally efﬁcient multimodal fusion frameworks for ECG heart beat classiﬁcation called Multimodal Image Fusion (MIF) and Multimodal Feature Fusion (MFF). At the input of these frameworks, we convert ECG signal into three types of images using Gramian Angular Field (GAF), Recurrence Plot (RP) and Markov Transition Field (MTF). In MIF, we ﬁrst perform image fusion by combining three input images to create a three channel single image which used as input to the CNN. In MFF, highly informative cues are pulled out from penultimate layer of CNN and they are fused and used as input for the SVM classiﬁer. We demonstrate the superiority of the proposed fusion frameworks by performing experiments on PhysionNet’s MIT-BIH for ﬁve different arrhythmias and on PTB diagnostics dataset for MI classiﬁcation. Experimental results prove that we beat the previous state-of-the-art in terms of classiﬁcation accuracy, precision and recall. The important ﬁnding of this study is that the multimodal fusion of modalities increases the performance of the machine learning task as compare to use the modalities individually."
2107.09869,"dataset provided, dataset",57,,,We perform experiments using both proposed fusion frameworks on PTB dataset with training and testing samples shown in Table 4 and with training parameters shown in Tables 1. Training and testing parts of the dataset are provided in [63] to train CNN models. The experimental results are shown in Tables 7 and 8
2107.09869,"github, data, code, dataset",289,,,"ABSTRACT Electrocardiogram (ECG) is an authoritative source to diagnose and counter critical cardiovascular syndromes such as arrhythmia and myocardial infarction (MI). Current machine learning techniques either depend on manually extracted features or large and complex deep learning networks which merely utilize the 1D ECG signal directly. Since intelligent multimodal fusion can perform at the stateof-the-art level with an efﬁcient deep network, therefore, in this paper, we propose two computationally efﬁcient multimodal fusion frameworks for ECG heart beat classiﬁcation called Multimodal Image Fusion (MIF) and Multimodal Feature Fusion (MFF). At the input of these frameworks, we convert the raw ECG data into three different images using Gramian Angular Field (GAF), Recurrence Plot (RP) and Markov Transition Field (MTF). In MIF, we ﬁrst perform image fusion by combining three imaging modalities to create a single image modality which serves as input to the Convolutional Neural Network (CNN). In MFF, we extracted features from penultimate layer of CNNs and fused them to get unique and interdependent information necessary for better performance of classiﬁer. These informational features are ﬁnally used to train a Support Vector Machine (SVM) classiﬁer for ECG heart-beat classiﬁcation. We demonstrate the superiority of the proposed fusion models by performing experiments on PhysioNet’s MIT-BIH dataset for ﬁve distinct conditions of arrhythmias which are consistent with the AAMI EC57 protocols and on PTB diagnostics dataset for Myocardial Infarction (MI) classiﬁcation. We achieved classiﬁcation accuracy of 99.7% and 99.2% on arrhythmia and MI classiﬁcation, respectively. Source code at https://github.com/zaamad/ECG-Heartbeat-Classiﬁcation-Using-Multimodal-Fusion"
2107.09869,"publicly available, data, dataset",273,,,"employed for MI classiﬁcation of different categories. MultiChannel Lightweight Convolutional Neural Network (MCLCNN) which uses squeeze convolution, the depth-wise convolution, and the point-wise convolution is proposed in [31] for MI classiﬁcation. Two end-to-end deep learning models based on CNN are proposed in [32]. These models are called two stage hierarchical model. Furthermore, generative adversarial networks (GANs) is used for data augmentation and to reduce the class imbalance. In [33], authors proposed a neural network model for precise classiﬁcation of heartbeats by following the AAMI inter-patient standards. This model works in two steps. In the ﬁrst step the signals are preprocessed and then features are extracted from the signals. In the second step, the classiﬁcation is performed by a two-layer classiﬁer in which each layer consists of two independent fully-connected neural networks. The experiments show that the proposed model precisely detects arrhythmia conditions. In [34], authors proposed a complex deep learning model consists of CNN and LSTM. This model classiﬁes six types of ECG signals by processing ten seconds ECG slices of MIT-BIH arrhythmia dataset. Experimental results proved that the proposed model could be used by cardiologists to detect arrhythmia. In [35], authors presented CNN based model for proper diagnoses of congestive heart failure using ECG. The testing and training of the proposed model was carried out on publicly available ECG datasets. Performance of the proposed model shows the authenticity of model for congestive heart failure detection."
2107.10483,"code available, data, code, github, download, dataset",125,,,"To ensure reproducibility, we have published the source code of the proposed method ENCO at https://github.com/phlippe/ENCO. The code includes instructions on how to download the datasets, and reproduce the experiments in Section 4 and additional experiments in Appendix D. Further, for all experiments of Section 4, we have included a detailed overview in Appendix C of (a) the used data and its generation process, (b) all hyperparameters used for all methods, and (c) additional details on the results. All experiments have been repeated with 5 to 25 seeds to obtain stable, reproducible results. Appendix C.1.2 outlines the packages that have been used for running the baselines."
2107.10483,"code available, data, code, publicly available, github, dataset",54,,,"We evaluate ENCO on structure learning on synthetic datasets for systematic comparisons and realworld datasets for benchmarking against other methods in the literature. The experiments focus on graphs with categorical variables, and experiments on continuous data are included in Appendix D.5. Our code is publicly available at https://github.com/phlippe/ENCO."
2107.10483,data,3,,,D.5 CONTINUOUS DATA
2107.10483,data,4,,,4.6 REAL-WORLD INSPIRED DATA
2107.10483,data,4,,,B.2.3 LIMITED DATA REGIME
2107.10483,data,4,,,D.5 Continuous data .
2107.10483,data,4,,,Data: N variables
2107.10483,data,5,,,D.7 NON-NEURAL BASED DATA SIMULATORS
2107.10483,data,8,,,"follow from the given, observational data."
2107.10483,data,11,,,• Xi and Xj are not independent under observational data.
2107.10483,data,12,,,"Derived from Multiparameter Single-Cell Data. Science, 308:523–529, 2005."
2107.10483,data,16,,,"expression data. Journal of computational biology, 7(3-4):601–620, 2000."
2107.10483,data,18,,,"• Finally, Theorem B.4 does not necessarily hold anymore since noise in our data can lead to"
2107.10483,data,18,,,• The expectations over the interventional data ˜p ˆI (X) is replaced by the joint distribution
2107.10483,data,25,,,"Next, we consider situations where data is very limited. Thereby, we consider two data sample axes: observational and interventional data."
2107.10483,data,31,,,"We ﬁrst experiment on synthetic graphs. We pick six common graph structures and sample 5,000 observational data points and 200 per intervention. The graphs chain and full represent the"
2107.10483,data,34,,,D.7 Non-neural based data simulators . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2107.10483,data,37,,,"Distribution ﬁtting trains a neural network fφi per variable Xi parameterized by φi to model its ...). The input to the network are all other variables, observational, conditional data distribution p(Xi|"
2107.10483,data,39,,,"Table 19: Experiments with a different data simulator, introducing independence among parents for each variable. Similar to the neural-based synthetic data, ENCO recovers most graphs with a minor error rate, outperforming other baselines."
2107.10483,data,42,,,"Rainer Opgen-Rhein and Korbinian Strimmer. From correlation to causation networks: a simple approximate learning algorithm and its application to high-dimensional plant gene expression data. BMC Systems Biology, 1(1):37, 2007. ISSN 1752-0509."
2107.10483,data,44,,,"Ruocheng Guo, Lu Cheng, Jundong Li, P. Richard Hahn, and Huan Liu. A Survey of Learning Causality with Data: Problems and Methods. ACM Comput. Surv., 53(4), 2020. ISSN 0360-0300."
2107.10483,data,47,,,"For the log-likelihood term, we start by reorganizing the expectations to simplify the gradient expression. The derivate term ∂ can be moved inside the two expectations over interventional data ∂γkl since those are independent of the graph parameters. Thus, we can write:"
2107.10483,data,48,,,"Figure 15: Results of ENCO for different graph structures under limited observational data sample size. Note the different scale of the y-axis for the six graphs. The structure learning performance remains good for sparse graphs, and suffers for graphs with larger parent sets."
2107.10483,data,51,,,"Table 12: Repeating experiments of Table 1 with very small sample sizes (20 samples per intervention, 1k observational samples). Despite the limited data, ENCO can recover graphs with small parent sets reasonably well, while the graphs collider and full suffer for all methods."
2107.10483,data,56,,,"For graphs that fulﬁll all conditions in the Theorems B.1 and B.4, ENCO is guaranteed to converge given sufﬁcient data and time. The conditions in the theorems ensure that there exist no local minima or saddle points in the loss surface of the objective in Equation 2 with respect to γ and θ."
2107.10483,data,58,,,"where f is an arbitrary function. The difﬁculty of deterministic variables is that a variable Xi can be fully replaced by its parents pa(Xi) in any conditional distribution. The only way we can identify deterministic variables is from interventional data, where an intervention on Xi breaks the dependency to its parents."
2107.10483,data,61,,,"Table 16: Experiments on graph with continuous data from Brouillard et al. (2020). The sufﬁx “-G” denotes that the neural networks model a Gaussian density, and “-DSF” a two-layer deep sigmoidal ﬂow. ENCO outperforms all baselines in this scenario, verifying that ENCO also works on continuous data well."
2107.10483,data,62,,,"In contrast to algorithms working on observational data, ENCO does not strictly require the faithfulness assumption. Hence, we can apply ENCO to graphs with deterministic variables. Deterministic variables have a distribution that is deﬁned by a one-to-one mapping of its parents’ inputs to an output value. In other words, we have the following distribution:"
2107.10483,data,64,,,"Figure 14: Results of ENCO for different graph structures under limited interventional data sample size. Note the different scale of the y-axis for the six graphs. While the general trend is the same for all graphs, i.e. decreasing performance with fewer samples, the order heuristic can reduce the SHD error by a considerable margin for most graphs."
2107.10483,data,64,,,"The number of samples provided as observational and interventional data is crucial for causal structure learning methods since the more data we have, the better we can estimate the underlying causal mechanisms. To gain further insights in the effect of the sample size on ENCO and the compared baselines, we repeat the experiments of Section 4.2 with different sample sizes."
2107.10483,data,65,,,"The following section describes in detail the derivation of the gradient estimators discussed in Section 3.3. We consider the problem of causal structure learning where we parameterize the graph by edge existence parameters γ and orientation parameters θ. Our objective is to optimize γ and θ such that we maximize the probability of interventional data, i.e., data generated from the true"
2107.10483,data,66,,,"Results are shown in Table 16, and the observations are the same as with categorical data. ENCO outperforms all other methods in all settings, especially for the more complex distributions. The higher error rate for the DSF setup is mostly due to overﬁtting of the ﬂow models. We conclude that ENCO works as accurately for both continuous and categorical data."
2107.10483,data,68,,,"Proof. If Xi and Xj are independent under observational data, the observational distributions would not identify any correlation among those two variables. Hence, transferring them for any graph to Xj, ...), thus making the objective invariant to the interventional data would have p(Xi| orientation of the edge, and removing any edge between Xi and Xj for sparsity."
2107.10483,data,68,,,"graphs under (arbitrary) interventions on single variables. Thereby, the likelihood estimates have been trained on observational data only. Additionally, we want to ensure that the graph is as sparse as possible to prevent unnecessary connections. Thus, an (cid:96)1 regularizer is added on top of the edge probabilities. The full objective can be written as follows:"
2107.10483,data,74,,,"Further, we also apply GES on the categorical data with an additional hyperparameter search over the penalty discount. The results in Table 18 give a similar conclusion as on the continuous data. While the baseline attains good scores for chains, it makes considerably more errors on all other graph structures than ENCO. This shows that ENCO is much more robust by jointly learning from observational and interventional data."
2107.10483,data,75,,,"Philippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre Drouin. Differentiable Causal Discovery from Interventional Data. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
2107.10483,data,76,,,"This condition is the inverse statement of Equation 23, in the sense that we consider interventions on the child/descendant Xj. In the data limit, this naturally follows from Equation 23 and Equation 24, but in the limited data regime, we might have violations of Equation 36 due to biases in our samples. Violations of Equation 36 are the cause of ENCO predicting cyclic graphs as seen in Section 4.2."
2107.10483,data,77,,,"In Section 3.5, we have discussed that ENCO can be extended to graph with latent confounders. For this, we have to record the gradients of γij for the interventional data on Xi and all other ij + γ(O) interventional data separately. We deﬁne γij = γ(I) is only updated with gradients from Equation 3 under interventions on Xi, and γ(O) confounders is:"
2107.10483,data,78,,,"To make the proof more accessible, we will ﬁrst discuss the assumptions that are needed for the guarantee, and then give a sketch of the proof. The proof will ﬁrst assume that we work in the data limit, i.e. have given sufﬁcient data, such that we can derive conditions that solely depend on the causal graphical model. In Appendix B.2.3, we extend the proof to the limited data setting."
2107.10483,data,79,,,"Table 8: Results of ENCO on detecting latent confounders averaged over 25 graphs with 25 nodes in the data limit (10k samples per intervention, 100k observational samples) and limited data (200 samples per intervention, 5k observational samples). In the data limit, only false negative predictions of latent confounders occured which did not affect other edge predictions. With little interventional data, more false positives occur reducing the precision."
2107.10483,data,82,,,"Limited observational data sample sizes Similarly as above, we repeat the experiments of Table 1 for ENCO but limit the observational sample size to 1000 and 2000 (5000 before) while keeping 200 samples per interventions. Observational data is important in ENCO for learning the conditional distributions. For variables with many parents, this becomes more difﬁcult when fewer samples are available, because the input space grows exponentially with the number of parents. Thus, we"
2107.10483,data,83,,,"Since we have the condition that θkl = θlk, the full gradient for θkl would therefore consist of the gradient above minus the gradient of Equation 21 with respect to θji. However, as discussed in Section 3.3, the orientation of an edge cannot be learned from observational data in this framework. Hence, we only want to use the gradients of θkl if we intervene on node Xk, which gives us the following gradient expression:"
2107.10483,data,88,,,"Enforcing acyclicity When the conditions are violated, e.g. by limited data, cycles can occur in the prediction. Since ENCO learns the orientations as a separate parameter, we can remove cycles by SN , with SN being the set of permutations, that maximizes ﬁnding the global order of variables O the pairwise orientation probabilities: arg maxO j=i+1 σ(θOi,Oj ). This utilizes the learned ancestor-descendant relations, making the algorithm more robust to noise in single interventions."
2107.10483,data,88,,,"Figure 5: Visualizing the gradient calculation for the incoming edges of X2 in an example graph with three variables. The intervention is being performed on X1, and the data is used to calculate the loglikelihood estimates under the three randomly sampled graphs: LC3 (X2). Those terms are assigned to the Monte-Carlo estimators for X2(X2), and LXi ﬁnally used to determine the gradients for γ and θ. The same process is performed for X3 as well."
2107.10483,data,89,,,"In this section, we show additional experiments performed as ablation studies of ENCO. First, we discuss further experiments We then discuss the effect of using our gradient estimators proposed in Section 3.4 compared to Bengio et al. (2020). Next, we show experiments on synthetic graphs with deterministic variables violating faithfulness, and experiments on continuous data with Normalizing Flows. Finally, we discuss experiments with different causal mechanism functions for generating synthetic, conditional categorical distributions besides neural networks."
2107.10483,data,92,,,"Discovering the graph G from samples of a joint distribution P is called causal structure learning or causal discovery, a fundamental problem in causality (Pearl, 2009; Peters et al., 2017). While often performed from observational data, i.e. samples from P (see Glymour et al. (2019) for an overview), we focus in this paper on algorithms that recover graphs from joint observational and interventional data. Commonly, such methods are grouped into constraint-based and score-based approaches."
2107.10483,data,94,,,"The only situation where Xi and Xj can become conditionally dependent under interventions on Xj is if Xi and Xj share a collider Xk, and Xi is being conditioned on the collider Xk and Xj. However, this requires that θki has negative gradients, i.e. θki increasing, when intervening on Xk. This cannot be the case since under interventions on Xk, Xi and Xk become conditionally independent, and the correlations learned from observational data cannot be transferred to the interventional setting. If Xk"
2107.10483,data,94,,,"Using neural networks to generate the simulated data might give SDI, DCDI and ENCO an advantage in our comparisons since they rely on similar neural networks to model the distribution. To verify that ENCO works for other simulated data similarly well, we run experiments on categorical data with other function forms for the causal mechanisms instead of neural networks. Since there is no straightforward way of deﬁning ’linear’ mechanisms for categorical data, we instead express a conditional distribution as a product of independent, single conditionals:"
2107.10483,data,99,,,"and Xi again share a collider, we can apply this argumentation recursively until a node Xn does not share a collider with Xi. The recursion will always come to an end as we have a ﬁnite set of nodes, and the causal graph is assumed to be acyclic. Xj in the causal graph, we Thus, if the conditions in Equations 23 and 24 hold for an edge Xi → can guarantee that with sufﬁcient time and data, the corresponding orientation parameter θij will converge to σ(θij) = 1."
2107.10483,data,104,,,"Figure 13: Left: Example of a latent confounder scenario, where Xl is not observed and introduces a dependency between Xi and Xj on observational data. The dots on the left and right represent eventual (observed) parents of Xi and Xj. Right: Plotting the average score lc(Xi, Xj) for confounders Xj in the true causal graph (orange) and maximum score of any other node pair (blue). Xi ← The plot shows the detection of latent confounders in ENCO is not sensitive to the speciﬁc value of τ ."
2107.10483,data,106,,,"For all pairs of variables that do not share a latent confounder, lc(Xi, Xj) converges to zero. The edges that are removed in Theorem B.4 converge to σ(γ(O) ij ) = 0 which sets lc(Xi, Xj) to zero. For edges that have been recovered, we state in Equation 24 that the gradient for interventional data must be negative for interventions on the parent. Hence, σ(γ(I) ji ) converges to one which brings lc(Xi, Xj) to zero again."
2107.10483,data,111,,,"Many scenarios of predicting false positive edges can, in theory, be solved by providing an undirected skeleton of the graph, for example, obtained from observational data. Still, one of the cornerstones of ENCO is that it does not assume faithfulness. Without faithfulness or any other assumption on the functional form of the causal mechanisms, the correct undirected graph cannot be recovered by any method. One of the future directions will be to include faithfulness in ENCO to solve the scenarios mentioned above, although this would imply that we might not be able to recover edges of deterministic variables anymore."
2107.10483,data,120,,,"Based on Equations 3 and 4, we obtain a tractable, unbiased gradient estimator by using Monte-Carlo sampling. Luckily, samples can be shared across variables, making training efﬁcient. We ﬁrst sample an intervention, a corresponding data batch, and K graphs from pγ,θ(C) (K usually between 20 and 100). We then evaluate the log likelihoods of all variables for these graphs on the batch, and estimate Xj (Xj) for all pairs of variables Xi and Xj by simply averaging the results LXi for the two cases separately. Finally, the estimates are used to determine the gradients for γ and θ."
2107.10483,data,120,,,"We propose ENCO, an efﬁcient causal structure learning method leveraging observational and interventional data. Compared to previous work, ENCO models the edge orientations as separate parameters and uses an objective unconstrained with respect to acyclicity. This allows for easier optimization and low-variance gradient estimators while having convergence guarantees. As a consequence, the algorithm can efﬁciently scale to graphs that are at least one order of magnitude larger graphs than what was possible. Experiments corroborate the capabilities of ENCO compared to the state-of-the-art on an extensive array of settings on graph sizes, sizes of observational and interventional data, latent confounding, as well as on both partial and full intervention sets."
2107.10483,data,124,,,"To guarantee that the whole gradient of θij is negative, we also need to show that for interventions on Xj, T (Xj, Xi) can only be positive. When intervening on Xj, Xi and Xj become independent Xj, ...) relying on Xj is removed in the intervened graph. A distribution p(Xi| as the edge Xi → correlations between Xi and Xj from observational data cannot achieve a better estimate than the same distribution when removing Xj. This is because the cross entropy is minimized when the sampled distribution, in this case p(Xi), is equal to the log-likelihood estimator (Cover & Thomas, 2005):"
2107.10483,data,127,,,"Limited interventional and observational data sample sizes Finally, we combine the smallest interventional and observational data sample sizes, and also include the results of the previously best baselines, SDI and DCDI, in Table 12. The results of ENCO show the combination of the previous two effects: graphs consisting of variables with small parent sets can still be recovered well by ENCO, while errors increase for the collider and full graph. Similar trends are observed for SDI, while DCDI showed a considerable decrease in performance for all graphs. In conclusion, ENCO still works well for graphs with smaller parent sets under a small observational and interventional data regime, and outperforms related baselines in this setting."
2107.10483,data,131,,,"Assumption 2 A common assumption in causal structure learning is that the data distribution over all variables p(X) is Markovian and faithful with respect to the causal graph we are trying to model. This means that the graph represents the (conditional) independence relations between variables in the data, and (conditional) independence relations in the data reﬂect the edges in the graph. For ENCO, faithfulness is not strictly required. This is because we work with interventional data. Instead, we rely on the Markov property and assume that for all variables, the parent set pa(Xi) reﬂects the inputs to the causal generation mechanism of Xi. This allows us to also handle deterministic variables."
2107.10483,data,134,,,"If they are dependent, we can follow a similar argument as in Theorem B.2. The causal mechanism Xi, ...) transfers from observational to interventional data on Xi since on interventions on p(Xj| Xi, the causal mechanism of Xj is not changed. Further, when intervening on Xj, Xi and Xj Xj, ...) cannot transfer except if Xi and Xj are become independent such that any mechanism p(Xi| independent under interventions on Xj. In this case, the edge will be again removed by the sparsity Xi cannot lead to a regularizer. This shows that for any setting, the orientation of the edge Xj → Xi will be removed as better estimate than Xi → well."
2107.10483,data,135,,,"Baselines. We compare ENCO to GIES (Hauser & Bühlmann, 2012) and IGSP (Wang et al., 2017; Yang et al., 2018) as greedy score-based approaches, and DCDI (Brouillard et al., 2020) and SDI (Ke et al., 2019) as continuous optimization methods. Further, as a common observational baseline, we apply GES (Chickering, 2002) on the observational data to obtain a graph skeleton, and orient each edge by learning the skeleton on the corresponding interventional distribution. We perform a separate hyperparameter search for all baselines, and use the same neural network setup for SDI, DCDI, and ENCO. Appendix C provides a detailed overview of the hyperparameters for all experiments."
2107.10483,data,151,,,"We make the following four contributions. Firstly, we propose ENCO, a causal structure learning method for observational and interventional data using continuous optimization. Different from recent methods, ENCO models the edge orientation as a separate parameter. Secondly, we derive unbiased, low-variance gradient estimators, which is crucial for scaling up the model to large numbers of variables. Thirdly, we show that ENCO is guaranteed to converge to the correct causal graph if interventions on all variables are available, despite not having any acyclicity constraints. Yet, we show in practice that the algorithm works on partial intervention sets as well. Fourthly, we extend ENCO to detecting latent confounders. In various experimental settings, ENCO recovers graphs accurately, making less than one error on graphs with 1,000 variables in less than nine hours of computation."
2107.10483,data,153,,,"EIXi ,X,C−ij EIXj ,X,C−ij The probability of taking an intervention on Xi is represented by p(IXi) (usually uniform across variables), and EIXi ,X,C−ij the same expectation as before under the intervention on Xi. When Xj improves the log-likelihood of Xj under intervention on Xi, then the the oriented edge Xi → Xi, the correlation ﬁrst part of the gradient increases θij. In contrast, when the true edge is Xj → between Xi and Xj learned from observational data would yield a worse likelihood estimate of Xj Xi, ...) does not on interventional data on Xi than without the edge Xj → stay invariant under intervening on Xi. The same dynamic holds for interventions on Xj. Lastly, for independent nodes, the expectation of the gradient is zero."
2107.10483,data,154,,,"where pI (I) is the distribution over which variable to intervene on (usually uniform), and ˜p ˆI (X) the joint distribution of all variables under the intervention ˆI. In other words, these two distributions represent our interventional data distribution. With pγ,θ(C), we denote the distribution over adjacency matrices C under γ, θ, where Cij ∼ LC(Xi) is the negative log-likelihood Ber(σ(γij)σ(θij)). estimate of variable Xi conditioned on the parents according to C: ,i (cid:12) − · X i). The second term of Equation 2 is an (cid:96)1-regularizer on the edge probabilities. It acts as a prior, selecting the sparsest graph of those with similar likelihood estimates by removing redundant edges."
2107.10483,data,155,,,"To converge to the mentioned values, especially of γ(O) , we need a similar condition as in Equation 7: Xj and conditioned on all the improvement on the log-likelihood estimate gained by the edge Xi → other parents of Xj needs to be larger than λsparse on interventional data excluding Xi and Xj. If this is not the case, the sparsity regularizer will instead remove the edge between Xi and Xj preventing any false predictions among observed variables. For all other pairs of variables, at least one of the terms in Equation 8 converges to zero. Thus, we can detect latent confounders by checking whether the score function lc(Xi, Xj) is greater than a threshold hyperparameter τ (0.0, 1.0). We discuss possible guarantees in Appendix B, and experimentally verify this approach in Section 4.5."
2107.10483,data,161,,,"Learning the structure of a causal graphical model using both observational and interventional data is a fundamental problem in many scientiﬁc ﬁelds. A promising direction is continuous optimization for score-based methods, which, however, require constrained optimization to enforce acyclicity or lack convergence guarantees. In this paper, we present ENCO, an efﬁcient structure learning method for directed, acyclic causal graphs leveraging observational and interventional data. ENCO formulates the graph search as an optimization of independent edge likelihoods, with the edge orientation being modeled as a separate parameter. Consequently, we provide for ENCO convergence guarantees when interventions on all variables are available, without having to constrain the score function with respect to acyclicity. In experiments, we show that ENCO can efﬁciently recover graphs with hundreds of nodes, an order of magnitude larger than what was previously possible, while handling deterministic variables and discovering latent confounders."
2107.10483,data,169,,,"Score-based methods, on the other hand, search through the space of all possible causal structures with the goal of optimizing a speciﬁed metric (Tsamardinos et al., 2006; Ke et al., 2019; Goudet et al., 2017; Zhu et al., 2020). This metric, also referred to as score function, is usually a combination of how well the structure ﬁts the data, for instance in terms of log-likelihood, as well as regularizers for encouraging sparsity. Since the search space of DAGs is super-exponential in the number of nodes, many methods rely on a greedy search, yet returning graphs in the true equivalence class (Meek, 1997; Hauser & Bühlmann, 2012; Wang et al., 2017; Yang et al., 2018). For instance, GIES (Hauser & Bühlmann, 2012) repeatedly adds, removes, and ﬂips the directions of edges in a proposal graph"
2107.10483,data,169,,,"would expect the collider and full graph suffer the most from having less observational data, and this is indeed the case as shown by the results in Figure 15. The results of all other graphs are less affected, although interestingly, some become even better with less observational data. For the chain ...XN , for instance, we observed that the learned conditional distributions picked X1 → up spurious correlations among variables, e.g., between X1 and X3 when modeling p(X3| X1, X2) which are, in the data limit, independent given X2. Since those correlations do not necessarily transfer to the interventional setting, it is easier to spot false positive edges, and we can obtain even better results than for the larger sample sizes. In conclusion, having sufﬁcient observational data is crucial in ENCO for graphs with variables that have larger parent sets, while being less important for sparser graphs."
2107.10483,data,175,,,"For small cycles it is easy to do this exhaustively by checking all permutations. For larger cycles, we apply a simple greedy search that works just as well. Once the order ˆO has been found, we remove all edges Xi → The intuition behind this heuristic is the following. Cycles are often caused by a single orientation pair being incorrect due to noise in the interventional data. For example, in a chain X1 → X2 → X5, it can happen that the orientation parameter θ14 is incorrectly learned as orientation X3 → X1 if the interventional data on X4 does not show the of the edge between X1 and X4 as X4 → independence of X1 and X4. However, most other orientation parameters, e.g. θ12, θ13, θ24, θ34, etc., have been likely learned correctly. Thus, it is easy to spot that θ14 is an outlier, and this is what the simple heuristic above implements."
2107.10483,data,177,,,"ENCO learns a causal graph from observational and interventional data by modelling a probability for every possible directed edge between pairs of variables. The goal is that the probabilities corresponding to the edges of the ground truth graph converge to one, while the probabilities of all other edges converge to zero. For this to happen, we exploit the idea of independent causal mechanisms (Pearl, 2009; Peters et al., 2016), according to which the conditional distributions for all variables in the ground-truth CGM stay invariant under an intervention, except for the intervened ones. By contrast, for graphs modelling the same joint distribution but with a ﬂipped or additional edge, this does not hold (Peters et al., 2016). In short, we search for the graph which generalizes best from observational to interventional data. To implement the optimization, we alternate between two learning stages, that is distribution ﬁtting and graph ﬁtting, visually summarized in Figure 1."
2107.10483,data,177,,,"− The two variables (cid:15)1, (cid:15)2 represent small constants close to zero. In this case, the graph can violate the condition in Equation 23 since intervening on X2 breaks the dependency between X1 and X2. The X2) learned from observational data relies on the dependency between conditional distribution p(X3| X3 is X1 and X2 which can make it to a worse estimate than p(X3). Note that if the edge X1 → learned by ENCO though, this will not constitute a problem anymore since with conditioning on X1, X3 will gain a gradient towards the correct graph. Thus, when γ i.e. p(X3| and θ are not initialized with the worst-case values, the graph with both X1 and X2 as parents of X3 can be sampled and provides gradients in the correct direction. Further, we did not observe any of these situations in the synthetic and real-world graphs we experimented on."
2107.10483,data,188,,,"Summary: in conclusion, for the discussed example, we can guarantee that ENCO converges to the correct causal graph if λsparse < 0.02. To experimentally verify this results, we applied ENCO on this graph with two hyperparameter settings for the sparsity regularizer: λsparse = 0.019 and λsparse = 0.021. We considered a very large sample size, more speciﬁcally 10k per intervention and 100k observational samples, to simulate the data limit regime. For λsparse = 0.019, ENCO was able X2 was, as expected, to recover the graph without errors while for λsparse = 0.021, the edge X1 → missed. This veriﬁes the theoretical result above with respect to λsparse. Note that if the condition is not fulﬁlled by selecting a too large sparsity regularizer, this does not necessarily mean that ENCO will not be able to recover the graph. This is because we consider the ’worst-case’ parent set in condition 3, while this case might not be in the true causal graph to which the other edges converge."
2107.10483,data,198,,,"Large sample size First, we use very large sample sizes to ﬁnd the upper bound performance level that we can expect from each method. For this, we sample 100k observational samples per graph, and 10k samples per intervention. We observed that this is sufﬁcient to model most conditional probabilities up to a negligible error. The results are shown in Table 11. We ﬁnd that, in line with the theoretical guarantees, ENCO can reliably recover most graphs, only making 0.3 mistakes on average on the full graph. Of the baselines, only DCDI is able to recover the collider graph without errors since its edges can be independently orientated. For all other graphs, DCDI converges to acyclic graphs, but incorrectly orients some edges and predicts false positive edges, while being 8 times slower than ENCO on the same hardware. All other baselines show improved SHD scores than in Table 1 as well, but are not able to match ENCO’s performance. This shows that, even in the data limit, ENCO achieves notably better results than concurrent methods."
2107.10483,data,226,,,"Below we give an example of a distribution which is not faithful with respect to its graph structure, X3. For but can yet be found by ENCO. Suppose we have a chain of three variables, X1 → simplicity, we assume here that all the variables are binary, but the argument can similarly hold for any categorical data. The distribution p(X1) is an arbitrary function with 0 < p(X1 = 1) < 1, and the other two conditionals are deterministic functions: p(X2| X2) = δ[X3 = X2]. This joint distribution is not faithful to the graph, since X3 is independent of X2 given X1, which is not implied by the graph. We will now focus our discussion on the edge X1 → X3 to show that, despite the independence, the proposed method ENCO can identify the true parent set of X3. The ﬁrst step of ENCO is to ﬁt the neural networks to the observational distributions, X1, X2). Now, the update of the edge parameter which include p(X3), p(X3| γ13 under interventions on X2 can be summarized as follows, where we marginalize out over graph samples:"
2107.10483,data,236,,,"Uncovering and understanding causal mechanisms is an important problem not only in machine learning (Schölkopf et al., 2021; Pearl, 2009) but also in various scientiﬁc disciplines such as computational biology (Friedman et al., 2000; Sachs et al., 2005), epidemiology (Robins et al., 2000; Vandenbroucke et al., 2016), and economics (Pearl, 2009; Hicks et al., 1980). A common task of interest is causal structure learning (Pearl, 2009; Peters et al., 2017), which aims at learning a directed acyclic graph (DAG) in which edges represent causal relations between variables. While observational data alone is in general not sufﬁcient to identify the DAG (Yang et al., 2018; Hauser & Bühlmann, 2012), interventional data can improve identiﬁability up to ﬁnding the exact graph (Eberhardt et al., 2005; Eberhardt, 2008). Unfortunately, the solution space of DAGs grows super-exponentially with the variable count, requiring efﬁcient methods for large graphs. Current methods are typically applied to a few dozens of variables and cannot scale so well, which is imperative for modern applications like learning causal relations with gene editing interventions (Dixit et al., 2016; Macosko et al., 2015)."
2107.10483,data,248,,,"When taking the next step to having M interventions provided, ENCO can create more incorrect predictions. For the variables for which interventions are provided, we can use the same convergence guarantees (Theorem B.1-B.4) since all conditions are independent across variables. For variables without interventions, we cannot rely on those. While we have observed that learning the missing θ’s from other interventions give reasonable results, we see a degradation of performance the further the distance is between a node and the closest intervened variable. As an example, suppose we have a X5, and we are provided with an intervention chain with 5 variables, i.e. X1 → X4 → X2 → on X1 only. This allows us to learn the orientation between X1 and X2. The orientation between X2 and X3 is often learned correctly as well because adding the edge X2 → X2 gives a greater decrease in overall log-likelihood, since part of the information from X3 to predict X2 is already included in X1. However, the further we go away from X1, the less information is shared between the child and the intervened variable. Moreover, the likelihood of a mistake occurring due to X5 is not always learned limited data further increases. This is why the orientation of the edge X4 → correctly, which can also cause false positive edges."
2107.10483,data,273,,,"γ-freezing stage For ENCO, one challenge of large graphs is that the orientation parameters θ are updated very sparsely. The gradients for θij require data from an intervention on one of its adjacent nodes Xi or Xj, which we evaluate less frequently with increasing N as we iterate over interventions on all N nodes. Hence, we require more iterations/epochs just for training the orientation parameters while wasting a lot of computational resources. To accelerate training of large graphs, we freeze γ in every second graph ﬁtting stage. Updating only θ allows us to use the same graph sample C ij Xj (Xj) since the log-likelihood estimate of Xj only needs to be for both evaluated for θij. With this gradient estimator, we experience that as little as 4 graph samples are sufﬁcient to obtain a reasonable gradient variance. Hence, it is possible to perform more gradient updates of θ in the same computation time. Note that this is estimator not efﬁcient when training γ as we require different C ij samples for every i. In experiments, we alternate the standard graph ﬁtting step with this pure θ-training stage. We want to emphasize that this approach can also be used for small graphs obtaining similar results as in Table 1. However, it is less needed because the orientation parameters are more frequently updated in the ﬁrst place. Such an approach is not possible for the baselines, SDI and DCDI, because they do not model the orientation as a separate variable."
2107.10483,data,277,,,"Causal structure learning algorithms such as the proposed method are mainly used to uncover and understand causal mechanisms from data. The knowledge of the underlying causal mechanisms can then be applied to decide on speciﬁc actions that inﬂuence variables or factors in a desired way. For instance, by knowing that the environmental pollution in a city has an impact on the risk of cancer of its residents, one can try to reduce the pollution to decrease the risk of cancer. The applications of causal structure learning are ranging across many scientiﬁc disciplines, including computational biology (Friedman et al., 2000; Sachs et al., 2005; Opgen-Rhein & Strimmer, 2007), epidemiology (Robins et al., 2000; Vandenbroucke et al., 2016), and economics (Pearl, 2009; Hicks et al., 1980). We envision that our work can have positive impacts on those ﬁelds. One example we want to highlight is the ﬁeld of genomics. Recent advances have enabled to perform gene knockdown experiments in a large scale, providing large amounts of interventional data (Dixit et al., 2016; Macosko et al., 2015). Gaining insights into how speciﬁc genes and diseases interact can lead to the development of novel pharmaceutic methods for treating current diseases. Since the number of variables in those experiments is tremendous, efﬁcient causal structure learning algorithms are needed. The proposed method constitutes a ﬁrst step towards this goal, and our work can foster future work for creating algorithms scaling beyond 10,000 variables."
2107.10483,data,320,,,"For categorical random variables Xi, we apply a softmax output where Mj ∼ activation for fφi, and for continuous ones, we use Normalizing Flows (Rezende & Mohamed, 2015). Graph ﬁtting uses the learned networks to score and compare different graphs on interventional data. N represents For parameterizing the edge probabilities, we use two sets of parameters: γ N the orientation of the edges. The likelihood of an the existence of edges in a graph, and θ σ(θij), with σ(...) being the sigmoid function and edge is determined by p(Xi → θji. The probability of the two orientations always sum to one. The beneﬁt of separating θij = the edge probabilities into two independent parameters γ and θ is that it gives us more control over the gradient updates. The existence of an (undirected) edge can usually be already learned from observational or arbitrary interventional data alone, excluding deterministic variables (Pearl, 2009). In contrast, the orientation can only be reliably detected from data for which an intervention is performed on its adjacent nodes, i.e., Xi or Xj for learning θij. While other interventions eventually provide information on the edge direction, e.g., intervening on a node Xk which is a child of Xi and a parent of Xj, we do not know the relation of Xk to Xi and Xj at this stage, as we are in the process of learning the structure. Despite having just one variable for the orientation, γij and γji are learned as two separate parameters. One reason is that on interventional data, an edge can improve the log-likelihood estimate in one direction, but not necessarily the other, leading to conﬂicting gradients."
2107.10483,data,324,,,"Thus, independent of what function the network has learned for p(X3| X1, X2), the difference above can be only greater X1, X2), there are value combinations that have never been or equals to zero. Note that for p(X3| = X2, such that in practice we can’t guarantee a speciﬁc distribution to be learned observed, i.e. X1 (cid:54) for such conditionals. Still, this does not constitute a problem for ﬁnding the graph as shown above. The second difference, EX [ X1) + log p(X3)], can be similarly reasoned. Since X1 is X1) cannot lead to a better estimator than p(X3), independent of X3 under interventions on X2, p(X3| X1) was trained on observational even when trained on the interventional data. However, since p(X3| data, where there exist a strong correlation between X1 and X3, this estimator must be strictly worse than p(X3). Hence, this difference will be strictly positive. To summarize, under interventions on X2, the edge X1 → X3 will be trained towards decreasing its probability. Further, under interventions on X1, the effect of X1 on X3 can be fully expressed by X3 goes conditioning on X2, making this gradient going to zero when the edge probability X2 → towards one. For the edge X2 → X3 itself, the same reasoning as here can be followed such that X3 is included in the graph or not, conditioning X3 on X2 independent of whether the edge X1 → can only lead to an improvement in its estimator. Therefore, ENCO is able to ﬁnd the correct graph despite it not being faithful."
2107.10483,data,324,,,"edge X1 → zero (note that positive gradients lead to a decrease since we minimize the objective). In the ﬁrst difference, EX [ X2) is the X2)], we know that p(X3| X1, X2) + log p(X3| optimal estimator, since the ground truth data is generated via this conditional. Thus, independent of what function the network has learned for p(X3| X1, X2), the difference above can be only greater X1, X2), there are value combinations that have never been or equals to zero. Note that for p(X3| = X2, such that in practice we can’t guarantee a speciﬁc distribution to be learned observed, i.e. X1 (cid:54) for such conditionals. Still, this does not constitute a problem for ﬁnding the graph as shown above. The second difference, EX [ X1) + log p(X3)], can be similarly reasoned. Since X1 is X1) cannot lead to a better estimator than p(X3), independent of X3 under interventions on X2, p(X3| X1) was trained on observational even when trained on the interventional data. However, since p(X3| data, where there exist a strong correlation between X1 and X3, this estimator must be strictly worse than p(X3). Hence, this difference will be strictly positive. To summarize, under interventions on X2, the edge X1 → X3 will be trained towards decreasing its probability. Further, under interventions on X1, the effect of X1 on X3 can be fully expressed by X3 goes conditioning on X2, making this gradient going to zero when the edge probability X2 → towards one."
2107.10483,data,325,,,"X i. For simplicity, we want this neural network to model the conditional of the variable Xi with respect to any possible set of parent variables. We, therefore, apply a dropout-like scheme to the input to simulate different sets of parents, similar as (Ke et al., 2019; Ivanov et al., 2019; Li et al., 2020; Brouillard et al., 2020). In that case, during training, we randomly set an input variable Xj to zero based on the probability of its corresponding edge Xj → min i (cid:12) φi Xi)). For categorical random variables Xi, we apply a softmax output where Mj ∼ activation for fφi, and for continuous ones, we use Normalizing Flows (Rezende & Mohamed, 2015). Graph ﬁtting uses the learned networks to score and compare different graphs on interventional data. N represents For parameterizing the edge probabilities, we use two sets of parameters: γ N the orientation of the edges. The likelihood of an the existence of edges in a graph, and θ σ(θij), with σ(...) being the sigmoid function and edge is determined by p(Xi → θji. The probability of the two orientations always sum to one. The beneﬁt of separating θij = the edge probabilities into two independent parameters γ and θ is that it gives us more control over the gradient updates. The existence of an (undirected) edge can usually be already learned from observational or arbitrary interventional data alone, excluding deterministic variables (Pearl, 2009). In contrast, the orientation can only be reliably detected from data for which an intervention is performed on its adjacent nodes, i.e., Xi or Xj for learning θij."
2107.10483,data,331,,,"Next, we consider interventions on Xj. If under the sampled adjacency matrix Xi is conditionally independent of Xj, the difference in the log-likelihood estimates T (Xj, Xi) is zero. The variables can be independent if Xi is conditioned on variables that d-separate Xi and Xj in the true causal graph. For instance, having the children of Xi as parents of Xi creates this scenario. However, for this scenario to take place, one or more orientation parameters of parent-child or ancestor-descendant pairs must be incorrectly converged. In case of a parent-child pair Xi, Xk, Theorem B.1 shows that σ(θik) will converge to one removing any possibility of a reversed edge to be sampled. In case of an ancestor-descendant pair Xi, Xl, we can apply a recursive argument: as Xl d-separates Xi and Xj, Xl must come before Xj in the causal order. If for the gradient θil, we have a similar scenario with Xi being conditionally independent of Xj, the same argument applies. This can be recursively applied until no more variables except direct children of Xi can d-separate Xi and Xj. In that case, σ(θik) will converge to one, which leads to all other orientation parameters to converge to one as well. If Xi is not conditionally independent of Xj, we can rely back on the argumentation of Theorem B.1 Xj: as in the intervened causal graph, Xi and Xj are independent, any when we have an edge Xi → correlation learned from observational data can only lead to a worse log-likelihood estimate. In cases of colliders, we can rely on the recursive argument from before. Thus, under interventions on Xj, the gradient of θij must be smaller or equals to zero in expectation, i.e.increases θij."
2107.10483,"data available, data",71,,,"Figure 1: Visualization of the two training stages of ENCO, distribution ﬁtting and graph ﬁtting, on an example graph with 3 variables (X1, X2, X3). The graph on the right further shows how the parameters γ and θ correspond to edge probabilities. We learn those parameters by comparing multiple graph samples on how well they generalize from observational to interventional data."
2107.10483,"data available, data",92,,,"Enforcing acyclicity ENCO is guaranteed to converge to acyclic graphs in the data limit; arguably, an assumption that does not always hold. In the presence of cycles, which can occur especially when low data is available, a simple heuristic is to keep the graph, which maximizes the orientation probabilities. Speciﬁcally, we aim to ﬁnd the order O SN , where SN represents the set of all permutations from 1 to the number of variables N , for which we maximize the following objective:"
2107.10483,"data available, data, dataset",188,,,"The results on continuous data are shown in Table 16. Since GES assumes linear mechanisms and gaussianity of the data, it is unsurprising that it performs better on the linear Gaussian dataset than on the non-linear datasets. However, on all the three datasets, it constitutes the lowest performance compared to the other methods, including ENCO. This highlights the beneﬁts of incorporating interventional data in the learning of the skeleton and graph structure. To gain further insights in comparison to the constraint-based baseline, we repeat the experiments with smaller sample sizes. The original dataset has 909 samples for observational data and per intervention, and we sub-sample 500 and 100 of those respectively for simulating smaller dataset sizes. The results of those experiments can be found in Table 17. It is apparent that the results of GES on the linear dataset get considerably worse with fewer data samples being available, while ENCO-G is able to reconstruct most graphs still without errors. Especially for the small dataset of 100 samples, we noticed that the skeletons"
2107.10483,"data, code",106,,,"Since the possible applications are fairly wide-ranging, there might be potential impacts we cannot forecast at the current time. This includes misuses of the method for unethical purposes. For instance, the method can be used to justify gender and race as causes for irrelevant variables if the output is misinterpreted, initial assumptions of the model are ignored, or the input data has been manipulated. Hence, the obligation to use this method in a correct way within ethical boundaries lies on the user. We emphasize this responsibility of the user in the license of our code."
2107.10483,"data, code, package, python, dataset",277,,,"Baseline implementation We used existing implementations to run the baselines GIES (Hauser & Bühlmann, 2012), IGSP (Wang et al., 2017), GES (Chickering, 2002) and DCDI (Brouillard et al., 2020). For GIES, we used the implementation from the R package pcalg2. To run categorical data, we used the GaussL0penIntScore score function. For IGSP, we used the implementation of the python package causaldag3. As IGSP uses conditional independence tests in its score function, we cast the categorical data into continuous space ﬁrst and experiment with different kernel-based independence tests. Due to its long runtime for large dataset sizes, we limit the interventional and observational data set size to 25k. Larger dataset sizes did not show any signiﬁcant improvements. For details on the observational GES experiments, see Section D.6. Finally, we have used the original python code for DCDI published by the authors4. We have added the same neural networks used by ENCO into the framework to perform structure learning on categorical data. Bugs in the original code were corrected to the best of our knowledge. Since SDI (Ke et al., 2019) has a similar learning structure as ENCO, we have implemented it in the same code base as ENCO. This allows us to compare the learning algorithms under exact same perquisites. Further, all methods with neural networks used the deep learning framework PyTorch (Paszke et al., 2019) which ensures a fair run time comparison across methods."
2107.10483,"data, dataset",51,,,"Assumption 1 We are given a dataset of observational data from the joint distribution p(X). Additionally, we have N interventional datasets for N variables where in each intervention a different node is intervened on (the intervention size for each dataset is therefore 1)."
2107.10483,"data, dataset",52,,,"found by GES on observational data already contained couple of mistakes. This shows that for small datasets, observational data alone might not be sufﬁcient to ﬁnd the correct skeleton while by jointly learning from observational and interventional data, we can yet ﬁnd the graph up to minor errors."
2107.10483,"data, dataset",61,,,"Assumption 5 We are given a sufﬁciently large interventional dataset such that sampling data points from it models the exact interventional distribution under the true causal graph. This can be achieved by, for example, sampling directly from the causal graph, or having an inﬁnitely large dataset. For the limited data setting, see Appendix B.2.3."
2107.10483,"data, dataset",62,,,"Table 17: Experiments on graph with continuous data from Brouillard et al. (2020) with smaller sample sizes for both observational and interventional datasets (in brackets). ENCO shows to perform much better in smaller sample sizes than a skeleton+orientation method, underlining the beneﬁt of learning the whole graph from observational and interventional data jointly."
2107.10483,"data, dataset",66,,,"Assumption (3) and (4) are taken with respect to the data limit such that the conditions derived in the next section solely depend on the given causal graphical model. However, in practice, we often have a limited data set. The proof presented for the data limit is straightforward to extend to this setting with the following modiﬁcation:"
2107.10483,"data, dataset",71,,,"Datasets We perform the experiments of interventions on fewer variables on the same graphs and datasets as used for the initial synthetic graphs (see Section C.1). To simulate having interventions on fewer variables, we randomly sample a subset of variables for which we include the interventional data, and remove for all others. The sampled variables are the same for both ENCO and DCDI, and"
2107.10483,"data, dataset",85,,,"Graphs and datasets. Given a ground-truth causal graphical model, all methods are tasked to recover the original DAG from a set of observational and interventional data points for each variable. In case of synthetic graphs, we follow the setup of Ke et al. (2019) and create the conditional distributions from neural networks. These networks take as input the categorical values of its variable’s parents, and are initialized orthogonally to output a non-trivial distribution."
2107.10483,"data, dataset",118,,,"We perform experiments on the same datasets as in Section 4.2, but provide interventional data only for a randomly sampled subset of the 25 variables of each graph. We compare ENCO to DCDI, which supports partial intervention sets, and plot the SHD over the number of intervened variables in Figure 4. Despite ENCO’s guarantees only holding for full interventions, it is still competitive and outperforms DCDI in most settings. Importantly, enforcing acyclicity has an even greater impact on fewer interventions as more orientations are trained on non-adjacent interventions (see Appendix B.4 for detailed discussion). We conclude that ENCO works competitively with partial interventions too."
2107.10483,"data, dataset",126,,,"To show the beneﬁt of learning a graph from observational and interventional data jointly, we compare ENCO to a simple observational baseline. This baseline ﬁrst learns the skeleton of the graph by applying greedy equivalence search (GES) (Chickering, 2002) on the observational data. Then, for each interventional dataset, we apply GES as well and use those skeletons to orientate the edges of Y is the original one. This can be done by checking for each undirected edge X − in the skeleton of interventions on X or not. As a reference implementation of GES, we have used the one provided in the Causal Discovery Toolbox (Kalainathan et al., 2020)."
2107.10483,"data, dataset",138,,,"average, every node has 8 in- and outgoing edges. We limit the number of parents to 10 per node since, otherwise, we cannot guarantee that the randomly sampled distributions take all parents faithfully into account. This is also in line with the real-world inspired graphs of the BnLearn repository, which have a maximum of 6 parents. To give an intuition on the complexity of such graphs, we show an example graph of 100 nodes in Figure 12a. Accordingly to the number of variables, we have increased the data set size to 4096 samples per intervention, and 100k observational samples. We did not apply the order heuristic on the predictions, since ENCO was able to recover acyclic graphs by itself with the given data."
2107.10483,"data, dataset",158,,,"To test the detection of latent confounders, we create a set of 25 random graphs with 5 additional latent confounders. The dataset is generated in the same way as before, except that we remove the latent variable from the input data and increase the observational and interventional sample size (see Appendix C.3 for ablation studies). After training, we predict the existence of a latent confounder on any pair of variables Xi and Xj if lc(Xi, Xj) is greater than τ . We choose τ = 0.4 but verify in Appendix C.3 that the method is not sensitive to the speciﬁc value of τ . As shown in Table 2, ENCO detects more than 95% of the latent confounders without any false positives. What is more, the few mistakes do not affect the detection of all other edges, which are recovered perfectly."
2107.10483,"data, dataset",161,,,"Limited interventional data sample sizes We repeat the experiments of Table 1 for ENCO while limiting the sample size per intervention to 20, 50, and 100 (200 before). The observational dataset size of 5000 samples is thereby kept constant. We plot the performance for all graph structures in Figure 14. Overall, the decrease of performance with lower interventional sample size is consistent across graph structures. With only 20 samples per intervention, it becomes especially hard to reason about variables with many parents, since the variable’s distribution is determined by many other parents as well. Yet, for four out of the six graphs, we obtain an SHD of less than 1 with 100 interventional samples, and less than 6 when only 20 samples are available. In conclusion, ENCO works well with little interventional data if most variables have a small parent set."
2107.10483,"data, dataset",169,,,"In order to train this objective on a dataset of interventional data, we can use Monte-Carlo sampling to obtain an unbiased gradient estimator. Note that the adjacency matrix samples to estimate Xj (Xj) are not required to be the same. For efﬁciency, we instead sample LXi K adjacency matrices from pγ,θ(C), evaluate the likelihood of a batch X under all these graphs. Afterwards, we assign the evaluated samples to one of the two cases, depending on Cij being zero or one. This way, we can reuse the same graph samples for all edge parameters γ. We visualize the gradient calculation in Figure 5. In the cases where we perform an intervention on Xi, we do not optimize γij for this step and set the gradients to zero. The same holds for gradient steps where we do not have any samples for one of the two log-likelihood estimates."
2107.10483,"data, dataset",212,,,"We verify that ENCO works just as well with continuous data by performing the experiments on datasets from Brouillard et al. (2020) that contained interventions on all variables. In these datasets, the graphs consist of 10 variables with an average of one edge per variable, and deploy three different causal mechanisms: linear, nonlinear additive noise models, and nonlinear models with non-additive noise using neural networks. The datasets contain 909 observational samples and 909 samples per intervention. All results of GIES, IGSP, and DCDI have been taken from Brouillard et al. (2020) (Appendix C.7, Table 22-24). We follow the setup of Brouillard et al. (2020) and compare two different neural network setups. First, we use MLPs that model a Gaussian density by predicting a mean and variance variable (denoted by sufﬁx G). The second setup uses normalizing ﬂows, more speciﬁcally a two-layer deep sigmoidal ﬂow (Huang et al., 2018), which is ﬂexible enough to model more complex distributions (denoted by sufﬁx DSF). The rest of the experimental setup in ENCO is identical to the categorical case."
2107.10483,"data, dataset",249,,,"Graph generation The graphs used for testing the latent confounding strategy are based on the random graphs from Section 4.2. We use graphs of 25 nodes, and add 5 extra nodes that represent latent confounders. Each latent confounder Xl is connected to two randomly sampled nodes Xi, Xj that do not have a direct connection. However, Xi and Xj can be an ancestor-descendant pair and have any other (shared) parent set (see Figure 13a). In the adjacency matrix, we add the edges Xj, and perform the data generation as for the previous graphs. After data Xl → generation, we remove the 5 latent confounders from both observational and interventional data. The task is to learn the graph structure of the remaining 25 observable variables, as well as detecting whether there exists a latent confounder between any pair of variables. We use the same setup in terms of dataset size as before for the observational samples, namely 5k, but increased the samples per intervention to 512. Little interventional data showed to cause a high variance in the interventional gradients, γ(I) ij , which is why more false positives occured. The results in for the limited data with 200 interventions, and results in the data limit, i.e. for 10k interventional samples and 100k observational samples, are shown in Table 8."
2107.10483,"data, dataset provided",65,,,"As mentioned in the text, condition 1 and 2 of Theorem 3.1 ensure that the orientation probabilities cannot converge to any local optima. Since the conditions explicitly involve the data distributions and implicitly the gradient estimators, we provide below an assumption from a data generation mechanism perspective as an alternative, that ensures condition 1 and 2 to be satisﬁed."
2107.10483,"data, dataset provided",85,,,"ENCO has been designed under the assumption that interventional data is provided. When we have interventional data on only a very small subset of variables, we might not optimally use the information that is provided by the observational data. To overcome this issue, we can run a causal discovery method that solely work on observational data and return an undirected graph. This skeleton can be used as a prior, and prevents false positive edges between conditionally independent variables."
2107.10483,"data, dataset provided",94,,,"The following section gives an overview and proves the conditions under which ENCO converges to the correct causal graph given sufﬁcient time and data. We emphasize that we provide conditions here for which no local optima exist, meaning that if ENCO converges, it returns the correct causal graph. This is a stronger statement than showing that the global optimum corresponds to the true graph, since a gradient-based algorithm can get stuck in a local optimum. We will discuss the conditions for the global optimum in Appendix B.2.5."
2107.10483,"data, dataset provided",132,,,"To construct a theoretical argument, we make the following assumptions. First, we assume that sparse interventions have been performed on all variables. Later, we show how to extend the algorithm to avoid this strong assumption. Further, given a CGM, we assume that its joint distribution p(X) is . In other words, the parent set pa(Xi) reﬂects the inputs Markovian with respect to the true graph to the causal generation mechanism of Xi. We assume that there exist no latent confounders in . G Also, we assume the neural networks in ENCO are sufﬁciently large and sufﬁcient observational data is provided to model the conditional distributions of the CGM up to an arbitrary small error."
2107.10483,"data, dataset provided",135,,,"Assumption 4 ENCO relies on neural networks to determine the conditional data distributions ...). Hence, for providing a guarantee, we assume that in the graph learning step the neural p(Xi| networks have been sufﬁciently trained such that they accurately model all possible conditional ...). In practice, the neural networks might have a slight error. However, as long as distribution p(Xi| enough data, network complexity, and training time is provided, it is fair to assume that the difference between the modeled distribution and the true conditional is smaller than an arbitrary constant (cid:15), based on the universal approximation theorem (Hornik et al., 1989). For the limited data setting, see Appendix B.2.3."
2107.10483,"data, dataset provided",145,,,"The condition in Equation 28 introduces a dependency between convergence guarantees and the regularizer parameter λsparse. The lower we set the regularization weight λsparse, the more edges we can guarantee to recover. If the regularization weight is set too high, we can eventually obtain false negative edge predictions. If the regularization weight is set very low, we take a longer time to converge as it requires lower gradient variance or more update steps, and is more sensitive in a limited data regime. Nonetheless, if sufﬁcient computational resources and data is provided, any value of λsparse > 0 can be used. Theorem B.4. Assume for all edges Xi → converged to one. Then, the likelihood of all other edges, i.e.σ(θlk) under the condition that λsparse > 0."
2107.10483,"data, used dataset, dataset",275,,,"Datasets We perform experiments on a collection of causal graphs from the Bayesian Network Repository (BnLearn) (Scutari, 2010). The repository contains graphs inspired by real-world applications that are used as benchmarks in literature. We chose the graphs to reﬂect a variety of sizes and different challenges (rare events, deterministic variables, etc.). The chosen graphs are cancer (Korb & Nicholson, 2010), earthquake (Korb & Nicholson, 2010), asia (Lauritzen & Spiegelhalter, 1988), sachs (Sachs et al., 2005), child (Spiegelhalter & Cowell, 1992), alarm (Beinlich et al., 1989), diabetes (Andreassen et al., 1991), and pigs (Scutari, 2010). The graphs have been downloaded from the BnLearn website5. For the small graphs, we have used a dataset size of 50k observational samples and 512 samples per intervention. This is a larger dataset size than for the synthetic graph because many edges in the real-world graphs have very small causal effects that cannot be recovered from limited data, and the goal of the experiment was to show that the convergence conditions also hold on real-world graphs. Hence, we need more observational and interventional samples. The results with a smaller dataset size, i.e. 5k observational and 200 interventional samples as before, are shown in Table 9. For the large graphs, we follow the dataset size for the scalability experiments (see Section C.2)."
2107.10483,dataset,1,,,Dataset
2107.10483,dataset,2,,,C.1.1 DATASETS
2107.10483,dataset,12,,,; observational dataset Dobs; interventional datasets Dint( ˆI) for
2107.10483,dataset,16,,,Table 4: Hyperparameter overview for the simulated graphs dataset experiments presented in Table 1.
2107.10483,dataset,23,,,"differ across graphs. The dataset size is the same as before, namely 200 samples per intervention and 5k observational datasets."
2107.10483,dataset,32,,,"If the conditions discussed above hold with respect to the given observational and interventional dataset, we can guarantee that ENCO will converge to the true causal graph given sufﬁcient time."
2107.10483,dataset,72,,,"Proof. To show this statement, we need to consider different independence relations between Xi and Xj. First, if Xi and Xj are independent in the observational dataset given any conditional set, the edge will be removed since any edge between two independent variables is removed for any λsparse > 0. The same holds if Xi and Xj are independent for interventions on Xi and Xj."
2107.10483,dataset,128,,,"Interventions on fewer variables. It is straightforward to extend ENCO to support interventions on fewer variables. Normally, in the graph ﬁtting stage, we sample one intervention at a time. We can, thus, simply restrict the sampling only to the interventions that are possible (or provided in the dataset). In this case, we update the orientation parameters θij of only those edges that connect to an intervened variable, either Xi or Xj, as before. For all other orientation parameters, we extend the gradient estimator to include interventions on all variables. Although this estimate is more noisy and does not have convergence guarantees, it can still be informative about the edge orientations."
2107.10483,dataset,174,,,"Condition 1 and 2 ensure that the orientations can be learned from interventions. Intuitively, ancestors and descendants in the graph have to be dependent when intervening on the ancestors. This aligns with the technical interpretation in Theorem 3.1 that the likelihood estimate of the child variable must improve when intervening and conditioning on its ancestor variables. Condition 3 states intuitively that the sparsity regularizer needs to be selected such that it chooses the sparsest graph among those graphs with equal joint distributions as the ground truth graph, without trading sparsity for worse distribution estimates. The speciﬁc condition in Theorem 3.1 ensures thereby that the set can be learned with a gradient-based algorithm. We emphasize that this condition only gives an upper bound for λsparse when sufﬁciently large datasets are available. In practice, the graph can thus be recovered with a sufﬁciently small sparsity regularizer and dependencies among variables under interventions. We provide more details for various settings and further intuition in Appendix B."
2107.10483,dataset,177,,,"ENCO for partial intervention sets While the theoretical guarantees for convergence to an acyclic graph apply when interventions on all variables are possible, it is straightforward to extend the ENCO algorithm to support partial interventions as well. Normally, in the graph ﬁtting stage, we sample one intervention at a time. We can, thus, simply restrict the sampling only to the interventions that are possible (or provided in the dataset). In this case, we update the orientation parameters θij of only those edges that connect to an intervened variable, either Xi or Xj, as before. All other orientation parameters would remain unchanged throughout the training, since their gradients rely on interventions missing from the dataset. Instead, we extend the gradient estimator in Equation 4 to not be exclusive to adjacent interventions, but include interventions on all variables. Speciﬁcally, for the orientation parameter θij without any interventions on Xi or Xj, we use the following gradient estimator:"
2107.10483,dataset,188,,,"Results The results including standard deviations can be found in Table 9. The low standard deviation for ENCO shows that the approach is stable across seeds, even for large graphs. SDI has a zero standard deviation for a few graphs. In those cases, SDI converged to the same graph across seeds, but not necessarily the correct graph. We have also applied DCDI (Brouillard et al., 2020) to the real-world datasets and report the results in Table 9 and 10. DCDI performs relatively similar to SDI, making a few more mistakes on the very small graphs (< 10 nodes) while being slightly better on sachs and child. Nonetheless, ENCO outperforms DCDI on all graphs. We do not report results of DCDI on the largest graphs, diabetes and pigs, because it ran out of memory for diabetes (larger number of max. categories per variable) and did not converge within the same time limitations as SDI and ENCO (see Section 4.3 for a comparison on scalability)."
2107.10483,dataset,188,,,"We consider the task of ﬁnding a directed acyclic graph G = (V, E) with N variables of an unknown CGM given observational and interventional samples. Firstly, we assume that: (1) The CGM is causally sufﬁcient, i.e., all common causes of variables are included and observable; (2) We have N interventional datasets, each sparsely intervening on a different variable; (3) The interventions are “perfect” and “stochastic”, meaning the intervention does not set the variable necessarily to a single value. Thereby, we do not strictly require faithfulness, thus also recovering some graphs violating faithfulness. We emphasize that we place no constraints on the domains of the variables (they can be discrete, continuous, or mixed) or the distributions of the interventions. We discuss later how to extend the algorithm to infer causal mechanisms in graphs with latent confounding causal variables. Further, we discuss how to extend the algorithm to support interventions to subsets of variables only."
2107.10483,dataset,201,,,"25 nodes. For all graphs larger than 100 nodes, we use the hyperparameters of Appendix C.2, i.e. the large-scale graphs. One exception is that we allow the ﬁne-tuning of the regularizer parameter for both sets. For ENCO, we used a slightly smaller regularizer, λsparse = 0.002, for the small graphs, and a larger one, λsparse = 0.02, for the large graphs. Due to the large amount of deterministic variables, ENCO tends to predict more false positives in the beginning before removing them one by one. For SDI, we also found a smaller regularizer, λsparse = 0.01, to work best for the small graphs. However, in line with the results of Ke et al. (2019), SDI was not able to detect all edges. Even lower regularizers showed to perform considerably worse on the child dataset, while minor improvements were made on the small graphs. Hence, we settled for λsparse = 0.01. In terms of run time, both methods used 100 epochs for the small graphs and 50 for the large graphs."
2107.10483,dataset,223,,,"SDI We focused the hyperparameter search for SDI on its two regularizers, λsparse and λDAG, as well as its learning rate for γ. The other hyperparameters with respect to the neural networks were kept the same as ENCO for a fair comparison. We show all details of the hyperparameter search in Table 4. The best combination of regularizers found was λsparse = 0.02 and λDAG = 0.5. Lower values of λsparse lead to more false positives, especially in sparse graphs, while a lower value of λDAG caused many two-variable loops. Compared to the reported hyperparameter by Ke et al. (2019) (λDAG = 0.5, λsparse = 0.1), we found a lower sparsity regularizer to work better. This is likely because of testing SDI on larger graphs. In contrast to ENCO, SDI needed a lower learning rate for γ due to its higher variance gradient estimators. To compensate for it, we ran it for 50 instead of 30 epochs. In general, SDI achieved lower scores than in the original experiments by Ke et al. (2019) which was because of the larger graph size and smaller dataset size. The average run time of SDI was 4mins per graph."
2107.10483,github,9,,,2https://cran.r-project.org/web/packages/pcalg/index.html 3https://github.com/uhlerlab/causaldag 4https://github.com/slachapelle/dcdi
2107.10483,github,15,,,"1The calculations can be found in the notebook called convergence_guarantees_ENCO.ipynb, https://github.com/phlippe/ENCO/blob/main/convergence_guarantees_ENCO."
2107.10483,package,15,,,Marco Scutari. Learning Bayesian Networks with the bnlearn R Package. Journal of Statistical
2107.10483,python,15,,,"relationships in Python. J. Mach. Learn. Res., 21:37–1, 2020."
2108.02756,data,56,,,"In this section, we present applications of our proposed framework along with the designation of the desired speciﬁcations. We remark that the choice of xd is not exclusive to data from the same distribution as that used to train the target model p. For example, BOSS can be used to generate an image"
2108.02756,data,150,,,"Given a single instance of an object, the human brain is capable of learning the concept of the underlying phenomena and recognize similar objects in future experience. This remarkable capacity for cognition is a holy grail of machine learning models for classiﬁcation tasks and beyond. Naturally, this raises questions for the development of classiﬁcation models with respect to their ability to (i) learn concepts from a single observed instance and (ii) robustly classify similar objects with similar conﬁdence for a given class. The former is being addressed under the area of one-shot learning [1], where prior knowledge, often in the form of a pre-trained model, is leveraged in order to learn a new concept from a single datum. The latter problem of robustness is being assessed in adversarial machine learning via additive perturbations to data"
2108.02756,data,244,,,"Abstract—The design of additive imperceptible perturbations to the inputs of deep classiﬁers to maximize their misclassiﬁcation rates is a central focus of adversarial machine learning. An alternative approach is to synthesize adversarial examples from scratch using GAN-like structures, albeit with the use of large amounts of training data. By contrast, this paper considers oneshot synthesis of adversarial examples; the inputs are synthesized from scratch to induce arbitrary soft predictions at the output of pre-trained models, while simultaneously maintaining high similarity to speciﬁed inputs. To this end, we present a problem that encodes objectives on the distance between the desired and output distributions of the trained model and the similarity between such inputs and the synthesized examples. We prove that the formulated problem is NP-complete. Then, we advance a generative approach to the solution in which the adversarial examples are obtained as the output of a generative network whose parameters are iteratively updated by optimizing surrogate loss functions for the dual-objective. We demonstrate the generality and versatility of the framework and approach proposed through applications to the design of targeted adversarial attacks, generation of decision boundary samples, and synthesis of low conﬁdence classiﬁcation inputs. The approach is further extended to an ensemble of models with different soft output speciﬁcations. The experimental results verify that the targeted and conﬁdence reduction attack methods developed perform on par with stateof-the-art algorithms."
2108.02756,data,341,,,"The proposed Bidirectional One-Shot Synthesis (BOSS) solution differs from existing synthesis and adversarial machine learning methods in a number of key ways. First, is the capacity for BOSS to handle user-deﬁned constraints on both the input and output directions of the given pre-trained classiﬁer p(. ; θ), where the output constraint is not limited to only targeting a speciﬁc class. These constraints come in the form of distance bounds between the given datum xd (distribution pd) and the synthesized input datum x (output inference p(x ; θ)). The capacity to handle constraints in both the input and output directions of the model has the additional beneﬁt of producing speciﬁc outcomes. For example, a malicious attacker may want to cause a classiﬁer trained to detect stop signs to instead classify these as yield or speed limit signs with equal conﬁdence, which can lead to dire consequences in autonomous driving scenarios. Such inherent vulnerabilities in machine learning models are difﬁcult to test without accounting for both the input (e.g., the synthesized datum x must look like a stop sign) and output (e.g., p‘yield’(x ; θ) ≈ p‘speed 70’(x ; θ) ≈ 0.5) directions of the model simultaneously. See Figure 1 for examples. Second, our one-shot synthesis approach requires only a single datum, which mitigates the excessive data requirements of popular methods based on Generative Adversarial Networks (GANs) [4]. Finally, it is worth noting that the use of additive perturbations to data has been widely explored in the literature to induce misclassiﬁcations and test the robustness of learning models such as [3, 5, 6, 7, 8]. Though such approaches also often require only a single datum, they function by perturbing said datum to enact the attack."
2108.02756,data,346,,,"First, is the capacity for BOSS to handle user-deﬁned constraints on both the input and output directions of the given pre-trained classiﬁer p(. ; θ), where the output constraint is not limited to only targeting a speciﬁc class. These constraints come in the form of distance bounds between the given datum xd (distribution pd) and the synthesized input datum x (output inference p(x ; θ)). The capacity to handle constraints in both the input and output directions of the model has the additional beneﬁt of producing speciﬁc outcomes. For example, a malicious attacker may want to cause a classiﬁer trained to detect stop signs to instead classify these as yield or speed limit signs with equal conﬁdence, which can lead to dire consequences in autonomous driving scenarios. Such inherent vulnerabilities in machine learning models are difﬁcult to test without accounting for both the input (e.g., the synthesized datum x must look like a stop sign) and output (e.g., p‘yield’(x ; θ) ≈ p‘speed 70’(x ; θ) ≈ 0.5) directions of the model simultaneously. See Figure 1 for examples. Second, our one-shot synthesis approach requires only a single datum, which mitigates the excessive data requirements of popular methods based on Generative Adversarial Networks (GANs) [4]. Finally, it is worth noting that the use of additive perturbations to data has been widely explored in the literature to induce misclassiﬁcations and test the robustness of learning models such as [3, 5, 6, 7, 8]. Though such approaches also often require only a single datum, they function by perturbing said datum to enact the attack. Our approach differs in that we synthesize a datum from scratch, thereby enabling the exploration and synthesis of data that exists outside the conﬁnes of typical perturbation bounds."
2108.02756,"data, dataset",212,,,"Our approach is related to generative modeling using GANs in that we utilize a similar structure and layer conﬁguration to its generator side, but there are key differences. Underlying GANs is the training of two sub-models: a generator model trained to generate fake examples resembling ones from an original dataset, and a discriminator model trained to classify examples as either real (i.e., from the domain) or fake (generated) [4]. The process of training a GAN requires a large amount of training data. Here, the underlying task, and the training process and its requirements are altogether different: given a trained model, we seek to generate an example that induces a predeﬁned Probability Mass Function (PMF) at its output and to simultaneously enforce similarity to a desired example, without access to any training dataset. This is accomplished in our implementation without the use of a discriminator. Additionally, the generator and the discriminator of a GAN are trained together in a zero-sum game until the discriminator is fooled about 50% of the time [4]. In our case, we update the parameters of the generator using loss functions"
2108.02756,database,17,,,"written digit database,” ATT Labs http://yann.lecun.com/exdb/mnist, vol. 2, 2010."
2108.02756,database,130,,,"Fig. 1: Problem demonstration (left) and examples (right). The true labels are placed on the left of each desired features (image) with bold font. First row of images represent the desired features where the desired PMFs are placed on the left of each sample. The second row presents the synthesized examples, by BOSS, with their corresponding predictions w.r.t their trained classiﬁers. From left to right, samples are picked from the MNIST digits [9], MNIST fashion [10], CIFAR-10 [11], GTSRB [12], and COVID-19 Chest X-ray [13] database, respectively. Details about the trained classiﬁers are given in the supplementary material."
2108.02756,dataset,35,,,"[10] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms,” CoRR, vol. abs/1708.07747, 2017."
2108.02756,dataset,37,,,We conduct two experiments to demonstrate the capability of our proposed algorithm in targeting two trained classiﬁers for the MNIST dataset p1(. ; θ1) and p2(. ; θ2). The goal
2108.02756,dataset,58,,,"Fig. 10: Samples from each class of the CIFAR-10 dataset (columns). The ﬁrst row shows the original examples. Rows 2-5 represent the synthesized images for BOSS-C, BOSS-B, BOSS-U, and BOSS-T, respectively. The labels and similarity measures are arranged in a similar fashion as in Figure 9."
2108.02756,dataset,68,,,"Our approach can also be used to synthesize near-decisionboundary samples that can be utilized in adversarial training for robust decision-making. The authors in [20] proposed a technique for adversarial training using such samples built on GANs. The idea is to ﬁne-tune the model parameters by augmenting the training dataset with generated near-boundary samples. During training, two loss functions are minimized: the"
2108.02756,dataset,89,,,"We augment a repeated version of vector z to create a small training dataset and utilize the back-propagation algorithm [14]. Given the two objectives of BOSS, and the utilization of the adjustable parameters of network h, φ, we introduce the surrogate losses Lh(p(g(z ; φ) ; θ), pd) and Lg(g(z ; φ), xd), and use the back-propagation algorithm to optimize φ based on the minimization"
2108.02756,dataset,91,,,"Fig. 13: Samples from each class of the MNIST dataset (columns) for the targeted attacks scenario against an ensemble of two models p1(. ; θ1) and p2(. ; θ2) where the target classes are the same (second row) and different (third row). The SSIM is displayed on top while the predicted label w.r.t. p1(. ; θ1) and p2(. ; θ2), respectively, are shown on the bottom."
2108.02756,dataset,97,,,"CGAN loss and the Kullback-Leibler (KL) divergence between the output of the trained classiﬁer and the uniform distribution. Our method is used to generate near-boundary examples, but does not require extensive training with a full dataset in order to produce such examples. We remark that decision boundary examples produced by our algorithm could also be utilized in other techniques such as Knowledge Distillation (KD), a method used to enhance the training of a ‘student’ network based on a trained ‘teacher’ network [21]."
2108.02756,dataset,119,,,"Samples of BOSS-C and NewtonFool are given in the second and third rows, respectively. While BOSS-C succeeds to maintain the correct classiﬁcation of these samples, NewtonFool fails to maintain that as seen from the images of digits ‘3’, ‘5’, ‘8’, and ‘9’. The rounded conﬁdence scores (in percentage) are given at the bottom of each synthesized image. Figure 10 shows samples from CIFAR-10 dataset. Figure 11 shows samples from the Chest X-ray dataset where labels 03 represent ‘Normal Lung’, ‘COVID-19’, ‘Ovine Pulmonary Adenocarcinoma’ (OPA), and ‘pneumonia’."
2108.02756,dataset,140,,,"Inspired by the defense mechanism presented in [20], we could use our method to generate boundary samples, which are examples that fall near the decision boundary of a pre-trained classiﬁer. Such examples can be used for adversarial training to enhance robustness or for knowledge distillation (e.g., see [20, 21]). Here, we focus on synthesizing such examples regardless of the application of interest. Unlike [20], we achieve this without the need to train a CGAN with a full dataset, and with the additional ﬂexibility of choosing a desired soft output pd (e.g., uniform over only a subset of the classes). The ﬁrst four synthesized samples in the bottom row of Figure 1 are all examples of near-decision-boundary samples."
2108.02756,dataset,155,,,"Fig. 9: Samples from each class of the MNIST dataset (columns). The ﬁrst row shows the original examples. Rows 2-7 represent the synthesized images for BOSS-C, NewtonFool, BOSS-B, BOSS-U, BOSS-T, and CW, respectively. The rounded percentage values of the conﬁdence level, c, of the BOSS-C and NewtonFool samples are placed on the bottom of each image along with the predicted label. For BOSS-B, the ﬁrst pair at the bottom of every image represents the highest two predicted labels along with their rounded classiﬁcation scores (second pair). For BOSS-U, the predicted label is placed together with the maximum and minimum rounded scores. Predicted labels are placed at the bottom of the CW and BOSS-T. The percentage of the rounded similarity measure (I) is placed on top of each generated example."
2108.02756,dataset,213,,,"Figure 5 illustrates the overall performance of BOSS w.r.t. the feature vectors from the MNIST dataset. The ﬁrst and second plots of Figure 5 show the similarity I(x, xd), and its Cumulative Distribution Function (CDF), and the third and fourth plots show D(p(x ; θ), pd) and its Complementary CDF (CCDF). For the similarity I, we observe that BOSSC achieves the best performance while other methods return nearly similar results. We remark that BOSS-T yields the largest PMF distance D since the requirement of inducing misclassiﬁcations to the target label (i.e., ensuring the mode of the distribution occurs at the target class) can still be attained by relaxing the distance upper bound δc to a relatively large value. BOSS-U yields the smallest JS distance since the outputs pm(x ; θ), ∀m ∈ [M ] of all nodes need to be equal (uniform distribution). Similar observations are noticed for BOSS-C and BOSS-B. This experiment underscores the ﬂexibility of our framework in achieving the goal of each application as speciﬁed by xd, pd, δc, and δs."
2108.02756,dataset,245,,,"Variants of GANs have also been developed in prior work to perform individual and universal attacks. Examples include [15], in which a GAN architecture is trained along with an attacker model in a three-way game for enhanced adversarial training and faster convergence, the Conditional GANs (CGANs) in [16], where the generator, discriminator, and the classiﬁer are trained to generate additive perturbations, and the Auxiliary Classiﬁer GAN (AC-GAN) [17], in which an auxiliary classiﬁer is used instead of the discriminator. More closely related to our paper is the work in [18] ([19]) which uses generative networks without discriminators to synthesize additive (nonadditive) adversarial samples, however, there are two major distinctions. First, unlike these works, we do not require a labeled dataset to train our system to synthesize adversarial samples at inference time. Second, the methods in [18] and [19] are only implemented for targeted and non-targeted attacks. Our approach, on the other hand, is unifying in that it offers full control over the desired output PMF of the trained classiﬁer (while maintaining similarity to speciﬁc input features), making it applicable to a wide range of scenarios, including targeted/non-targetted attacks, conﬁdence reduction attacks, and synthesis of decision boundary examples."
2108.02756,"used dataset, dataset",206,,,"We use the MNIST digits dataset [26] in the evaluation of the different applications described in Section V and present additional examples from other datasets such as MNIST Fashion [10], CIFAR-10 [11], GTSRB [12] and COVID-19 Radiography [13] in Figure 1. More examples using these datasets are also included in the supplementary document showing similar performance trends. We compare BOSS-T and BOSS-C to the CW and NewtonFool attacks, respectively. We use the l2 variant of CW with a maximum number of iterations 3000, conﬁdence 0.9, learning rate 0.01, and binary search steps 10. For NewtonFool, we use 50 iterations and set the small perturbations parameter as η = 0.01. The details of the architectures of the pre-trained classiﬁers and the generative networks used are provided in the supplementary document. The initial loss weights are chosen as λ0 = λ0 v = 0.001. The random vector z of dimension Q = 100 is generated from a uniform distribution over the interval [0, 1]. The parameters are updated using the ADAM optimizer [27] with step size 0.025."
2108.09702,"code, dataset",137,,,"67.1 67.7+0.6 68.1+1.0 68.2+1.1 68.5+1.4 Table 1. Ablation study results (mIoU, %) on the val sets of three datasets: PASCAL VOC 2012 (PC) [11], MS-COCO 2014 (MC) [32] with image-level class labels (WSSS), and Cityscapes (CS) with pixel-level class labels (FSSS). The WSSS baseline is CONTA [64]+SPGNet [7], and the FSSS baseline is OCRNet [62]. Their results are shown in the ﬁrst row. “MEA” indicates the MEA loss. “SR-F” and “SR-L” indicate the other two losses respectively (see Eq. 5). (cid:91) means the result is produced by us using public code."
2108.09702,data,55,,,"[58] Ting-Bing Xu and Cheng-Lin Liu. Data-distortion guided self-distillation for deep neural networks. In AAAI, 2019. 3 [59] Yang Xu, Zebin Wu, Jocelyn Chanussot, and Zhihui Wei. Nonlocal patch tensor sparse representation for hyperspectral image super-resolution. In TIP, 2019. 3"
2108.09702,data,218,,,"In this paper, we seek reasons for the two major failure cases in Semantic Segmentation (SS): 1) missing small objects or minor object parts, and 2) mislabeling minor parts of large objects as wrong classes. We have an interesting ﬁnding that Failure-1 is due to the underuse of detailed features and Failure-2 is due to the underuse of visual contexts. To help the model learn a better trade-off, we introduce several Self-Regulation (SR) losses for training SS neural networks. By “self”, we mean that the losses are from the model per se without using any additional data or supervision. By applying the SR losses, the deep layer features are regulated by the shallow ones to preserve more details; meanwhile, shallow layer classiﬁcation logits are regulated by the deep ones to capture more semantics. We conduct extensive experiments on both weakly and fully supervised SS tasks, and the results show that our approach consistently surpasses the baselines. We also validate that SR losses are easy to implement in various state-of-the-art SS models, e.g., SPGNet [7] and OCRNet [62], incurring little computational overhead during training and none for testing1."
2108.09702,data,234,,,"Now, we present our overall approach, called SelfRegulation (SR), which has three major advantages: 1) generic to be implemented in any deep backbones, 2) without using any auxiliary data or supervision, and 3) with little overhead for training and none for testing. As shown in Figure 2, given a backbone network, we ﬁrst add a pair of classiﬁer and segmenter at each layer (e.g., head networks for multi-label classiﬁcation and semantic segmentation), and feed them with the corresponding feature maps. Then, we apply the proposed Self-Regulation for the features and logits by introducing the following three operations: I) Regulating the predictions of each pair with ground-truth labels, i.e., the image-level classiﬁcation labels and the pixellevel mask labels, respectively; II) Using the feature map of the shallowest layer — Shallow Teacher — to regulate all the subsequent deeper-layer feature maps — Deep Students; III) Using the prediction of the deepest layer classiﬁcation logits — Deep Teacher — to regulate all the previous shallow-layer classiﬁcation logits — Shallow Students. for pixel-level feature maps, we Here is our punchline: use one Shallow Teacher to teach many Deep Students; for image-level classiﬁcation logits, we use one Deep Teacher to teach many Shallow Students."
2108.09702,"data, dataset",333,,,"Datasets. Our WSSS experiments were carried out on two widely-used benchmarks: PASCAL VOC 2012 (PC) [11] and MS-COCO 2014 (MC) [32]. PC dataset consists of 21 classes (20 for objects and 1 for background) and splits 1, 464 images for training, 1, 449 for val, and 1, 456 for test. Following related works [1, 24, 27, 64], we used an enlarged training set including 10, 582 images. MC dataset consists of 81 classes (80 for objects and 1 for background) with 80k and 40k images respectively for training and val. In the training phase of WSSS, only the image-level class labels were used as ground truth. For data augmentation, we followed the same strategy as in [66], including gaussian blur, color augmentation, random horizontal ﬂip, randomly rotate (from −10◦ to +10◦), and random scaling (using the scaling rates between 0.5× and 2×). Baseline Models. The WSSS framework includes two steps: pseudo-mask generation and semantic segmentation training with pseudo-masks. For pseudo-mask generation, we deployed two popular ones, namely IRNet [1] and SEAM [54], and the state-of-the-art (SOTA) CONTA [64]. For semantic segmentation, we deployed DeepLab-v2 [5] with ResNet-38 [55], SegNet [2] with ResNet-101 [20] and the SOTA SPGNet [7] with ResNet-50 [20], respectively. Baseline models are trained on the conventional arch and using only the cross-entropy loss. Baseline+SR models are ours which apply our proposed SR loss terms (see Eq. 5) and train the model on the MEA arch (see Figure 2). Training Details. Our major settings followed close related"
2108.09702,"data, dataset",337,,,"Datasets. Our FSSS experiments were carried out on two challenging benchmarks: Cityscapes (CS) [8], and PASCAL-Context (PAC) [39]. CS dataset consists of 19 classes with 2, 975 images for training, 500 for val and 1, 525 for test. Please note that we only used these ﬁnely annotated images, although this dataset offers 20, 000 coarsely annotated images. For the PAC dataset, we followed [66, 62, 19] and used the most popular version consisted of 60 classes (including the background class). There are 4, 998 and 5, 015 images for training and test, respectively. For data augmentation, we followed [62, 67, 65] to use random horizontal ﬂipping, random scaling (using the the scaling rates between 0.5× and 2×), and random brightness jittering (in the range from −10◦ to +10◦). Baseline Models. We implemented our SR onto a popular method DBES [29] with ResNet-101 [20], and the SOTA method OCRNet [62] with HRNetV2-W48 [52]. Please note that HRNetV2-W48 is a backbone network. Training Details. Our major settings were the same as those in baseline methods [52, 62]. The input images were cropped into 969 × 969 and 520 × 520 on CS and PAC datasets, respectively. The mini-batch SGD momentum optimizer was used in the training phase with batch size as 8 and momentum as 0.9. The initial LRs were set to 0.01 and 0.001 for CS and PAC, respectively. The same “poly” schedule was deployed. The L2 regularization term weights were set to 0.0005 and 0.0001 for CS and PAC, respectively. All our implemented models were trained from 580 epochs on CS and 100 epochs on PAC."
2108.09702,database,36,,,"[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 6"
2108.09702,dataset,52,,,"[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 1, 6, 7, 8"
2108.09702,dataset,115,,,"works [1, 7, 54, 64]. The input images were cropped into ﬁxed sizes as 512 × 512 and 448 × 448 for PC and MC, respectively. Zero padding was used if needed. The minibatch SGD momentum optimizer was used to train all SS models with batch size as 16, momentum as 0.9 and weight decay as 0.0001. The initial learning rates (LR) were 0.01 and 0.04 for PC and MC datasets, respectively. A “poly” schedule for LR was deployed, i.e., updating LR as (1 − )0.9. All our implemented models were trained for"
2108.09702,dataset,143,,,"Ablation Study. We conducted the ablation study on the val sets of PC, MC and CS datasets, and reported the results in Table 1. “MEA” denotes using the MEA loss consisting of LSR-seg and LSR-cls. We used the SOTA WSSS method, namely CONTA [64]+SPGNet [7], as the baseline here and show its results in row 1. Comparing row 5 to row 1, we can see the proposed SR loss brought clear performance gains, e.g., 1.4% on PC dataset. Intriguingly, comparing row 5 to row 2 (and row 1), we ﬁnd that distillation-based loss terms (LSR-L and LSR-F) brought higher improvement mar SR-L (cid:55) (cid:55) (cid:51) (cid:55) (cid:51)"
2108.09702,dataset,220,,,"Visualizations. In Figure 4, we visualize four segmentation samples on PC (val) and CS (val) datasets. The top two show the results of WSSS models, and the bottom two for FSSS models. From PC samples, we can see that many of the failure regions (highlighted with white dash boxes) using baseline models were corrected by adding MEA losses. This is a gain of strengthening the semantics and details on every individual layer of the model. On the top of it, using the overall SR loss makes the models more effective to mark out minor object parts, e.g., “horse legs” and “horse tail”. On both datasets, we also saw some failure cases. For example when segmenting “monitors”, all models are missing “neck” regions (see green dashed boxes). We think the reason is that the pseudo mask module (which is basically a classiﬁcation model) in WSSS rarely attends to “monitor neck” when training the classiﬁer of “monitor”. Another failure case is on the second row of CS: tiny objects such as distant trafﬁc signs are missing. We believe those signs are"
2108.09702,dataset,235,,,"and CONTA for generating pseudo masks, respectively. It also boosted SegNet and SPGNet on PC respectively by 1.0% and 1.4% when using CONTA. On the test set of PC, it achieved the best performance which is higher than the SOTA method (CONTA w/ SPGNet) by 1.2% mIoU. Its superiority is also obvious when comparing its best results to those of the methods in the top block, e.g., it surpassed EME by 1.3% on PC val set and MCS by 2.3% on PC test set. Comparing to SOTA in FSSS. In the task of FSSS, SOTA methods include DBES [29] on ResNet-101 [20] (backbone) and OCRNet [62] on HRNet [52] (backbone), on both CS and PAC datasets. We present their original results and also show our results (by plugging SR in these methods) in Table 4 and Table 5 respectively for two datasets. We can see from both tables that our SR becomes the new state-ofthe-art. Impressively, our SR with little computing overhead boosted the large-scale network OCRNet by a clear margin of 1.4% mIoU on the val set and 0.9% mIoU on the test set of CS, and 0.8% on the more challenging PAC."
2108.09702,dataset,335,,,"gins than using only MEA loss. As we mentioned under Eq. 5, this is because our SR-L and SR-F terms encourage each individual layer to learn the “soft knowledge” (from a superior layer) which is richer than the “hard knowledge” in one-hot labels (used for computing the MEA loss). This phenomenon is consistent across all datasets. Model Efﬁciency. Our SR brings performance improvements without increasing much computational costs. To validate this, we show the statistics of Params, i.e., the number of network parameters, and FLOPs, i.e., the speed of training, in Table 2. It is clearly shown that SR introduced very little computational overheads for both SS tasks. For example in WSSS, applying MEA, SR-L and SR-F losses increased 1.1M, 0.0M, and 0.2M Params, respectively. Using MEA increased only 0.3B FLOPs, and using SR-L and SR-F for 0. This overhead is mainly caused by using additional convolutional layers in constructing MEA. Comparing to SOTA in WSSS. To compare with SOTA methods, we implemented three pseudo-mask generation approaches, i.e., IRNet [1], SEAM [54], and CONTA [64], and used three baseline SS models, i.e., DeepLab-v2 [5], SegNet [2], and SPGNet [7]. We plugged our SR in all of them. We report the results on the val and test sets of the PC dataset, and the val set of the MC dataset in Table 3. It is shown that using our SR consistently improved the performance of all implemented methods and achieved the top performance on two datasets. For example, it boosted DeepLab-v2 on the val set of PC by 1.1%, 1.1%, and 0.7% mIoU improvements when using IRNet, SEAM"
2108.09702,"github, code available, code",13,,,*Corresponding author. 1The code is available at: https://github.com/dongzhang89/SR-SS
2108.12026,"data available, data",140,,,"Drawing inspiration from reinforcement learning-based graph to sequence architecture [Chen et al., 2020],a fully Transformer-based reinforcement learning generator evaluator architecture is introduced to tackle question generation problems. Transfer learning has become an almost de facto technique in NLP like in Computer Vision, thanks to the massive and rich content data available on the internet like C4 (Colossal Clean Crawled Corpus), large models like GPT, BERT, T5 [Radford et al., 2020, Devlin et al., 2019, Raffel et al., 2020] can be trained through unsupervised or self-supervised learning with a large amount of unlabeled data. More general-purpose pre-training achieves better performance at a ﬁne-tuning step on a downstream task like summarization, text classiﬁcation, question answering, and language modeling."
2108.12026,"data, dataset",93,,,"The training and the inference of the developed architectures were conducted with the benchmark reading comprehension dataset SQUAD v1.1 [Rajpurkar et al., 2016], which consists of 100,000+ (context, question, answer) triple posed by crowdworkers on a set of Wikipedia articles. The data is originally split into two parts: the training and test. The test set remains unchanged in our experiment and the training set was randomly split into training and development set with the factors 94% and 6% respectively."
2108.12026,database,84,,,"Experiments on question-answer extraction were conducted by [Lewis et al., 2021]. They introduced Probably Asked Question, a large semi-structured knowledge database of about 65M pairs. The QA database was built using a question generation model and corpora from Wikipedia. To ensure the worthiness of generated pair, they introduced a globalﬁltering algorithm that ranks the generated pairs. Such a huge QA database could be useful to extend research on question generation and question answering tasks."
2108.12026,dataset,64,,,"Question Generation by Transformers [Kriangchaivech and Wangperawong, 2019] explores the potential of the vanilla architecture of Transformer to deal with generating questions. Instead of fully encoding the context and answer as they appear in the dataset, some transformations including the change of the named entities by their corresponding NER tags were applied both on the context and answer."
2108.12026,dataset,158,,,"Following the previous works, the n-gram-based metric BLEU was used to evaluate the syntactic reconstruction ability of the models. Besides the n-gram based metrics, the semantic side of the generated questions was assessed with the use of semantic-based metrics, namely BERTScore [Zhang et al., 2020], NUBIA [Kane et al., 2020]. The semantic relation and logical agreement scores were drawn from NUBIA. The evaluation was performed solely on the test set provided in the original dataset. The table 1 summarizes the evaluation for the metric BLEU and the table 2 refers to the evaluation for metric NUBIA. To ensure that the semantic-based adversarial training effectively improves the performance of the model toward the chosen metric, the following models were independently trained (1) T5base + RLbase(BLEU), (2) T5base + RLbase(BLEU + Semantic)."
2108.12026,dataset,266,,,"Question generation is a conditioned language generation task that consists in generating a contextaware question given a context and the targeted answer. Train language modelling with a mere likelihood maximization has been widely used while suffering from exposure bias and the discordance between the training and the test metrics. In the way of addressing this issue, The presented work portrays a fully Transformer-based reinforcement learning generator-evaluation architecture for neural question generation. To edge the ﬂexibility of the generation, a semantic-based reward score was externally infused during the training to drive the training of the language model. The global architecture is laid out in a generator-evaluator fashion optimized directly to n-gram and semanticbased metrics. Evaluation metrics for language modelling only based on n-gram overlapping do not consider semantic relations between reference and candidate sequences. To improve the evaluation step, a two-fold evaluation was carried out. On the one side, an n-gram overlapping evaluation using the BLEU score. On the other side, a semantic-based assessment using BERTScore and NUBIA. The results were corroborated by a binary human evaluation of the semantic relatedness of the generated question and the ground truth. The results obtained showed that use a semantic-based REINFORCE algorithm for the question generation syntactically reshapes the generated questions while preserving their underlying semantic meaning. Many downstream applications can be drawn from a successful question generation including the enlargement of question answering datasets, the improvement of conversational systems, the enhancement of autonomous educational assessment systems, and so forth."
2108.12026,dataset,321,,,"the analysis of the semantic score of the two conﬁgurations. Albeit the BLEU score of the conﬁguration (ii) is less than the BLEU score of the conﬁguration (i), the semantic-based scores of the conﬁguration (ii) are commensurable with or even better than the semantic-based scores of the conﬁguration (i). This reveals a change in the syntactic structure of the generated question while the semantic relatedness between the ground truth remains unchanged or even improved. This second fact corroborates the aforesaid hypothesis stating that the model’s parameters are optimized toward enhancing the performance regarding the metric used in the discriminator component. This observation can be justiﬁed by the fact that the values of the hyper-parameters, namely α, γ convey that the embedding cosine similarity of the representation outputs by the discriminator has a high contribution to the reward score and the reinforcement learning loss component has a high contribution to the overall loss. Both architectures developed outperforms the vanilla architecture of the T5 base, this authenticates the usefulness of the generator-evaluator architecture set to address the question generation. In terms of BLEU score. The exploration of the large architectures of T5 was not carried out in this work. But instead, the potential of relatively small architecture to perform on par with the larger ones leading the leaderboard. In addition to automatic evaluation, a binary human-based evaluation of the semantic correlation between the ground truth and the prediction was carried out. 500 questions were randomly sampled from the initial dataset (SQuAD v1.1) and the model used was the T5base + RLbase(BLEU + Semantic) conﬁguration. The generated questions were classiﬁed in different WH-question classes, then for each class, the proportion of questions positively ranked by 5 crowdworkers was calculated."
2108.12026,dataset,332,,,"This reveals a change in the syntactic structure of the generated question while the semantic relatedness between the ground truth remains unchanged or even improved. This second fact corroborates the aforesaid hypothesis stating that the model’s parameters are optimized toward enhancing the performance regarding the metric used in the discriminator component. This observation can be justiﬁed by the fact that the values of the hyper-parameters, namely α, γ convey that the embedding cosine similarity of the representation outputs by the discriminator has a high contribution to the reward score and the reinforcement learning loss component has a high contribution to the overall loss. Both architectures developed outperforms the vanilla architecture of the T5 base, this authenticates the usefulness of the generator-evaluator architecture set to address the question generation. In terms of BLEU score. The exploration of the large architectures of T5 was not carried out in this work. But instead, the potential of relatively small architecture to perform on par with the larger ones leading the leaderboard. In addition to automatic evaluation, a binary human-based evaluation of the semantic correlation between the ground truth and the prediction was carried out. 500 questions were randomly sampled from the initial dataset (SQuAD v1.1) and the model used was the T5base + RLbase(BLEU + Semantic) conﬁguration. The generated questions were classiﬁed in different WH-question classes, then for each class, the proportion of questions positively ranked by 5 crowdworkers was calculated. Based on the results in table 3, it is noted that the model hardly generates questions related to explanation whilst it has an affordable performance when the questions are related to time and date. The aim of this human evaluation step is to track the type of questions where the model has a good/bad performance to design a speciﬁc discriminator for each type of question in further work."
2108.12966,data,24,,,"(8) where Σ2 is the predicted variance of data noise, which is also called aleatoric uncertainty. Since Σ2 is pixelwise"
2108.12966,data,79,,,"[1] Henrik Aanæs, Rasmus Ramsbøl Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision, pages 1–16, 2016. [2] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A randomized correspondence algorithm for structural image editing. ACM Trans. Graph., 28(3):24, 2009."
2108.12966,data,91,,,"where D1,τ represent the output of a randomly transformed multi-view images. All images (I1, Ij) are transformed by data-augmentation operations (τ1, τj) randomly. In the framework, we utilize standard data-augmentation operations [36] which do not move pixels, such as color jitter, gamma correction, random crop and etc. The output of the augmented input is required to be consistent with the pseudo label D1 on the valid regions ﬁltered by (cid:98)U1."
2108.12966,data,112,,,"In Bayesian deep learning, the uncertainty is categorized into two types [8]: aleatoric and epistemic uncertainty. Aleatoric uncertainty models the inherent noise in the training data, and epistemic uncertainty accounts for what is not included in the training data. As shown in Fig. 8, a toy example of aleatoric and epistemic uncertainty is provided. In Fig. 8(a), aleatoric uncertainty models the regions which have noisy labels. In Fig. 8(b), it shows that epistemic uncertainty models what current model ignores, for example, the regions without certain supervision or label."
2108.12966,data,121,,,"pretraining and pseudo-label post-training. In the ﬁrst stage of self-supervised pre-training stage, the overall loss Lssp includes photometric consistency loss Lpc and ﬂow-depth consistency loss Lf c. As suggested by [20], the uncertainty is involved in Lssp to construct the modiﬁed loss L(cid:48) ssp, which is used for training. In the second stage of pseudolabel post-training stage, the pseudo-label and uncertainty map are estimated from the pre-trained model in previous In the uncertaintystage via Monte-Carlo Dropout [20]. aware self-training loss Luc, the pseudo-label ﬁltered by the uncertainty map is used to supervised the model. Standard random data-augmentation operations are involved in the post-training stage."
2108.12966,data,294,,,"Unsupervised / Self-supervised Multi-view Stereo: The burgeoning ﬁeld of self-supervision [13] provide a competitive alternative for amazing performance and requiring no ground truth data. In Unsup MVS [21], the predicted depth map and the input images are utilized to reconstruct the image on another view by homography warping, thus the photometric consistency is enforced to minimize the difference between the original and reconstructed images. MVS2 [7] predicts the per-view depth maps simultaneously and automatically infer the occlusion relationship among views. M3VSNet [17] enforce the consistency between surface normal and depth map to regularize the MVS pipeline. JDACS [36] revisit the color constancy hypothesis of selfsupervision and propose a uniﬁed framework to enhance the robustness of self-supervision signal towards the natural color disturbance in multi-view images. Uncertainty: The uncertainty [8] in deep learning models for vision tasks can be classiﬁed as aleatoric uncertainty and epistemic uncertainty. Aleatoric uncertainty captures the noise inherent in the training data, while epistemic uncertainty provides interpretation for the uncertainty in the model which can be remedied with enough data. [20] study the beneﬁts of modeling epistemic and aleatoric uncertainty in Bayesian deep learning models for vision tasks. In this work, we aim to reject the unreliable pixels estimated by the epistemic uncertainty. Similar idea also appears in [29]. Conﬁdence estimation is applied in MVS to ﬁlter the unreliable predictions, such as [23, 24]. UCS-Net [6] progressively reconstruct high-resolution depth map with a coarseto-ﬁne manner. The depth hypothesis of each stage adapts to the uncertainties of previous per-pixel depth predictions."
2108.12966,"data, dataset",104,,,"and the real ﬂow are enforced to be consistent. (2) To handle invalid supervision in background, we suggest to ﬁlter the unreliable supervision signals on invalid regions, and propose an uncertainty-aware self-training consistency loss. In a totally unsupervised setting, we ﬁrstly annotate the dataset with a self-supervisedly pretrained model, while acquiring the uncertainty map with Monte-Carlo Dropout. Then the pseudo label ﬁltered by the uncertainty map is used to supervise the model. Random data-augmentations on the input multi-view images are appended to enforce the robustness towards disturbance on the areas with valid supervision."
2108.12966,dataset,17,,,Figure 12. Visualization of all scenes on the intermediate partition of Tanks&Temples dataset.
2108.12966,dataset,17,,,Figure 13. Visualization of all scenes on the advanced partition of Tanks&Temples dataset.
2108.12966,dataset,96,,,"RGB2Flow Module: In the RGB2Flow module, we utilize a self-supervised method5 to train an optical ﬂow estimation network, PWC-Net [33], from the scratch on DTU dataset. The two-view pairs for estimating optical ﬂow are selected by combining the reference view with each of the source views in the multi-view pairs provided by MVSNet [39]. After self-supervisedly pretraining the PWC-Net, it is able to predict the optical ﬂow from RGB images in the RGB2Flow module. No extra ground truth is used in this module."
2108.12966,dataset,132,,,"Dataset: DTU [1] is a large-scale indoor MVS dataset collected by robotic arms. For each of the 124 scenes in total, high-resolution images are captured on 49 different views with 7 controlled light conditions. Tanks&Temples [22] is a outdoor MVS dataset, which contains challenging realistic scenes. Following the ofﬁcial split of MVSNet [39], we train the model on DTU training set and test on the DTU evaluation set. To validate the generalization performance of the proposed method, we test it on the intermediate and advanced partition of Tanks&Temples without any ﬁnetuning. Error Metrics: In the DTU benchmark, Accuracy is measured as the distance from the result to the structured light"
2108.12966,dataset,189,,,"reference, encapsulating the quality of reconstruction; Completeness is measured as the distance from the ground truth reference to the reconstructed result, encapsulating how much of the surface is captured; Overall is a the average of Accuracy and Completeness, acting as a composite error metric. In the Tanks&Temples benchmark, F-score in each scene is calculated following the ofﬁcial evaluation process. Implementation Details: The backbone of our U-MVS framework is inherited from the consice open implementations of MVSNet [39] and CascadeMVSNet [15]. In the preparation phase, we utilize a self-supervised method [25] to train an optical ﬂow estimation network, PWC-Net [33], from the scratch on DTU dataset. The two-view pairs for optical ﬂow estimation are selected by combining the reference view with each of the source views provided by MVSNet [39]. Then, we utilize the self-supervised pretrained PWC-Net to estimate the optical ﬂow from the aforementioned two-view pairs in the RGB2Flow module. More implementation details are provided in the supplementary materials."
2108.12966,dataset,210,,,"In order to evaluate the generalization ability of the proposed method, we compare the performance of our proposed method with state-of-the-art supervised and unsupervised methods on Tanks and Temples benchmark. For a fair comparison, we utilize the model merely trained on DTU dataset without any ﬁnetuning to test on Tanks&Temples dataset. For evaluation, the input image is set to 1920 × 1056, and the number of views is 7. We use CascadeMVSNet as backbone without using any ground truth in the training phase. The quantitative comparisons of performance on the intermediate partition of Tanks and Temples benchmark is presented in Table 4. The experimental results in the ta ImageDepthUncertaintyDepthUncertaintyw/o Uncertainty Guidencew Uncertainty GuidenceImageDepthUncertaintyDepthUncertaintyw/o Flow Guidencew Flow Guidence(a) Uncertainty visualization about the effect of flow-depth consistency loss(b) Uncertainty visualization about the effect of uncertain-aware self-training lossMethod OpenMVG [30] + MVE [9] OpenMVG [30] + OpenMVS [4] COLMAP [31] MVSNet [39] CIDER [37] R-MVSNet [40] CVP-MVSNet [38] CascadeMVSNet [15] MVS2 [7] M3VSNet [17] JDACS [36] Ours + CascadeMVSNet"
2108.12966,dataset,223,,,"RGB2Flow module: We utilize a self-supervised method [25] to train a PWC-Net [33] on the dataset from scratch. All two-view pairs are enumerated among the provided multi-view pairs from the target MVS dataset. After unsupervisedly pretrained on the MVS dataset, the PWCNet is used to predict the optical ﬂow in the RGB2Flow module. As shown in Fig. 3, all two-view pairs combined with reference view and arbitrary source view are fed to RGB2Flow module. The output includes the forward ﬂow and backward ﬂow among each of the two views. The forward ﬂow F1j models the projection from reference view to source view j. In contrast, the backward ﬂow Fj1 represents the optical ﬂow from source view j to reference view. Loss function: The predicted depth map D1 can be converted to virtual cross-view optical ﬂow (cid:98)F1j by Depth2Flow module. The output of RGB2Flow module is forward ﬂow F1j and backward ﬂow Fj1, which should be consistent with the virtual ﬂow (cid:98)F1j. For non-occluded pixels, the forward ﬂow F1j should be the inverese of the backward ﬂow Fj1. To avoid learning incorrect deformation in occluded pixels, we mask out the occluded parts via the occlusion"
2108.12966,github,3,,,2https://github.com/pmorerio/dl-uncertainty
2108.12966,github,6,,,3https://github.com/xy-guo/MVSNet_pytorch 4https://github.com/alibaba/cascade-stereo/
2108.12966,github,9,,,*Corresponding author. 1Code: https://github.com/ToughStoneX/U-MVS
2108.12966,"github, code",13,,,5https://github.com/lliuz/ARFlow 6The code will be released on Github in the future
2109.08237,"code available, publicly available, code",14,,,"In the spirit of reproducible research, our code is publicly available here:"
2109.08237,data,2,,,Raw Data
2109.08237,data,3,,,Subtle Data Crimes
2109.08237,data,3,,,Subtle data crimes
2109.08237,data,4,,,SUBTLE DATA CRIMES:
2109.08237,data,5,,,Subtle data crime II experiments
2109.08237,data,6,,,Subtle Crime II: JPEG-compressed data
2109.08237,data,7,,,Subtle Crime I: zero-padded k-space data
2109.08237,data,20,,,"[37] S. Ravishankar and Y. Bresler, “MR image reconstruction from highly undersampled k-space data by dictionary"
2109.08237,data,22,,,1. Brain data. In the experiment presented in Figure 3 we used a single 320 × 320 brain image.
2109.08237,data,23,,,Keywords Big data · machine learning · deep learning · inverse problem · MRI · bias · image reconstruction · subtle data crimes
2109.08237,data,26,,,"[13] S. Ravishankar, J. C. Ye, and J. A. Fessler, “Image reconstruction: From sparsity to data-adaptive methods and"
2109.08237,data,30,,,"[19] F. Ong, S. Amin, S. Vasanawala, and M. Lustig, “Mridata.org: An open archive for sharing MRI raw data,” in"
2109.08237,data,31,,,"The authors acknowledge funding from grants U24 EB029240-01, R01EB009690, R01HL136965. The authors thank Shreyas Vasanawala for his assistance with identifying the pathology cases in the FastMRI data."
2109.08237,data,35,,,Table 2: Subtle data crime II statistics: the mean NRMSE and SSIM values measured for the test set. All three algorithms yield overly-optimistic results when trained and evaluated using JPEG-compressed data.
2109.08237,data,48,,,"In summary, our experiment demonstrates that when processed data are retrospectively subsampled with a VD scheme, there is increased sampling density of ""true"" data. In the experiments described in the next section we demonstrate that this gives rise to overly-optimistic algorithm performance."
2109.08237,data,52,,,Raw k-spacedataZero-padded dataCommon Clinical Scanner PipelineArtificial data(a)InterpolatedmagnitudeimageRSSF-1databaseFRetro-spectiveexp. (c)Common Storage Pipeline(b)databaseFScanner pipeline (a)JPEGJPEG imagesynthesizedk-spaceRetro-spectiveexp. (c)Recon algorithm Input:fullysampled synthesizedk-space data(c)“Gold standard”RecRetrospective ExperimentError MetricsNRMSESSIMsamplingorSubtle Data Crimes
2109.08237,data,55,,,We quantiﬁed the subtle data crimes effects by studying how the data processing pipelines inﬂuence two highly common image quality metrics: the Normalized Root Mean Square Error (NRMSE) and the Structural Similarity Index (SSIM) [41]; the latter was implemented using the SSIM-PIL library [60].
2109.08237,data,60,,,"[7] K. Hammernik, T. Klatzer, E. Kobler, M. P. Recht, D. K. Sodickson, T. Pock, and F. Knoll, “Learning a variational network for reconstruction of accelerated MRI data,” Magnetic resonance in medicine, vol. 79, no. 6, pp. 3055– 3071, 2018."
2109.08237,data,64,,,"We designed our research framework such that it would enable isolating the bias related to the subtle data crimes in a controlled setup. Additionally, since a side-result of this study was the benchmarking of the studied algorithms, we also dedicated signiﬁcant efforts to ensuring their fair comparison. Here we detail the steps that were taken for these two aims."
2109.08237,data,69,,,"The ﬁrst experiment examines the effect of the commercial-scanner data processing pipeline (Figure 1a) on the CS algorithm. The results show that this algorithm produces increasingly sharper reconstructions as the k-space zeropadding factor grows, for both weak and strong VD sampling schemes (Figure 3). This effect is reﬂected by an artiﬁcial reduction of the NRMSE as a function of the zero-padding."
2109.08237,data,72,,,"3. Knee Proton Density (PD) data. In the large-scale experiments that were done for demonstrating the two subtle data crimes (Figures 5-7), we used data from multi-coil Proton Density (PD) scans. Speciﬁcally; we used 1427 slices obtained from 80 subjects for training and 122 slices obtained from 7 subjects as the test set. All the slices were chosen randomly."
2109.08237,data,73,,,"2. Knee Fat-Saturated Proton Density (FSPD) data. In the knee pathology experiment (Figure 4) we used data from multi-coil FSPD scans, since knee pathology is usually observed in this type of MRI scans. The training set consisted of 2849 randomly-chosen slices obtained from 300 subjects, and the test case was a speciﬁc image that contains a pathology (shown in Figure 4)."
2109.08237,data,75,,,"The CS algorithm has one tunable parameter, λ. We calibrated it through a grid search, where the grid included values in λ ∈ [1e − 9, 1e − 1]. We ran the grid search over 10 images from a subset of the data that was reserved for hyperparameter tuning. We then computed the mean NRMSE over those 10 images, and the value of λ that corresponded"
2109.08237,data,77,,,"Similarly, the experiments that demonstrate subtle data crime II (Figures 6 and 7) required training and evaluating twelve instances of each algorithm, for the twelve combinations of the four studied compression scenarios and three reduction factors. The CS computation time was several hours. However, the DictL runs were conducted for about eight days on 100 CPUs, and the DL runs were conducted for six days on 12 GPUs."
2109.08237,data,77,,,"The JPEG compression effect was further observed in a statistical analysis of a large-scale experiment, where the algorithms were trained and tested on the four types of data (NC, QF=75, QF=50, QF=20) (Figure 7). As illustrated, the error metrics exhibit a consistent improvement with the compression; notably, this effect is systematically observed for all the studied algorithms and reduction factors (Table 2)."
2109.08237,data,80,,,"to the lowest mean NRMSE was chosen. Since in the experiments of subtle data crime I the image size varies with the zero-padding, we repeated this procedure for each image size separately. However, we empirically observed that the same λ value was chosen for all image sizes. The chosen values were λ = 0.005 for the brain data (Figure 3) and λ = 0.001 for the knee data (Figures 4-7)."
2109.08237,data,87,,,"Public databases are an important driving force in the current Deep Learning (DL) revolution; ImageNet [1] is a well-known example. However, due to the growing availability of open-access data and the general hype around AI, databases are sometimes used in an ""off-label"" manner: data published for one task are used for different ones. Here we aim to show that such naïve and seemingly-appropriate usage of open-access data could lead to biased, overly-optimistic results."
2109.08237,data,101,,,"Figure 3: Example for subtle data crime I: CS reconstructions from retrospectively-subsampled k-space of processed images. Notice how the reconstruction quality improves (both visually in terms of NRMSE) with the zero-padding (data processing) extent. This improvement is completely artiﬁcial; it stems from the coupling of early processing and retrospective subsampling which leads to an increased sampling of ""true"" non-padded data (as illustrated in Figure 2). The artiﬁcial improvement is more signiﬁcant when the sampling is stronger around k-space center (bottom row, strong VD)."
2109.08237,data,107,,,"This study reveals that naive usage of open-access data in development of MRI reconstruction algorithms could give rise to overly-optimistic results. The underlying cause is that open-access data are commonly prepared with hidden data processing pipelines that implicitly affect the data properties. Our large-scale study demonstrates that CS, DictL and DL algorithms exhibit biased results for data prepared with common data processing pipelines. Since this form of bias is largely unknown, it is frequently not addressed in research literature; we introduce a framework for studying such bias and coin the term subtle data crimes to facilitate research in this ﬁeld."
2109.08237,data,109,,,"where Dw(x) is the output of a Convolutional Neural Network (CNN). This optimization problem is solved using an unrolled deep neural network which includes interleaved CNNs and DC blocks. The DC blocks ensure consistency of the solution with the k-space measurements; the backpropagation through them is implemented using the Conjugate Gradient (CG) algorithm [55]. The MoDL unrolled network is trained in an end-to-end supervised manner, where the input is an aliased image obtained from the zero-ﬁlled subsampled k-space data and the target is a ""gold standard"" image obtained from the fully-sampled k-space."
2109.08237,data,112,,,"Biased performance of machine learning models due to faulty construction of data cohorts or research pipelines has been recently identiﬁed for various tasks, including gender classiﬁcation [2], COVID-19 prediction [3] and natural language processing [4]. However, to the best of our knowledge, it was not yet studied for inverse problem solvers. We address this gap by highlighting scenarios that lead to biased performance of algorithms developed for image reconstruction from undersampled Magnetic Resonance Imaging (MRI) measurements; the latter is a real-world example of an inverse problem and a current frontier of DL research [5–13]."
2109.08237,data,116,,,"Figure 5: Subtle data crime I statistics. The CS, DictL and DL algorithms were trained and evaluated using data with various data processing extents. The processing pipeline, which is typically implemented inside commercial scanners, includes k-space zero-padding (Figure 1a). Retrospective subsampling experiments were performed with Variable Density (VD) subsampling with R = 4. The curves display the mean and STD of the NRMSE and SSIM error metrics for the test set. Notice that both metrics show an artiﬁcial improvement that is correlated with the data processing extent. This demonstrates that algorithms evaluated on retrospectively-subsampled processed data tend to yield overly-optimistic evaluation."
2109.08237,data,116,,,"MRI measurements are fundamentally acquired in the Fourier domain, which is known as ""k-space"". Sub-Nyquist sampling is commonly applied for shortening the traditionally lengthy MRI scan time, and image reconstruction algorithms are used for recovering images from the undersampled data [14–17]. The development of such algorithms should therefore ideally be done using raw k-space data. However, the development of DL methods requires thousands of examples, and databases containing raw k-space data are scarce. To date, there are only several databases that offer such data, e.g. [18–21], while there are many more that offer reconstructed and processed Magnetic Resonance"
2109.08237,data,116,,,"where x is the image to be reconstructed, y are the k-space measurements, E is an encoding operator that describes the imaging system, R(x) is a regularization term, and λ is trainable parameter that controls the tradeoff between the Data Consistency (DC) term (the ﬁrst term in Eq. [1]) and the regularization term. In MRI, the encoding operator E is typically described as E = UF, where F is the Fourier transform and U is an operator that describes the k-space subsampling. The studied algorithms differ in their regularization terms and optimization techniques, as described next."
2109.08237,data,123,,,"Figure 6: Example for subtle data crime II. The DL algorithm was trained and tested on non-compressed and JPEGcompressed data. Although the compression reduces the visual image quality, the NRMSE surprisingly reduces with increased compression, reﬂecting a seemingly-better image quality. The reason is that in retrospective experiments both the ""gold standard"" and reconstructed images are based on processed data, hence the error metric is blind to the data processing and prone to bias. Strikingly, although the reconstructions from non-compressed and default-compressed data are visually similar, the NRMSE of the latter is lower by 30%. This demonstrates the subtle bias induced by training and evaluating algorithms on JPEG-compressed data."
2109.08237,data,123,,,"Let us assume that the scanner image is later downloaded and used for synthesizing new k-space data, with the aim of using those data for training a reconstruction algorithm. The synthesized k-space has two interesting features not originally present: it is larger than the original raw k-space (due to the zero-padding), and it has non-zero values everywhere (due to the non-linear RSS step). In other words, the ""true"" data now lie in the k-space center, while artiﬁcial data appear in its periphery (Figure 1a). However, since this k-space looks fully sampled, it is considered as ""ground truth"" and used for algorithm development."
2109.08237,data,125,,,"Experiments. The experiment for demonstrating subtle data crime I with a single brain image (Figure 3) required only about several minutes on a standard laptop. In contrast, the experiment for demonstrating this subtle data crime using fat-saturated PD knee data (Figure 4) required one day of computations on two GPUs and 40 CPUs. The experiments for obtaining the statistics of subtle data crime I (Figure 5) were computationally more demanding, since they required training and testing ten different instances of each algorithm, for the ten combinations of the studied zero-padding ratios and VD sampling schemes. The compute time of these experiments was one week on a single GPU and 200 CPUs."
2109.08237,data,143,,,"In summary, this research aims to raise a red ﬂag regarding naive off-label usage of open-access data in development of machine learning algorithms. We showed that such usage may lead to biased results of inverse problem solvers. Furthermore, we demonstrated that training MRI reconstruction algorithms using such data could yield an overlyoptimistic evaluation of their ability to reconstruct small clinically-relevant details and pathology; this increases the risk of translation of biased algorithms into clinical practice. We therefore call for attention of researchers and reviewers; data usage and pipeline adequacy should be considered carefully, reproducible research should be encouraged, and research transparency should be required. By introducing the framework for studying subtle data crimes we hope to raise community awareness, stimulate discussions and set the ground for future studies of data usage."
2109.08237,data,144,,,"We ﬁrst consider a data processing pipeline that is implemented inside many commercial MRI scanners to reconstruct the scanner output (i.e. the MR image). The k-space data are typically acquired using a multi-coil array and the pipeline includes the following steps (Figure 1a): (1) image interpolation, implemented by zero-padding the raw multi-coil k-space data; (2) application of the inverse Discrete Fourier Transform (DFT), and (3) multi-coil image combination via a square Root Sum of Squares (RSS) step. Notice that although the acquired data are complex-valued, the RSS step produces a magnitude (real and non-negative) image. The scanner output is therefore an interpolated real-valued non-negative image; this is the type of images most prevalent in online MRI databases."
2109.08237,data,148,,,"The main contributions of this work are threefold. First, we reveal scenarios in which algorithmic bias of inverse problem solvers may arise from off-label usage of open-access databases. We also analyze the effects of inverse crimes on complex high-dimensional learning systems via large-scale statistics. Secondly, we expose that CS, DictL and DL algorithms are all prone to this form of subtle bias. While recent studies identiﬁed stability issues of MRI reconstruction algorithms [5, 40], to the best of our knowledge this is the ﬁrst study that identiﬁes a common vulnerability of canonical algorithms to such data-related bias. Third, by introducing the concept of subtle data crimes and setting a framework for studying them, we hope to raise community awareness to the growing problem of bias stemming from off-label usage of open access data."
2109.08237,data,151,,,"A research pipeline that is commonly used in the development of MRI reconstruction algorithms is based on retrospectivesubsampling, where sub-Nyquist sampling is simulated using a binary sampling mask and applied to a fully-sampled k-space (Figure 1c). In the scenario of subtle data crime I, such retrospective subsampling is applied to the synthesized k-space, which includes artiﬁcial data. Common subsampling masks are typically based on Variable-Density (VD) sampling schemes, which sample the center of k-space more densely than its periphery; VD schemes are used because they produce incoherent aliasing artifacts that can be removed by sparsity-promoting optimization-based reconstruction algorithms [36]. Importantly, because k-space was zero-padded earlier in the pipeline, application of a VD mask to the entire area of the synthesized k-space results in higher effective sampling density of the ""true"" k-space data."
2109.08237,data,264,,,"In the retrospective-subsampling experiments, we generated random 2D subsampling masks from pre-deﬁned PDFs using Monte-Carlo experiments. We implemented three subsampling schemes: (1) random-uniform, in which the PDF was constant and equal to 1/R (R is the acceleration factor); (2) weak VD, in which the PDF was constructed by the function f (r) = (1 − r)p, where r is the distance from k-space center and p is the power [36], which was set to p = 7 in this case; and (3) strong VD, in which the PDF was also constructed by f (r) = (1 − r)p and the power was set to p = 1, p = 2 and p = 3 for reduction factors of R = 2, R = 3, and R = 4 correspondingly. All the sampling masks included a small fully-sampled area in the center of k-space. In parallel imaging this area is often known as the calibration region [16], and in single-coil MRI experiments this region ensures sampling of the low-frequency data and helps stabilize the computational results. The calibration region size was 12 × 7 pixels for the 640 × 372 knee images and 6 × 6 pixels for the 320 × 320 brain image. In the zero-padding experiments, where the image size varied, the calibration region size scaled with the image size."
2109.08237,"data available, data",40,,,Table 1: Subtle data crime I statistical results: the mean NRMSE and SSIM values measured for the test set. All three algorithms yield overly-optimistic results when trained and evaluated on zero-padded (processed) MRI data.
2109.08237,"data available, data",200,,,"This work offers insights into subtle mechanisms that lead to biased performance of modern reconstruction algorithms. Our main observation is that such bias stems from the unintentional coupling of hidden data processing pipelines with later retrospective-subsampling experiments; the data processing implicitly improves the inverse problem conditioning, and the retrospective subsampling enables the algorithms to beneﬁt from that. This process may appear in different forms. In subtle data crime I, the zero-padding concentrates the ""true"" k-space data to the center, and when VD sampling is later applied, those data are densely sampled; the increased amount of ""true"" data that becomes available to the algorithm makes the inverse problem easier to solve, hence algorithms tend to exhibit misleadingly-good results. In subtle data crime II, the JPEG compression reduces the data entropy, i.e. it increases their sparsity and yields a more compact representation in a sparsifying transform domain. Modern reconstruction algorithms leverage sparsity priors or learn the compact representation from training data [6, 36, 43, 44]; therefore, they beneﬁt from the compression and yield biased results."
2109.08237,"data available, data",237,,,"While open databases are an important resource in the Deep Learning (DL) era, they are sometimes used ""off-label"": data published for one task are used for training algorithms for a different one. This work aims to highlight that in some cases, this common practice may lead to biased, overlyoptimistic results. We demonstrate this phenomenon for inverse problem solvers and show how their biased performance stems from hidden data processing pipelines. We describe two data processing pipelines typical of open-access databases and study their effects on three well-established algorithms developed for Magnetic Resonance Imaging (MRI) reconstruction: Compressed Sensing (CS), Dictionary Learning (DictL), and DL. In this large-scale study we performed extensive computations. Our results demonstrate that the CS, DictL and DL algorithms yield systematically biased results when naively trained on seemingly-appropriate data: the Normalized Root Mean Square Error (NRMSE) improves consistently with the data processing extent, showing an artiﬁcial increase of 25%-48% in some cases. Since this phenomenon is generally unknown, biased results are sometimes published as state-of-the-art; we refer to that as subtle data crimes. This work hence raises a red ﬂag regarding naive off-label usage of Big Data and reveals the vulnerability of modern inverse problem solvers to the resulting bias."
2109.08237,"data available, data, dataset",153,,,"Here we introduce two subtle forms of algorithmic bias that were not yet considered and are relevant to the current DL era. We show how they arise from two hidden data processing pipelines that characterize many open-access MRI databases: a commercial scanner pipeline and a JPEG data storage pipeline. To demonstrate these scenarios, we took raw MRI data and ""spoiled"" them with carefully-controlled data processing steps; we then used the processed datasets for training and evaluation of algorithms from three well-established MRI reconstruction frameworks: Compressed Sensing (CS) with a Wavelet transform [36], Dictionary Learning (DictL) [37], and DL [38]. Our large-scale experiments demonstrate that these algorithms yield overly-optimistic results when trained and evaluated on processed data. Preliminary results of this work were published in a conference proceeding [39]."
2109.08237,"data available, publicly available, data",211,,,"This study also sheds light on a new type of sensitivity of MRI reconstruction algorithms. At present there is growing interest in identifying sensitivities of such algorithms [5, 40, 45–48]. However, recent studies focused mainly on investigating sensitivities with respect to adversarial attacks. While these attacks are an important research tool, they are not observed in practice since MRI scanners are closed systems. Here, on the other hand, we focused on sensitivity related to a more common cause: off-label usage of public databases. While reviewing papers, we noticed that such usage is becoming increasingly more common due to the growing availability of public databases that offer various types of MRI data. Subtle data crime I may be common since MR images found in public databases are often based on images produced by commercial scanners, where the data processing pipeline described in Figure 1a is often applied by default. Additionally, subtle data crime II may be common since JPEG images are highly prevalent; 73.3% of the Internet websites contain JPEG-format data [49]. These factors suggest that the subtle data crimes might be more common that intuitively expected."
2109.08237,"data available, publicly available, data",235,,,"Figure 2: An experiment demonstrating how retrospective-subsampling of k-space that was synthesized from processed data leads to increased effective sampling density of the ""true"" k-space data. (a) Subsampling masks were generated for different combinations of zero-padding factors (left-right) and subsampling schemes (top-down). The masks were generated from symmetric 2D Probability Density Functions (PDFs) (proﬁles displayed), with 17% sampling in all cases. The regions covering of the original non-padded k-space data are marked with yellow boxes. Notice that the zero-padding squashes the original data to the center, so when a variable density scheme is used, those data are sampled with an increased rate. (b) Effective sampling rate, which is the subsampling rate inside the original k-space area (yellow boxes in (a)), vs. the zero-padding. Notice that for the VD schemes, the effective rate is much higher than the global rate (17%) and may rise above 55%. (c) Real-world examples for k-space data generated from MR images found in public open-access data [22, 23] show evidence of zero-padding (the yellow box is our estimation). These examples indicate that training algorithms using data from public databases could lead to increased effective sampling."
2109.08237,"data available, publicly available, data, database",20,,,All the data used in this research is publicly available as part of the FastMRI database [18].
2109.08237,"data https, dataset",17,,,"[22] “IXI dataset.” http://brain-development.org/ixi-dataset/, 2010. Accessed: 2021-03-22."
2109.08237,"data https, dataset",19,,,"[25] “Oasis dataset,.” https://www.oasis-brains.org/, 2007. Accessed: 2021-05-02."
2109.08237,"data https, dataset",22,,,"[24] “AccelMR dataset,.” https://accelmrorg.wordpress.com/organizers/#jp-carousel-135/, 2020. Ac cessed: 2021-05-02."
2109.08237,"data, code",135,,,"DictL algorithm. The DictL algorithm reconstructs the image x by jointly learning a dictionary D and a sparse code A. The dictionary, which is used for reconstruction of patches, is a sparsifying transform that is learned directly from the subsampled k-space data. In this method, the image is reconstructed by representing it as a sparse linear combination of dictionary atoms, x = DA. The DictL algorithm jointly solves for the image x, the dictionary D, and the sparse code A [37]. The algorithm learns the dictionary adaptively from the subsampled k-space while reconstructing the image, i.e. it is trained without any other examples or without access to the fully-sampled k-space data; the learning is done over image patches."
2109.08237,"data, data https, dataset",19,,,"[26] “ADNI dataset,.” http://adni.loni.usc.edu/data-samples/data-types/, 2004. Accessed: 2021-05-02."
2109.08237,"data, data https, dataset",19,,,"[29] “Brain Tumor MRI dataset,.” https://www.kaggle.com/shlezinger/brain-mri-data/, 2020. Accessed:"
2109.08237,"data, database",113,,,"Another main insight from this study is that in retrospective-subsampling experiments, the error metrics might show a misleading evaluation. That occurs because they measure the difference between two images (the gold standard and reconstructed image) that are based on the same processed data. Ideally, the error metrics should measure the difference between the reconstructed image and the original unprocessed one, but because the latter is unavailable (since it was not stored in the database), the metrics become blind to the data processing. As a result, they cannot reﬂect the true reconstruction quality, and they might produce misleading results."
2109.08237,"data, database",149,,,"The second studied pipeline involves JPEG compression of the scanner image (Figure 1b). Such compression is commonly used to reduce storage footprint, and it is sometimes applied as part of the DICOM data saving pipeline, which is highly prevalent for storage of medical images. To demonstrate the JPEG effect, here we neglect the zeropadding scenario, although the two effects are sometimes combined. In the scenario of subtle data crime II, the JPEG-compressed image is stored in an online database and later downloaded and used for synthesizing a new k-space, which is used for algorithm development (Figure 1c). However, since JPEG compression reduces the data entropy, using JPEG data in retrospective-subsampling experiments leads to improved reconstruction ﬁdelity; we aim to show that this leads to an artiﬁcial improvement of image reconstruction algorithms."
2109.08237,"data, database",210,,,"Figure 1: Subtle data crimes: how retrospective subsampling of processed data leads to biased results. (a) A common data processing pipeline, which is often implemented inside commercial MRI scanners, includes: k-space zero-padding, application of the inverse Fourier Transform, and coil combination via a Root Sum-of-Squares (RSS) step. The output image, which is interpolated and non-negative, is stored in a database. In subtle data crime I, this image is later used for synthesizing new k-space data; this yields artiﬁcial data in previously zero-padded areas. (b) A common data storage pipeline includes JPEG compression. In subtle data crime II, the compressed image is later used for retrospective experiments. (c) Standard research pipelines commonly involve retrospective subsampling of fully-sampled k-space data. In the subtle data crimes scenarios, the fully-sampled data are based on processed data, hence image reconstruction algorithms beneﬁt from the early data processing. Moreover, since the ""gold standard"" image is based on the same processed data as the reconstructed one, error metrics become blind to the data processing and they are therefore also prone to bias."
2109.08237,"data, database",263,,,"(MR) images, e.g. [22–29]. The latter offer images for post-reconstruction tasks such as segmentation and biomarker discovery. Nevertheless, due to their abundance, they are often downloaded and used for synthesizing ""raw"" k-space data using the forward Fourier transform; the synthesized data are then used for the development of reconstruction algorithms. We identiﬁed that this common approach can lead to undesirable consequences; the underlying cause is that the non-raw MR images are commonly processed using hidden pipelines. These pipelines, which are implemented by commercial scanner software or during database storage, include a full set or a subset of the following steps: image reconstruction, ﬁltering, storage of magnitude data only (i.e. loss of the MRI complex values), lossy compression, and conversion to DICOM or NIFTI formats; these reduce the data entropy. We aim to highlight that when modern algorithms are trained and evaluated using such data, they beneﬁt from the data processing and hence tend to exhibit overly-optimistic results as compared to performance on raw, unprocessed data. Since this phenomenon is largely unknown, such biased results are sometimes published as state-of-the-art, without reporting the data processing pipelines or addressing their effects. In order to raise community awareness to this growing problem, we coin the term subtle data crimes to describe such publications, in reference to the more obvious inverse crime scenario [30] which is described next."
2109.08237,"data, database, dataset",66,,,"To demonstrate the effects of the hidden data processing pipelines, we took raw MRI data and ""spoiled"" them with carefully-controlled processing steps. The raw data were obtained from the FastMRI database [18]. This section describes the raw datasets; the data processing steps were described in the main part of the paper for each subtle data crime separately."
2109.08237,"data, dataset",65,,,In the second set of experiments we studied how the performance of reconstruction algorithms is inﬂuenced by JPEG compression of the underlying data. We prepared the processed datasets using the standard JPEG implementation found in the PILLOW library [59]. In the JPEG experiments the reduction factor ranged from R = 2 to R = 4 (see Table 2).
2109.08237,"data, dataset",83,,,"Figure 4: Subtle data crime I. The CS, DictL and DL algorithms were trained and tested using two versions of the same knee MRI dataset, processed without zero-padding and with 2x zero padding. In the latter case, which represents the scenario of subtle data crime I, the reconstructions exhibit sharper images, with improved visibility of small clinically-relevant details. This illustrates that training inverse problem solvers using processed data may lead to overly-optimistic results."
2109.08237,"data, dataset",105,,,"Figure 7: Results of a large-scale experiment demonstrating subtle data crime II. The CS, DictL and DL algorithms were applied to datasets with No Compression (NC) and increasing JPEG compression levels. The graphs depict the mean and STD computed for the test set. Notice that all the curves show the same trend: the error metrics improve consistently with increased JPEG compression. This improvement is artiﬁcial and stems only from the data processing, which reduces the data entropy. The results therefore demonstrate the subtle bias caused by training inverse problem solvers on JPEG-compressed data."
2109.08237,"data, dataset",108,,,"In the second experiment we implemented the three algorithms and applied them to two versions of the same knee MRI dataset: one prepared without zero-padding, and the other prepared with 2x zero-padding. The algorithms were trained on each dataset separately, and then tested with the corresponding version of a test image that includes ﬁne details and a knee pathology (Figure 4). As can be seen, all the algorithms produced sharper images in the subtle data crime I scenario, where the data were zero-padded: the ﬁne details and the pathology became more visible than in the non-padded case."
2109.08237,"data, dataset",124,,,"To demonstrate the JPEG compression effect, we performed experiments in which the algorithms were trained and tested on different versions of the same underlying dataset. The JPEG compression level is determined by a Quality Factor (QF), where QF = 75 is the default (that yields lossy compression), and values such as QF = 50 and QF = 20 yield increasingly lossy compression [42]. For reference, our experiments also include the case of image reconstruction from Non-Compressed (NC) data. In all cases, the hyperparameter calibration, algorithm training and inference were done on the same type of data (i.e. with NC or a speciﬁc QF)."
2109.08237,"data, dataset",171,,,"These results were further conﬁrmed in a large set of experiments, where the algorithms were trained and tested on ﬁve versions of the underlying knee dataset representing ﬁve data processing scenarios; each dataset contained 2971 images. The hyperparameter calibration, training and testing was performed for each dataset separately, to optimize the algorithmic results for each data processing scenario. We then computed the statistics of two image quality metrics, the NRMSE and Structural Similarity Index (SSIM) [41], and plotted them against the zero-padding rate. Markedly, the results of the three algorithms exhibit the same behaviour: their NRMSE and SSIM values improve consistently with the zero-padding extent (Figure 5). This improvement is completely artiﬁcial and stems only from the data processing. Strikingly, for the 2x zero-padding case, which is often the default in commercial scanners, the NRMSE exhibits a large improvement of 26%-42% (Table 1)."
2109.08237,"data, dataset",195,,,"It is worth mentioning that this work did not aim to benchmark the studied algorithms; instead, it aimed to show they are all affected similarly by the subtle data crimes. However, as a side beneﬁt, we did obtain benchmark comparisons. To ensure a fair comparison, we dedicated signiﬁcant efforts to calibrating the hyperparameters of each algorithm for each processed version of the underlying dataset separately (see Materials and Methods); speciﬁcally, we dedicated one month of computations to tuning the DictL algorithm parameters through a vast search over a huge search space. Moreover, we ensured that the algorithms were calibrated, trained and tested using identical datasets. We empirically observed that the studied algorithms perform overall on-par, with an advantage of CS over DictL and a slight advantage of DL over both. However, due to the pipelines of the subtle data crimes, all of our computations were performed with single-coil magnitude non-negative images; the benchmarking of the algorithms for multi-coil, complex-valued MRI data is beyond the scope of this work and remains for future research."
2109.08237,"data, dataset",225,,,"In the ﬁrst experiment, the DL algorithm was trained on the different datasets. Figure 6 displays an example from the test set, which shows the gold standard images and the DL reconstructions for data undersampled with R = 4. Generally the visual quality of all the images (both gold standard and reconstructed ones) reduces with increased JPEG compression level (left-to-right in Figure 6); this is expected from compressed data. However, the NRMSE metric shows an unexpected effect: it improves with the compression, i.e. the reconstruction error reduces although the image visual quality degrades. The reason for this phenomenon is that in retrospective experiments the reconstruction quality is measured w.r.t. to a ""gold standard"" image that is also processed (see the pipeline in Figure 1c); the error metric is therefore blind to data processing. Strikingly, the NRMSE could show a misleading improvement even when the human eye cannot see any difference, as demonstrated in the left two columns of Figure 6: although the reconstructions from NC and QF = 75 are visually similar, the NRMSE of the latter is lower by 30%. This reﬂects the subtle bias induced by the pipeline of subtle data crime II."
2109.08237,"data, dataset provided",5,,,Subtle data crime I experiments
2109.08237,"data, dataset provided",36,,,We studied the effects of the hidden data processing pipelines by simulating those pipelines and conducting a large-scale study using the carefully-controlled processed data. Implementation details are provided in the Materials and Methods section.
2109.08237,"data, provide implementation",37,,,"In this section, we provide implementation details regarding the experiments that were performed for demonstrating the effects of subtle data crime I. In the ﬁrst experiment, which demonstrates the difference between global and effective"
2109.08237,dataset,60,,,"When constructing the knee PD and FSPD datasets we used only slices from central anatomical regions, i.e. edge slices that contain mostly noise were removed. Additionally, for each dataset, we chose 10 random slices obtained from 10 different subjects and reserved them for tuning the hyperparameters of the studied algorithms; these slices were not"
2109.08237,dataset,81,,,"[21] A. D. Desai, A. M. Schmidt, E. B. Rubin, C. M. Sandino, M. S. Black, V. Mazzoli, K. J. Stevens, R. Boutin, C. Re, G. E. Gold, et al., “SKM-TEA: A dataset for accelerated MRI reconstruction with dense image labels for quantitative clinical evaluation,” in Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021."
2109.08237,"download, data, database, dataset",169,,,"First, to mimic a scenario in which users download a database from an online resource and then optimize the parameters of their algorithm for that speciﬁc dataset, we prepared separate processed datasets for each instance of the data processing parameters (i.e. for each zero-padding factor or JPEG QF), and ensured that there is no mixture between the datasets. We then calibrated, trained and tested the algorithms on each processed dataset separately. This ensured that each algorithm was evaluated using instance-optimal parameters; it therefore mitigated bias related to hyperparameter tuning. Secondly, we applied the three algorithms to identical datasets; their results are therefore comparable. Finally, we generated sampling masks on-the-ﬂy, i.e. a different random mask was generated for each k-space example during the training and test sessions. This technique enables generating a large number of sampling masks while maintaining their statistics, hence it prevents over-ﬁtting to any particular sampling mask."
2109.08237,github,3,,,https://github.com/mikgroup/subtle_data_crimes
2109.08237,"github, python, repo",48,,,"[57] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization.” arXiv preprint, arXiv:1412.6980, 2014. [58] J. Tamir, “DeepInPy: Deep Inverse Problems for Python git repo,.” https://github.com/utcsilab/"
2109.08237,publicly available,48,,,"[39] E. Shimron, J. I. Tamir, K. Wang, and M. Lustig, “Subtle inverse crimes: Naïvely using publicly available images could make reconstruction results seem misleadingly better!,” in Proceedings of the Annual Meeting of ISMRM, 2021."
2109.08237,"publicly available, data",237,,,"Bias stemming from the underlying data has been previously recognized in a few scenarios related to inverse problems. The term inverse crime describes a scenario in which an algorithm is tested using simulated data, and the simulation resonates with the algorithm such that it leads to improved results [30–34]. Speciﬁcally, the authors of [33] described an inverse crime as a situation where the same discrete model is used for simulating k-space measurements and reconstructing an MR image from them; they showed that this leads to reduced ringing artifacts compared with reconstruction from raw or analytically-computed measurements. A second example is evaluation of MRI reconstruction algorithms on real-valued magnitude images; in this case k-space exhibits conjugate symmetry, hence it is sufﬁcient to use only about half of it for full image recovery. This symmetry is often leveraged in Partial Fourier methods such as Homodyne [15] and POCS [35], where additional steps are applied for recovery of the full complex data. However, neglecting the data complexity creates a better-conditioned inverse problem and may hence lead to an obvious advantage when evaluating the algorithm on such data as opposed to raw k-space data. However, to the best of our knowledge, inverse crimes were not yet studied in the context of machine learning or public data usage."
2109.08237,"publicly available, data https, dataset",12,,,[20] “Calgary Campinas Public Dataset.” https://sites.google.com/view/calgary-campinas-dataset/
2109.08237,"publicly available, dataset",80,,,"[18] F. Knoll, J. Zbontar, A. Sriram, M. J. Muckley, M. Bruno, A. Defazio, M. Parente, K. J. Geras, J. Katsnelson, H. Chandarana, et al., “fastMRI: A publicly available raw k-space and DICOM dataset of knee images for accelerated MR image reconstruction using machine learning,” Radiology: Artiﬁcial Intelligence, vol. 2, no. 1, p. e190007, 2020."
2109.08237,python,50,,,where Ψ is the wavelet transform. Eq. [2] can be solved using different optimization techniques; here it was solved using the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) [50]. Our implementation was based on the SigPy python toolbox [51].
2109.08237,"python, code, open-source code, open-source",52,,,"We implemented the DictL algorithm in python using our open-source code [52]. The algorithm solves eq. [3] through alternating minimization, with K-SVD [53] for the dictionary update and Orthogonal Matching Pursuit (OMP) [54] for the sparse code update."
2109.08237,"python, package",27,,,"[51] F. Ong and M. Lustig, “SigPy: a python package for high performance iterative reconstruction,” in Proc. ISMRM,"
2109.12907,data,17,,,[25] Bryon Jacob and Jonathan Ortiz. 2017. Data.world: A Platform for Global-Scale
2109.12907,data,30,,,[11] Jaana Taakis et al. 2015. Crowdsourced semantic annotation of scientific publica tions and tabular data in PDF. SEMANTICS’15 (2015).
2109.12907,data,35,,,"[22] Silvio Peroni et al. 2012. Scholarly publishing and Linked Data: describing roles, statuses, temporal and contextual extents. i-Semantics 2012 (2012). https: //doi.org/10.1145/2362499.2362502"
2109.12907,data,42,,,"only be an openly accessible structured data container for scientific findings or claims, but they can also be used for different types of meta-level assertions as well, for example for statements or assessments about other nanopublications [23]."
2109.12907,data,51,,,[8] David Shotton et al. 2009. Adventures in semantic publishing: Exemplar semantic enhancements of a research article. PLoS computational biology 5 (2009). Issue 4. [9] Emek Demir et al. 2010. The BioPAX community standard for pathway data
2109.12907,data,127,,,"The individual pair-wise agreements between the four experts are visualized in Figure 2. To calculate these agreements, we use a value of 1 when the two participants fully agree on the status of an individual formalization (either “best”, “mistake”, or no mark), a value of 0.5 when they are different but without conflict (i.e. not “best” and “mistake” at the same time), and a value of 0 in the case of conflicting “best” and “mistake” marks. We then take the average of these values of the 100 data points per expert pair (4 formalization candidates for each of 25 claims)."
2109.12907,data,135,,,"Nanopublications [19] are a concept and technology of Linked Data containers that can be used to represent and share different kinds of scientific knowledge. They are expressed in a way that is fully formal, thus machine-interpretable and can be regarded as “minimal publications” due to the fact that they contain just three basic elements (represented in RDF): an assertion containing a small unit of information that is its main content (such as a scientific finding), provenance of the assertion (e.g. linking to the scientific methods used to derive the scientific finding in the assertion) and a publication information part about the nanopublication as a whole (e.g. when it was created and by whom). Nanopublications can not"
2109.12907,data,136,,,"Data infrastructures such as Linked Open Data allow for connecting data between scientific publications [41], but this needs additional steps as concepts and relations need to be identified beforehand, and the content of the articles need to be interconnected semantically, usually by using humans in the loop [18, 39]. Moreover, more complex approaches that attempt to automate the process of extracting formal semantics from traditionally-structured scientific articles include techniques like the compositional and iterative semantic enhancement method (CSIE) [32], semantic lenses [5], and modelling the context of sentences from scientific articles in conceptual frameworks [13]. Semantic Web technologies are therefore extensively used but the underlying publishing paradigm has stayed the same [44]."
2109.12907,data,203,,,"New initiatives that try to change this old paradigm of publishing are proposed, together with new publishing workflows. This paradigm shift entails to move from the textual representation of information to a more data-centric one, similar to the proposal for the next-generation Web [1], where instead of documents, the interest shifts from the syntactic (e.g. HTML) to the semantic level (e.g. RDF, OWL). On the semantic level, we would be able to express the content and not just the structure of what is now in narrative documents. Moreover, formats that are based on HTML, like RASH [33], have been proposed, where scientific articles that include semantic annotations can be represented. Other initiatives involve using semantic representations from the start, written by the actual authors of the research in what is named genuine semantic publishing [27], and we have proposed in our earlier work to move from the monolithic form structure of scientific articles to smaller, more granular interconnected parts that each contain single scientific claims or statements from the start [7]."
2109.12907,data,232,,,"Step 4: find identifiers in existing ontologies. Next we tried to find identifiers in existing ontologies for the classes needed for the super-pattern instantiation from the last step. For this, we used (1) Wikidata5, the free knowledge base in which new concepts and properties can be added in a structured way, (2) BioOntology6, one of the biggest repositories containing biomedical ontologies and (3) Linked Open Vocabularies (or LOV)7, a big curated data collection of vocabularies that are reviewed and added continuously. All these chosen sources contain indexed vocabularies and ontologies and as such permit full text searches of concepts and relations. If we found multiple applicable candidate identifiers, we selected the one that seemed most suitable or best documented. For the cases where we could not find a matching identifier, we tried to find identifiers for parts of the required concept. These parts could then be used to construct a (partial) class definition, such as “obesity together with metabolic abnormality”, which can be defined as the intersection of the conditions “obesity” and “metabolic abnormality”. We mint new identifiers for such complex classes and also for the cases where we could not find any existing identifiers at all."
2109.12907,"data available, data, data https",99,,,"We have therefore designed a three-stage formalization study where the four co-authors of this article participated as knowledge representation experts. In the first stage, the experts independently instantiate the super-pattern for the given claims. In the second stage, each expert is asked to review all four formalizations (which are anonymized and randomly shuffled) from the first stage and select the best ones. In the last stage, all experts meet, discuss their choices, and are given the opportunity to adjust their selection. All data can be found online8."
2109.12907,"data available, data, data https, dataset",110,,,"3.2 Dataset A In order to see if the content of high-level scientific claims from different disciplines can be expressed with our super-pattern, we created a set of randomly selected scientific articles from Semantic Scholar. All data can be found online3. The general methodology for creating this Dataset A was as follows: (1) selecting a random sample of articles from different disciplines; (2) identifying a high-level scientific claim from each article; (3) applying the super-pattern on this claim with informal classes; and (4) formalizing the classes by finding existing identifiers or defining new ones."
2109.12907,"data, data https",43,,,[26] Tobias Kuhn. 2018. Using the AIDA Language to Formally Organize Scientific Claims. CNL 2018 (2018). https://doi.org/10.3233/978-1-61499-904-1-52 [27] Tobias Kuhn and Michel Dumontier. 2017. Genuine semantic publishing. Data
2109.12907,"data, data https",75,,,"[31] Silvio Peroni. 2014. The Digital Publishing Revolution. In Semantic Web Technologies and Legal Scholarly Publishing, Pompeu Casanovas and Giovanni Sartor (Eds.). Law, Governance and Technology Series, Vol. 15. Springer International Publishing, Switzerland, Chapter 2, 7–43. https://doi.org/10.1007/978-3-319-04777-5_2 [32] Silvio Peroni. 2017. Automating semantic publishing. Data Science 1 (2017),"
2109.12907,"data, data https, open-source",43,,,"[38] Bahar Sateli and René Witte. 2016. From Papers to Triples: An Open Source Workflow for Semantic Publishing Experiments. In Semantics, Analytics, Visualization. Enhancing Scholarly Data. Springer, 39–44. https://doi.org/10.1007/9783-319-53637-8_5"
2109.12907,database,33,,,[39] Pedro Sernadela and Jose Luis Oliveira. 2017. A semantic-based workflow for biomedical literature annotation. Database (Oxford) (2017). https://doi.org/10. 1093/database/bax088
2109.12907,database,40,,,[14] Marcus C. Chibucos et al. 2014. Standardized description of scientific evidence using the Evidence Ontology (ECO). Database : the journal of biological databases and curation (2014). https://doi.org/10.1093/database/bau075
2109.12907,database,52,,,[21] Sumit Madan et al. 2019. The extraction of complex relationships and their conversion to biological expression language (BEL) overview of the BioCreative VI (2017) BEL track. Database : the journal of biological databases and curation (2019). https://doi.org/10.1093/database/baz084
2109.12907,dataset,10,,,Table 2: Usage of qualifiers in the dataset.
2109.12907,dataset,10,,,Table 3: Usage of relations in the dataset.
2109.12907,dataset,50,,,"In contrast to Dataset A, Step 3 for Dataset B was performed several times independently by different knowledge representation experts. Only after the independent super-pattern instances were created, the experts discussed and tried to reach an agreement. This will be explained in more detail below."
2109.12907,dataset,68,,,"4 EVALUATION In this section we will present the design and results of the evaluation for our approach. We performed a descriptive analysis and a vocabulary use analysis for the datasets introduced above. In order to assess how well and how consistently the super-pattern can be applied by knowledge representation experts, we ran a formalization study where such experts independently perform super-pattern based formalizations."
2109.12907,dataset,108,,,"3 APPROACH AND METHODS We present here our approach to formalize high-level scientific claims by proposing what we call a “super-pattern”. This superpattern is a general template of a logical statement that can be instantiated to represent scientific claims in formal logic. To aid the development of this super-pattern, we created a dataset of 50 claims from scientific publications (Dataset A). After the super-pattern had been finalized, we created a second set of another 25 claims (Dataset B), which did not influence our design decisions of the super-pattern and is therefore not biased towards it."
2109.12907,dataset,116,,,"3.3 Dataset B As the super-pattern was developed based on the claims found during the generation of Dataset A, we created an additional dataset that did not influence the super-pattern and can therefore provide a more reliable picture about what the range of scientific claims the super-pattern is able to express. For this Dataset B, we selected an extra set of 25 claims from another set of 25 randomly selected articles from Semantic Scholar, following the same procedure and criteria as for Dataset A above. We therefore arrived at another 25 formalizations in the form of instantiated super-patterns (to the extent it applied; more on this later)."
2109.12907,dataset,124,,,"4.3 Formalization Study Results For each of the 25 claims from Dataset B we have four different possible formalizations, therefore 100 formalizations in total. After Stage 2 (before the joint discussion), 38% of the individual formalizations were marked as containing a mistake by at least one of the experts. This dropped to just 6% after the discussion, showing agreement being increased by the discussion. Given the complexity of the task, these mistake ratios seem reasonably low. Overall, just 2% of the individual formalizations had both, at least one mistake mark as well as at least one best mark (this value didn’t change after discussion)."
2109.12907,dataset,126,,,"Stage 1. The first stage started with an introduction session where the participating knowledge representation experts learned about the details of the super-pattern and could discuss any questions. The formalizations of Dataset A were used as examples for all the experts to have a common and consistent understanding of the super-pattern. After this joint introduction session, all experts worked independently to apply the super-pattern on the 25 claims of Dataset B. For each of these claims, the participants also had to rate (on a scale from 1 to 5) how confident they were in their formalization. For the class slots, the experts were only asked to provide class names, but not existing class identifiers."
2109.12907,dataset,170,,,"For Dataset A we can do an analysis of the classes used from the existing ontologies. We only restrict ourselves here to the toplevel classes that could directly be used in the slots of the superpattern, and we exclude here the classes and relations from existing ontologies that can be used to build a complex class definition. In terms of the vocabulary usage for the classes for the subject, context, and object slots of the super-pattern, we noticed that most of these classes were not present in existing vocabularies, but had to be newly minted. In only 18 out of the 128 classes (14%), we could directly use an existing class identifier. Most of the time (16 cases), these identifiers came from Wikidata. Therefore we can say that formalizations that use the super-pattern mostly depend on defining new classes, which can typically only be partially defined from existing concepts and relations."
2109.12907,dataset,232,,,"Next we can look at the distribution of qualifiers and relations for both datasets, which is shown in Tables 2 and 3. As we can see from Table 2, the most used qualifier is “generally” in almost 39% of cases (29 claims), together with its modal counterpart, “can generally” in 23% of cases (17 claims). The modal negative of qualifiers was never used, while using the negative for qualifiers seems to be less common, in just 6% of cases (6 claims), while the most used qualifiers are positive with 57% (43 claims) and modal positive with 25% (19 claims). In terms of the relations used in the datasets, Table 3 shows that relations that express causal relations are the most common with 57% (43 claims), then the equivalency relation “is same as” is the next most used with 16% (12 claims), then in a smaller ratio, the relations marking numerical comparisons (the “compares to” relations) and the relations about spatio-temporal relationships (the “has spatio-temporal relationship with”) are used in about 6-7% of cases (7 and 6 claims, respectively)."
2109.12907,dataset,329,,,"ABSTRACT The use of semantic technologies is gaining significant traction in science communication with a wide array of applications in disciplines including the life sciences, computer science, and the social sciences. Languages like RDF, OWL, and other formalisms based on formal logic are applied to make scientific knowledge accessible not only to human readers but also to automated systems. These approaches have mostly focused on the structure of scientific publications themselves, on the used scientific methods and equipment, or on the structure of the used datasets. The core claims or hypotheses of scientific work have only been covered in a shallow manner, such as by linking mentioned entities to established identifiers. In this research, we therefore want to find out whether we can use existing semantic formalisms to fully express the content of high-level scientific claims using formal semantics in a systematic way. Analyzing the main claims from a sample of scientific articles from all disciplines, we find that their semantics are more complex than what a straight-forward application of formalisms like RDF or OWL account for, but we managed to elicit a clear semantic pattern which we call the “super-pattern”. We show here how the instantiation of the five slots of this super-pattern leads to a strictly defined statement in higher-order logic. We successfully applied this super-pattern to an enlarged sample of scientific claims. We show that knowledge representation experts, when instructed to independently instantiate the super-pattern with given scientific claims, show a high degree of consistency and convergence given the complexity of the task and the subject. These results therefore open the door on the longer run for allowing researchers to express their high-level scientific findings in a manner they can be automatically interpreted. This in turn will allow for automated consistency checking, question answering, aggregation, and much more."
2109.12907,github,3,,,1https://larahack.github.io/linkflows_superpattern/doc/sp/index-en.html
2109.12907,github,3,,,8https://github.com/LaraHack/linkflows_formalization_study
2109.12907,github,6,,,3https://github.com/LaraHack/linkflows_claims_dataset 4https://www.semanticscholar.org/
2109.12907,"used dataset, dataset",36,,,"4.1 Descriptive Analysis We performed a descriptive analysis on the combined datasets used in this research, dataset A and dataset B, one with 50 claims and the other with 25 other claims respectively."
2110.00074,data,6,,,4 The point location data structure
2110.00074,data,17,,,of the point-location data structure. This structure will be used for the point location query.
2110.00074,data,23,,,"Next, the MSSP data structure of Lemma 4, MSSP(Gp, fp), is computed and stored as part"
2110.00074,data,25,,,in time O(log n) per query. The data structure can be preprocessed in O(n log n) time.
2110.00074,data,38,,,"[15] Dekel Tsur. Succinct data structures for nearest colored node in a tree. Information Processing Letters, 132:6–10, 2018. URL: https://www.sciencedirect.com/science/article/ pii/S0020019017301710, doi:https://doi.org/10.1016/j.ipl.2017.10.001."
2110.00074,data,42,,,"and Robert E. Tarjan. Making data structures persistent. Journal of Computer and System Sciences, 38(1):86–124, 1989. URL: https://www.sciencedirect.com/science/article/pii/ 0022000089900342, doi:https://doi.org/10.1016/0022-0000(89)90034-2."
2110.00074,data,45,,,"Space complexity The space used for storing the MSSP structure is O(|V | log |V |) by Lemma 4. For each centroid, we store a constant amount of data, so the space required for storing the centroid decompositions is"
2110.00074,data,70,,,"Space complexity The decomposition tree T can be represented with O(|V | log |V |) space and D with O(s(n)) space. For each node G(cid:48) ∈ T , we store data structures S1(G(cid:48)) and S2(G(cid:48)), so by Lemma 2 and 3, we get"
2110.00074,data,73,,,"Lemma 4. Let G = (V, E, ω) be an edge-weighted planar graph, f be a face of G and let Tu denote the shortest path tree rooted at u. Then there exists a data structure MSSP(G, f ) with O(n log n) space which given u ∈ V (f ) and c, v ∈ V supports queries"
2110.00074,data,115,,,"Lemma 3. Let G = (V, E, ω) be an undirected, planar embedded, edge-weighted graph with labeling l : V → L and let p be a shortest path in G. There is a data structure OG,p with O(|V | log |V |) space which given u ∈ V and λ ∈ L returns a subset C ⊂ V of constant size, s.t. if v is the vertex with label λ closest to u and v (cid:32) u intersects p, then v ∈ C. Each such query takes time at most O(log2 |V |)."
2110.00074,data,117,,,"Eﬃciently answering shortest path distance queries between pairs of vertices in a graph is a fundamental algorithmic problem with a wide range of applications. An algorithm like Dijkstra’s can answer such a query in near-linear time in the size of the graph. If we allow for precomputations, we can break this bound, for instance by simply storing the answers to all possible queries in a look-up table. However, a fast query time should preferably not come at the cost of a large space requirement. A distance oracle is a compact data structure that can answer a shortest path distance query in constant or close to constant time."
2110.00074,data,180,,,"Let us brieﬂy recall the statement of Lemma 3; that is, we let G be an undirected, edge-weighted, planar embedded graph with associated labeling l : V → L and let p = p1 (cid:32) pk be a shortest path in G. Given a query (u, λ) ∈ V × L, we want to identify a small “candidate” set of vertices C ⊆ V such that if v is the vertex with label λ closest to u and u (cid:32) v intersects p, then v ∈ C. Here, we ﬁrst describe how to compute a data structure which provides the guarantees of Lemma 3, but restricts itself to the case only where u (cid:32) v intersects p from the left. The description of the data structure for handling paths that intersect p from the right is symmetric (e.g. by swapping the endpoints of p). Lemma 3 thus readily follows from the existence of such structures."
2110.00074,data,185,,,"Our point-location data-structure uses techniques similar to those of [7] for point location in additively weighted Voronoi diagrams, but with some crucial diﬀerences in order to save space. Both structures rely on being able to determine left/right turns of shortest paths in shortest path trees rooted at sites in G, but to facilitate this, the data structure of [7] explicitly stores an (augmented) shortest path tree rooted at each site as well as a data structure for answering least common ancestor (LCA) queries. The point location structure thus requires Θ(|S|n) space n) (corresponding to the size of a where S is the number of sites, and since S may be large as Θ( sparse balanced separator in a planar graph), this translates to Θ(n3/2) space for their problem. This will not work in our case since the number of sites can in fact be as high as Θ(n), leading to a quadratic space bound."
2110.00074,data,187,,,"To compactly represent shortest path trees, our point location structure uses an augmented version of the multiple-source shortest-path (MSSP) data structure of Klein [10]. It cleverly uses the persistence techniques of [5] in conjunction with top trees [1] to obtain an implicit representation of shortest path trees rooted at each site. Top-trees allow for shortest path distance queries and least-common ancestor (LCA) queries in time O(log n) per query while using O(n log n) space, and can easily be augmented to support turn queries, as we shall see shortly. To be used as a black box, the MSSP structure relies on being initialized from a face of G. In our construction, we wish to use it for querying left/right turns of paths and distances from vertices residing on shortest paths of fundamental cycle separators, and thus some further preprocessing is required. The guarantees of the augmented MSSP structure used for the point location structure are summarized in the following lemma:"
2110.00074,data,211,,,"of M for which M (j) = M (j + 1) = v where v is the vertex with label λ closest to p(cid:48) j for k ≤ j < l. This implies that the number of sites is proportional to the number of vertices with label λ instead of the length of the original separator p: By Lemma 2, |M | = O(|Sλ|) and since |R| = |M | it follows that |R| = O(|Sλ|) bounds the complexity of T ∗ p,λ, which is stored as part of the data structure. As aforementioned, each centroid c ∈ T ∗ p,λ corresponds to some degree three Voronoi vertex, f ∗ c , with vertices, {x1, x2, x3} in the corresponding primal face fc s.t. each xj belongs to a diﬀerent Voronoi site rij for j ∈ {1, 2, 3}. For each such j, the centroid c stores a pointer to its corresponding face fc, the ﬁrst vertex pkj of p(cid:48) on rij xj and the weight ω(pkj rij )."
2110.00074,data,231,,,"Given an undirected n-vertex planar graph G = (V, E, ω) with non-negative edge weight function ω : E → R and given an assigned label to each vertex, a vertex-labeled distance oracle is a data structure which for any query consisting of a vertex u and a label λ reports the shortest path distance from u to the nearest vertex with label λ. We show that if there is a distance oracle for undirected n-vertex planar graphs with non-negative edge weights using s(n) space and with query time q(n), then there is a vertex-labeled distance oracle with ˜O(s(n))1 space and ˜O(q(n)) query time. Using the state-of-the-art distance oracle of Long and Pettie [12], our construction produces a vertex-labeled distance oracle using n1+o(1) space and query time ˜O(1) at one extreme, ˜O(n) space and no(1) query time at the other extreme, as well as such oracles for the full tradeoﬀ between space and query time obtained in their paper. This is the ﬁrst non-trivial exact vertex-labeled distance oracle for planar graphs and, to our knowledge, for any interesting graph class other than trees."
2110.00074,data,249,,,"Given the input graph G = (V, E, ω), the preprocessing phase initially computes the decomposition tree, T , of Lemma 2. Associated with each non-leaf node G(cid:48) ∈ T is a fundamental cycle separator of ab ∈ E(G(cid:48)) w.r.t. the shortest path tree T rooted at some c ∈ V (G(cid:48)). For such a G(cid:48) we shall refer to S1(G(cid:48)) = c (cid:32)G(cid:48) a and S2(G(cid:48)) = c (cid:32)G(cid:48) b. Thus the fundamental cycle separator is given by S1(G(cid:48)) ◦ ab ◦ S2(G(cid:48)). The preprocessing phase proceeds as follows: For all non-leaf nodes G(cid:48) ∈ T , compute and store data structures OG(cid:48),S1(G(cid:48)) and OG(cid:48),S2(G(cid:48)) of Lemma 3. Finally, a distance oracle D with O(s(|V |)) space capable of reporting vertex-to-vertex shortest path distances in time O(t(|V |)) is computed for G and stored alongside the decomposition tree and the point location structures."
2110.00074,data,270,,,"Let G(cid:48) ∈ T and consider the query dG(cid:48)(u, λ). If G(cid:48) is a leaf node, the query is resolved in time O(t(n)) by querying D once for each vertex of G(cid:48). If G(cid:48) is a non-leaf node, the query is handled as follows: First, data structures OG(cid:48),S1(G(cid:48)) and OG(cid:48),S2(G(cid:48)) are queried with u and λ, resulting in two “candidate sets”, C1 and C2, one for each query. By Lemma 3, C1 ∪ C2 contains the nearest vertex with label λ for which u (cid:32)G(cid:48) v(cid:48) intersects either S1 or S2 if such a vertex exists. Compute dG(cid:48) = min {dG(u, c) | c ∈ C1 ∪ C2} ∪ {∞} by querying D once for each vertex of C1 ∪ C2. The query then recursively resolves dG(cid:48)(cid:48) = dG(cid:48)(cid:48) (u, λ) where G(cid:48)(cid:48) is a child of G(cid:48) in T containing u. Finally, the query returns min {dG(cid:48), dG(cid:48)(cid:48) }."
2110.01989,data,129,,,"Defocus multi-height diversity, on the other hand, introduces different object-to-detector distances for diffraction data acquisition. In 1968, the concept was first introduced for electron microscopy, termed ‘focus series reconstructions’ [7]. In the optical region, it has been demonstrated in wavefront reconstruction [8, 9] and shows great potentials for lensless on-chip microscopy [10, 11]. A typical implementation of defocus multiheight phase retrieval is shown in Fig. 1(b), where the specimen is axially translated to different defocus distances for image acquisition. In the reconstruction process, the complex object solution is iteratively propagated to different defocus planes and the captured images are enforced as magnitude constraints."
2110.01989,open-source,348,,,"interference,"" Acta Crystallographica Section a-Crystal Physics Diffraction Theoretical and General Crystallography, 495-& (1969). 17. G. Zheng, R. Horstmeyer, and C. Yang, ""Wide-field, highresolution Fourier ptychographic microscopy,"" Nature photonics 7, 739 (2013). 18. G. Zheng, C. Shen, S. Jiang, P. Song, and C. Yang, ""Concept, implementations and applications of Fourier ptychography,"" Nature Reviews Physics 3, 207-223 (2021). 19. D. Batey, T. Edo, C. Rau, U. Wagner, Z. Pešić, T. Waigh, and J. Rodenburg, real-space oversampling in x-ray ptychography,"" Physical Review A 89, 043812 (2014). 20. C. Guo, Z. Bian, S. Jiang, M. Murphy, J. Zhu, R. Wang, P. Song, X. Shao, Y. Zhang, and G. Zheng, ""OpenWSI: a low-cost, highthroughput whole single-frame imaging autofocusing and open-source hardware,"" Optics Letters 45, 260-263 (2020). 21. Z. Bian, C. Guo, S. Jiang, J. Zhu, R. Wang, P. Song, Z. Zhang, K. Hoshino, and G. Zheng, ""Autofocusing technologies for whole slide imaging and automated microscopy,"" Journal of Biophotonics 13, e202000227 (2020). 22. Z. Bian, S. Jiang, P. Song, H. Zhang, P. Hoveida, K. Hoshino, and G. Zheng, ""Ptychographic modulation engine: a low-cost DIY microscope add-on for coherent super-resolution imaging,"" Journal of Physics D: Applied Physics 53, 014005 (2019). 23. R. Wang, P. Song, S. Jiang, C. Yan, J. Zhu, C. Guo, Z. Bian, T. Wang, and G. Zheng, ""Virtual brightfield and fluorescence staining for Fourier ptychography via unsupervised deep learning,"" Optics Letters 45, 5405-5408 (2020)."
2110.05428,data,6,,,Temporal Data Causally-related Factors Nonparametric Expression
2110.05428,data,9,,,LEARNING TEMPORALLY CAUSAL LATENT PROCESSES FROM GENERAL TEMPORAL DATA
2110.05428,data,12,,,Supplementary Materials for Learning Temporally Causal Latent Processes from General Temporal Data
2110.05428,data,15,,,Figure 5: MCC trajectories of LEAP for temporal data with clear assumption violations.
2110.05428,data,19,,,Time index Variable element (channel) index Time lag index Random permutated variable index across the data batch
2110.05428,data,19,,,"subsampled data. In Proc. 32th International Conference on Machine Learning (ICML 2015), 2015."
2110.05428,data,21,,,(cid:124) sources are considered as special cases where the nonstationary regime u = 1 for all data samples.
2110.05428,data,21,,,"Encoder for Synthetic/MoCap Data Observed time series 128 neurons, LeakyReLU 128 neurons, LeakyReLU 128 neurons, LeakyReLU Temporal embeddings"
2110.05428,data,22,,,"(Violation) Instantaneous Causal Relations For instantaneous causal relations, we generate 45,000 data points according to Eq. 56:"
2110.05428,data,22,,,"(Violation) Regime-Variant Causal Relations For regime-variant causal relations, we generate 240,000 data points according to Eq. 55:"
2110.05428,data,22,,,"Decoder for Synthetic/MoCap Data Sampled latent variables 128 neurons, LeakyReLU 128 neurons, LeakyReLU i_dim neurons, reconstructed ˆx1:T"
2110.05428,data,23,,,"and trigger it when data falls into this category: p( ˆ(cid:15)i|u) = pN (0,1)"
2110.05428,data,39,,,"David Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. arXiv preprint arXiv:2007.10930, 2020."
2110.05428,data,48,,,"Further details of this example can be found in Appendix B of (Hyvarinen et al., 2019). In summary, we need the modulation by u to have diverse (i.e., distinct inﬂuences) and complex impacts on the underlying data generation process."
2110.05428,data,52,,,"Figure D.1: Latent traversal comparisons between LEAP and the baselines. LEAP represents the data with causally-related factors, thus can represent the data with only much fewer latent variables (three vs eight) with smooth transitions dynamics. Video demonstrations are in: https://bit. ly/3kEVQhf."
2110.05428,data,52,,,Observation data Reconstructed observation Auxiliary nonstationary regime variable Underlying sources Time-delayed latent causal variables Set of direct cause nodes/parents of node zit Measurement error Forward or backward embeddings in bidirectional RNN Soft mask vector Modulation parameter vector State transition matrix Process noise term Estimated process noise term Estimated sources True underlying sources
2110.05428,data,82,,,"Figure 2: LEAP: Encoder (A) and Decoder (D) with MLP or CNN for speciﬁc data types; (B) Bidirectional inference network that approximates the posteriors of latent variables ˆz1:T , and (C) Causal process network that (1) models nonstationary latent causal processes ˆzt with Independent Noise constraint (Thm 1) or (2) models the linear transition matrix with Laplacian constraints (Thm 2)."
2110.05428,data,134,,,"Latent Traversal We ﬁt LEAP and the baseline models using the same latent size n = 8 and the maximum time lags L = 2. As shown in Fig. D.1, LEAP represents the data with causally-related factors, thus explaining the data with fewer latent variables and smooth transitions dynamics. Only three latent variables are in fact used by LEAP and while the other ﬁve latent variables only encode random noise as seen from the video demonstration. BetaVAE and SlowVAE, however, need to use all the latent variables to represent the data. Furthermore, we ﬁnd the three latent variables discovered by LEAP encode pitch, yaw, and roll rotations of walking cycles, which is close to how human beings perceive walking movement."
2110.05428,data,137,,,"Applicability The nonstationarity of process noise seems to be prominent in many kinds of temporal data. For example, nonstationary variances are seen in EEG/MEG, natural video, and closely related to changes in volatility in ﬁnancial time series (Hyvarinen & Morioka, 2016). As we assume the transition functions fi are ﬁxed across regimes, the data that most likely satisfy the proposed condition is a collection of multiple trials/segments of data with slightly different temporal dynamics in between, where the differences can be well modeled by different noise distributions. For instance, in MEG data, temporal nonstationarity can be modeled by segmenting the measured data into different sessions (e.g., stimuli, rest, etc.) where the session index modulates the noise variance."
2110.05428,data,144,,,"observation data x with u, deep generative models ﬁt with such tuples (x, u) may be identiﬁable in function space; they can recover independent factors up to a certain transformation of the original latent variables under proper assumptions (note that we use “latent factor” and “latent processe” interchangeably). Although the temporal structure is widely used for nonlinear ICA, existing work that establishes identiﬁability results considers only independent sources, or further with linear transitions. However, these assumptions may severely distort the results if the real latent factors have causal relations in between, or if the relations are nonlinear. It is not clear yet how the temporal structure may help in learning temporally causally-related latent factors, together with their causal structure, from temporal observation data."
2110.05428,data,160,,,"In the parametric (VAR) processes in Theorem 2, we exploit the non-Gaussianity of noise perturbations to achieve identiﬁability. Speciﬁcally, we constrain the process noise distribution to be within the generalized Laplacian distribution family in this paper. This L1-sparse temporal prior is motivated by the natural statistics of video data, where the uncertainty could have sharp impacts on some latent factors, but most other factors are not perturbed between two adjacent frames. This transition prior has strong connections with slow feature analysis (Sprekeler et al., 2014; Klindt et al., 2020) which measures slowness in terms of the L2 distance between temporally adjacent encodings as temporal constraints for nonlinear ICA. Note that although the Laplacian-like distributional form is pre-deﬁned, the generalized Laplacian distrbution can still be used to ﬁt a broad family of perturbations with different shapes by changing α and λ of the distribution."
2110.05428,data,162,,,"Baselines and Ablation We experimented with three kinds of nonlinear ICA baselines: (1) BetaVAE (Higgins et al., 2016) and FactorVAE (Kim & Mnih, 2018) which ignore both history and nonstationarity information; (2) iVAE (Khemakhem et al., 2020) and TCL (Hyvarinen & Morioka, 2016) which exploit nonstationarity to establish identiﬁability, and (3) SlowVAE (Klindt et al., 2020) and PCL (Hyvarinen & Morioka, 2017) which exploit temporal constraints but assume independent sources. Model variants are built to disentangle the contributions of different modules. As in Table 2, we start with BetaVAE and add our proposed modules successively without any change on the training settings. Finally, (4), we ﬁt our LEAP that uses linear transitions (LEAPVAR) with nonstationary data to show if linear relation assumptions distort identiﬁability."
2110.05428,data,165,,,"Sufﬁcient Variability The sufﬁcient variability condition was introduced in GCL (Hyvarinen et al., 2019) to extend the modulated exponential families (Hyvarinen & Morioka, 2016) to general modulated distributions. Essentially, the condition says that the nonstationary regimes u must have a sufﬁciently complex and diverse effect on the transition distributions. In other words, if the underlying distributions are composed of relatively many domains of data, the condition generally holds true. For instance, in the linear Auto-Regressive (AR) model with Gaussian innovations where only the noise variance changes, the condition reduces to the statement in (Matsuoka et al., 1995) that the variance of each noise term ﬂuctuates somewhat independently of each other in different nonstationary regimes. Then the condition is easily attained if the variance vector of noise terms in any regime is not a linear combination of variance vectors of noise terms in other regimes."
2110.05428,data,201,,,"Figure 8: Mass-Spring system results: (a) MCC for causally-related sources; (b) entries of B1,2. Nonparametric Transition – CMU-MoCap We ﬁt LEAP with nonparametric transitions on 12 trials of motion capture data for subject 7 with 62 observed variables of skeleton-based measurements at each time step. The 12 trials contain walk cycles with slightly different dynamics (e.g., walk, slow walk, brisk walk). We set latent size n = 8 and lag number L = 2. The differences between trials are modeled by nonstationary noise with one regime for each trial. The results are in Fig. 9. Three latent variables (which seem to be pitch, yaw, roll rotations, respectively) are found to explain most of the variances of human walk cycles (Panel c). The learned latent coordinates show smooth cyclic patterns with slight differences among trials (Panel a). Finally, we ﬁnd that pitch (e.g., limb movement) and roll (e.g., shoulder movement) of human walking are coupled while yaw has independent dynamics (Panel b)."
2110.05428,data,216,,,"Temporal dependencies and nonstationarities were recently used as side information u to achieve identiﬁability of nonlinear ICA on latent space z. Hyvarinen & Morioka (2016) proposed timecontrastive learning (TCL) based on the independent sources assumption. It gave the very ﬁrst identiﬁability results for a nonlinear mixing model with nonstationary data segmentation. Hyvarinen & Morioka (2017) developed a permutation-based contrastive (PCL) learning framework to separate independent sources using temporal dependencies. Their approach learns to discriminate between true time series and permuted time series, and the model is identiﬁable under the uniformly dependent assumption. Hälvä & Hyvarinen (2020) combined nonlinear ICA with a Hidden Markov Model (HMM) to automatically model nonstationarity without the need for manual data segmentation. Khemakhem et al. (2020) introduced VAEs to approximate the true joint distribution over observed and auxiliary nonstationary regimes. The conditional distribution in their work p(z|u) is assumed to be within exponential families to achieve identiﬁability on the latent space. A more recent study in causally-related nonlinear ICA was given by (Yang et al., 2021), which introduced a linear causal layer to transform independent exogenous factors into endogenous causal variables."
2110.05428,data,224,,,"Nonstationary Noise For nonparametric processes, temporal constraints are not sufﬁcient for the identiﬁcation of latent causal transition dynamics whose functional or distributional form is not constrained. Otherwise, there is no need for Theorem 2 to assume the generalized Laplacian noise and the full-rankness of state transitions at all. In this paper, an alternative way is to exploit the (temporal) nonstationarity of the data caused by changing noise distribution (hence called nonstationary noise condition). We assume the functions of the temporal causal inﬂuences denoted by fi remain the same across across the |u| regimes or domains of data we have observed, but the distributions p(cid:15)i|u, of noise terms that serve as arguments to the structural equation models, may change. One special case of this principle uses nonstationary variances, i.e., the noise variances change across nonstationary regimes. This kind of perturbation has been widely used in linear ICA (Matsuoka et al., 1995). Additionally, the nonstationary noise condition in this paper allows for any kinds of modulation of noise distribution by nonstationary regimes u, such as changing distributional forms, scale, and location by u, as long as the modulated sources satisfy sufﬁcient variability condition described below."
2110.05428,data,235,,,"As one can see from the proofs in Appendix A.3, what have been assumed for the estimation framework are the conditional factorial properties of q(ˆzt|{ˆzt−τ }, u) where ˆzt = h−1(zt) and the model of temporal nonstationarities through nonstationary noises. The conditional factorial properties have been injected using the reparameterization trick (Eq. 7) with the IN condition in causal transition prior and the enforcing of spatiotemporal independence of estimated residuals through contrastive learning. The nonstationary noises are modeled with ﬂow-based density estimators. We share the weights of the other modules (e.g., encoder, transition function, decoder, inference network, etc.) across nonstationary regimes while using separate ﬂow models to estimate the density of residuals and evaluate the prior scores in each regime. We also use componentwise ﬂow models so the learned residuals will not interact with each other in the estimation framework. Finally, in nonparametric processes, we warm-start the ﬂow models to generate standard Gaussian noise. In parametric processes, the ﬂow models are initialized to generate standard Laplacian noise. Note that the other assumed conditions in the two theorems, such as sufﬁcient variability and nonsingular state transitions, are data properties and do not need to be encoded as constraints in the estimation framework."
2110.05428,data,241,,,"In this paper, we focus on the scenario where the observed temporal variables xt do not have direct causal edges but are generated by latent processes or confounders zt that have time-delayed causal relations in between. That is, the observed data xt are unknown nonlinear (but invertible) mixtures of the underlying sources: xt = g(zt). Our ﬁrst goal is hence to understand under what conditions the latent temporally causal processes can be identiﬁed. Inspired by real situations, we consider both a nonparametric, nonstationary setting and a parametric setting for the latent processes. In the nonparametric setting, the generating process of each latent causal factor zit is characterized by nonparametric assignment zit = fi(Pa(zit), (cid:15)it), in which the parents of zit (i.e., the set of latent factors that directly cause zit) together with noise term (cid:15)it ∼ p(cid:15) (where p(cid:15) denotes the distribution of (cid:15)it) generate zit via unknown nonparametric function fi with some time delay. In the parametric setting, the time-delayed causal inﬂuences among latent factors follow a linear form. In both settings, we establish the identiﬁability of the latent factors and their causal inﬂuences, rendering them recoverable from the observed data."
2110.05428,data,329,,,"Causal discovery seeks to identify the underlying structure of the data generation process by exploiting an appropriate class of assumptions (Spirtes et al., 1993; Pearl, 2000). Despite its success in certain domains, most existing work either focuses on estimating the causal relations between observed variables (Spirtes & Glymour, 1991; Chickering, 2002; Shimizu et al., 2006), or starts from the premise that causal variables are given beforehand (Spirtes et al., 2013). Real-world observations (e.g., image pixels, sensor measurements, etc.), however, are not structured into causal variables to begin with. Estimating latent causal variable graphs from observations is particularly challenging as the latent variables, even with independent factors of variation (Locatello et al., 2019), are not identiﬁable or “uniquely” recoverable in the most general case (Hyvärinen & Pajunen, 1999). There exist several pieces of work aiming to uncover causally related latent variables. For instance, by exploiting the vanishing Tetrad conditions (Spearman, 1928) one is able to identify latent variables in linear-Gaussian models (Silva et al., 2006), and the so-called Generalized Independent Noise (GIN) condition was proposed to estimate linear, non-Gaussian latent variable causal graph (Xie et al., 2020), with follow-up studies such as (Adams et al., 2021). However, these approaches are constrained within linear relations, need certain types of sparsity or minimality assumptions, and require a relatively large number of measured variables as children of the latent variables. The work of (Bengio et al., 2019; Ke et al., 2019) used “quick adaptation” as the training criteria for learning latent structure but the identiﬁability results have not been theoretically established yet."
2110.05428,"data, database, dataset",144,,,"4.2 REAL-WORLD APPLICATIONS: PERCEPTUAL CAUSALITY Three public datasets including KiTTiMask (Klindt et al., 2020), Mass-Spring system (Li et al., 2020), and CMU MoCap database are used. The data descriptions are in Appendix B.2; depending on its property (e.g., whether it has multiple regimes), we apply the corresponding method. We ﬁrst compare the MCC performances between our approach and the baselines on KiTTiMask and Mass-Spring system in Fig. 6. Our parametric method considerably outperforms the baselines that do not use history information. Because the true latent variables for CMU-MoCap are unknown, we visualize the latent traversals and the recovered skeletons in Appendix D.1, qualitatively comparing our nonparametric method with the baselines in terms of how intuitively sensible the recovered processes and skeletons are."
2110.05428,"data, dataset",56,,,"Parametric (VAR) Dataset For parametric processes, we generate 50,000 data points according to Eq. 4. The noises (cid:15)it are sampled from i.i.d. Laplacian distribution (σ = 0.1). The entries of state transition matrices Bτ are uniformly distributed between [−0.5, 0.5]."
2110.05428,"data, dataset",90,,,"4.1 SYNTHETIC EXPERIMENTS We ﬁrst design synthetic datasets with the properties required by NonParametric (NP) and parametric Vector AutoRegressive (VAR) conditions in Section 2.2. We set latent size n = 8. The lag number of the process is set to L = 2. The mixing function g is a random three-layer MLP with LeakyReLU units. We give the data generation procedures for nonparametric conditions, parametric conditions, and ﬁve types of datasets that violate our assumptions separately in Appendix B.1."
2110.05428,"data, dataset",92,,,"Nonparametric (NP) Dataset For nonparametric processes, we generate 150,000 data points according to Eq. 2. The noises (cid:15)it are sampled from i.i.d. Gaussian distribution with variance modulated by 20 different nonstationary regimes. In each regime, the variance entries are uniformly sampled between 0 and 1. A 2-layer MLP with LeakyReLU units is used as the state transition function fi. When we need sparse causal structure for visualization, a random binary mask is added to the input nodes."
2110.05428,"data, dataset",127,,,"Our second goal is then to develop a theoretically-grounded training framework that enforces the assumed conditions through proper constraints. To this end, we propose Latent tEmporally cAusal Processes estimation (LEAP), a novel architecture that extends VAEs with a learned causal process prior network that enforces the Independent Noise (IN) condition and models possible nonstationarity through ﬂow-based estimators. We evaluate LEAP on a number of synthetic and real-world datasets, including video and motion capture data with the properties required by our conditions. Experimental results demonstrate that temporally causal latent processes are reliably identiﬁed from observed variables under different dependency structures, and that our approach considerably outperforms existing methods which do not leverage history or nonstationarity information."
2110.05428,"data, dataset",134,,,"3.3 ENCODER AND DECODER The reconstruction likelihood is LRecon = preconstruct(xt|ˆzt), where preconstruct is the decoder distribution. For synthetic and point cloud data, we use MLP with LeakyReLU units as encoder and decoder and MSE loss is used for reconstruction. For video datasets with single objects (e.g., KiTTiMask), vanilla CNNs and binary cross-entropy loss are used. For videos with multiple objects, we apply a disentangled design (Kulkarni et al., 2019) with two separate CNNs, one for extracting visual features and the other for locating object locations with spatial softmax units. The decoder retrieves object features using object locations and reconstructs the scene with MSE loss. The network architecture details are given in Appendix C.1."
2110.05428,"data, dataset",151,,,"Evaluation Metrics To measure the identiﬁability of latent causal variables, we compute Mean Correlation Coefﬁcient (MCC) on the validation dataset, a standard metric in the ICA literature for continuous variables. MCC reaches 1 when latent variables are perfectly identiﬁable up to permutation and componentwise invertible transformation in the noiseless case (we use Pearson correlation and rank correlation for linearly and nonlinearly related latent processes, respectively). To evaluate the recovery performance on causal relations, we use different approaches for (1) linear and (2) nonlinear transitions: (1) the entries of estimated state transition matrices are compared with the true ones after permutation, signs, and scaling are adjusted, and (2) the estimated causal skeleton is compared with the true data structure, and Structural Hamming Distance (SHD) is computed."
2110.05428,"data, dataset",160,,,"In this work, we proposed two provable conditions under which temporally causal latent processes can be identiﬁed from their observed nonlinear mixtures. The theories have been validated on a number of datasets with the properties required by the conditions. The main limitations of this work lie in our two major assumptions: (1) there is no instantaneous causal inﬂuence between latent causal processes, and (2) causal inﬂuences do not change across regimes. Both of them may not be true for some speciﬁc types of temporal data. The existence of instantaneous relations distorts identiﬁability results, but the amount of these relations can be controlled by time resolution. While we do not establish theories under changing causal relations, we have demonstrated through experiments the possibilities of generalizing our identiﬁability results to changing dynamics. Extending our identiﬁability theories and framework to accommodate such properties is our future directions."
2110.05428,"data, dataset",167,,,"However, it is known (Mita et al., 2021) that the performances of VAEs might change drastically as a function of the regularization strength. We thus conduct a sensitivity analysis on the impacts of hyperparameters on our identiﬁability performances. We report the MCC scores for the synthetic NP and VAR datasets on a hyperparameter grid. We found that the values of β and γ have a larger impact on the identiﬁability results, while the effects of σ are relatively smaller. Furthermore, we have veriﬁed the robustness of our approach under different maximum time lags L and latent size n settings. The ﬁnal MCC scores only show marginal differences, indicating that our approach is robust to the choices of n and L. In summary, the performance of our approach is robust to the values of some of the hyperparameters, and for the remaining hyperparameters, we use separate validation data to set their values."
2110.05428,"data, dataset",185,,,"Our goal is to recover time-delayed latent causal variables and identify their relations from measured temporal data. Estimating causally-related latent variables from observations is particularly challenging as the latent variables are not uniquely recoverable in the most general case. In this work, we consider both a nonparametric, nonstationary setting and a parametric setting for the latent processes and propose two provable conditions under which temporally causal latent processes can be identiﬁed from their nonlinear mixtures. We propose LEAP, a theoretically-grounded framework that extends Variational AutoEncoders (VAEs) by enforcing our conditions through proper constraints in causal process prior. Experimental results on various datasets demonstrate that temporally causal latent processes are reliably identiﬁed from observed variables under different dependency structures and that our approach considerably outperforms baselines that do not properly leverage history or nonstationarity information. This demonstrates that using temporal information to learn latent processes from their invertible nonlinear mixtures in an unsupervised manner, for which we believe our work is one of the ﬁrst, seems promising even without sparsity or minimality assumptions."
2110.05428,"data, dataset",228,,,"(ii) the VAR transitions, a group of state transition matrices, one for each time lag, is used. log (|det (J)|) computation. For the NP transitions, the Jacobian matrix entries ∂ri are com∂ ˆzit puted using torch.autograd.functional.jacobian method. The log determinant is then evaluated by summing over log transformations of absolute values of Jacobian terms for ˆzit. For the VAR transitions, because of the additive noise assumption, the log determinant is directly 0. (iii) Spline ﬂow model. Componentwise spline ﬂow models use monotonic linear rational splines to transform standard Gaussian distribution to the estimated noise distribution. We use eight bins for the linear splines and set the bound as ﬁve so data points lying outside [−5, 5] are evaluated using N (0, 1) directly while the data points within the region are evaluated by spline ﬂow models. For the nonparametric processes, we always warm-start the spline ﬂows by training it on a dataset of standard Gaussian noises with steps=5000 and learning rate=0.001. For the parametric processes, we warm-start it instead on a dataset of standard Laplacian noises. All the three components of the transition prior network are set to be learnable during the VAE updates."
2110.05428,"database, dataset",53,,,"B.2 REAL-WORLD DATASET Three public datasets, including KiTTiMask, Mass-Spring System, and CMU MoCap database, are used. The observations together with the true temporally causal latent processes are showcased in Fig. B.1. For CMU MoCap, the true latent causal variables and time-delayed relations are unknown."
2110.05428,dataset,3,,,B.1 SYNTHETIC DATASET
2110.05428,dataset,3,,,B.1 Synthetic Dataset
2110.05428,dataset,3,,,B.2 Real-world Dataset
2110.05428,dataset,9,,,D.1 COMPARISONS BETWEEN LEAP AND BASELINES ON CMU-MOCAP DATASET
2110.05428,dataset,10,,,Figure C.1: Impacts of hyperparameters on VAR dataset.
2110.05428,dataset,10,,,Figure C.2: Impacts of hyperparameters on NP dataset.
2110.05428,dataset,17,,,D.1 Comparisons between LEAP and Baselines on CMU-Mocap Dataset . . . . . . . .
2110.05428,dataset,17,,,Figure D.3: Visualization of recovered latent variables and the estimated skeletons for Mass-Spring system dataset.
2110.05428,dataset,26,,,We comparatively evaluate LEAP on a number of temporal datasets with the required assumptions satisﬁed or violated. We aim to answer the following questions:
2110.05428,dataset,40,,,Figure 9: MoCap dataset results: (a) latent coordinates dynamics for 12 trials; (b) estimated skeleton; and (c) latent traversal by rendering the reconstructed point clouds into the video frame.
2110.05428,dataset,42,,,Applicability L1-sparse transition priors are widely used to model video datasets and natural scene measurements. This condition is applicable to video datasets where the external factors have sharp effects on some but not all latent factors in two adjacent frames.
2110.05428,dataset,44,,,Figure 7: KiTTiMask dataset results: (a) MCC for independent sources; (b) scatterplots between estimated and true factors; (c) entries of B1; and (d) latent traversal on a ﬁxed video frame.
2110.05428,dataset,53,,,Figure 4: Results for synthetic parametric processes (VAR) datasets: (a) MCC for causally-related factors; (b) scatterplots of the entries of Bτ ; (c) scatterplots between estimated and true factors; and (4) MCC trajectories comparisons between LEAP and baselines.
2110.05428,dataset,54,,,Figure 3: Results for synthetic nonparametric processes (NP) datasets: (a) MCC for causally-related factors; (b) recovered causal skeletons with (SHD=5); (c) scatterplots between estimated and true factors; and (d) MCC trajectories comparisons between LEAP and baselines.
2110.05428,dataset,54,,,"Figure B.1: Real-world datasets: (a) KiTTiMask is a video dataset of binary pedestrian masks, (b) Mass-Spring system is a video dataset with ball movement rendered in color and invisible springs, and (c) CMU MoCap is a 3D point cloud dataset of skeleton-based signals."
2110.05428,dataset,55,,,"exploits the nonstationarity in the noise distribution, which is more natural in real-world datasets. Finally, iVAE assumes modulated exponential families in Eq. 54 while our nonparametric condition (Theorem 1) allows any kinds of modulation by side information u without those strong assumptions on the transition functions or distributions."
2110.05428,dataset,62,,,"(Violation) Low-rank State Transition For this dataset, the transition matrix Bτ in Eq. 4 is lowrank instead of full-rank. The datasets are created following the steps in the VAR dataset, but we restrict the rank of state transition matrix Bτ to 4 and time lag L = 1. The full matrix rank is 8."
2110.05428,dataset,64,,,"(Violation) Gaussian Noise Distribution For this dataset, the noise terms (cid:15)it in Eq. 4 follow the Gaussian distribution (αi = 2) instead of Generalized Laplacian distribution (αi < 2). In particular, the noise terms (cid:15)it are sampled from i.i.d. Gaussian distribution (σ = 0.1)."
2110.05428,dataset,64,,,"KiTTiMask The KiTTiMask dataset consists of pedestrian segmentation masks sampled from the autonomous driving vision benchmark KiTTi-MOTS. For each given frame, the position (vertical and horizontal) and the scale of the pedestrian masks are set using measured values. The difference in the sample time (e.g., ∆t = 0.15s) generates the sparse Laplacian innovations between frames."
2110.05428,dataset,68,,,"The hyperparameters of LEAP include [β, γ, σ], which are the weights of each term in the augmented ELBO objective, as well as the latent size n and maximum time lag L. We use the ELBO loss on the validation dataset to select the best pair of [β, γ, σ] because low ELBO loss always leads to high"
2110.05428,dataset,70,,,"Seven synthetic datasets, including two datasets (NP and VAR) which satisfy our assumptions, and ﬁve datasets, which violate each of the assumption in the proposed theorems, are used in this paper. We set the latent size n = 8 and the lag number of the process L = 2. The mixing function g is a random three-layer MLP with LeakyReLU units."
2110.05428,dataset,72,,,"• (1,2) MLP-Encoder and MLP-Decoder: These modules are used for the synthetic and motion capture datasets. They are composed of a series of fully-connected neural networks with LeakyReLU as the activation function. The universal approximation theorem guarantees that our model can approximate the mixing function. The encoder maps the raw observations into features, while the decoder maps the latent variables back to the inputs."
2110.05428,dataset,79,,,"MCC. We always set a larger latent size than the true latent size. This is critical in video datasets because the image pixels contain more information than the annotated latent causal variables, and restricting the latent size will hurt the reconstruction performances. For the maximum time lag L, we set it by the rule of thumb. For instance, we use L = 2 for temporal datasets with a latent physics process."
2110.05428,dataset,88,,,"(Violation) Insufﬁcient Variability For this dataset, we create datasets that violate the nonstationary noise condition and sufﬁcient variability by restricting the number of nonstationary regimes observed in the NP dataset. When only one regime is observed, we violate the nonstationary noise condition by using stationary noise. Furthermore, we vary the number of the observed regimes |u| ∈ {1, 5, 10, 15, 20} to assess the impacts of variability on the recovery of nonparametric processes."
2110.05428,dataset,97,,,Training Stability We have used several standard tricks to improve training stability: (1) we use a slightly larger latent size than the true latent size for real-world datasets in order to make sure the meaningful latent variables are among the recovered latents; (2) we use AdamW optimizer as a regularizer to prevent training from being interrupted by overﬂow or underﬂow of variance terms of VAE; (3) we use a larger learning rate for the VAE than for the noise discriminator to prevent extreme extrapolation behavior of discriminator.
2110.05428,dataset,126,,,"The supplementary materials are divided into ﬁve main sections. In Appendix A, we provide the explanations of each assumption and give the proof of the identiﬁability theory. We also give sideby-side comparisons, by providing the mathematical formulations of the closest works and making comparisons in terms of problem setups and critical assumptions. Finally, how the theory is connected to the training framework is discussed. In Appendix B, we provide the details of the synthetic and real-world datasets and explain the evaluation metrics. In Appendix C, we describe our network architecture, hyperparameters setting, and training details. The additional experiment results are given in Appendix D. The related work is summarized in Appendix E."
2110.05428,dataset,129,,,"• (3,4) CNN-Encoder and CNN-Decoder: For the KiTTiMask dataset, vanilla CNNs are used for both the encoder and decoder. For the Mass-Spring system dataset, the time-delayed causal variables use objects as the building blocks to factorize the scene. The object-centric representations contain object locations and some other attributes (e.g., color, size, etc.). We thus use two separate CNNs, one for extracting visual features (see Feature Extractor in Table C.2) and the other for locating object locations with a spatial softmax unit (see Keypoint Predictor in Table C.2). The decoder retrieves object features from feature maps using object locations and reconstructs the scene (see Reﬁner in Table C.2)."
2110.05428,dataset,133,,,"Nonparametric (NP) Dataset Similarly, we have performed a grid search of β ∈ [2E-4, 2E-3, 2E-2] and γ ∈ [2E-3, 2E-2, 2E-1] on the NP dataset and reported the results in Fig. C.2(a). The best conﬁguration is [β, γ] = [2E-3, 2E-2]. The ﬁnal MCC scores under parameter grids of [β, γ] are shown in Fig. C.2(b). The optimal conﬁguration for σ is 1E-6 in the search space σ ∈ [1E-7, 1E-6, 1E-5] with the optimal [β, γ] value, as shown in Fig. C.2(c). We verify the robust 27"
2110.05428,dataset,141,,,"The results for VAR datasets are in Fig. 4. Similarly, the latent processes are recovered, as indicated by (a) high MCC and (b) recovery of state transition matrices. The latent causal variables are estimated up to permutation and scaling (c). The baselines without using history or assume independent sources again fail to recover the latent processes (d). We show the contributions of different components of LEAP in Table 2. Causal process prior and nonstationary ﬂow signiﬁcantly improve identiﬁability. Noise discriminator further increases MCC and reduces variance. Note that we use a dense network without the input masks on the NP dataset for ablation studies, to validate that our proposed framework does not rely on sparse causal structure for latent causal discovery."
2110.05428,dataset,143,,,"Main Results Fig. 3 gives the results on NP datasets. The latent processes are successfully recovered, as indicated by (a) high MCC for the casually-related factors and (b) the recovery of the causal relations (SHD=5). Panel (c) suggests that the latent causal variables are estimated up to permutation and componentwise invertible transformation. The comparisons with baselines are in (d). In general, the baselines that do not exploit history or nonstationarity cannot recover the latent processes. SlowVAE and PCL distort the results due to independent source assumptions. Interestingly, LEAP-VAR, which uses linear causal transitions, gains partial identiﬁability on NP datasets. This might promote the usage of linear components of transition signals to guide the learning of latent causal variables."
2110.05428,dataset,178,,,"Parametric Transition – KiTTiMask LEAP with VAR transitions is used. We set latent size n = 10 and the lag number L = 1. The gap between our result and that by SlowVAE is relatively small, as seen in Fig. 6; this is because the latent processes on this dataset seem rather independent (according to the transition matrix learned by LEAP, given in Fig. 7(c)) and when the latent processes are independent, our VAR method reduces to SlowVAE, as its special case. As shown in Fig. 7, the latent causal processes are recovered, as seen from (a) high MCC for independent sources; (b) latent factors estimated up to componentwise transformation; (c) the estimated state transition matrix, which is almost diagonal (independent sources); and (d) latent traversals conﬁrming that the three latent causal variables correspond to the vertical, horizontal, and scale of pedestrian masks."
2110.05428,dataset,228,,,"Parametric (VAR) Dataset We have performed a grid search of β ∈ [3E-4, 3E-3, 3E-2] and γ ∈ [9E-4, 9E-3, 9E-2] and reported the results in Fig. C.1(a). The best conﬁguration is [β, γ] = [3E-3, 9E-3]. We plot the ﬁnal MCC score as a function of the value of the two hyperparameters in Fig. C.1(b). For σ, we compare the MCC scores under different σ ∈ [5E-7, 1E-6, 1E-5] with the optimal (β, γ) value. The optimal conﬁguration for σ is 1E-6 as shown in Fig. C.1(c). Furthermore, we verify the robustness of our approach under different time lags L ∈ [2, 3, 4] and latent dimensions n ∈ [4, 6, 8] with [β, γ, σ] = [3E-3, 9E-3, 1E-6] and the results are shown in Fig. C.1(d). We can see that the ﬁnal MCC scores in all cases are around 0.9 with marginal differences, indicating that latent recovery performances of our approach is robust to the choices of n and L."
2110.05428,dataset,230,,,"Robustness We show the consequences of the violation of each of the assumptions on the synthetic datasets to mimic real situations. For VAR processes, we create datasets: (1) with causal relations changing over regime, and (2) with instantaneous causal relations, (3) with Gaussian noise, and (4) with low-rank state transition matrices. For NP processes, we violate (5) the sufﬁcient variability by creating datasets with fewer than the required 2n + 1 = 17 regimes. We ﬁt LEAP on these datasets without any modiﬁcation. Our framework can gain partial identiﬁability under (1,4) but violating (2) conditional independence and (3) non-Gaussianity distort the results obviously. Our approach, although designed to model nonstationarity by noise, can be extended to model changing causal relations. The partial identiﬁability of (4) is because the low-dimensional projections of the latent processes are recovered, as illustrated by a numerical example in Appendix A.2.4. For NP processes, nonstationarity is necessary for identiﬁability. Furthermore, the differences of the MCC trajectories under 15 and 20 regimes seem marginal, suggesting that our approach does not always require at least 2n + 1 = 17 regimes to achieve full identiﬁability of the latent processes."
2110.05428,"dataset, open-source data, open-source",97,,,"CMU MoCap CMU MoCap (http://mocap.cs.cmu.edu/) is an open-source human motion capture dataset with various motion capture recordings (e.g., walk, jump, basketball, etc.) performed by over 140 subjects. In this work, we ﬁt our model on 12 trials of “walk” recordings (Subject 7). Skeleton-based measurements have 62 observed variables corresponding to the locations of joints (e.g., head, foot, shoulder, wrist, throat, etc.) of the human body at each time step."
2110.05428,"github, code",9,,,∗ Equal contribution. Code: https://github.com/weirayao/leap
2110.05428,"github, data, code",60,,,"Our code for the proposed framework and experiments can be found at https://github.com/ weirayao/leap. For theoretical results, the assumptions and complete proof of the claims are in Appendix A. For synthetic experiments, the data generation process is described in Appendix B.1. The implementation details of our framework are given in Appendix C."
2110.08645,open-source,105,,,"In addition to gaining insights using simulation, our goal is also to ﬁnd a pathway from autonomous simulated agents to real-world assistant agents. This requires an open source research infrastructure, which supports simulation and real-world prototyping on different levels, and which enables the building of knowledge-based assistant agents, some of which may be decision support. Although knowledge-based approaches are useful for transparency, they may be combined with other approaches, such as neural networks. The required research platform needs to support exploration of differing theories and deﬁnitions, without forcing a commitment to any particular one."
2110.10854,data,36,,,"[14] G. Ungerboeck, “Adaptive maximum-likelihood receiver for carriermodulated data-transmission systems,” IEEE Trans. Commun., vol. COM-22, no. 5, pp. 624-636, May 1974."
2110.10854,data,51,,,"σ2 a τ T In this letter, in order to provide more stringent covertness requirement for the legitimate data transmission, h(t) and τ are assumed to be perfectly known by Willie. After the process of matched ﬁlter, the received signal can be represented by"
2110.10854,data,83,,,"In this letter, we have analyzed the performance of covert communications under FTN signaling in the Rayleigh block fading channel. Both Bayesian criterion- and KL divergencebased covertness constraints have been considered. Especially, for KL divergence-based one, we have proved that the maximum transmit power and covert rate were higher using FTN signaling than those under Nyquist signaling. Numerical results have coincided with our analysis and validated the advantages of FTN signaling to realize covert data transmission."
2110.10854,data,86,,,"Abstract—In this letter, we analyze the performance of covert communications under faster-than-Nyquist (FTN) signaling in the Rayleigh block fading channel. Both Bayesian criterion- and Kullback-Leibler (KL) divergence-based covertness constraints are considered. Especially, for KL divergence-based one, we prove that both the maximum transmit power and covert rate under FTN signaling are higher than those under Nyquist signaling. Numerical results coincide with our analysis and validate the advantages of FTN signaling to realize covert data transmission."
2110.10854,data,108,,,"In this paper, we consider a covert communication system under FTN signaling as shown in Fig. 1, where a legitimate transmitter Alice transmits signals to its receiver Bob, while an eavesdropper Willie wants to detect the existence of the data transmission. In addition, there is a secret key shared between Alice and Bob to ensure their communication [11], [12]. For instance, the secret key can help Bob synchronize data or choose codebook. We assume that channel A-B and AW follow the Rayleigh block fading. The transmitted signals of Alice can be represented by"
2110.10854,data,218,,,"On the other hand, besides degrading the transmit rate of the legitimate data transmission, the incurred ISI can also aggravate the detection or decoding of illegal eavesdroppers. Therefore, some existing work has exploited the ISI incurred by FTN signaling to enhance the security of the data transmission [6]–[8]. In [6], the eigenvalue decomposition based scheme had been investigated to improve the secrecy rate of FTN signaling in quasi-static fading channel. In [7] and [8], the time-varied coefﬁcients of shaped ﬁlter and symbol duration were introduced to incur more severe interference to ensure secure transmission, respectively. However, the aforementioned work all focuses on exploiting FTN signaling to avoid the information from being decoded by illegal eavesdroppers, i.e., physical layer security. In contrast, covert communications, which aim to prevent the communication between the transmitter Alice and the receiver Bob, from being detected by the eavesdropper Willie, can provide stronger security [9]–[13]. For instance, in military wireless communications, preventing communications from being detected has a higher priority. Also, Alice can transmit no more than (√n) bits to Bob"
2110.11575,code,10,,,How many code ﬁles are there? (number)
2110.11575,code,10,,,What percentage of code is comments? (percentage)
2110.11575,code,11,,,"Is code modularized? (yes, no∗ , n/a)"
2110.11575,code,11,,,"Is code modularized? (yes=1, no∗=0, n/a=0)"
2110.11575,code,11,,,Number of code lines in text-based ﬁles. (number)
2110.11575,code,13,,,"Source code URL? (set of url, n/a, unclear)"
2110.11575,code,17,,,"What percentage of code is comments? (10%+=1, <10%=0)"
2110.11575,code,18,,,"Are the code identiﬁers consistent, distinctive, and meaningful? (yes, no∗ , n/a)"
2110.11575,code,18,,,"Are the code identiﬁers consistent, distinctive, and meaningful? (yes=2, no∗=0, n/a=0)"
2110.11575,code,20,,,"Is there any information on how code is reviewed, or how to contribute? (yes∗, no)"
2110.11575,code,20,,,"Is there any information on how code is reviewed, or how to contribute? (yes∗=1, no=0)"
2110.11575,code,22,,,"How many code ﬁles are there? (0-9=0, 10-49=1, 50-99=3, 100-299=4, 300-599=5, 600-999=6, 1000+=8)"
2110.11575,code,38,,,"1. The software functionality must fall within the identiﬁed domain. 2. The source code must be viewable. 3. The empirical measures listed in Section 9 should ideally be available, which implies a"
2110.11575,code,40,,,"What issue tracking tool is employed? (set of Trac, JIRA, Redmine, e-mail, discussion board, sourceforge, google code, git, BitBucket, none, unclear, other∗) ∗ given via string"
2110.11575,code,47,,,"What tools or techniques are used to build conﬁdence of correctness? (literate programming, automated testing, symbolic execution, model checking, assertions used in the code, Sphinx, Doxygen, Javadoc, conﬂuence, unclear, other∗) ∗ given via string"
2110.11575,code,66,,,"8. When designing the software, did you consider the ease of future changes? For example, will it be hard to change the structure of the system, modules or code blocks? What measures have been taken to ensure the ease of future changes and maintains? [research question 5d (maintainability), research question 5c (modiﬁability)]"
2110.11575,code,68,,,"Associated with each variability are its parameters of variation, which summarize the possible values for that variability, along with their potential binding time. The binding time is when the value of the variability is set. It could be set as speciﬁcation time, build time (when the program is being compiled) or run time (when the code is executing)."
2110.11575,code,108,,,"If we can add measures for modiﬁability and usability, we can start to measure the quality impact of software development processes, tools and techniques. For instance, we have a project (called Drasil (Szymczak et al., 2016)), which facilitates a development process that focuses on knowledge capture, followed by generation of code and other software artifacts from the captured knowledge. To understand the advantages and disadvantages of Drasil, we could measure its quality. In particular, we would like to see the impact of a generative process on the quality of usability and modiﬁability."
2110.11575,code,112,,,Last commit date. Programming languages used. Number of text-based ﬁles. Number of total lines in text-based ﬁles. Number of code lines in text-based ﬁles. Number of comment lines in text-based ﬁles. Number of blank lines in text-based ﬁles. Number of binary ﬁles. Number of total lines added to text-based ﬁles. Number of total lines deleted from text-based ﬁles. Number of total commits. Numbers of commits by year in the last 5 years. (Count from as early as possible if the project is younger than 5 years.) Numbers of commits by month in the last 12 months.
2110.11575,code,189,,,"We have outlined a methodology for assessing the state of the practice for any given research software domain. (Although the scope of the current work has been on research software, there is little in the methodology that is speciﬁc to research software, except for the interview question related to the quality of reproducibility.) When applying the methodology to a given domain, we provide a means to answer the following questions: i) What artifacts (documents, code, test cases, etc.) are present? ii) What tools are used? iii) What principles, process and methodologies are used? iv) What are the pain points for developers? v) What actions are used to improve qualities like maintainability and reproducibility? vi) What speciﬁc actions are taken to achieve the qualities of usability, traceability, modiﬁability, maintainability, correctness, understandability, unambiguity, reproducibility and visibility/transparency? vii) How does software designated as high quality by this methodology compare with top rated software by the community?"
2110.11575,code,205,,,"Given the importance of research software, scientists and engineers are pushing for methods and tools to sustainably develop high quality software. This is evident from the existence of such groups as the Software Sustainability Institute (SEI) and Better Scientiﬁc Software (BSS). Sustainability promoting groups such as these are necessary because unfortunately the current “state of the practice” for research software often does not incorporate “state of the art” Software Engineering (SE) tools and methods (Johanson and Hasselbring, 2018). The lack of SE tools and methods contributes to sustainability and reliability problems (Faulk et al., 2009). Problems with the current state of the practice are evident from embarrassing failures, like a retraction of derived molecular protein structures (Miller, 2006), false reproduction of sonoluminescent fusion (Post and Votta, 2005), and ﬁxing and then reintroducing the same error in a large code base three times in 20 year (Milewicz and Raybourn, 2018). To improve this situation, we need to ﬁrst fully understand the current state of the practice for research software development."
2110.11575,"code available, code open-source, open-source, code, open-source code",181,,,"The full template consists of 108 questions categorized under 9 qualities. The questions were designed to be unambiguous, quantiﬁable and measurable with limited time and domain knowledge. The measures are grouped under headings for each quality, and one for summary information. The summary information (shown in Figure 1) is the ﬁrst section of the template. This section summarizes general information, such as the software name, number of developers, etc. We follow the deﬁnitions given by Gewaltig and Cannon (2012) for the software categories. Public means software intended for public use. Private means software aimed only at a speciﬁc group, while the concept category is used for software written simply to demonstrate algorithms or concepts. The three categories of development models are: open source, where source code is freely available under an open source license; free-ware, where a binary or executable is provided for free; and, commercial, where the user must pay for the software product."
2110.11575,"code available, data, code package, code, package",317,,,"To improve software development methods and tools for research software, we ﬁrst need to understand the current state of the practice. Therefore, we have developed a methodology for assessing the state of the software development practices for a given research software domain. The methodology is applied to one domain at a time in recognition that software development in diﬀerent domains is likely to have adopted diﬀerent best practices. Moreover, providing a means to measure diﬀerent domains facilitates comparison of development practices between domains. For each domain we wish to answer questions such as: i) What artifacts (documents, code, test cases, etc.) are present? ii) What tools are used? iii) What principles, process and methodologies are used? iv) What are the pain points for developers? v) What actions are used to improve qualities like maintainability and reproducibility? To answer these questions, our methodology prescribes the following steps: i) Identify the domain; ii) Identify a list of candidate software packages; iii) Filter the list to a length of about 30 packages; iv) Gather source code and documentation for each package; v) Collect repository related data on each software package, like number of stars, number of open issues, number of lines of code; vi) Fill in the measurement template (the template consists of 108 questions to assess 9 qualities (including the qualities of installability, usability and visibility)); vii) Interview developers (the interview consists of 20 questions and takes about an hour); viii) Rank the software using the Analytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the questions posed above."
2110.11575,"code available, data, code package, code, package",347,,,"The methodology is applied to one domain at a time in recognition that software development in diﬀerent domains is likely to have adopted diﬀerent best practices. Moreover, providing a means to measure diﬀerent domains facilitates comparison of development practices between domains. For each domain we wish to answer questions such as: i) What artifacts (documents, code, test cases, etc.) are present? ii) What tools are used? iii) What principles, process and methodologies are used? iv) What are the pain points for developers? v) What actions are used to improve qualities like maintainability and reproducibility? To answer these questions, our methodology prescribes the following steps: i) Identify the domain; ii) Identify a list of candidate software packages; iii) Filter the list to a length of about 30 packages; iv) Gather source code and documentation for each package; v) Collect repository related data on each software package, like number of stars, number of open issues, number of lines of code; vi) Fill in the measurement template (the template consists of 108 questions to assess 9 qualities (including the qualities of installability, usability and visibility)); vii) Interview developers (the interview consists of 20 questions and takes about an hour); viii) Rank the software using the Analytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the questions posed above. A domain expert should be engaged throughout the process, to ensure that implicit information about the domain is properly represented and to assist with conducting an analysis of the commonalities and variabilities between the 30 selected packages. Using our methodology, spreadsheet templates and AHP tool, we estimate (based on our experience with using the process) the time to complete an assessment for a given domain at 173 person hours."
2110.11575,"code available, data, code package, code, package, repo",163,,,Initial 1 hour meeting with the Domain Expert plus meeting prep Identify broad list of candidate software (Section 6) Filter software list (Section 7) (10 minutes per package for 30 packages) Review software list with Domain Expert (Section 5) Domain analysis (with help of Domain Expert) (Section 8) Vet domain analysis with Domain Expert (Section 5) Gather source code and documentation for each package (10 minutes per package for 30 packages)) Collect repository based data (Section 9) (10 minutes per package for 30 packages) Measure using measurement template (Section 10) (2.5 hours per repo for 30 repos) Solicit developers for interviews Conduct interviews (1.5 hour interviews with 10 developers (assuming 1 in 3 developers agree to an interview)) AHP ranking Work with Domain Expert to vet AHP ranking Analyze data and answer research questions
2110.11575,"code package, code, data, package",132,,,"The methodology follows a systematic procedure that begins with identifying the domain and ends with answering the research questions posed above. In between we collect an authoritative list of about 30 software packages. For each package in the list we ﬁll in our measurement template. The template consists of repository related data (like number of open issues, number of lines of code, etc.) and 108 measures/questions related to 9 qualities: installability, correctness/veriﬁability, reliability, robustness, usability, maintainability, reusability, understandability and visibility/transparency. Filling in the template requires installing the software, running simple tests (like completing the getting started instructions (if present)), and searching the code, documentation and test ﬁles."
2110.11575,"code package, code, package",46,,,"Status of software package as either dead or alive, where alive is deﬁned as the presence of repository commits or software package version releases in the last 18 months. Percentage of identiﬁed issues that are closed. Percentage of code that is comments."
2110.11575,"code package, code, package",171,,,"Our methodology suggests requesting interviews with a developer from each of the 30 software package. Requests for interviews are sent to all packages so as to not cause a potential bias by singling out any subset of the list. Moreover, since not every developer will agree to the interview request, asking 30 times will typically yield a reasonable number of responses. In our experience, the response rate is between 15% and 30%. In some cases multiple developers from the same project will agree to be interviewed. When sending out interview requests, we recommend ﬁnding the contacts on the projects’ website, or code repository, or publications, or the biographic pages of the teams’ institutions. We send at most two interview request emails to a contact for each software package. Meeting will typically be held using on-line meeting software, like Zoom or Teams. This facilitates recording and automatic transcription of the meetings."
2110.11575,"code package, code, package",248,,,"The Analytical Hierarchy Process (AHP) is a decision-making technique that can be used when comparing multiple options by multiple criteria. AHP focuses on pair-wise comparisons between all options for all criteria. The advantage of pair-wise comparisons is that they facilitates a separation of concerns. Rather than worry about the entire problem, the decision maker can focus on one comparison at a time. In our work AHP is used for comparing and ranking the software packages of a domain using the quality scores that are gathered in the Measurement Template (Appendix B). AHP performs a pairwise analysis between each of the 9 quality options for each of the 30 software packages. This results in a matrix, which is used to generate an overall score for each software package for the given criteria. Smith et al. (2016a) shows how AHP is applied to ranking software based on quality measures. We have developed a tool for conducting this process. The tool includes an AHP JAR script and a sensitivity analysis JAR script that is used to ensure that the software package rankings are appropriate with respect to the uncertainty of the quality scores. The README ﬁle outlines the requirements for, and conﬁguration and usage of, the JAR scripts. The JAR scripts, source code, and required libraries are located in the same folder as the README ﬁle."
2110.11575,"code package, code, package, code available",176,,,"research questions, measurement template, and developer interviews. (Section 5) 4. Identify the broad list of candidate software packages in the domain. (Section 6) 5. Preliminary ﬁlter of software packages list. (Section 7) 6. Review software list with Domain Expert. (Section 5) 7. Domain Analysis (with the help of the Domain Expert). (Section 8) 8. Ask Domain Expert to vet domain analysis. (Section 5) 9. Gather source code and documentation for each prospective software package. 10. Collect repository based information. (Section 9) 11. Measure using measurement template. (Section 10) 12. Survey developers. (Section 11) 13. Use AHP process to rank the software packages. (Section 12) 14. Ask Domain Expert to vet AHP ranking. (Section 5) 15. Answer research questions (from Section 2) and document answers."
2110.11575,"code package, data, package",94,,,The data for each domain is used to rank the software package according to each quality dimension using AHP. The ranking is not intended to identify a single best software package. Instead the ranking is intended to provide insights on the top set of software for each quality. The top performers can be contrasted with the lesser performers to gain insight into what practices in the domain are working. Deeper insight can be obtained by combining this data with the interview data from asking each recruited developer 20 questions.
2110.11575,"code package, data, package",111,,,"The next experiment is designed to gather qualitative data regarding the modiﬁability of each software package. This proposed experiment also involves experimental subjects/participants, who in this case are asked to modify a set of software packages. The speciﬁc modiﬁcations requested will again depend on the software domain. In advance of the experiment the Domain Expert will be asked the likely changes for software in this domain. We emphasize likely changes, instead of any changes because software cannot be designed so that everything is equally easy to change (Parnas and Clements, 1986). The procedure could be along the following lines:"
2110.11575,"code package, package",6,,,software package in the domain.
2110.11575,"code package, package",13,,,5. Review installation documentation and attempt to install the software package on a
2110.11575,"code package, package",14,,,3. Participants perform modiﬁcation tasks for likely changes on each software package being
2110.11575,"code package, package",53,,,1. Tasks should be executable for subjects with novice to intermediate experience. 2. All tasks should take no more than one hour. 3. Tasks should include the basic/common use cases of the software package. 4. Include tasks that require sequential or hierarchical steps for completion.
2110.11575,"code package, package",59,,,"3. Age: The older software packages (age being measured by the last date when a change was made) are eliminated, except in the cases where an older software package appears to be highly recommended and currently in use. (The Domain Expert can be consulted on this question, if necessary.)"
2110.11575,"code package, package",110,,,"In the future, we propose an experiment for assessing the usability of a given software package. Some initial thoughts on how this might be done are recorded in this section. To do the experiment we need an experimental subject, who will be required to complete tasks with the software being studied. The interaction with the software will allow the study subject to experience the software’s usability. The tasks for the subject to complete would vary by domain; therefore, the tasks would be selected with the help of the Domain Expert. Criteria for selecting candidate tasks are as follows:"
2110.11575,"code package, package",118,,,"The current methodology is constrained by limited resources. A 4 hour cap on the measurement time for each software package limits what can be assessed. Within this limit, we can’t measure some important qualities, like usability and modiﬁability. In the future, we propose a more time-consuming process that would capture these other quality measures. To improve the feasibility, the more time consuming measurements would not have to be completed for all 30 packages. Instead, a short list could be identiﬁed using the output of the AHP ranking to select the top projects, or to select a sample of interesting projects across the quality spectrum."
2110.11575,"code package, package",136,,,"The Measurement Template is found in Appendix B. This template is used to track measurements and quality scores for all of the software packages in the domain. For each software package, we ﬁll-in the template questions. This process can take between 1 to 4 hours for each package. Project developers can be contacted for help regarding installation, if necessary, but a cap of about 2 hours should be imposed on the entire installation process, to keep the overall measurement time feasible. To save time, a blank measurement template spreadsheet has been prepared, with the measures as rows. An excerpt of the spreadsheet is shown in Figure 1. A column should be added to this template for each software package to be measured."
2110.11575,data,3,,,9.1 Raw Data
2110.11575,data,3,,,9.2 Processed Data
2110.11575,data,10,,,The following measures are calculated from the raw data:
2110.11575,data,10,,,The following raw data measures are extracted from repositories:
2110.11575,data,14,,,"1. Survey participants to collect pre-experiment data (background, experience of subjects"
2110.11575,data,14,,,"2. Survey study participants to collect pre-experiment data (background, experience of"
2110.11575,data,28,,,"On most of the taps of this web page, the data can be downloaded for more analytics by clicking the menu button beside the data-range section."
2110.11575,data,33,,,"Most of the data to be collected should be straightforward from reviewing the measurement template. However, in a few cases extra guidance is necessary to eliminate ambiguity, as follows:"
2110.11575,data,36,,,"To begin our re-boot of the previous methodology we critically assessed, and subsequently modiﬁed, our previous set of measures. In addition, the following data has been added to the new methodology:"
2110.11575,data,38,,,This appendix covers the tools used for collecting repository based data (Section 9). The two tools covered are git_stats and scc. The tools do not have to be used in any particular order.
2110.11575,data,41,,,"Does the software handle unexpected/unanticipated input (like data of the wrong type, empty input, missing ﬁles or links) reasonably? (a reasonable response can include an appropriate error message.) (yes, no∗ )"
2110.11575,data,41,,,"Does the software handle unexpected/unanticipated input (like data of the wrong type, empty input, missing ﬁles or links) reasonably? (a reasonable response can include an appropriate error message.) (yes=5, no∗=0)"
2110.11575,data,47,,,"To answer the above research questions (Section 2), we systematically measure the quality of the software through data collection. An overview of the measurement process is given in the following steps, starting from determining a domain that is suitable for measurement:"
2110.11575,data,48,,,"The following are the research questions that we wish to answer for each of our selected domains. In addition to answering the questions for each domain, we also wish to combine the data from multiple domains to answer these questions for research software in general."
2110.11575,data,52,,,"The Domain Experts are asked to vet the collected data and analysis. In particular, they are asked to vet the proposed list of software packages, the domain analysis and the AHP ranking. These interactions can be done either electronically or with in-person (or virtual) meetings."
2110.11575,data,70,,,"Combining the quantitative data from the measurement template with the interview results, along with the domain experts knowledge, we can determine the current state of the practice for domain X. Using our methodology, spreadsheet templates and AHP tool, we estimate (based on our experience with using the process) the time to complete an assessment for a given domain at 173 person hours."
2110.11575,data,72,,,"With the wealth of data from assessing the state of practice for multiple domains, the next step is a meta-analysis. We would look at how the diﬀerent domains compare. What lessons from one domain could be applied in other domains? What (if any) diﬀerences exist in the pain points between domains? Are there diﬀerences in the tools, processes, and documentation between domains?"
2110.11575,data,74,,,"The full methodology is presented in the sections that follow. Section 2 highlights the research questions that are to be answered for each measured domain. These questions are answered by the data collected using the process outlined in Section 3. The major steps in the process are outlined in Sections 4 – 12. Following this, the time required for assessing a single domain is estimated in Section 13."
2110.11575,data,79,,,"Some quality measurements rely on gathering raw and processed data from software repositories. We focus on data that is reasonably easy to collect, which we combine and analyze. The measures that are collected relate to the research questions (Section 2). For instance, we collect data to see how large a project is, to ascertain a project’s popularity, and to determine whether to project is being actively developed."
2110.11575,data,82,,,9 9.1 Raw Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 9.2 Processed Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2110.11575,data,101,,,"In the previous “state of the practice” project, we measured 30 software projects for each domain, but the measures were relatively shallow. With this re-boot we still target about 30 software examples from each domain, but we are now collecting more data. In keeping with the previous project, we still have the constraint that the work load for applying the methodology to a given domain needs to be feasible for a team as small as one individual, and for a time that is short, ideally around a person month per domain.1"
2110.11575,data,120,,,"In the proposed methodology, the collected data is combined to rank the software within each domain using the Analytic Hierarchy Process (AHP) (Saaty, 1980). As in the previous measurement exercise, we use AHP to develop a list of software ranked by quality. However, in the new process we do not stop with this list. The Domain Expert is consulted to verify the ordering, and to discuss the decisions that led to the ranking. The AHP process is used to facilitate a conversation with the Domain Expert as a means to deepen our understanding of the software in the domain, and the needs of typical developers."
2110.11575,data,122,,,"As mentioned in the introduction (Section 1), the Domain Expert is an important member of the state of the practice assessment team. Pitfalls exist if non-domain experts attempt to acquire an authoritative list of software, or perform a commonality analysis, or try to deﬁnitively rank the software. The main source of problems for non-domain experts is that they can only rely on information that is available on-line, but on-line data has two potential problems: i) the on-line resources could have false or inaccurate information; and, ii) the on-line resources could leave out relevant information that is so in-grained with experts that nobody thinks to explicitly record the information."
2110.11575,data,255,,,"The table below lists the full set of measures that are assessed for each software product. The measures are grouped under headings for each quality, and one for summary information. Following each measure, the type for a valid result is given in brackets. Many of the types are given as enumerated sets. For instance, the response on many of the questions is one of “yes,” “no,” or “unclear.” The type “number” means natural number, a positive integer. The types for date and url are not explicitly deﬁned, but they are what one would expect from their names. In some cases the response for a given question is not necessarily limited to one answer, such as the question on what platforms are supported by the software product. Case like this are indicated by “set of” preceding the type of an individual answer. The type in these cases are then the power set of the individual response type. In some cases a superscript ∗ is used to indicate that a response of this type should be accompanied by explanatory text. For instance, if problems were caused by uninstall, the reviewer should note what problems were caused. The template also include 3 sections at the end for summarizing the repository based metrics. A blank measurement template spreadsheet is available to save time with data entry."
2110.11575,"data, code",38,,,"commonality analysis. Project repository related data, such as the number of ﬁles, number of lines of code, percentage of issues that are closed, etc. Interviews with software developers in domain X."
2110.11575,"download, data",5,,,6. Download the data
2110.11575,"download, package",15,,,"Follow the oﬃcial instructions, or the following demo, download the installation package:"
2110.11575,github,5,,,preference for GitHub-style repositories.
2110.11575,github,11,,,go get -u github . com / boyter / scc /
2110.11575,github,25,,,"Which version control system is in use? (svn, cvs, git, Github, unclear, other∗) ∗ given via string"
2110.11575,github,29,,,# change [ project path ] to your desired folder cd [ project path ] git clone https :// github . com / nroduit / Weasis . git
2110.11575,github,40,,,"5. Continuous integration: Search the software artifacts for any mention of continuous integration. The search function on GitHub can help. In some cases, yaml ﬁles will provide a hint that continuous integration is employed."
2110.11575,github,65,,,"Once the domain of interest is identiﬁed, the candidate software for measuring can be found through search engine queries targeting authoritative lists of software. Potential places to search include GitHub, swMATH and domain related publications, such as review articles. Domain Experts are also asked for their suggestions and are asked to review the initial draft of the software list."
2110.11575,github,96,,,"1. Initial release date: Mark the release year if an exact date is not available. 2. Publications about the software: A list of publications can be found directly on the website of some software packages. For others use Google Scholar or a similar index. 3. Is there evidence that performance was considered?: Search the software artifacts for any mention of speed, storage, throughput, performance optimization, parallelism, multi-core processing, or similar considerations. The search function on GitHub can help."
2110.11575,"github, code",43,,,"Are artifacts available? (List every type of ﬁle that is not a code ﬁle – for examples please look at the ‘Artifact Name’ column of https://gitlab.cas.mcmaster.ca/SEforSC/se4sc/-/blob/gitsvn/GradStudents/Olu/ResearchProposal/Artifacts_MiningV3.xlsx) (yes∗, no, unclear) ∗list via string"
2110.11575,"github, code",46,,,Are artifacts available? (List every type of ﬁle that is not a code ﬁle – for examples please look at the ‘Artifact Name’ column of https://gitlab.cas.mcmaster.ca/SEforSC/se4sc/-/blob/gitsvn/GradStudents/Olu/ResearchProposal/Artifacts_MiningV3.xlsx) (Rate 0 – 2 depending on how many and perceived quality)
2110.11575,"github, code package, repo, package",29,,,"4. If the software package is found on git, gather the measurements for the Repo Metrics the GitHub section found near the bottom of the spreadsheet."
2110.11575,"github, data repos, publicly available, data",111,,,"So that the collected data for a given domain can beneﬁt the scientiﬁc community, our recommendation is that all collected data be made public. For instance, the data collection for each domain can be put on a GitHub repository. In addition to the project record left on GitHub, the ﬁnal data can be exported to Mendeley Data. As an example, the measurements for the state of the practice for GIS software are available on Mendeley. Ideally the full analysis of the state of the practice for domain X will also be published in a suitable journal, allowing for dissemination/feedback and communication."
2110.11575,"github, data, data repository, code",159,,,"Section 9.1 lists the raw data that is collected. Some of this data can be observed from GitHub repository metrics. The rest can be collected using freeware tools. GitStats is used to measure the number of binary ﬁles as well as the number of added and deleted lines in a repository. This tool is also used to measure the number of commits over diﬀerent intervals of time. Sloc Cloc and Code (scc) is used to measure the number of text based ﬁles as well as the number of total, code, comment, and blank lines in a repository. These tools were selected due to their installability, usability, and ability to gather the empirical measures listed below. Details on installing and running the tools can be found in Appendix A. Section 9.2 introduces the required processed data, which is calculated using the raw data."
2110.11575,"github, repo",5,,,Oﬃcial Manual: GitHub repo
2110.11575,"github, repo",7,,,Repo Metrics (Measured via GitHub)
2110.11575,"github, repo",38,,,# change [ git path ] to the url of your target repo git clone [ git path ] # e . g . git clone https :// github . com / nroduit / Weasis . git
2110.11575,"github, repo",38,,,"Make sure the target repo (the repo to be analyzed, not the repo of this tool) is on your machine. In this demo, the target repo is downloaded from a GitHub repo:"
2110.11575,"github, repo, code",5,,,Source Code: GitHub repo
2110.11575,open-source,11,,,3. The software packages must have open source options.
2110.11575,open-source,13,,,"Development model? (open source, freeware, commercial, unclear)"
2110.11575,package,13,,,"Are required package versions listed? (yes, no, n/a)"
2110.11575,package,13,,,"Are required package versions listed? (yes=1, no=0, n/a=1)"
2110.11575,package,127,,,"As in Smith et al. (2016a), Virtual machines (VMs) are used to provide an optimal testing environments for each package. VMs were used because it is easier to start with a fresh environment without having to worry about existing libraries and conﬂicts. Moreover, when the tests are complete the VM can be deleted, without any impact on the host operating system. The most signiﬁcant advantage of using VMs is to level the playing ﬁeld. Every software install starts from a clean slate, which removes “works-on-my-computer” errors. When ﬁlling in the measurement template spreadsheet, the the details for each VM should be noted, including hypervisor and operating system version."
2110.11575,publicly available,29,,,"What issue tracking tool is employed? (nothing=0, email of other private=1, anything public or accessible by all devs (eg git) = 2)"
2110.11575,publicly available,47,,,"Software Category? The concept category includes software that does not have an oﬃcially released version. Public software has a released version in the public domain. Private software has a released version available to authorized users only. (concept, public, private)"
2110.11575,python,41,,,"Programming language(s)? (set of FORTRAN, Matlab, C, C++, Java, R, Ruby, Python, Cython, BASIC, Pascal, IDL, unclear, other∗) ∗ given via string"
2110.11575,repo,6,,,3. Prepare the target repo
2110.11575,repo,10,,,Number of people watching this repo. (number)
2110.11575,repo,13,,,Repo Metrics - GitStats section found near the bottom of the spreadsheet.
2110.11575,repo,13,,,Repo Metrics – SCC section found near the bottom of the spreadsheet.
2110.11575,repo,24,,,# make sure [ repo path ] is the target repo path cd [ repo path ] # use scc to generate analytics scc
2110.11575,repo,26,,,Number of developers (all developers that have contributed at least one commit to the project) (use repo commit logs) (number)
2110.11575,repo,67,,,# make sure [ repo path ] is the target repo path # the [ output path ] can be anywhere you desire git_stats generate -p [ repo path ] -o [ output path ] # e . g . git_stats generate -p / home / user / git - stats / Weasis -o # / home / user / git - stats / Weasis - analytics
2110.15667,data,31,,,"[36] T. Hur, L. Kim, and D. K. Park, “Quantum convolutional neural network for classical data classiﬁcation,” arXiv preprint arXiv:2108.00661, 2021."
2110.15667,data,35,,,"[40] R. LaRose and B. Coyle, “Robust data encodings for quantum classiﬁers,” Physical Review A, vol. 102, no. 3, p. 032420, 2020."
2110.15667,data,39,,,"[24] S. Y.-C. Chen, T.-C. Wei, C. Zhang, H. Yu, and S. Yoo, “Quantum convolutional neural networks for high energy physics data analysis,” arXiv preprint arXiv:2012.12177, 2020."
2110.15667,data,43,,,"[49] M. Weigold, J. Barzen, F. Leymann, and M. Salm, “Data encoding patterns for quantum computing,” in HILLSIDE Proc. of Conf. on Pattern Lang. of Prog. 22, 2019."
2110.15667,data,53,,,"[16] R. Varatharajan, G. Manogaran, and M. Priyan, “A big data classiﬁcation approach using lda with an enhanced svm method for ecg signals in cloud computing,” Multimedia Tools and Applications, vol. 77, no. 8, pp. 10195–10215, 2018."
2110.15667,data,54,,,"[15] F. Yuan, A. Karatzoglou, I. Arapakis, J. M. Jose, and X. He, “A simple convolutional generative network for next item recommendation,” in Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pp. 582–590, 2019."
2110.15667,data,89,,,"3.1.3. Training Setup. In Experiment A, after applying the non-trainable quantum ﬁlter to transform the original image data into feature maps, we use a mini-batch of 32 and Adam optimizer with a learning rate of 0.01 to train each model for 30 epochs. In Experiment B, due to the computational cost of training parametric quantum circuits involved in the trainable quantum ﬁlter, we reduce the batch size to four and train all models for 20 epochs with other hyperparameters remaining unchanged."
2110.15667,data,120,,,"• ENCODING MODULE. In this module, classical data is encoded into a quantum state which will be further processed in the quantum convolutional circuit. There exist various encoding methods such as angle encoding, amplitude encoding and basis encoding. A summary of them can be found in the literature [49]. Among these methods, angle encoding is the most commonly used encoding approach. In this encoding scheme, the classical input is treated as the rotation angle of a single-qubit rotation gate (e.g. RY rotation gate). For example, a classical variable or feature a can be encoded by RY (a) which is applied on some"
2110.15667,data,179,,,"Convolutional neural networks (CNNs), proposed by Yann LeCun et al [1] in 1989, are one of the most powerful algorithms in the context of deep learning. The main advantage of CNNs is that they use multiple feature extraction stages to automatically and accurately learn important features from the data without any human supervision. Due to this advantage, CNNs have been tremendously successful in a broad array of high-level computer vision problems, including image recognition [2], [3], [4], [5] , object detection [6], [7], [8], and image segmentation [9], [10], [11]. In recent years, with further development in deep learning , CNNs have also been demonstrated to show promising performances in other machine learning areas such as time series forecasting [12], [13], speech recognition [14] and recommendation system [15]."
2110.15667,data,183,,,"The key difference between our method and existing QCNNs is that the dilated convolution is employed for the quantum convolutional layer. So the quantum layer in QDCNNs is called quantum dilated convolutional (QDC) layer. An example of a QDC layer is illustrated in Fig. 1. Due to the mechanism of dilated convolution , the quantum kernel in our model generally covers larger image patches (i.e. receptive ﬁelds ). For example, a 2 × 2 quantum dilated convolution with dilation rate of 3 has a receptive ﬁeld of 4×4 while the standard quantum convolution with the same kernel size has only a receptive ﬁeld of 2 × 2 . It is noteworthy that even though the quantum dilated convolution is able to expand the receptive ﬁeld the number of data points that are fed into the quantum convolution circuit is the same as the one for the standard quantum convolution. This means that the quantum dilated convolution does not requires more qubits than the standard quantum convolution with the same kernel size."
2110.15667,data,322,,,"A few works have been done to investigated how to reduce the runtime complexity of QCNNs. In the ﬁrst family of works, a small number of qubits required for the quantum circuit is achieved by using classical data pre-processing techniques to reduce the dimension of the input features fed into the quantum (convolutional) layer. For instance, Pramanik et al. [35] employ principal component analysis (PCA) to reduce the VGG-16 features for the quantum variational classiﬁer (VQC), while Hur et al. [36] adopt autoencoding (AutoEnc) for the dimensionality reduction. Nevertheless, the performance of the model trained in this way is likely to be compromised by the limited expressive power of the reduced features, as shown in [35]. The second family of works focus on how to efﬁciently encode classical data into quantum states. Schuld and Killoran [37] propose and implement the amplitude encoding for variational quantum circuits, which is explored further in [38] for Flexible Representation of Quantum Images (FRQI). This type of encoding method is efﬁcient in terms of required qubits for data encoding but it relies on too deep quantum circuits which are unpractical on NISQ devices. In a different direction, some recent researchers [26], [39], [40] propose angel encoding (also referred to as qubit encoding) and its variants (e.g. dense angel encoding) which use a constant quantum circuit depth for state preparation. This encoding scheme requires one qubit to encode one or a limited number of components of the input feature vector and thus is not efﬁcient for high-dimensional input features from a resource prospective. To trades off these two encoding methods mentioned above, Hur et al."
2110.15667,data,332,,,"[36] adopt autoencoding (AutoEnc) for the dimensionality reduction. Nevertheless, the performance of the model trained in this way is likely to be compromised by the limited expressive power of the reduced features, as shown in [35]. The second family of works focus on how to efﬁciently encode classical data into quantum states. Schuld and Killoran [37] propose and implement the amplitude encoding for variational quantum circuits, which is explored further in [38] for Flexible Representation of Quantum Images (FRQI). This type of encoding method is efﬁcient in terms of required qubits for data encoding but it relies on too deep quantum circuits which are unpractical on NISQ devices. In a different direction, some recent researchers [26], [39], [40] propose angel encoding (also referred to as qubit encoding) and its variants (e.g. dense angel encoding) which use a constant quantum circuit depth for state preparation. This encoding scheme requires one qubit to encode one or a limited number of components of the input feature vector and thus is not efﬁcient for high-dimensional input features from a resource prospective. To trades off these two encoding methods mentioned above, Hur et al. [36] further develop a hybrid encoding approach which requires fewer number of qubits than the angel encoding and use shallower quantum circuit depth than the amplitude encoding. Moreover, Henderson et al. [22] employ a threshold based encoding technique to reduce the input-state space and made it possible to obtain the output feature map through a look-up table during the quantum convolution process without needing to execute the same quantum circuit repeatedly on image segments. This method is easy to implement, but it is infeasible on real quantum devices, as mentioned in [22]."
2110.15667,"data, python, open-source",189,,,"3.1.4. Experimental Environment. Experiments are conducted on the local computer with a 6-core CPU (2.2 GHz) by using PennyLane [53], Qulacs [54] and PyTorch [55]. PennyLane is an open source python-based framework that enables the automatic differentiation for hybrid quantumclassical computations. It is compatible with mainstream machine learning frameworks such as TensorFlow [56] and PyTorch, and it has a large plugin ecosystem which offers access to numerous quantum devices (i.e. simulators and hardware) from different vendors including IBM, Google, Microsoft, Rigetti and QunaSys. In Experiment A, we perform the quantum processing of the original image data by using the Qulacs simulator [57] which is a high-performance C++ quantum simulator and made available through the community contributed PennyLane-Qulacs plugin [58]. In Experiment B, considering the large amount of quantum circuit executions required in the scheme of parameter-shift rule, we train all hybrid models by using instead the builtin Pennylane simulator default.qubit which supports backpropagation method for PyTorch interface."
2110.15667,database,45,,,"[50] L. Deng, “The mnist database of handwritten digit images for machine learning research [best of the web],” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 141–142, 2012."
2110.15667,dataset,1,,,Dataset
2110.15667,dataset,21,,,• We conduct experiments using MNIST and FashinMNIST datasets and demonstrate the superior performance of QDCNN models over QCNN models.
2110.15667,dataset,24,,,"3.1.1. Dataset. We choose the image benchmark MNIST and Fashion-MNIST datasets [50], [51] for our experiments."
2110.15667,dataset,25,,,"TABLE 2: Results of Experiment B on MNIST and FashionMNIST datasets, reported for three hybrid-classical neural network models with trainable quantum ﬁlters."
2110.15667,dataset,33,,,"[51] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms,” arXiv preprint arXiv:1708.07747, 2017."
2110.15667,dataset,70,,,"TABLE 1: Results of Experiment A on MNIST and FashionMNIST datasets, reported for three hybrid-classical neural network models with non-trainable quantum ﬁlters. QDCNN_r2 and QDCNN_r3 are hybrid models with one QDC layer with dilation rate r = 2 and r = 3, respectively. QCNN represents the hybrid model with one standard quantum convolutional layer or equivalently QDC layer with dilation rate r = 1."
2110.15667,dataset,96,,,"The MNIST dataset contains 10 different classes of handwritten digits from ‘0’ to ‘9’ , while the Fashion-MNIST dataset is a collection of 10 different shapes of t-shirts, dresses, shoes, etc. Both of these datasets have 60,000 training samples, and 10,000 test samples of 28-by-28 gray scale pixel images. Due to the expensive training and validation, we pick two subsets of the entire MNIST and FashionMNIST datasets, respectively, both of which consist of 1,000 balanced training samples and 200 balanced testing samples."
2110.15667,dataset,133,,,"In this work, we propose the QDCNN model, which adopts the idea of dilated convolution in deep learning to the quantum neural network. We show through empirical evidence that the QDCNN model outperforms the recent QCNN method in terms of computation time and recognition accuracy. In particular, we ﬁnd that the quantum dilated convolution with a larger dilation rate generally contribute to a better model performance. Dilated convolution has been extensively studied in the area of deep learning, but little work has been done to explore it in the context of quantum machine learning. Our work constitutes a ﬁrst step in this direction. With the promising results on both MNIST and Fashion-MNIST datasets, our QDCNN approach deserves further investigation in the future."
2110.15667,dataset,170,,,"Abstract—In recent years, with rapid progress in the development of quantum technologies, quantum machine learning has attracted a lot of interest. In particular, a family of hybrid quantum-classical neural networks, consisting of classical and quantum elements, has been massively explored for the purpose of improving the performance of classical neural networks. In this paper, we propose a novel hybrid quantum-classical algorithm called quantum dilated convolutional neural networks (QDCNNs). Our method extends the concept of dilated convolution, which has been widely applied in modern deep learning algorithms, to the context of hybrid neural networks. The proposed QDCNNs are able to capture larger context during the quantum convolution process while reducing the computational cost. We perform empirical experiments on MNIST and Fashion-MNIST datasets for the task of image recognition and demonstrate that QDCNN models generally enjoy better performances in terms of both accuracy and computation efﬁciency compared to existing quantum convolutional neural networks (QCNNs)."
2110.15667,dataset,317,,,"• QDCNN Model. We employ the architecture of the most basic convolution-inspired hybrid quantumclassical neural network. Our QDCNN model consists of one QDC layer with one ﬁlter and one fullyconnected layer with 10 neurons. The kernel size and stride for the QDC layer is selected as 2 × 2 and 2 respectively without speciﬁcation. The quantum circuit ansatz of the QDC layer is designed as below. The 1 variable/qubit encoding scheme is adopted to encode the input image. Speciﬁcally, 2 × 2 pixels are encoded into a 4-qubit state using RY rotation gates. Note that, these 2 × 2 pixels are not adjacent to each other in the input image, due to the quantum dilated convolution. The resulting 4-qubit state is further transformed by a following random parameterized quantum circuit which might creates the entanglement. The decoding method follows the same spirit of [52], in which each expectation value is mapped to a different channel of a single output pixel. Consequently, even though there is only one ﬁlter, the quantum layer can transform the input 2-D image into four feature maps. This type of quantum layer might beneﬁt the model performance as it allows for correlation among channels of the output feature maps. In both cases of MNIST and Fashion-MNIST datasets, the QDC layer extracts from the 28 × 28 input image a feature tensor of size 13 × 13 × 4, which is then transformed to 10 output probabilities by the fully-connected layer with softmax activation. To evaluate how the dilation rate impacts the model performance, we consider two QDCNN models with dilation rate r = 2 and r = 3. We refer to these two models as QDCNN r2 and QDCNN r3 respectively for the rest of the paper."
2111.01351,data,53,,,"[Ding et al., 2019] Xinfang Ding, Xinxin Yue, Rui Zheng, Cheng Bi, Dai Li, and Guizhong Yao. Classifying major depression patients and healthy controls using eeg, eye tracking and galvanic skin response data. Journal of affective disorders, 251:156–161, 2019."
2111.01351,data,66,,,"[Sun et al., 2019] Shuting Sun, Xiaowei Li, Jing Zhu, Ying Wang, Rong La, Xuemin Zhang, Liuqing Wei, and Bin Hu. Graph theory analysis of functional connectivity in major depression disorder with high-density resting state eeg data. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 27(3):429–439, 2019."
2111.01351,data,122,,,"Recently, [Liu et al., 2020] used the PLI method to calculate the connectivity matrix and the graph theory-based method to measure the topological structure of the BFCN between different frequency bands in patients with depression. Ding et al. used a multimodal machine learning method, including EEG, eye tracking, and dynamic skin response data as input to classify depression patients and healthy controls [Ding et al., 2019]. However, most of these methods use single-layer networks to simulate the associations between brain functions, which is not conducive to in-depth analysis of the complex relationships between brain functions and exploring the cognitive and neural mechanisms that lead to MDD."
2111.01351,"publicly available, dataset",89,,,"3 Experiments and Analysis This study uses the public dataset MODMA [Sun et al., 2019] to construct the single-brain and the multi-BFCN to recognize MDD. A total of patients diagnosed with depression (female/male=11/13, 30.88±10.37 years old) 24 MDD were used as experimental subjects, and 29 NC (female/male=9/20, 31.45±9.15 years old) were used as control groups. There was no signiﬁcant difference in age (t=0.214, p=0.832) or gender (x2=1.224, p=0.269) between groups."
2111.03516,data,3,,,Counterfactual Data Augmentation
2111.03516,data,6,,,2.4 Using Counterfactuals for Data Augmentation
2111.03516,data,7,,,4 Competitive Tests of Data Augmentation Methods
2111.03516,data,7,,,Using a Counterfactual Method for Data Augmentation
2111.03516,data,11,,,2.1 Data Sampling Methods for the Class Imbalance Problem: SMOTE
2111.03516,data,12,,,"Keywords: Counterfactual, Class Imbalance Problem, Case-based Reasoning, Data"
2111.03516,data,12,,,"Knowledge and Data Engineering, 21(9), 1263-1284."
2111.03516,data,12,,,"for Balancing Machine Learning Training Data. SIGKDD Explor, 20–29."
2111.03516,data,12,,,problems using an instance-based counterfactual method that generates synthetic data-points in the
2111.03516,data,14,,,"From a data augmentation perspective, basing synthetic counterfactuals on sparse pairs also makes"
2111.03516,data,14,,,"Overall, the counterfactual data-augmentation method (CFA) performs better than all other"
2111.03516,data,14,,,Table 4: AUC values for the RF classifier for each Data Augmentation Method
2111.03516,data,14,,,Table 5: AUC values for the k-NN classifier for each Data Augmentation Method
2111.03516,data,14,,,Table 6: AUC values for the LR classifier for each Data Augmentation Method
2111.03516,data,14,,,Table 7: AUC values for the MLP classifier for each Data Augmentation Method
2111.03516,data,14,,,Two subtle differences that distinguish this data augmentation version of the algorithm from its
2111.03516,data,14,,,"existing datapoints. Accordingly, the extension of these techniques to data augmentation problems"
2111.03516,data,14,,,generate synthetic data for a crop-growth prediction problem. Their problem domain involved a
2111.03516,data,15,,,Data level solutions to the class imbalance problem are dominated by three main approaches:
2111.03516,data,15,,,Foundation Ireland (SFI) to the Insight Centre for Data Analytics under Grant Number
2111.03516,data,15,,,data-level sampling techniques for the class imbalance problem and (ii) counterfactual methods for
2111.03516,data,15,,,"likely to be preserved in sparse-pairs; hence, generated synthetic data-points using these sparse"
2111.03516,data,15,,,limited observational examples.  Pitis et al. [56] proposed Counterfactual Data Augmentation
2111.03516,data,15,,,"seen as supporting the creation of synthetic data-points in the minority class, using information"
2111.03516,data,15,,,"specific measurements of climate and grass growth on dairy farms in Ireland (N=70,091 data-points"
2111.03516,data,16,,,"Insight Centre for Data Analytics, University College Dublin, Belfield, Dublin 4, Ireland"
2111.03516,data,16,,,"SMOTE-RSB does quite well; However, when data augmentation can make a contribution, it"
2111.03516,data,16,,,"did not consider using their counterfactual method for data augmentation. In a student project,"
2111.03516,data,16,,,"from these known counterfactual pairs. However, few XAI techniques have been applied to data"
2111.03516,data,16,,,minority instances may be better than others to use in this data-generation process. Another issue
2111.03516,data,16,,,"models.  In this paper, we advance a novel data augmentation method (adapted from"
2111.03516,data,16,,,the classes (SMOTE-ENN [5] uses a related data-cleaning approach involving the Edited Nearest
2111.03516,data,16,,,"their “blind” perturbation of test-items, they sometimes generate out-of-distribution, invalid data 10"
2111.03516,data,17,,,"In this paper, we explore a novel approach to both the class imbalance and data augmentation"
2111.03516,data,17,,,"already in the training data.  In data augmentation, the test instances used to generate synthetic"
2111.03516,data,17,,,"data augmentation to handling the class imbalanced problem. On the other hand, Figure 4 shows"
2111.03516,data,17,,,evaluate classification models for imbalanced data sets [26]. The ROC curve is a two-dimensional
2111.03516,data,17,,,on solutions at the data or algorithm levels. Data level solutions attempt to change the distribution
2111.03516,data,17,,,"using any data augmentation method. Finally, to determine the optimal values for all classifiers,"
2111.03516,data,18,,,"data augmentation.  Temraz, et al. [67] used the present instance-guided counterfactual method to"
2111.03516,data,18,,,"have also proved useful in data augmentation for deep learning models, when new, synthetic data 2"
2111.03516,data,18,,,"instances” are called unpaired instances.  For data augmentation purposes, the “test instances” are"
2111.03516,data,19,,,"[47]. Classically, this problem arises in binary classification tasks when most of data comes from"
2111.03516,data,19,,,"individual farms in the coming week using this historical data.  But, with climate change, there are"
2111.03516,data,20,,,"[29] Hasan, M. G. (2020). Use case of counterfactual examples: Data augmentation."
2111.03516,data,20,,,"adding multiple copies of some of the minority classes to the training data [46]. Whereas, with"
2111.03516,data,20,,,begun to apply these counterfactual XAI methods to class-imbalance and data augmentation prob lems (see section 2.4).
2111.03516,data,20,,,"examples from the majority class, data can be discarded that may be important [53]. The third"
2111.03516,data,20,,,instances in the training data that do not take part in native counterfactuals; this is why the “test
2111.03516,data,21,,,"to comprehend [22, 23].  However, this rationale from XAI does not apply to data augmentation."
2111.03516,data,23,,,"[31] He, H., & Garcia, E. (2009). Learning from imbalanced data. IEEE Transactions on"
2111.03516,data,24,,,"Provost, F. (2000). Machine Learning from Imbalanced Data Sets 101. AAAI’2000 Workshop on Imbalanced Data Sets."
2111.03516,data,26,,,1 An unpublished paper [48] reports an identical method to Wachter et al.’s. [70] counterfactual optimization method for data
2111.03516,data,26,,,method increases the size of available training data with counterfactual examples by stitching to gether locally-independent subsamples from the environment. They found that CoDA significantly
2111.03516,data,27,,,"[26] Han, J., & Kamber, M. (2006). Data Mining: Concepts and Technique (2nd ed.)."
2111.03516,data,27,,,"[74] Wong, S. C., Gatt, A., Stamatescu, V., & McDonnell, M. D. (2016). Understanding Data"
2111.03516,data,28,,,"Figure 3: Selected examples for ROC curves where CFA outperformed SMOTE-based methods, obtained for the six methods using the four classifiers on different data sets."
2111.03516,data,28,,,"Figure 4: Selected examples for ROC curves where SMOTE variants outperformed CFA, obtained for the six methods using the four classifiers on different data sets."
2111.03516,data,29,,,"In data augmentation, the 2-feature-difference may work because it produces very minimally-dif ferent counterfactual pairs; so, they produce very simple adaptation rules in which most features"
2111.03516,data,29,,,"Which is Best for Handling Unbalanced Classes with Unequal Error Costs?. In Proceedings of the 2007 International Conference on Data Mining (DMIN), 35-41."
2111.03516,data,31,,,Over-Sampling Technique (SMOTE) has become a very popular method for solving class-imbal ance issues in traditional ML and has also been applied to the data augmentation problem in deep
2111.03516,data,31,,,"SMOTE, B-SMOTE, ADASYN, SVM-SMOTE, SL-SMOTE and SMOTE-RSB. For our experi ments, we oversample the minority class using each of the data augmentation methods until we"
2111.03516,data,31,,,"Zheng, Z., Wu, X., & Srihari, R. (2004). Feature selection for text categorization on imbalanced data. ACM SIGKDD Explorations, 80-89."
2111.03516,data,31,,,"data-points. Interestingly, Temraz et al’s experiments showed that that the instance-guided meth ods did better than optimization-methods in this problem domain (specifically, the DICE method"
2111.03516,data,31,,,"of the imbalanced data by re-sampling the original data [9, 10, 11, 12, 27, 30, 31, 46, 58, 59];"
2111.03516,data,32,,,"Pitis, S., Creager, E., & Garg, A. (2020). Counterfactual data augmentation using locally factored dynamics. Advances in Neural Information Processing Systems."
2111.03516,data,32,,,"XAI is the backdrop and motivation for applying this counterfactual method to the data augmenta tion. As we saw earlier, initial tests on a crop-growth prediction problem showed that generated"
2111.03516,data,32,,,"different ways. For example, SVM-SMOTE [54] uses an SVM to approximate the decision bound ary and then generates new synthetic data along the lines joining each minority-class support-vector"
2111.03516,data,32,,,the actual feature-values of instances (not interpolated values) that are the close to existing in stances thus populating the minority class with plausible adaptations of existing data.    If this
2111.03516,data,33,,,"Shorten, C., & Khoshgoftaar, T. M. (2019). A survey on image data augmentation for deep learning. Journal of Big Data 6, 1, 1-48."
2111.03516,data,33,,,applied to generate additional minority instances; this method acts like a data-cleaning step to re move generated instances that might be noise.  Many of these methods improve on B-SMOTE’s
2111.03516,data,35,,,"ADASYN, proposed by He et al. [30], generates minority class instances according to their distri butions, generating more synthetic data from minority instances that are harder to learn compared"
2111.03516,data,35,,,"pairs should be more likely to be valid and within-distribution.  Second, there is a critical differ ence between the XAI and data augmentation contexts with respect to the selection of test instances."
2111.03516,data,35,,,"without any data augmentation applied. Several standard measures were used to assess the perfor mance of the four methods; namely, Precision, Recall, F1, and plots of ROC curves."
2111.03516,data,36,,,"plausible counterfactuals and appears to avoid the out-of-distribution pitfalls that arise in optimi zation techniques (see [17, 39, 64]).  From a data augmentation perspective, this method can be"
2111.03516,data,39,,,"Jiang, K., Lu, J., & Xia, K. (2016). A Novel Algorithm for Imbalance Data Classification Based on Genetic Algorithm Improved SMOTE. Arabian Journal for Science and Engineering, 3255–3266."
2111.03516,data,39,,,"Ling, C., & Li, C. (1998). Data Mining for Direct Marketing Problems and Solutions. In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining (KDD 98)."
2111.03516,data,40,,,"[27] Han, H., Wang, W.-Y., & Mao, B.-H. (2005). Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning. In International Conference on Intelligent Computing, 878-887."
2111.03516,data,40,,,"[50] Maciejewski, T., & Stefanowski, J. (2011). Local neighbourhood extension of SMOTE for mining imbalanced data. IEEE Symposium on Computational Intelligence and Data Mining (CIDM), 104-111."
2111.03516,data,40,,,"[54] Nguyen, H. M., Cooper, E. W., & Kamei, K. (2009). Borderline over-sampling for imbalanced data classification. International Journal of Knowledge Engineering and Soft Data Paradigms, 4–21."
2111.03516,data,41,,,"Fernandez, A., Garcia, S., Herrera, F., & Chawla, N. V. (2018). SMOTE for Learning from Imbalanced Data: Progress and Challenges, Marking the 15-year Anniversary. The Journal 863-905. of"
2111.03516,data,41,,,"Luengo, J., Fernandez, A., Garcia, S., & Herrera, F. (2011). Addressing data complexity for imbalanced data sets: analysis of SMOTE-based oversampling and evolutionally underdamping. Soft Computing, 1909-1936."
2111.03516,data,43,,,"Jeni, L., Cohn, J., & Torre, F. (2013). Facing Imbalanced Data Recommendations for the Use of Performance Metrics. In Proceedings of the Humaine Association Conference on Affective Computing and Intelligent Interaction. 245-251."
2111.03516,data,43,,,"[41] Krawczyk, B., Koziarski, M., & Woźniak, M. (2019). Radial-based oversampling for multiclass imbalanced data classification. IEEE transactions on neural networks and learning systems, 31(8), 2818-2831."
2111.03516,data,45,,,"Lash, M. T., Lin, Q., Street, N., Robinson, J. G., & Ohlmann, J. (2017). Generalized Inverse Classification. In Proceedings of the 2017 SIAM International Conference on Data Mining, 162-170."
2111.03516,data,45,,,"[71] Wang, J., Xu, M., Wang, H., & Zhang, J. (2006). Classification of Imbalanced Data by Using the SMOTE Algorithm and Locally Linear Embedding. 8th international Conference on Signal Processing."
2111.03516,data,46,,,"In XAI, counterfactual methods have been found to create plausible, synthetic datapoints for ex planatory purposes; indeed, the evaluative metrics in XAI show that these explanatory counterfac tuals are generally valid, within-distribution and close to existing data-points. This experience in"
2111.03516,data,47,,,This paper advances the use of counterfactual methods for data augmentation as a solution to pop ulating the minority class with more synthetic data to solve class-imbalance problems. The appli cation of this XAI method to data augmentation was motivated by the observation that it seemed
2111.03516,data,48,,,does not interpolate instances when the k-nearest neighbors show a preponderance of majority in stances (see also ADYSYN). This is one way to take the majority class into account. Other meth ods explore the relationship between classes to undersample the majority class using data-cleaning
2111.03516,data,49,,,"presented in Figure 3 and 4. Figure 3 shows selected examples of ROC curves where CFA outper formed SMOTE-based methods, obtained for the four methods using the four classifiers on differ ent data sets. According to Figure 3, CFA clearly outperformed other data augmentation methods"
2111.03516,data,50,,,"[9] Bunkhumpornpat, C., Sinapiromsaran, K., & Lursinsap, C. (2009). Safe-Level-SMOTE: Safe-Level-Synthetic Minority Over-Sampling TEchnique for Handling the Class Imbalanced Problem. In Proceedings of the 13th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, 475–482."
2111.03516,data,51,,,"[34] Hu, S., Liang, Y., Ma, L., & He, Y. (2009). MSMOTE: Improving Classification Performance When Training Data is Imbalanced. In Proceedings of the 2009 Second International Workshop on Computer Science and Engineering, 13–17."
2111.03516,data,51,,,"[73] Wen, Q., Sun, L., Yang, F., Song, X., Gao, J., Xue, W., & Huan, X. (2020). Time Series Data Augmentation for Deep Learning: A Survey. arXiv preprint arXiv:2002.12478."
2111.03516,data,51,,,"training folds, and finally, validate the classifier on the remaining fold. For CFA, the native coun terfactuals in the training data were computed (CF-Set) and then all the remaining unpaired major ity-instances were run through CFA to create the synthetic counterfactual instances in the minority"
2111.03516,data,52,,,"an increasing number of climate-disruptive events, events that diverge significantly from the sce narios recorded in the historical data (e.g., extreme values for key weather variables like solar ra diation or soil moisture). For example, in 2018 there was a significant drought across Europe, that"
2111.03516,data,66,,,"Temraz, M., Kenny, E., Ruelle, E., Shalloo, L., Smyth, B., & Keane, M. T. (2021). Handling Climate Change Using Counterfactuals: Using Counterfactuals in Data Augmentation to Predict Crop Growth in an Uncertain Climate Future. In International Conference on CaseBased Reasoning (ICCBR-21), Springer. Germany. 216-231."
2111.03516,data,73,,,"Beyond XAI, our hypothesis is that counterfactual methods can also play a role in data augmenta tion to solve class-imbalance problems, that generated synthetic counterfactual cases could im prove the predictive accuracy of AI models. Although, there are now 100s of papers on counter factuals in XAI, only a handful of papers consider their use in data augmentation [29, 56, 65, 77]."
2111.03516,"data available, data",29,,,"al. [77] proposed the Counterfactual Generator, which generates counterfactual examples for tex tual data and found that generated counterfactuals improved the generalizability of models under"
2111.03516,"data available, dataset",14,,,"availability of good native counterfactuals in the dataset. Without tolerance, fewer counterfactuals"
2111.03516,"data available, dataset",31,,,"Hasan [29] did and tried to determine whether an augmented dataset based on generated counter factuals could act as a proxy dataset, but only found modest success."
2111.03516,"data, data repository, dataset",64,,,"[2] Alcalá-Fdez, J., Fernandez, A., Luengo, J., Derrac, J., García, S., Sánchez, L., & Herrera, F. (2011). KEEL Data-Mining Software Tool: Data Set Repository, Integration of Algorithms and Experimental Analysis Framework. Journal of Multiple-Valued Logic and Soft Computing, 7, 255-287."
2111.03516,"data, dataset",15,,,compared with the original dataset (without data augmentation) and the datasets generated by
2111.03516,"data, dataset",16,,,subsets being used as the training data. Different datasets have different-sized minority classes (see
2111.03516,"data, dataset",16,,,techniques for data augmentation. Subbaswamy and Saria [65] considered the problem of dataset
2111.03516,"data, dataset",17,,,"counterfactual-data have to be in the training dataset; specifically, they are all the majority class"
2111.03516,"data, dataset",19,,,"§ Vehicle dataset: The Vehicle data set is a multi-class dataset, with 4 classes. The problem"
2111.03516,"data, dataset",29,,,"most popular being oversampling techniques (such as SMOTE). These methods gen erate synthetic instances in the minority class, to balance the dataset, performing data"
2111.03516,"data, dataset",34,,,"Figure 2 shows the F1 comparisons for each of the data augmentation methods (SMOTE, B SMOTE, ADASYN, SVM-SMOTE, SL-SMOTE, SMOTE-RSB, and CFA) on 25 datasets using"
2111.03516,"data, dataset",36,,,"each dataset is randomly partitioned into 5 disjoint subsets, where each subset included approxi mately equal size of data; then, a single subset was retained as a test set with the remaining k-1"
2111.03516,"data, dataset",37,,,"[18] Douzas, G., & Bacao, F. (2017). Self-Organizing Map Oversampling (SOMO) for imbal anced data set learning. Expert Systems with Applications, 40 – 52."
2111.03516,"data, dataset",37,,,"generate a set of counterfactuals that can substitute for the original dataset (calling it substitutabil ity); that is, if the set of generated counterfactuals were plausible and close to the original data then"
2111.03516,"data, dataset",54,,,"[59] Ramentol, E., Caballero, Y., Bello, R., & Herrera, F. (2012). SMOTE-RSB*: a hybrid preprocessing approach based on oversampling and undersampling for high imbalanced data-sets using SMOTE and rough sets theory. Knowledge and Information Systems, 245– 265."
2111.03516,"data, dataset",54,,,"of 25 datasets when using k-NN classifier. It outperformed the Baseline (with no data augmenta tion) and SMOTE variants. Similarly, when applying LR classifier, CFA achieved the best perfor mance in 22 out of 25 datasets, although the improvement varied with different data sets. Finally,"
2111.03516,"data, dataset",58,,,"with the Baseline (no data augmentation), SMOTE, B-SMOTE, ADASYN, SVM-SMOTE, SL SMOTE, SMOTE-RSB and CFA.  Tables 4-7 report the main metric (AUC-ROC) for each clas sifier on the datasets. For the RF classifier (Table 4), the results show that CFA does better than"
2111.03516,dataset,1,,,Dataset
2111.03516,dataset,3,,,Dataset Baseline SMOTE
2111.03516,dataset,5,,,10 base datasets were:
2111.03516,dataset,6,,,4.1 Method: Datasets & Setup
2111.03516,dataset,10,,,"case-base/dataset, by providing plausible transformations of known datapoints."
2111.03516,dataset,11,,,Learning from class imbalanced datasets poses challenges for many machine learning
2111.03516,dataset,11,,,"datasets are reported, which show that this Counterfactual Augmentation method"
2111.03516,dataset,12,,,"self-organizing map to transform high-dimensional datasets into a two-dimensional space, and"
2111.03516,dataset,13,,,"counterfactuals in the minority class improved performance, specifically dealing with the dataset"
2111.03516,dataset,13,,,"instance, in credit-card fraud-detection, datasets always have many more non-fraudulent instances"
2111.03516,dataset,13,,,method is that it oversamples by adaptively combining actual feature-values from dataset instances
2111.03516,dataset,14,,,Imbalanced datasets create significant problems for machine learning (ML) in classification tasks
2111.03516,dataset,14,,,"counterfactual methods developed for XAI can be usefully deployed to augment datasets, with"
2111.03516,dataset,14,,,difference constraint works here for 25 datasets with 9-12 features suggests that this constraint
2111.03516,dataset,14,,,"patient has diabetes, based on certain diagnostic measurements included in the dataset."
2111.03516,dataset,15,,,Baseline-Control for a given classifier recorded the performance of the model on a given dataset
2111.03516,dataset,15,,,"Quality of the Dataset.  Fundamentally, CFA depends on the set of native counterfactuals"
2111.03516,dataset,15,,,a multi-class classification into one binary classification dataset for each pair of classes. Whereas
2111.03516,dataset,15,,,"classifiers and datasets; and, specifically, to datasets where class-imbalance problems arise."
2111.03516,dataset,15,,,"datasets. SMOTE, B-SMOTE, SVM-SMOTE and SMOTE-RSB had the highest AUC-ROC in 1"
2111.03516,dataset,15,,,the native counterfactuals build from the dataset involve no more than two feature-differences. This
2111.03516,dataset,16,,,"For example, the loan dataset could contain two existing cases that are counterfactually related;"
2111.03516,dataset,16,,,Table 2: Datasets & DataSet Variants Used in the Experiment (IR= Imbalance Ratio)
2111.03516,dataset,16,,,classifier on the ‘PIMA’ dataset we can see that SMOTE-based methods had better performance
2111.03516,dataset,16,,,"datasets. Whereas, SMOTE-RSB doing better in only 5 datasets, with SVM-SMOTE being the"
2111.03516,dataset,16,,,each feature until changes in classification of the original dataset arose and then chose a relative
2111.03516,dataset,16,,,k-NN model –called PBI-CBR -- for grass growth prediction that relies on an historical dataset of
2111.03516,dataset,16,,,percentage affect performance. What we do know is that for many datasets the current parameters
2111.03516,dataset,16,,,"points need to be generated to create the large, labelled datasets required for better performance"
2111.03516,dataset,16,,,§ Abalone dataset: A multi-class dataset analyzed to find the age of abalone from physical
2111.03516,dataset,16,,,§ Phoneme dataset: A binary-class dataset used to distinguish between nasal and oral sounds.
2111.03516,dataset,16,,,§ Pima Indians Diabetes dataset: A binary-class dataset used to predict whether or not a
2111.03516,dataset,17,,,"As the focus is on binary classification problems and some of these datasets are multi-class, they"
2111.03516,dataset,17,,,"For each classifier based on the original imbalanced dataset was also run as a baseline, without"
2111.03516,dataset,17,,,Table 2 shows the main characteristics of the datasets drawn from both UCI and KEEL repositories.
2111.03516,dataset,17,,,"all the other SMOTE-based methods in 21 datasets out of 25, with SVM-SMOTE being the next"
2111.03516,dataset,17,,,best in only 2 datasets. Both ADASYN and SL-SMOTE had the highest AUC-ROC for one dataset
2111.03516,dataset,17,,,class; this augmented dataset was then used for testing. These generated datasets from CFA were
2111.03516,dataset,17,,,"datasets for each method. Notably, these results show for certain datasets and classifiers, the"
2111.03516,dataset,17,,,"in the MLP classifier, CFA still achieved highest AUC in 21 out of 15 datasets."
2111.03516,dataset,17,,,"may be more representative of valid instance-differences in the dataset and, hence, be more likely"
2111.03516,dataset,17,,,"their predictive performance should parallel that of the original dataset. However, Mothilal et al."
2111.03516,dataset,17,,,"where the datasets are more separable.  Still others, such as G-SMOTE and ADASYN, explore"
2111.03516,dataset,17,,,§ Page-Blocks dataset: A multi-class dataset used to classify all the blocks of the page layout
2111.03516,dataset,17,,,"§ Poker dataset: A multi-class dataset, with 10 classes used to predict poker hands."
2111.03516,dataset,17,,,"§ Yeast dataset: A multi-class dataset used to predict the cellular localization sites of proteins,"
2111.03516,dataset,18,,,"is under-represented in the dataset in this way, a classifier’s performance can be compromised in"
2111.03516,dataset,18,,,"know whether similar results would be found for other datasets, though the fact that the 2-feature 39"
2111.03516,dataset,18,,,"next best with 3 datasets. Baseline, B-SMOTE, and ADASYN had the highest AUC-ROC in 2"
2111.03516,dataset,18,,,"of datasets, ML models and different imbalance ratios. But why does it work so well?"
2111.03516,dataset,18,,,"other SMOTE-based methods in 19 out of 25 datasets, with ADASYN being the next best with 2"
2111.03516,dataset,18,,,when running a RF classifier on the ‘Abalone-9-vs-13’ dataset we can see that CFA had better
2111.03516,dataset,19,,,§ Wine Quality dataset (Red and White): Two datasets related to red and white vinho verde
2111.03516,dataset,20,,,CFA achieved higher F1 values in 23 out of 25 datasets when using the RF classifier and in 22 out
2111.03516,dataset,20,,,"Table 2 for details).  For each of the SMOTE methods, we split each dataset into training and"
2111.03516,dataset,20,,,"be imbalanced, in that most cows will tend to be healthy rather than ill. An analysis of this dataset"
2111.03516,dataset,20,,,"have the same number of instances in each class. As a result, fully balanced datasets were created."
2111.03516,dataset,20,,,minority and majority instances in the datasets (see Figures for results). Area Under Curve (AUC)
2111.03516,dataset,20,,,"the negative class (majority). In this paper, the datasets were modified using both methods (one"
2111.03516,dataset,20,,,"§ Ecoli dataset: This dataset is a multi-class dataset, with 8 classes. The problem is to classify"
2111.03516,dataset,21,,,"and SL-SMOTE doing better in 2 datasets. Finally, for the MLP classifier (Table 7), again the"
2111.03516,dataset,21,,,"conditions under which CFA is likely to be less good, with respect to (i) the quality of dataset"
2111.03516,dataset,21,,,where 𝛿 is a random number between 0 and 1. This new instance is then added to the dataset for
2111.03516,dataset,22,,,"[52]). However, this work only considers one specific problem domain, classifier, and dataset. It"
2111.03516,dataset,22,,,"dataset for each method. For the LR classifier (Table 6), the results are quite different in that they"
2111.03516,dataset,22,,,method per dataset) to vary the class ratio of class imbalance among the datasets (see Table 2). The
2111.03516,dataset,23,,,"KEEL datasets [2, 3, 4, 8], from which 25 dataset-variants were produced, with four different ML"
2111.03516,dataset,23,,,"class of a cow from healthy to ill.  So, if we want to fix the class imbalance in this dataset using"
2111.03516,dataset,23,,,"in the dataset for its success. To put this another way, there needs to be a rich and diverse set of"
2111.03516,dataset,23,,,"“good” native counterfactuals, 𝑐𝑓(𝑥, 𝑝), are initially computed over the whole dataset, 𝑇,"
2111.03516,dataset,24,,,"Step 1 Compute the CF-Set for the dataset, 𝒄𝒇(𝒙, 𝒑): CFA first finds all possible “good”"
2111.03516,dataset,24,,,"class. Unlike other oversampling techniques, this method adaptively combines ex isting instances from the dataset, using actual feature-values rather than interpolating"
2111.03516,dataset,26,,,"fall ill (e.g., mastitis in cows; see [60]).  The dataset recording a herd of cows on most farms will"
2111.03516,dataset,27,,,"binary datasets using two of the most well-known strategies; 1v1 [7, 33] and 1vR [7, 13, 69] (see"
2111.03516,dataset,29,,,"rare in most datasets (<1% of all instances), but with some tolerance in feature-matching they can be increased (to ~5%)."
2111.03516,dataset,31,,,"and Multilayer Perceptron (MLP) models. Several alternative ML models were used because dif ferent models find different decision boundaries for a given dataset, differences that could impact"
2111.03516,dataset,34,,,"SMOTE On the Borderline. The idea that different regions in the dataset need to be han dled differently, owes a lot to the intuition that minority instances close to the decision boundary"
2111.03516,dataset,36,,,"Sandhan, T., & Choi, J. Y. (2014). Handling Imbalanced Datasets by Partially Guided Hybrid Sampling for Pattern Recognition. 2014 22nd International Conference on Pattern Recognition, 1449-1453."
2111.03516,dataset,36,,,"takes part in a so-called explanation case (xc). An explanation case captures a counterfactual rela tion between existing instances in the dataset, that are in opposing classes either side of a decision"
2111.03516,dataset,37,,,"show that for AUC-ROC metric, CFA doing better in only 10 out of 25 datasets, with the SMOTE RSB being the next best with 5 datasets. Whereas Baseline, B-SMOTE, ADASYN, SVM-SMOTE"
2111.03516,dataset,37,,,"§ Glass dataset: A multi-class dataset used to classify glass-types based on the chemical anal ysis, consisting of 7 classes, that modified using 1vR to treat the class ’3’ as the minority"
2111.03516,dataset,38,,,"Figure 2: F1 values for the different conditions, across 25 datasets for the four classifiers (a) RF classifier, (b) k-NN, (c) LR, and (d) MLP"
2111.03516,dataset,38,,,"is to classify a given silhouette as one of four types of vehicle, using a set of features ex tracted from the silhouette. Again, this dataset was modified using 1vR. So that the class"
2111.03516,dataset,40,,,"SMOTE-based methods on the main metrics reported, for most of the classifiers tested (see Ta bles 4-7). Recall, the cross-validated datasets were tested on four classifiers (RF, k-NN, LR, MLP)"
2111.03516,dataset,40,,,"Subbaswamy, A., & Saria, S. (2018). Counterfactual normalization: Proactively addressing dataset shift using causal mechanisms. In Proceedings of the 34th Conference on Uncertainty in Artificial Intelligence (UAI), 947-957."
2111.03516,dataset,40,,,"Table 8: The number of datasets for each method showing the highest Precision and Recall scores for a given method (SMOTE, B-SMOTE, ADASYN, SVM-SMOTE, SL-SMOTE, SMOTE-RSB and CFA) on a selected classifier"
2111.03516,dataset,41,,,"𝑐𝑓(𝑥, 𝑝), pair instances either side of decision boundary (they are called native be cause, in one sense, they already exist in the dataset). Each of these native pairs has a"
2111.03516,dataset,45,,,method can successfully introduce new synthetic minority examples by leveraging known coun terfactuals in the dataset; and iii) this method can outperform many key benchmark SMOTE vari ants on a wide range of datasets with differing imbalance ratios using representative ML models.
2111.03516,dataset,47,,,"good counterfactual records the minimal feature-changes that result in a class change (as in Bor derline-SMOTE and SVM-SMOTE).  Second, this instance-based method relies on native coun terfactuals in the dataset, pairings between existing majority and minority instances and, as such,"
2111.03516,dataset,55,,,within +/-10% of the standard deviation from the mean all the values for that feature.  This toler ance was applied uniformly across all of our datasets. Keane & Smyth [39] used a more sophisti cated tolerance scheme that tailored the tolerance to each dataset; they varied the tolerances for
2111.03516,dataset,58,,,"dataset [31]. Although these methods can re-balance the original dataset, they have some draw backs. Since ROS merely copies minority-class instances, no new information is added to the da taset and, hence, it can lead to overfitting [72]. On the other hand, since RUS randomly removes"
2111.03516,dataset,71,,,"soil moisture drops, then grass stops growing, indeed high solar radiation will burn grass). Ac cordingly, the PBI-CBR model does not do very well at predicting grass growth for these climate disrupted months of 2018 because they are historically unique.  Temraz et al. defined a climate based class boundary in the PBI-CBR dataset, creating a division between “normal cases” (with"
2111.03516,dataset,80,,,"matrix summarizes the performance of classifiers for the four possible outcomes of a given classi fication: a true positive (TP), true negative (TN), false positive (FP) and false negative (FN). Ac curacy was not used as a measure because, as discussed earlier, it can be spuriously high for im balanced datasets. It should be noted that all datasets used in our experiments were converted to"
2111.03516,dataset,129,,,"Figure 1: Counterfactual Augmentation (CFA): An unpaired instance, 𝒙& (grey circle), finds a nearest neighbor, 𝒙 (blue circle), taking part in a “good” native counterfactual-pair in the dataset, 𝒄𝒇(𝒙, 𝒑) (pairing of blue circle and yellow box) and then uses the difference-features of the counterfactual-instance, 𝒑 (yellow box), to generate a new synthetic counterfactual-instance, 𝒑& (green box), combining them with the matching-features of the original unpaired instance, 𝒙& (grey circle). The generated synthetic instance, 𝒑& (green box), is then added to the dataset to improve future prediction."
2111.04287,code,9,,,Listing 2. Code snippet to simulate school ﬁshes
2111.04287,code,36,,,Code. We set the topology as the one-peer variant of grid topology. The code snippet using BlueFog is shown in Listing 7. The complete code can be referred to BlueFog online tutorial8.
2111.04287,code,52,,,where N (i) is the in-coming neighbors of node i. Code. We set the topology as the static ring graph in the following Exact-Diffusion implementation. The code snippet using BlueFog is shown in Listing 6. The complete code can be referred to BlueFog online tutorial7.
2111.04287,code,113,,,"where N (i) is the in-coming neighbors of node i. Code. We set the topology as the static exponential graph in the following DGD implementation, which is established in [33] to be both sparse and well-connected. Note that such static exponential graph and its associated weight matrix W has already been implemented in the BlueFog library. Users can directly set it as the default topology over which DGD runs. The code snippet of the DGD implementation using BlueFog is shown in Listing 1. The complete code can be referred to BlueFog online tutorial4. It is observed that the DGD implementation using"
2111.04287,code,116,,,"where pi is a scalar correct xi and is initialized as value 1, and δj stands for some index difference since there is no global synchronized index in the asynchronous setting. When the weight matrix W = [wij] is column stochastic, i.e. (cid:80) i will converge to the unbiased consensus value x(cid:63). For this reason, we will utilize the asynchronous primitives provided by BlueFog in the push-style. Code. The following code snippet is to illustrate how to use partial averaging in the asynchronous and push-style mode. The complete code and experimental results can be referred to BlueFog online tutorial5."
2111.04287,code,340,,,"The objective of the ﬁsh school is to estimate the predator’s location in a fully decentralized manner and take actions such as disperse or encircle. The estimation of the predator’s location can be formulated into a distributed optimization problem. For ﬁsh i at time k, it will have a local estimate of the distance d(k) between the predator and itself. If we denote the direction vector as u(k) sin θ(k) , and the i predator’s position w(cid:63) can be characterized as d(k) i − w(cid:63)) + n(k) denotes additive noise. If we let fi(w) = 1 i (xi − w)]2 to be the local loss function of ﬁsh i to estimate the predator’s position w(cid:63), the global loss function of the ﬁsh schools to estimate the predator’s position can be formulated as w(cid:63) = arg minw{(cid:80)n i=1 fi(w)}, which is a distributed optimization problem and can be solved with decentralized stochastic gradient descent (4) and (5). When w(k+1) is estimated at time k, each ﬁsh will either escape from the predator or encircle it at time k + 1. The detail to formulate these ﬁsh behaviors can be referred to [75]. Code. The simulation of ﬁsh schools with BlueFog is to illustrate how to use partial averaging over time-varying graphs. The code snippet is shown in Listing 2. Note the neighborhood at each iteration is determined by the argument src weights, which is a dictionary mapping the rank to the scaling weights. The argument src weights is updated at each iteration through the neighbor location collections function and Metropolis-Hastings Rule."
2111.04287,code,345,,,"In this subsection, we simulate the above behaviour of ﬁsh schools with BlueFog. We mimic each ﬁsh with one process in a CPU cluster. With the system-level neighbor-allreduce, neighbor-allgather, and the topologyoriented APIs provided by BlueFog, we can easily schedule the time-varying ﬁsh topology, recognize the dynamic neighborhood, and conduct local information exchange between neighboring ﬁshes. Note that the topology of ﬁsh schools is highly dynamic especially when ﬁshes are escaping or encircling, this simulation will test how robust BlueFog is to dynamic topology scheduling. Problem formulation. Consider n ﬁshes distributed over some spatial region. Two ﬁshes are said to be neighbors if they are within a predeﬁned distance. The objective of the ﬁsh school is to estimate the predator’s location in a fully decentralized manner and take actions such as disperse or encircle. The estimation of the predator’s location can be formulated into a distributed optimization problem. For ﬁsh i at time k, it will have a local estimate of the distance d(k) between the predator and itself. If we denote the direction vector as u(k) sin θ(k) , and the i predator’s position w(cid:63) can be characterized as d(k) i − w(cid:63)) + n(k) denotes additive noise. If we let fi(w) = 1 i (xi − w)]2 to be the local loss function of ﬁsh i to estimate the predator’s position w(cid:63), the global loss function of the ﬁsh schools to estimate the predator’s position can be formulated as w(cid:63) = arg minw{(cid:80)n i=1 fi(w)}, which is a distributed optimization problem and can be solved with decentralized stochastic gradient descent (4) and (5)."
2111.04287,"code package, python, package",147,,,"We can write most known decentralized algorithms in a few lines of equations. However, realizing them in a computer network is quite complicated. In particular, implementing a set of coordinated communication steps between speciﬁc pairs of nodes requires a skillful application of low-level communication libraries. Therefore, despite the development of decentralized algorithms over the past two decades, we are yet to see a software package for their rapid implementations. Most decentralized algorithm developers have been simulating their algorithms on a single computer in Matlab or Python. The very few packages that can run in a computer network were written for speciﬁc algorithms and network topologies. Adapting them to another project requires modifying lowerlevel communication. The recent development further involves asynchrony, time-varying network topologies, and asymmetric communication have created an even higher implementation barrier."
2111.04287,data,15,,,"learning on heterogeneous data,” in International Conference on Machine Learning, 2021."
2111.04287,data,16,,,3The exact memory size in bytes will be determined by the data type and systems.
2111.04287,data,17,,,"11 12 for batch_idx, (data, target) in enumerate(train_loader): 13"
2111.04287,data,21,,,"[38] Kun Yuan and Sulaiman A Alghunaim, “Removing data heterogeneity inﬂuence enhances network topology dependence of decentralized"
2111.04287,data,22,,,"decentralized optimization,” in 2019 IEEE Data Science Workshop (DSW). IEEE, 2019, pp. 315–321."
2111.04287,data,27,,,"# Forward and backward computation. output = model(data) loss = F.cross_entropy(output, target) loss.backward() opt.step() ..."
2111.04287,data,31,,,"[52] Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu, “d2: Decentralized training over decentralized data,” in International"
2111.04287,data,51,,,"[78] “NVIDIA DGX-1,” https://www.nvidia.com/en-us/data-center/dgx-1/. [79] Paul J Werbos, “Backpropagation through time: what it does and how to do it,” Proceedings of the IEEE, vol. 78, no. 10, pp. 1550–1560,"
2111.04287,data,57,,,"1 import torch 2 import bluefog.torch as bf 3 # initialization and model and data settings. 4 ... 5 opt = optim.SGD(model.parameters(), lr=0.01) 6 # BlueFog wraps the standard optimizer to use the decentralized communication 7 # to fuse the information with neighbors. 8 opt = bf.DistributedAdaptThenCombineOptimizer( 9"
2111.04287,data,59,,,"[39] Thijs Vogels, Lie He, Anastasia Koloskova, Tao Lin, Sai Praneeth Karimireddy, Sebastian U Stich, and Martin Jaggi, “Relaysum for decentralized deep learning on heterogeneous data,” To appear on the 35th conference on Advances in Neural Information Processing Systems. Also avaialbe at arXiv:2110.04175, 2021."
2111.04287,data,63,,,"In this section, we compare the communication time between neighbor_allreduce and allreduce. We carry out two experiments with CPUs and GPUs on AWS. In each experiment, we utilize the three different communication approaches to process the synthetic data of 1 megabytes (MB) for CPUs and 10 MB for GPUs as GPUs typically handle bigger computing capacity."
2111.04287,data,65,,,"where Ai and bi are local data kept by node i. The target is to let each node i achieve the optimal solution x(cid:63). Decentralized gradient descent. Decentralized gradient descent (DGD) is among the most widely-used approaches to solving problem (15). In particular, DGD with static graph topology will iterate as follows:"
2111.04287,data,95,,,"The asynchronous primitives’ implementation is based on the window operations provided after MPI-3 standard. Internally, we maintain a state dictionary that maps from the unique window name to the registered window object. Each window object is also associated with a distributed mutex that can be locked and unlocked by the local process and the corresponding neighbor process. Since each window object may associate with multiple neighbors, each data manipulation primitive will automatically calculate the displacement value for the remote window based on the tensor shape and the rank."
2111.04287,data,100,,,"Fig. 12. Throughput performance comparison over ResNet-50, VGG-16, and BERT-large models. Label H-ATC represents the hierarchical neighbor allreduce with ATC-style over dynamic exponential 2 topologies. H-AWC is the corresponding AWCstyle one. ATC and AWC represents the neighbor allreduce over dynamic inner and outer exponenetial 2 topology with ATCand AWC- style algorithms. The batch-size or tokens are for one GPU. One machine has 8 GPUs. Hence, for 4 and 8 GPUs data-point, which corresponding to one machine, we use neighbor-allreduce result for hierarchical neighbor-allreduce’s one."
2111.04287,data,109,,,"To fairly compare different neighbor allreduce methods, we select neighbor allreduce on the ring topology and dynamic neighbor allreduce on the inner-outer exponential-2 graph, supported by BlueFog, so that the data size for transfer in each iteration matches. As we can see from Fig. 11, the proposed neighbor allreduce primitives in BlueFog takes much less time for communication than allreduce on both CPUs and GPUs, especially with more computation cores. In addition, the time consumption for neighbor allreduce increases much slowly compared to allreduce as the number of cores increases, indicating better scalability of the neighbor communication methods."
2111.04287,data,176,,,"BlueFog next provides three communication primitives neighbor_win_put, neighbor_win_get and neighbor_win_accumulate (the names are borrowed from MPI protocol) to manipulate the remote memory. As their name indicate, neighbor_win_put puts local tensors to the window buffers maintained by its neighbors while neighbor_win_get fetches neighbors’ local tensors to its own local window buffers, see the illustration in Fig. 4. Primitive neighbor_win_accumulate performs similarly to neighbor_win_put, but the former adds local tensor to the existing window buffers maintained by its neighbor while the latter overwrites those buffers. During the window creation, we utilize static topology since frequently allocating and de-allocating windows are expensive. BlueFog provides the dst weights and src weights dictionary as arguments to the following three primitives to enable asynchronous data transferring deﬁned over the dynamic topology. Note the window allocation is associated with the global static topology, which implies the ranks used in dst weights and src weights should be the subset of the neighbors deﬁned under the global static topology."
2111.04287,data,293,,,"BlueFog can implement all these algorithms including those use global averaging. Brief history of decentralized optimization. Decentralized optimization can be traced back to [42]. Since then, it has been intensively studied in the control and signal processing communities. The ﬁrst decentralized algorithms on general optimization problems include decentralized gradient descent [19], diffusion [34], [20], [43], and dual averaging [44]. Various primal-dual algorithms come out to further speed up the convergence, and they are based on alternating direction method of multipliers (ADMM) [45], [46], explicit bias-correction [47], [48], [49], gradient tracking [23], [24], [25], [26], and dual acceleration [50], [51]. In deep learning tasks, decentralize SGD also attracted a lot of attentions recently. Many efforts have been made to extend the algorithm to time-varying topologies [29], directed topologies [3], [27], asynchronous settings [2], and data-heterogeneous scenarios [52], [53], [31], [37], [38], [54], [39]. Techniques such as quantization/compression [55], [56], [57], [58], [59], periodic updates [60], [29], [61], and lazy communication [62], [63] were also integrated into decentralized SGD to improve communications."
2111.04287,data,303,,,"effective in aggregating information than global averaging, some decentralized algorithms can match or exceed the performance of global-averaging-based distributed algorithms: [1], [29] established that decentralized SGD can achieve the same asymptotic linear speedup in convergence rate as (parameter server based) distributed SGD; [3], [33] used exponential graph topologies to realize both efﬁcient communication and effective aggregation by partial averaging; [37], [38], [31], [39] improved the convergence rate of decentralized SGD by removing data heterogeneity between nodes; [40], [4], [30], [41] enhanced the effectiveness of partial averaging by periodically calling global averaging. BlueFog can implement all these algorithms including those use global averaging. Brief history of decentralized optimization. Decentralized optimization can be traced back to [42]. Since then, it has been intensively studied in the control and signal processing communities. The ﬁrst decentralized algorithms on general optimization problems include decentralized gradient descent [19], diffusion [34], [20], [43], and dual averaging [44]. Various primal-dual algorithms come out to further speed up the convergence, and they are based on alternating direction method of multipliers (ADMM) [45], [46], explicit bias-correction [47], [48], [49], gradient tracking [23], [24], [25], [26], and dual acceleration [50], [51]. In deep learning tasks, decentralize SGD also attracted a lot of attentions recently."
2111.04287,"data available, data",49,,,"decentralized (stochastic) gradient descent. Problem. Partition a set of data D to n computing nodes, where node i has access to local data Di, i = 1, . . . , n. Suppose they collaborate to solve the distributed optimization problem:"
2111.04287,"data, code",164,,,"In the code snippet above, we omit most initialization, model, and data setting. This is because the BlueFog optimizer should be non-intrusive by design and all parts unrelated to the optimizer should remain the same as the standard distributed training code. Basically, what the user need to do is just to wrap the standard optimizer with the BlueFog API, transforming the original optimizer into a decentralized one. As seen in the above example, the wrapped optimizer also accepts the arguments like src_weights, communication_types, etc, to conﬁgure the communication to be executed at each iteration. These settings should be placed before the forward and backward computation because the communication may trigger during the forward propagation, the backward propagation, or the step function, depended on what kind of decentralized optimizer is used. Next, we delve into a few key components that BlueFog adopts under these high-level APIs."
2111.04287,"data, dataset",126,,,"Modern deep-learning models are growing quickly in size [6], [7], and training them require extremely large datasets [8], [9], [10]. With the rapid increase of our ability to gather data, some classic signal processing and statistical learning problems have also grown very large. However, the performance of each computing core stopped improving over a decade ago. Therefore, solving large optimization problems in reasonable time requires parallel, distributed computation. Most existing distributed optimization methods are based on iterative local computation by individual agents and global communication, for example, computing the average of distributed model parameters or taking the sum of distributed gradients."
2111.04287,"data, python",121,,,"1) Users with no experience with distributed systems can implement a decentralized algorithm in just a few clean lines of Python codes of a single program. The program will be run on multiple agents (with their different data) to achieve parallelism, which is commonly known as single program, multiple data (SPMD). 2) We design BlueFog to support a series of features for advanced users and future development. They include: the controls of partial-averaging coefﬁcients, one-way communication in the pull and push forms, asynchrony, time-varying networks. We managed to keep the advanced communication primitives relatively simple through a uniﬁed abstraction of different decentralized communication operations."
2111.04287,database,34,,,"[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in 2009"
2111.04287,dataset,28,,,"[10] T Nathan Mundhenk, Goran Konjevod, Wesam A Sakla, and Koﬁ Boakye, “A large contextual dataset for classiﬁcation, detection and"
2111.04287,dataset,33,,,TABLE III TOP-1 VALIDATION ACCURACY AND WALL-CLOCK TIME (IN HOURS) COMPARISON WITH DIFFERENT MODELS AND ALGORITHMS ON IMAGENET DATASET OVER STATIC/DYNAMIC EXPONENTIAL TOPOLOGY (8X8 GPUS)[33].
2111.04287,dataset,94,,,"Decentralized computational methods are distributed computational methods running without a centralized server. They do not directly perform global operations such as computing the average of distributed numbers. They can, however, obtain the same results through local dynamics, namely, a series of computation and agent-to-agent direct communication steps. On large-scale optimization tasks involving distributed datasets, recent decentralized computational methods have shown strong, sometimes superior, performance [1], [2], [3], [4], [5]."
2111.04287,dataset,165,,,"Learning Curves. As Bluefog utilizes inexact averaging over different typologies, we further run a series of image classiﬁcation experiments on ImageNet-1k[8] dataset to validate the performance and generalisation of our system. We train ResNet-50 model following the training protocol of [88]. The Nesterov momentum SGD optimizer is used with a linear scaling learning rate strategy. The left and middle sub-ﬁgures in Fig. 13 show the evolution of training loss and accuracy in terms of wall clock time. Compared to Horovod, our implementation of decentralized communication gains 1.3x - 1.43x speed-up with similar convergence. The rightmost sub-ﬁgure in Fig. 13 shows the top-1 validation accuracy of the aforementioned decentralized training methods. More convergence results of decentralized optimization algorithms applied on DNN training tasks have also been reported in [3], [2], [68], which should give similar results if implemented in BlueFog."
2111.04287,github,5,,,4https://github.com/Bluefog-Lib/bluefog-tutorial/tree/master/Section%203
2111.04287,github,5,,,5https://github.com/Bluefog-Lib/bluefog-tutorial/tree/master/Section%206
2111.04287,github,5,,,7https://github.com/Bluefog-Lib/bluefog-tutorial/tree/master/Section%204
2111.04287,github,5,,,8https://github.com/Bluefog-Lib/bluefog-tutorial/tree/master/Section%205
2111.04287,"github, code, open-source",84,,,"BlueFog is an open-source project that was ﬁrst available on GitHub in December 2019. It keeps evolving during the past two years, and the progress was reported in some conferences1. BlueFog has provided demos for several distributed and decentralized algorithms introduced in [32], and it has supported all deep learning experiments in [4], [5], [33]. All BlueFog code and supporting documents can be found at https://github.com/Bluefog-Lib."
2111.04287,open-source,80,,,"BAGUA [69] is a recent open-source library that supports both global and partial averaging, offers full- and low-precision operations, and focuses on efﬁcient deep learning. It does not support asynchronous communication, diverse and time-varying network topologies, and directed communications in pull- and push styles, which are supported by BlueFog to implement algorithms such as push-sum [3] and push-pull [70], [71], as well as more"
2111.04287,open-source,112,,,"In this paper, we present BlueFog, an open-source library for efﬁcient and high-performance implementation of decentralized algorithms in optimization and deep learning. Through a uniﬁed abstraction of different decentralized communication operations, BlueFog provides simple and consistent communication primitives to support diverse decentralized algorithms. BlueFog can be used with PyTorch to train deep neural networks. The system design philosophy and detailed implementation of BlueFog are carefully presented to demonstrate the superior performance. The usage of BlueFog is illustrated with various application examples in optimization and signal processing. Deep learning experiments support that BlueFog outperforms Horovod, a state-of-the-art distributed training framework based on Ring-Allreduce."
2111.04287,open-source,349,,,"Libraries that support global averaging. Parameter Server (PS) [11] is a well-known architecture adopted early in TensorFlow [64] where all workers communicate with the central parameter server(s). It easily suffers from communication bottlenecks. If every worker sends a message of size M to the central server and the network interface is saturated by every message, the total communication in PS is n( M B + L). Like PS, the Ring-Allreduce [12] architecture organizes all nodes on a ring and divides each local gradient tensor into n chunks for parallel communication. Ring-Allreduce achieves a remarkable communication time of 2M B + 2nL. The bandwidth-bound time 2M B is optimal [65], where constant 2 is not improvable (without hardware additions). Ring-Allreduce has been implemented in Horovod and Pytorch. Fig. 1 illustrates the Parameter Server and Ring-Allreduce operations. Another framework BytePS [36] is based on PS-lite [66] and can reduce the bandwidth cost from 2M B to M B using n additional CPU servers. Instead of passing each of the n chunks over a ring, each worker pushes its ith chunk to server i and pulls the accumulated chunk back, achieving a total communication time of M B + nL. The communication overheads of Parameter Server, Ring-Allreduce, and BytePS are listed in Table I. Libraries that support partial averaging. The open-source codes or libraries to provide the system-level partial averaging are limited. The codes brought along with [3], [67] implemented only the decentralized algorithms proposed therein. Prague [68] wraps all-reduce or broadcast operations, which is smart but relatively inefﬁcient and restrictive. In particular, Prague forms a ring from a random subset of nodes and then ring-allreduces over them, thus ruling out other effective topologies such as the exponential graph [3], [33]."
2111.04287,"open-source, code package, package, github, python, dataset",226,,,"Decentralized algorithm is a form of computation that achieves a global goal through local dynamics that relies on low-cost communication between directly-connected agents. On large-scale optimization tasks involving distributed datasets, decentralized algorithms have shown strong, sometimes superior, performance over distributed algorithms with a central node. Recently, developing decentralized algorithms for deep learning has attracted great attention. They are considered as low-communication-overhead alternatives to those using a parameter server or the Ring-Allreduce protocol. However, the lack of an easy-to-use and efﬁcient software package has kept most decentralized algorithms merely on paper. To ﬁll the gap, we introduce BlueFog, a python library for straightforward, high-performance implementations of diverse decentralized algorithms. Based on a uniﬁed abstraction of various communication operations, BlueFog offers intuitive interfaces to implement a spectrum of decentralized algorithms, from those using a static, undirected graph for synchronous operations to those using dynamic and directed graphs for asynchronous operations. BlueFog also adopts several system-level acceleration techniques to further optimize the performance on the deep learning tasks. On mainstream DNN training tasks, BlueFog reaches a much higher throughput and achieves an overall 1.2× ∼ 1.8× speedup over Horovod, a state-of-the-art distributed deep learning package based on Ring-Allreduce. BlueFog is open source at https://github.com/Bluefog-Lib/bluefog."
2111.04287,package,94,,,"The BlueFog package comes with several examples as a tutorial. They include major decentralized algorithms for signal processing: decentralized gradient descent [19], [20], exact diffusion [21], [22], and gradient tracking [23], [24], [25], [26], [27]. We also include a deep-learning-based image classiﬁcation example where BlueFog is 1.2 ∼ 1.8× faster than Horovod [28], a state-of-the-art distributed training package based on Ring-Allreduce."
2111.04287,python,39,,,"We ﬁll the gap between the rapid development of algorithms on paper and the lack of practice in real world with the introduction of BlueFog, a python library for straightforward, high-performance implementations of diverse decentralized algorithms."
2111.04287,python,54,,,"The system layer consists of two threads. The main thread, created by Python, is responsible for computation and generating communication requests. After the communication API like neighbor_allreduce is called by application layer, BlueFog transforms them into a proper request structure and push that into the shared queue. The"
2111.04287,python,62,,,BlueFog fully integrates with the PyTorch library [81] so that the BlueFog APIs can be easily called in Python for decentralized tensor/vector computation and the core communication logic is implemented in C++ for better efﬁciency and faster interaction with low-level communication libraries like OpenMPI and NCCL. It is easy to install BlueFog by simply running pip install bluefog.
2111.04287,python,75,,,"The application layer consists of three types of APIs: 1) topology management APIs such as set_topology and set_machine_topology; 2) low-level communication APIs such as allreduce, neighbor_allreduce, neighbor_win_get, etc.; 3) high-level distributed optimizer wrappers. In addition, BlueFog provides bfrun, a thin wrapper over mpirun to initiate BlueFog processes, and ibfrun to use BlueFog in interactive python environment such as Jupyter Notebook."
2111.04287,"python, code",71,,,"BlueFog is pretty strait-forward; the code is basically the python interpretation of the math equation (16)–(17). In addition to the exponential graph, many other directed or undirected static topologies have also been implemented in BlueFog such as ring, star, mesh, and fully-connected topology. Users can also set up their own topology in BlueFog to facilate decentralized algorithms."
2111.05792,data,2,,,.com/us/solutions/cloud/data-directory-2810741.pdf.
2111.05792,data,6,,,B. Data Collection and Preparation
2111.05792,data,15,,,[7] Federal Trade Commission. Data brokers: A call for transparency and
2111.05792,data,16,,,[16] Chris Stokel-Walker. Facebook’s ad data may put millions of gay
2111.05792,data,29,,,"[13] Ashkan Soltani and Barton Gellman. New documents show how the NSA infers relationships based on mobile location data. The Washington Post, 2013."
2111.05792,data,31,,,"For the user proﬁling model, we train a separate model for each interest segment in the Oracle Data Cloud Registry to predict whether this interest segment will be triggered by"
2111.05792,data,43,,,"[83] Alexander Sj¨osten, Steven Van Acker, and Andrei Sabelfeld. Discovering browser extensions via web accessible resources. In Proceedings of the Seventh ACM on Conference on Data and Application Security and Privacy, pages 329–336, 2017."
2111.05792,data,45,,,"The authors would like to thank John Cook for his help with initial data collection. This work is supported in part by the Robert N. Noyce Trust and the National Science Foundation under grant numbers 1956435, 1901488, 2051592, and 2103439."
2111.05792,data,46,,,"Data collection for surrogate models. We construct 10,000 personas to collect data from real-world user proﬁling and ad targeting models in order to train the surrogate models. The proportion of obfuscation URLs, α, in each persona varies between 0 and 0.2."
2111.05792,data,56,,,"To train the bid and segment predictors, we start by randomly constructing a set of user personas. Then, we collect training data (by the Oracle Registry for the user proﬁling model and multiple bidders for the ad targeting model) and use supervised learning, see Section IV for more details."
2111.05792,data,68,,,"RL agent. We train and test the RL agent with the data collected from the 15,000 personas. Speciﬁcally, we train the RL agent using the surrogate models to collect the reward and run the training for 300 rounds. We test the RL agent using surrogate models for 10 rounds. At each training or testing round we generate a batch of 50 personas."
2111.05792,data,75,,,"Next, we evaluate the effectiveness of HARPO and baselines against real-world user proﬁling and ad targeting models. To this end, we replace surrogate models with the real-world user proﬁling model by Oracle Data Cloud Registry and ad targeting models of 10 different bidders. Table IVa reports the effectiveness of HARPO and baselines against real-world user proﬁling (L1 and L2) and ad targeting (L3 and L4) models."
2111.05792,data,76,,,"Surrogate models. For each of the 176 surrogate models (121 interest segment and 55 bid models), we utilize 80% of the data collected from the 10,000 personas for training and 20% for testing. We train each model via stochastic gradient descent [68] with a batch size of 32 personas for 30 training rounds, where all the training data are used once at each training round."
2111.05792,data,79,,,"Paper Organization: The rest of the paper is organized as follows. Section II describes the threat model. Section III presents the design and implementation of HARPO. We describe the experimental setup, including data collection and in Section IV. training process for HARPO and baselines, Section V presents the evaluation results. We discuss ethical issues and limitations in Section VI. Section VII summarizes prior literature before concluding with Section VIII."
2111.05792,data,81,,,"[75] Finn Brunton and Helen Nissenbaum. Political and ethical perspectives on data obfuscation. Privacy, due process and the computational turn: The philosophy of law meets the philosophy of technology, 2013. [76] Garrett A Johnson, Scott K Shriver, and Shaoyin Du. Consumer privacy choice in online advertising: Who opts out and at what cost to industry? Marketing Science, 39(1):33–51, 2020."
2111.05792,data,89,,,"models. To this end, we create 100 personas with 100 URLs each. For each persona we start with a fresh browser proﬁle in OpenWPM [4]. For ad targeting, we immediately collect the triggered bid values as we visit the 100 URLs of the persona. For user proﬁling, we save the browser state after visiting all 100 URLs of the persona, and wait for 2 days to access the Oracle Data Cloud Registry and collect the triggered interest segments."
2111.05792,data,97,,,"Data preparation for surrogate models. We ﬁrst clean the data by removing unrelated interest segments such as those related to geographic location or device type, and by removing zero bids. Then, for each user persona for which we collected some feedback, we extract content features from the visited web pages, concatenate the document embedding vectors of all visited URLs of the persona into an embedding matrix, and use this matrix as the input to the surrogate models. We use a surrogate model for each interest segment, where the"
2111.05792,data,112,,,"In this section, we contextualize our work with respect to prior literature on enhancing user privacy in online behavioral advertising. Online advertising platforms typically do not allow users to meaningfully opt in/out of tracking. The notable exception is Apple’s newly introduced App Tracking Transparency feature that requires apps to get permission from users to track them for targeted advertising [22]. Unfortunately, a vast majority of data brokers do not give users any meaningful choice about tracking. Thus, as we discuss next, the privacy community has developed a number of privacy-enhancing blocking and obfuscation tools geared towards online behavioral advertising."
2111.05792,data,120,,,"Data collection for RL agent. We construct 15,000 personas, 50 for each of 300 training rounds of the RL agent, to train the RL agent. Each persona consists of 100 URLs. Recall that user URLs are selected using the MC model from the IAB categories, and the obfuscation URLs are selected from the intent category based on the actions generated by the RL agent. The ﬁrst 20 URLs are selected randomly from the user URL set for initialization. The remaining 80 URLs are either obfuscation URLs (with probability α) or user URLs (with probability 1−α). Thus, we have on average 80·α obfuscation URLs per persona."
2111.05792,data,123,,,"of the visited URLs up to time step t, where S denotes the state space of the MDP. Note that this state deﬁnition means the state space will grow indeﬁnitely. Yet we do so because the retention time of URLs by data brokers, including the Oracle Data Cloud Registry, are often in the order of 12 to 18 months [50]. Thus, we want to select an obfuscation URL based on the entire browsing proﬁle of a persona. While such a state space complicates analytical treatment, as we discuss later in Section III-D, we use a recurrent model as part of our RL model which allows us to handle this effectively."
2111.05792,data,125,,,"To address the privacy concerns of online behavioral advertising, some platforms now allow users to opt in/out of tracking. Notably, iOS 14.5 introduced a new App Tracking Transparency feature that requires apps to get permission from users to track them for targeted advertising [22]. Unfortunately, the vast majority of data brokers do not give users any meaningful choice about tracking. The privacy community has also developed privacy-enhancing tools to enable users to outright block online advertising and tracking. These blocking tools, available as browser extensions such as uBlock Origin [23] and Ghostery [24], are now used by millions of users. However, advertisers and trackers can often circumvent these"
2111.05792,data,139,,,"HARPO modules discussed in Section III-D in the following order: (i) we use the doc2vec embedding model to extract features for pages visited by each persona, (ii) we crawl data to train surrogate user proﬁling and ad targeting models and use them to train the RL agent, (iii) we use the RL agent to select obfuscation URLs, and (iv) we use the URL agent to create obfuscated personas. Then, we evaluate HARPO’s effectiveness in protecting user privacy as compared to the baselines against real-world user proﬁling and ad targeting models. Finally, we analyze HARPO’s performance from three key perspectives: overhead, stealthiness (using an adversarial detection model introduced later in Section V-D), and adaptiveness."
2111.05792,data,142,,,"We also assume that the tracker’s goal is to train machine learning models to proﬁle and target arbitrary users rather than a particular user with a known identity (e.g., email address, account identiﬁer). In the latter case, the tracker can trivially gather information by collaborating with a ﬁrst-party publisher (e.g., social network or e-commerce site). We assume that it is not the case. Even when this assumption is invalid, we contend that a privacy-conscious user would be able to leverage data deletion requests under privacy regulations, such as GDPR [46] or CCPA [47], to remove their identity or information. In summary, we assume that the tracker does not have the user’s non-obfuscated browsing proﬁle to begin with."
2111.05792,data,153,,,"Using these personas we collect feedback from real-world user proﬁling and ad targeting models as follows: For each persona, we start with a fresh browser proﬁle in OpenWPM [4]. For ad targeting, we access bidding sites to collect the triggered bids immediately after visiting the 20 URLs. For user proﬁling, since we observe that it takes on average two days for triggered interest segments to appear in Oracle Data Cloud Registry, we save the browser state after visiting the 20 URLs and reload it after 2 days to collect the triggered interest segments. In total, we collect 184 different interest segments from the Oracle Data Cloud Registry and bids placed by 10 different bidders on 16 different ad slots. Note that for each bidding site, there could be multiple bidders that might place different bids for different ad slots."
2111.05792,data,179,,,"User proﬁling and ad targeting models. Many advertisers provide interest segments, such as “travel-europe”, “petsdogs”, “health-dementia”, inferred by their user proﬁling models for transparency [49], [50]. Furthermore, the bids placed by advertisers as part of the real time bidding (RTB) protocol are often visible in the browser [51]. We leverage this blackbox access to user proﬁling (i.e., interest segments) and ad targeting models (i.e., bid values2) to collect the data needed to train our own surrogate models. Speciﬁcally, we extract the interest segments made available by the Oracle Data Cloud Registry [41] which gathers data based primarily on cookies, and the bid values placed by advertisers in the Prebid.js header bidding implementation [42], [52]. Note that Oracle is a wellestablished data broker that combines data from more than 70 partners/trackers [53] and is accessible without needing a"
2111.05792,data,235,,,"Collecting data from real-world models is a costly operation. Thus, we determine the suitable length of a persona based on the following analysis, keeping data collection efﬁciency in our mind. Let N be the average number of URLs per persona, which we wish to determine. Let n be the fraction of personas for which we are able to collect some feedback (i.e., the trackers return no feedback for the rest). 10, 000 · n is the total number of personas we can collect feedback for and 10, 000 · N is the total number of URLs among all personas. We choose to select N such that we maximize n/N for the following reason: While longer personas with more URLs will likely trigger more feedback, computational overheads (e.g., CPU and memory) are also proportional to the total number of URLs. Thus, the most efﬁcient choice is to maximize the feedback we collect per URL, and n/N represents the number of personas with non-empty feedback per URL. The above procedure yields a value of N equal to 20, and we use the MC model described above to select the user URLs, and HARPO to select obfuscation URLs from the intent URL category, for a total of 20 URLs per user persona."
2111.05792,"data available, data",51,,,"6While proﬁle registries like the Oracle Data Cloud Registry are required by law to allow users access to their proﬁles, these proﬁles may be updated every few days. Thus, it would take months to collect enough samples to train the RL agent solely by accessing such registries."
2111.05792,"data available, data, data https",42,,,"[50] Tobias Urban, Dennis Tatang, Martin Degeling, Thorsten Holz, and Norbert Pohlmann. A study on subject data access in online adverIn International Workshop on Data Privacy tising after the GDPR. Management. 2019."
2111.05792,"data, data https",12,,,[41] Oracle Data Cloud Registry Information. https://datacloudoptout.orac
2111.05792,"data, data https",14,,,[53] Oracle Data Cloud Registry 2019 Data Directory. https://www.oracle
2111.05792,"data, data https",29,,,[46] General Data Protection Regulation (GDPR). https://gdpr-info.eu/. [47] California Consumer Privacy Act (CCPA). https://oag.ca.gov/privacy
2111.05792,"data, data https",129,,,"Online behavioral advertising poses a real privacy threat due to its reliance on sophisticated and opaque tracking techniques for user proﬁling and subsequent ad targeting [1]–[6]. The tracking information compiled by data brokers for the sake of online behavioral advertising is often outright creepy and scarily detailed [7]–[10]. Furthermore, the surveillance capitalism business model of the “free” web naturally aligns with mass surveillance efforts by governments [5], [11]–[14]. Finally, beyond privacy, the targeting capabilities of online behavioral advertising are routinely abused for discrimination [15]–[17] and manipulation [18]–[21]."
2111.05792,"data, dataset",13,,,Model type Number of models Dataset size Positive data Average FPR Average TPR
2111.05792,"data, dataset",91,,,"ad targeting models with good accuracy, which achieve 18.28% FPR and 73.43% TPR on average as shown in Table II. The FPRs of these 10 surrogate models range from 12.37% to 16.93% and the TPRs vary from 70.27% to 78.35%. Last, among the 10 datasets training the top 10 surrogate ad targeting models, the percentage of positive data (indicating a high bid is triggered) is 11.26% on average, ranging from 8.55% to 15.38%."
2111.05792,"data, dataset",121,,,"User proﬁling. In general, the trained surrogate user proﬁling models have reasonable accuracy. As reported in Table II, the average FPR and TPR of the 20 most accurate surrogate user proﬁling models are 3.92% and 96.57% respectively. The FPRs of these 20 surrogate user proﬁling models ranges from 1.82% to 19.23% and the TPRs vary from 81.43% to 100.00%. Last, among the 20 datasets training the top 20 surrogate user proﬁling models, the percentage of data points with label value 1 (positive data, indicating the segment is triggered) varies from 3.91% to 15.87%, with an average value of 7.04%."
2111.05792,"data, dataset",254,,,"Adversarial detection. To build a supervised detection model, the tracker needs to gather training data comprising of both non-obfuscated and obfuscated browsing proﬁles. To this end, we assume a strong adversary that has access to sufﬁcient non-obfuscated browsing proﬁles as well as black-box access to obfuscators (including HARPO) that can be used to gather obfuscated browsing proﬁles. To train the classiﬁcation model, we assume that the tracker extracts embedding based content features for all the URLs in the available positive and negative labeled browsing proﬁles. Thus, we assume that the tracker: (1) can track all the URLs in a user’s browsing proﬁle; (2) is able to extract content features for any URL that a user may visit; and (3) has sufﬁcient resources to gather training data and train an ML-based supervised detection model. Based on these assumptions, we design a binary ML classiﬁer that uses the doc2vec embeddings as features of a user browsing proﬁle and outputs a binary detection decision to indicate whether or not a given persona is obfuscated by HARPO (or other obfuscators under consideration). We gather a dataset of obfuscated and non-obfuscated personas containing a total of 20,000 URLs and use a similar 80-20 split to train and test this detector. We then use the detection error as a metric to measure stealthiness–obfuscation is more/less stealthy if the detection error is higher/lower."
2111.05792,dataset,15,,,(c) Effectiveness against real-world tracker models using real user personas from AOL dataset
2111.05792,dataset,64,,,User traces. We evaluated HARPO using both real user traces from the 15 year old AOL dataset and synthetic traces based on our user persona model. We acknowledge that the characteristics of the AOL and synthetic traces might be different than those of current real users. A future line of research would be to evaluate HARPO by recruiting real users.
2111.05792,dataset,67,,,"User persona modelDocument embedding modelRL agentSurrogate trackermodelsUser profilingAd targetingAdversarial detection modelNon-obfuscated personasPrivacy evaluationStealthinessevaluationContent featuresURL agentObfuscated personasHarpoTrain RLReal-world tracker modelsOracleBidderspersonas. To this end, we randomly sample 100 real-world user personas from the AOL dataset and use them as nonobfuscated personas. Then, we use HARPO and baselines approaches to generate 100 obfuscated personas and evaluate the effectiveness of obfuscation against real-world tracker models."
2111.05792,dataset,83,,,"Section IV-C discusses how we train the surrogate models using supervised learning. Let a dataset refer to all the user persona embedding matrices and the associated labels collected. If the percentage of labels with value 1 in a dataset is less than 5%, we remove it because it is likely not sufﬁcient for training surrogate models later. We end up with 121 interest segment datasets and 50 bid datasets for training a total of 171 surrogate models."
2111.05792,dataset,84,,,"[72] Ryan Amos, Gunes Acar, Elena Lucherini, Mihir Kshirsagar, Arvind Narayanan, and Jonathan Mayer. Privacy policies over time: Curation and analysis of a million-document dataset. arXiv:2008.09159, 2020. [73] Matthew W Vail, Julia B Earp, and Annie I Ant´on. An empirical study of consumer perceptions and comprehension of web site privacy policies. IEEE Transactions on Engineering Management, 55(3):442– 454, 2008."
2111.05792,dataset,109,,,"In the rest of the paper, we use the aforementioned model to generate web browsing proﬁles for user personas. Since the most popular categories are not necessarily the same for each user persona, we select the 100 most common combinations of the 3 most popular URL categories from the AOL dataset and deﬁne 100 user types. Then, every time we want to generate a web browsing proﬁle for a user persona, we randomly select one user type which sets the speciﬁc 3 most popular categories for this user, and use the MC model to generate the user URLs as described above."
2111.05792,dataset,125,,,"To specify the model parameters, ﬁrst we need to decide how many popular categories will have their own state. We do so by assigning a separate state to categories whose URLs represent more than 10% of the total URLs in the dataset. Figure 3a plots the percentage of URLs in a user’s web browsing proﬁle from the ith most popular URL category for this user, averaged over all users. From the ﬁgure we conclude that the 3 most popular categories satisfy our criteria. Thus, we set the total number of states of the MC to 4, one for each of the 3 most popular categories and one collectively for the 13 remaining categories."
2111.05792,dataset,139,,,"one state to another for a jth-order MC depends on the j most recent states. That said, the higher the order the higher the complexity of the MC, as the state space grows exponentially, see, for example, [66]. Following standard practice, we use the autocorrelation function to measure the correlation in the AOL dataset and experiment with different order MCs to identify the smallest order required for a good ﬁt. Figure 3b shows that a 1st order MC is enough to achieve a good ﬁt. Last, given the order and number of states of the MC, we ﬁt the stationary distribution and transition probabilities of our MC model to the statistics of the dataset (see Figure 4 for the ﬁnal MC model)."
2111.05792,dataset,168,,,"Speciﬁcally, we start with the AOL dataset [63] which consists of millions of distinct URLs and web browsing proﬁles of millions of users.8 We then randomly sample users with more than 100 visited URLs each, and leverage WhoisXMLAPI [64] to map each URL into one of the 16 IAB categories from Alexa [65]. We observe that real web browsing proﬁles consist of URLs from a handful of preferred URL categories. Motivated by this, we use a Markov Chain (MC) model to generate web browsing proﬁles as follows: a MC state dictates the category from which a URL is selected. We assign a separate state to each of the most popular categories, and a single state collectively to the rest of the categories. As the MC transits from state to state, a URL is randomly selected from the URL category (or categories) that corresponds to the current state."
2111.05792,github,30,,,"[23] Raymond Hill. An efﬁcient blocker for Chromium and Firefox. Fast and lean, uBlock Origin. https://github.com/gorhill/uBlock\#ublock-o rigin, 2019."
2111.05792,"github, code available, code",118,,,"In this paper we presented HARPO, a principled reinforcement learning-based obfuscation approach to subvert online targeted advertising. HARPO signiﬁcantly outperforms existing obfuscation tools by as much as 16× for the same overhead. Additionally, for the same level of privacy, HARPO provides better stealthiness against potential countermeasures. Thus, the privacy protections offered by HARPO are better suited for the arms race than existing obfuscation tools. We hope that HARPO and follow-up research will lead to a new class of obfuscation-driven effective, practical, and long lasting privacy protections against online behavioral advertising. To facilitate follow-up research, HARPO’s source code is available at https://github.com/bitzj2015/Harpo-NDSS22."
2111.05792,"publicly available, dataset",37,,,"8While the AOL dataset is somewhat dated, it is one of the largest publicly available datasets of real-world user browsing proﬁles and captures well the browsing behavior of a large, diverse set of users."
2111.06021,data,46,,,"[3] Karsten M Borgwardt, Arthur Gretton, Malte J Rasch, HansPeter Kriegel, Bernhard Sch¨olkopf, and Alex J Smola. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49–e57, 2006."
2111.06021,data,55,,,"Different from the seminal UDA framework, where unlabeled target data are utilized to explicitly minimize the domain divergence, recent UDA methods [24, 17, 53] have been proposed to explore the data structure of unlabeled data. Our proposed method in this paper belongs to this type of approaches."
2111.06021,data,61,,,"Semi-Supervised Learning (SSL) [2, 31, 15] aims to leverage the vast amount of unlabeled data with limited labeled data to improve performance. [71] classiﬁes the SSL methods into ﬁve categories, i.e., generative methods, consistency regularization methods, graph-based methods, pseudo-labeling methods, and hybrid methods. Recently,"
2111.06021,data,101,,,"Contrastive learning [5, 56, 20, 19, 4, 8, 16, 10, 62] is a framework that learns similar/dissimilar representations from data that are organized into similar/dissimilar pairs. Since there is no label information, an instance discrimination pretext task [65] is used, where a query and a key form a positive pair if they are data-augmented versions of the same image. They form a negative pair otherwise. In these works, an effective contrastive loss function, called InfoNCE [41] is widely adopted."
2111.06021,data,102,,,"Deep neural networks based on fully-supervised learning strategies have made great progress in image classiﬁcation. [30, 21, 22]. However, such fully-supervised learning algorithms require the training data and test data to be collected from the same distribution. Directly applying the model trained in the old domain (source) to the new domain (target) often encounters performance degradation. In machine learning community, domain adaptation methods [26, 40] have been proposed to address this issue, which aims to transfer knowledge from source domain to target domain."
2111.06021,data,118,,,"The quantitative evaluation results on CIFAR-10/100 are In the case of extremely limited lareported in Table 5. beled data (4 samples per class), our method get signiﬁcant gains on CIFAR-10 (+4.8%) and CIFAR-100 (+4.0%) compared with the baseline FixMatch. It well demonstrates the effectiveness of our method. It is worth noting that our performance is superior to SsCL and CoMatch that use feature contrast learning. In particular, CoMatch boosts the original feature contrastive learning through memory-smoothed pseudo-labeling and graph structure, while our method does not require any additional techniques. We believe these techniques can further enhance the performance of probabilistic contrastive learning."
2111.06021,data,133,,,"the consistency-based approach has attracted the attention of many reseachers. Mean teacher [55] uses two different models to ensure consistency across similar images. MixMatch [2] and ReMixMatch [1] use interpolation between labeled and unlabeled data to generate perturbed features. FixMatch [49] achieves impressive performance by generating the conﬁdent pseudo labels of the unlabeled samples and treating them as labels for the perturbed samples. Due to the effectiveness and simplicity of FixMatch, it is also widely applied in other semi-supervised tasks, such as semantic segmentation [9, 80] and object detection [50, 67, 54]. In this work, we consider semi-supervised image classiﬁcation task and also use FixMatch as a strong baseline."
2111.06021,data,147,,,"In this section, we ﬁrst review feature contrastive learning widely used in unsupervised learning, and then elaborate on our probabilistic contrastive learning. Formally, let B = {(xi, ˜xi)}N i=1 be a batch of data pairs, where N is the batch size, and xi and ˜xi are two random transformations of a sample. We deﬁne the model M = E ◦ F with the feature extractor E and the classiﬁer F . Here F has the parameters W = (w1, ..., wC), where C is the number of classes, and wk is the class weights of the k-th class (also called class prototype). We use the E to extract the features from B, and get F = {(fi, ˜fi)}N"
2111.06021,data,166,,,"Our main contributions are three-folds. 1) To the best of our knowledge, we are the ﬁrst to clearly point out that traditional feature contrastive learning cannot work well for domain adaptation because it does not consider class information to narrow the distance between features and weights. 2) We designed a simple yet effective probabilistic contrastive loss, which only needs to replace the features in FCL with probability and remove the (cid:96)2 normalization. As a result, our method can enforce the learned features to be distributed around the class weights. 3) We conduct extensive experiments on two domain adaptation tasks to verify the effectiveness of our proposed PCL, i.e., UDA and SSDA. The results well show the superiority of our method to previous state-of-art methods in terms of simplicity and performance. In addition, our method also works well on semi-supervised tasks when the annotated data is scarce."
2111.06021,data,168,,,"Semi-Supervised Domain Adaptation (SSDA) aims to reduce the discrepancy between the source and target distribution in the presence of limited labeled target samples. [46] ﬁrst proposes to align the distributions using adversarial training on entropy. [28] shows the presence of intradomain discrepancy in the target distribution and introduced a framework to mitigate it. [23] uses consistency alongside multiple adversarial strategies on top of MME. [32] presents a meta-learning framework for SSDA. [70] breaks down the SSDA problem into two subproblems, namely, SSL in the target domain and UDA problem across the source and target domains, and then proposes to learn the optimal weights of the network using co-training. [33] proposes an adversarial adaptive clustering loss to group the features of unlabeled target data into clusters and perform cluster-wise feature alignment across the source and target domains. It uses FixMatch to improve the performance."
2111.06021,data,169,,,"feature center in feature space. Actually, contrastive learning only achieves the ﬁrst one, and does not take into consideration the second one. This is because the current contrastive learning methods [5, 66, 6, 78] usually use the features before the classiﬁer to calculate the contrastive loss, while the class weight information is not involved in the optimization process. Therefore, feature contrastive learning cannot enforce the features to cluster around the class weight. For the domain adaptation tasks, however, there is a signiﬁcant domain shift problem between the source domain and target domain. For such a situation, it is quite difﬁcult for the class weights learned from the source domain to locate at the class center of the target domain data. Consequently, the features of the target domain are hard to be classiﬁed correctly even if they are semantically compact, as shown in Figure 1(b)."
2111.06021,data,184,,,"In fact, the applicable scenarios of PCL are not limited to the domain adaptation. As long as the features of unlabeled data cannot be clustered around the class weights, our method has a good potential to improve the performance. In this section, we consider the case where the source do A→C A→P A→R C→A C→P C→R P→A P→C P→R R→A R→C R→P Avg Method Source-Only 46.1 38.5 34.9 TAT (ICML’19) 65.8 59.5 51.6 67.2 63.6 47.7 SymNet (CVPR’19) MDD (ICML’19) 68.1 61.2 54.9 BNM (CVPR’20) 69.4 62.4 56.2 72.7 65.8 58.1 FixBi (CVPR’21) ToAlign (NeurIPS’21) 72.0 67.8 57.9 SCDA (ICCV’21) 73.1 68.9 60.7 70.7 64.6 TCM (ICCV’21) 58.6 70.4 65.2 57.0 GVB (CVPR’20) 71.3 65.7 + MetaAlign(CVPR’21) 59.3 72.3 67.0 59.7 + Our PCL 73.5 68.7 59.8 + FixMatch 74.5 69.9 + Our PCL + FixMatch 60.8"
2111.06021,data,194,,,"In contrastive learning, due to lack of labels, there may be false negative samples that belong to the same category with positive sample data. Many methods [16, 27, 66] point out that the false negative samples are harmful to contrastive learning. Therefore, a natural question is whether we can make feature contrast learning work well in domain adaptation tasks by alleviating the false negative problem without speciﬁcally designing probabilistic contrastive learning. An appropriate way to solve the false negative problem is to use supervised feature contrastive learning (SFCL) [16, 27]. Therefore, in this section, we mainly compare PCL with SFCL to verify whether PCL is necessary in domain adaptation tasks. For SFCL, in addition to the positive samples constructed by different transformations, it also selects the samples which are similar to the positive samples from the negative samples as positive samples. For convenience, we deﬁne Ifi = {fi}N 1 \fi, which means the set of all features except fi. The loss function is deﬁned as:"
2111.06021,data,217,,,"For the domain adaptation task, it is difﬁcult for the features in target domain to form semantically compact clusters as the target data lacks the supervision signals. Consequently, various categories of the target domain are hard to be distinguished, as shown in Figure 1(a). As we all know, the instance contrastive learning methods based on InfoNCE loss [41] can learn semantically compact feature representations on unlabeled data, which means that contrastive learning tends to cluster together semantically similar features [5, 66, 6, 27]. Many works have shown that the model trained in this way has good transferability [20, 7, 78]. Therefore, a straightforward idea for domain adaptation is that we perform supervised learning in source domain while performing contrastive learning in target domain, where it is expected that each category in the target domain is easy to be distinguished with semantically compact representations, as shown in Figure 1(b). However, our study experiments found that the gain from this way is very limited, and even no obvious gain is got for the model equipped with a strong consistency constraint, as shown in Figure 2."
2111.06021,"data, dataset",217,,,"Recent feature contrastive learning (FCL) has shown promising performance in self-supervised representation learning. For domain adaptation, however, FCL cannot show overwhelming gains since the class weights are not involved during optimization, which does not guarantee the produced features to be clustered around the class weights learned from source data. To tackle this issue, we propose a novel probabilistic contrastive learning (PCL) in this paper, which not only produces compact features but also enforces them to be distributed around the class weights. Speciﬁcally, we propose to use the output probabilities after softmax to perform contrastive learning instead of the extracted features and remove the (cid:96)2 normalization in the traditional FCL. In this way, the probability will approximate the onehot form, thereby narrowing the distance between the features and the class weights. Our proposed PCL is simple and effective. We conduct extensive experiments on two domain adaptation tasks, i.e., unsupervised domain adaptation and semi-supervised domain adaptation. The results on multiple datasets demonstrate that our PCL can consistently get considerable gains and achieves the state-of-theart performance. In addition, our method also obtains considerable gains on semi-supervised tasks when labeled data is scarce."
2111.06021,"data, dataset",292,,,"main and target domain come from the same distribution, i.e., semi-supervised learning. In particular, for the semisupervised tasks, the unlabeled features will deviate from the class weight when the labeled data is very scarce. Therefore, we consider the situation of very scarce labels. In this work, 4 samples per class is particularly adopted for evaluation. Datasets. We conduct SSL experiments on the CIFAR10 and CIFAR-100 datasets. CIFAR-10 (CIFAR-100) [29] contains 50,000 images of size 32 × 32 from 10 (100) classes. Similarly, we take FixMatch [49] as our baseline, and the backbone and training hyperparameters are exactly the same as FixMatch. We evaluate on 5 runs with different random seeds, and then report the mean and standard variance. Experimental Details. Following [2, 49], we report the performance of an EMA model and use a Wide ResNet-282 [73] for CIFAR-10 and use a WRN-28-8 for CIFAR-100. The models are trained using SGD with a momentum of 0.9 and a weight decay of 0.0005. We follow the original papers [2, 49] and train all models for 1024 epochs, using an learning rate of 0.03 with a cosine decay schedule. For the hyperparameters used in FixMatch, we follow FixMatch and set λcls = 1, τ = 0.95, and µ = 7, B = 64. For the hyper-parameters in PCL, we set s = 4, λ = 0.02 for CIFAR-10 and s = 7 ,λ = 0.05 for CIFAR-100. Comparison with SOTA. We compare our method"
2111.06021,"data, dataset",328,,,"Datasets. We evaluate the effectiveness of our proposed approach on two SSDA image classiﬁcation benchmarks, i.e., DomainNet [42] and Ofﬁce-Home. DomainNet is initially a multi-source domain adaptation benchmark. Similar to MME [46], we only select 4 domains Real, Clipart, Painting, and Sketch (abbr. R, C, P, and S), each of which contains images of 126 categories. Ofﬁce-Home is a widely used UDA benchmark and consists of Real, Clipart, Art, and Product (abbr. R, C, A, and P) domains with 65 classes. For fair comparison, the settings of our benchmark datasets refer to the existing SSDA approaches [46, 44, 28], including adaptation scenarios of each dataset, the number of labeled target data (typically 1-shot or 3-shot per class), sample selection strategies, etc. In particular, we choose MME [46] as our baseline and report the results on both ResNet34 [21] and AlexNet [30]. Experimental Details. Following MME [46], we remove the last linear layer of AlexNet and ResNet34, while adding a new classiﬁer F . We also use the model pre-trained on ImageNet to initialize all layers except F . We adopt SGD with a momentum of 0.9 and set the initial learning rate is 0.01 for fully-connected layers whereas it is set 0.001 for other layers. The max iteration number is set to 50k. For the hyper-parameters in PCL, we set s = 7 in all experiments. We set λ = 0.1 under the setting of 3-shot for AlexNet and set λ = 0.2 under the setting of 3-shot for ResNet34 in DomainNet. For all other experiments, we set λ = 0.05. Comparison with SOTA."
2111.06021,"data, dataset",334,,,"Ofﬁce-Home is a widely used UDA benchmark and consists of Real, Clipart, Art, and Product (abbr. R, C, A, and P) domains with 65 classes. For fair comparison, the settings of our benchmark datasets refer to the existing SSDA approaches [46, 44, 28], including adaptation scenarios of each dataset, the number of labeled target data (typically 1-shot or 3-shot per class), sample selection strategies, etc. In particular, we choose MME [46] as our baseline and report the results on both ResNet34 [21] and AlexNet [30]. Experimental Details. Following MME [46], we remove the last linear layer of AlexNet and ResNet34, while adding a new classiﬁer F . We also use the model pre-trained on ImageNet to initialize all layers except F . We adopt SGD with a momentum of 0.9 and set the initial learning rate is 0.01 for fully-connected layers whereas it is set 0.001 for other layers. The max iteration number is set to 50k. For the hyper-parameters in PCL, we set s = 7 in all experiments. We set λ = 0.1 under the setting of 3-shot for AlexNet and set λ = 0.2 under the setting of 3-shot for ResNet34 in DomainNet. For all other experiments, we set λ = 0.05. Comparison with SOTA. We compare our method with including “S+T”, previous state-of-the-art approaches, “MME” [46], “UODA” [44], “BiAT” [23], “Meta-MME” [32], “APE” [28], “ECACL-P” [35], and “CDAC” [33]. Here the “S+T” refers to the model trained using labeled source and labeled target data only."
2111.06021,"data, dataset",341,,,"Datasets. We evaluated our method in the following two standard benchmarks for UDA. Ofﬁce-Home [59] consists of images of everyday objects organized into four domains: Artistic (Ar), Clipart (Cl), Product (Pr), and Real-world (Rw). It contains 15,500 images of 65 classes. VisDA2017 [43] is a large-scale dataset for synthetic-to-real domain adaptation. It contains 152,397 synthetic images for the source domain and 55,388 real-world images for the target domain. Following the standard transductive setting [77] for UDA, we use all labeled source data and all unlabeled target data, and test on the same unlabeled target data. Experimental Details. We adopt the GVB-GD [14] architecture as our baseline. That is, GVB is adopted for both the generator and discriminator, and we follow the original experimental settings in GVB-GD. We use the ResNet-50 [21] model pre-trained on ImageNet as the backbone network for both Ofﬁce-Home and VisDA-2017. For network training, we use mini-batch stochastic gradient descent (SGD) with a momentum of 0.9 and a weight decay of 0.001. The initial learning rate is set to 0.001 for Ofﬁce-Home, and 0.0003 for VisDA-2017. The max iteration number is set to 20k. For the hyper-parameters in PCL, we set s = 7.0 and λ = 0.05 and ﬁx them. Comparison with SOTA. Here we compare different including “DAN” [38], representative UDA methods, “DANN” [18], “GTA” [48], “TAT” [37], “SymNet” [75], “MDD” [63], “FixBi” [40], “ToAlign” [64], “SCDA” [36], “TCM” [72], and “GVB” [14]."
2111.06021,dataset,40,,,Figure 4. The t-SNE visualization of learned features. We focus on the relationship between features and class weights on the C→S task of DomainNet dataset with Resnet34 under the setting of 3shot. Best viewed in color.
2111.06021,dataset,45,,,"[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016."
2111.06021,dataset,110,,,"Table 3 and Table 4 give the results on DomainNet and Ofﬁce-Home. It can be seen that our method on all datasets and all settings can get a signiﬁcant gain compared with the baseline MME∗. For DomainNet, under four different settings, our method can obtain a gain of more than 5%. It strongly proves the effectiveness of our method. Furthermore, our method outperforms other methods except CDAC and ECACL-P that adopt the FixMatch technique. For fair comparison, we also add FixMatch to our method. We can see that our method combined with FixMatch can achieve new state-of-the-art performance."
2111.06021,dataset,114,,,"In this paper, we found that the traditional feature contrastive learning can only cluster the features of similar semantics and cannot enforce the learned features to be distributed around the class weights due to the class weights are not involved during optimization. To solve this problem, we propose a novel probabilistic contrastive learning. Speciﬁcally, we use the probabilities after Softmax instead of the features, and remove the (cid:96)2-norm normalization widely used in FCL. We experimentally veriﬁed the effectiveness of our proposed methods in three tasks with multiple datasets. We believe that our PCL provides an innovative route for various visual tasks."
2111.12360,data,42,,,"[14] M. Obst, L. Hobert, and P. Reisdorf, “Multi-sensor data fusion for checking plausibility of V2V communications by vision-based tracking,” IEEE Vehicular Networking Conference, multiple-object VNC, pp. 143–150, 2015."
2111.12360,data,49,,,occlusions occurring in this scenario. Errors associated with such occluded ground-truth objects cannot be detected by the ego vehicle sensor. We thus expect an even higher recall in a real scenario where sensor input is used for the primary instead of the ground truth data.
2111.12360,data,99,,,"2) Theoretical analysis of position errors: A position error can be seen as a combination of a false positive and a false negative detection, as described above. The capability of the sensor check monitor component to detect position errors is determined by the grid size cgrid, determining the spatial resolution, the expected noise Σ(o) of the sensor input, and a safety margin denoted δsa f e. Robustness against noise and other non-faulty deviations of the sensor and object data is needed to assure a low false alarm rate."
2111.12360,data,172,,,"The development of automated vehicles (AVs) is one of the great technological challenges of today’s society. While signiﬁcant progress was made in the last years on the functional side, it remains yet unsolved how the safe operation of an AV can be assured. As illustrated in Fig. 1, the AV system represents a complex, multi-stage compute stack involving most notably sensing, perception, planning and actuation, constituting the primary channel. This compute chain is exposed to various sources of random faults, such as noise in the input data, hardware execution errors due to cosmic radiation, or systematic errors like software and hardware bugs, which can compromise the safe operation of the AV. Particularly, this holds in the presence of artiﬁcial intelligence (AI)-based components, which are well-established in the perception and planning domain, but are for example highly affected by unbalanced or incomplete training data [1]."
2111.12360,dataset,26,,,Fig. 9: Sensor check detection performance for permanent position errors (right) using the NuScenes dataset [21] (left).
2111.12360,dataset,53,,,"[21] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous driving,” arXiv preprint arXiv:1903.11027, 2019."
2111.12360,dataset,101,,,"featuring not only pedestrians but also cars and other vehicles. Those two setups were chosen to provide a diverse perception input to the ego vehicle, in terms of environment constellations and object types. In addition to that, we have tested position errors with the NuScenes dataset [21] as an example of non-simulated LiDAR information. Each scenario duration was sufﬁciently long for the environment to contain more than 1000 relevant object states in scope, in order to guarantee statistical signiﬁcance. All experiments use the parametrization of Tab. I unless stated otherwise."
2111.12360,dataset,232,,,"We presented a lightweight monitor architecture for automated perception systems, which is realized by a combination of plausibility checks of an object’s motion history, and sensor checks that use LiDAR information. To improve overall safety, we envision that this perception monitor is coupled with a supervisor for the AD planning system, such as for example RSS. Compared to existing monitor approaches, our concept is characterized by two key advantages: i) The checks are designed to be effective yet simple, in order to reduce computational load, ii) We combine diverse sensor-dependent and sensor-independent methods to address both fault-tolerance against commoncause sensor failures and SOTIF. We evaluated the monitor in simulation, using CARLA, and performed additional tests with the NuScenes dataset. Our experiments demonstrate a high recall and precision (both > 90%) for the detection of both permanent and random position errors larger than at most 0.7m. A comparable detection performance is found for transient speed errors greater than 2m/s or permanent ones greater than 6m/s. Except for the permanent speed errors, those values are more than sufﬁcient to detect the targeted errors of about 1m position and 3m/s velocity deviations, which we identiﬁed as a representative estimate for safetycritical perception errors in a wide spectrum of situations."
2111.12360,"dataset provided, data, dataset",78,,,"Finally, we also explore the error detection capability of our monitor with real LiDAR data from the NuScenes dataset [21]. Since no ground truth velocity data is provided here, we restrict ourselves to the analysis of permanent position faults with sensor checks only. As before, the ground truth environment model with injected position errors is used as a primary channel, while the LiDAR data enables the monitor sensor checks."
2111.12360,"python, code",211,,,"Our tests show (Fig. 10) that the sensor checks require an average of 12ms on the Core™ system using the conﬁguration of Table I (covering a 100m × 100m area around the ego vehicle, which should be sufﬁcient for urban driving). On the Intel® Atom™ the average latency increases to an average 18ms. Plausibility checking here takes an average of 5ms on the Core™ system to process about 30 objects. On the Atom™ the latency increases to an average 13ms. Those results indicate that our monitor architecture is indeed lightweight, meaning that it is feasible to run the veriﬁcation process at pseudo-real-time (typically ≈ 50ms − 100ms) on a safety-certiﬁable hardware. Note that the minimum latency target for an unobstructed system execution is eventually determined by the processing time of the parallel primary perception channel. The implementation of the code is not yet optimized and runs on a single core. The plausibility checks are currently implemented in Python and singlethreaded. With parallelism and other optimization techniques the latency could therefore be reduced further if required. Also using a non-uniform grid representation [22] can improve the latency of the sensor checks."
2111.13172,data,5,,,UDR: Unified Data Repository
2111.13172,data,26,,,2) The 5G system will establish the PDU session for location information and tracking data exchange and validation between the UAV/UAV-C and the USS/UTM.
2111.13172,data,32,,,"3) The UAV/UAV-C initiates the A&A request with the UAAF as UP data providing the UAV/UAV-C identity, USS/UTM identity if already known, and application level information."
2111.13172,data,36,,,"the scope of The 3GPP for broadcasting the RID, U8: The interface between the UAV and the UAV-C for transporting C2 data over a network that is beyond the scope of The 3GPP,"
2111.13172,data,37,,,"3) The UAS node sends the ﬂight operation permission request as UP data to the UTM. This request may include the UAV identity, its current location, planned trajectory, and so forth."
2111.13172,data,52,,,"Poly1305 and 256-bit keys with the advanced encryption standards, respectively [15]. It is critical to standardize and enforce encryption for all communications, including UAS originating or terminating data to prevent eavesdropping, location tracking, data breaches, and other attacks to privacy and security integrity."
2111.13172,data,58,,,"Figure 5 illustrates the 3GPP workﬂow for secure C2 communications link establishment between the UAV and the its controller, which can be the UAV-C or the USS/UTM. This procedure is an application programming interface (API)based solution that facilitates a secondary authentication with the USS/UTM for the PDU sessions via the 5G data network"
2111.13172,data,59,,,2) A request message is sent from the UAV to the AMF for establishing the PDU session with the USS/UTM. This message includes the CAA-level UAV ID and data network name/single-network slice selection assistance information (DNN/S-NSSAI). The AMF uses the subscription information of the UAV and the DNN/S-NSSAI to determine the appropriate SMF.
2111.13172,data,96,,,"data, and control UAVs. These entities are deﬁned under the umbrella of the third party authorized entity (TPAE). The TPAE can be an application server in the data network from the perspective of the 3GPP network. The control of a UAV for BVLOS operations can be performed either by the UAV-C or by the TPAE, where the C2 packets may be exchanged between the UAV and the UAV-C, UTM, or TPAE. The network interfaces are also illustrated in Fig. 1 and introduced in continuation."
2111.13172,data,112,,,"Potential threats: The location information can be compromised through a spooﬁng attack to create false location reports and force the USS/UTM to mislead the airspace management decisions into inaccurate and dangerous directions. The falsiﬁed location data created by a spoofer can lead to costly cyberphysical or kinetic attacks on the UAS and, for example, steer the UAV to ﬂy over unauthorized or prohibited airspace, deceive the maneuver strategy to create air conﬂicts, or confuse authorities or pilots about the location of UAVs. The location spooﬁng attacks can be carried out by external means through a fake GNSS or cell ID transmitter [14]."
2111.13172,data,137,,,"and user plane (UP) communications services for UASs and provide wireless connectivity between the UAS and non-3GPP aviation entities, such as the UAS Service Supplier (USS) and the UAS Trafﬁc Management (UTM) for beyond visual line of sighs (BVLOS) operations. The USS and UTM entities are responsible for providing various functions to ensure the safety and security of the UAS operations. These functions include C2 services, services to civil aviation authority (CAA), telematics, UAS-generated data, RID, authorization, enforcement, and regulation of UAS operation. The UTM/USS can be integrated in the 3GPP framework as an application function (AF), operating as a CP network function, or an application server in the data network."
2111.13172,data,146,,,"UAVs, UAV-Cs, and the USS/UTM need to be connected reliably and securely. In order to enable ﬂexible and safe operations of UAVs, the cellular communications network is being considered to carry UAS data and control signals and the corresponding interfaces and protocols are being standardized for emerging 5G networks. This paper has presented the 3GPP architecture for UASs connected to the 5G network and has discussed the critical security threats, the 3GPP procedures, and the remaining research and standardization opportunities related to A&A, location information privacy, and C2 signaling. Research needs to feed the standardization process of this rapidly evolving technology. Experimental research can further highlight the importance of rigorously specifying the security framework procedures, parameters, and conﬁgurations in the standards and ensuring that they are fully implemented and tested."
2111.13172,data,160,,,"• UAV ﬂight enablement subsystem (UFES): The UFES is implemented to serve as a single interface to the USS/UTM. Principally, the UFES performs the USS/UTM discovery mechanisms and the selection procedures without requiring other 3GPP network nodes. The USS/UTM selection by the UFES is based on the CAA-Level UAV ID, which provides RID&T information to the TPAE/USS/UTM that may be monitoring a UAV. The UFES supports delivery of the UAV external ID as the 3GPP UAV ID to the USS/UTM, and can retrieve relevant subscription information from the uniﬁed data management (UDM) and/or receive policy control information from the policy control function (PCF). The UFES determines a protocol data unit (PDU) session for the UAV operation through the session management function (SMF) to transmit the operation updates from the USS containing the updated authorized UAV and UAV-C pairing information."
2111.13172,data,211,,,"UAS authentication and authorization is the prerequisite to enable overruling the UAV-C in case of suspicious access after tracking the UAV data by the TPAE that can take over the control of the UAV. Consequently, the connection request must be authenticated and authorized by the 3GPP network differently from a normal UAV-C, UAV, or UE. The 3GPP network must follow certain policies regarding the unsuccessful authentication and authorization where the UAAF may inform the SMF to prevent the registration and/or the cancellation of illegitimate PDU sessions by an unapproved UAV or UAV-C. Potential threats: A weak UAS authentication process can grant access to an untrusted UAV or UAV-C to receive UAS services via the 3GPP network. This can cause leakage of critical data such as UAS system capabilities, location, and encryption keys. Unauthorized UAVs may attempt to imitate the behavior of legitimate UAVs to launch man-inthe-middle or replay attacks [13]. An unauthorized node that is able to obtain the credentials of authorized nodes could then inject false data. In a surveillance scenario, for example, an unauthorized UAV may deliberately alter and provide false data (e.g. altered pictures or video streams)."
2111.13172,"data open-source , data, open-source",82,,,"data links [5]. The threat model has shifted since sophisticated software radio hardware and software became widely available. Targeted wireless attacks to cellular networks, such as eavesdropping, jamming, and spooﬁng of control and data channels, can be implemented with open-source software investments [6], [7]. The 3GPP has therefore initiated a study on security aspects of network connected UAVs to identify key issues and solutions [8]."
2111.13172,"data, data repository",5,,,UDM: Unified Data Management
2111.13172,"data, open-source",131,,,"We have reviewed the standardization efforts for facilitating secure cellular connected UAS contexts. Various challenges remain that we identify in continuation and that we recommend to be considered as research and standardization work items. • Encryption: The broadcast nature of communications links between the UAV and the UAV-C or among 3GPP entities for both payload data and C2 packets, make them vulnerable to eavesdropping and adversarial attacks. Encryption of transmitted signals among UAS entities has not been standardized yet within the scope of The 3GPP, but there are efforts that address this problem as part of open source and commercial software projects for UAVs. For example, the Paparazzi and DJI open source UAV projects have managed to implement encrypted protocols using Chacha20 with"
2111.13172,"data, supplementary data",124,,,"of location information, including the absolute position, e.g., global navigation satellite system (GNSS) coordinates, and the relative position, such as cell ID or tracking area based coordinates. The reported location information may be used by the USS/UTM to deﬁne the optimal set of actions needed to ensure safe aerial operations. The reporting of location information can be veriﬁed using UAS application layer mechanisms such as the networked RID. In addition, it is preferable to advocate the position reporting for both UAV/UAV-C and USS/UTM via network assisted positioning mechanisms offered by the 3GPP network. The 3GPP network forwards the estimated location information to the USS/UTM as supplementary data when it is requested."
2111.13172,"publicly available, data, supplementary data",51,,,"nodes with desirable characteristics such as non-repudiation and tunable tradeoffs between operator privacy and public transparency. The blockchain can supplement ﬂight data recording to ensure that the data exchange over the cellular network is secure, tamper proof, and traceable for the entire UAS mission without human intervention."
2112.01565,code,122,,,"where αt ∈ (0, 1] is the step size. Accompanying the rise of deep learning, many variations of Q-learning have been developed, among which Deep Q-Network (DQN) (Mnih et al., 2015) has gained popularity due to its success in playing Atari games. There exist several major improvements of DQN, including Double DQN (Van Hasselt et al., 2016), Prioritized Replay (Schaul et al., 2015), and Dueling DQN (Wang et al., 2016). In this work, we choose Double DQN and Prioritized Replay as components of SparRL since they signiﬁcantly improve the vanilla DQN over small code changes."
2112.01565,data,17,,,"Collective classiﬁcation in network data. AI magazine, 29(3):93–93, 2008."
2112.01565,data,20,,,"diameters. ACM Trans. Knowl. Discov. Data, 1(1):2–es, March 2007."
2112.01565,data,33,,,"Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 631–636, 2006."
2112.01565,data,38,,,"Venu Satuluri, Srinivasan Parthasarathy, and Yiye Ruan. Local graph sparsiﬁcation for scalable clustering. In Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 721–732. ACM, 2011a."
2112.01565,data,43,,,"Michael Mathioudakis, Francesco Bonchi, Carlos Castillo, Aristides Gionis, and Antti Ukkonen. Sparsiﬁcation of inﬂuence networks. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 529–537, 2011."
2112.01565,data,47,,,"Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. Local higher-order graph clustering. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 555–564. Association for Computing Machinery, 2017."
2112.01565,data,54,,,"Lu Wang, Wenchao Yu, Wei Wang, Wei Cheng, Wei Zhang, Hongyuan Zha, Xiaofeng He, and Haifeng Chen. Learning robust representations with graph denoising policy network. In 2019 IEEE International Conference on Data Mining (ICDM), pp. 1378–1383. IEEE, 2019."
2112.01565,data,62,,,"Anand Padmanabha Iyer, Aurojit Panda, Shivaram Venkataraman, Mosharaf Chowdhury, Aditya Akella, Scott Shenker, and Ion Stoica. Bridging the gap: Towards approximate graph analytics. In Proceedings of the 1st ACM SIGMOD Joint International Workshop on Graph Data Management Experiences & Systems (GRADES) and Network Data Analytics (NDA), 2018."
2112.01565,data,89,,,"Graph sparsiﬁcation concerns data reduction where an edge-reduced graph of a similar structure is preferred. Existing methods are mostly sampling-based, which introduce high computation complexity in general and lack of ﬂexibility for a different reduction objective. We present SparRL, the ﬁrst general and effective reinforcement learning-based framework for graph sparsiﬁcation. SparRL can easily adapt to different reduction goals and promise graph-size-independent complexity. Extensive experiments show that SparRL outperforms all prevailing sparsiﬁcation methods in producing high-quality sparsiﬁed graphs concerning a variety of objectives."
2112.01565,"data available, data",18,,,Shang-Hua Teng. Scalable algorithms for data and network analysis. Found. Trends Theor. Comput.
2112.01565,"data, data https",50,,,"Lei Lin, Weizi Li, and Srinivas Peeta. Efﬁcient data collection and accurate travel time estimation in a connected vehicle environment via real-time compressive sensing. Journal of Big Data Analytics in Transportation, 1(2):95–107, 2019b. doi: https://doi.org/10.1007/s42421-019-00009-5."
2112.01565,"data, data https, dataset",33,,,"• Twitter (Leskovec & Mcauley, 2012): A social network dataset from the SNAP project (https://snap.stanford.edu/data). The graph has 81,306 nodes and 1,768,149 edges."
2112.01565,"data, data repository",16,,,Ryan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph analytics
2112.01565,"data, dataset",60,,,"• Email-Eu-Core (Email) (Leskovec et al., 2007; Yin et al., 2017; Leskovec & Sosiˇc, 2016): An interaction network of email data from a large European research institution, where nodes represent people and edges represent sent emails between people. The dataset consists of 1,005 nodes and 25,571 edges."
2112.01565,database,45,,,"Kook Jin Ahn, Sudipto Guha, and Andrew McGregor. Graph sketches: sparsiﬁcation, spanners, and subgraphs. In Proceedings of the 31st ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, PODS, pp. 5–14. ACM, 2012."
2112.01565,dataset,3,,,Dataset Twitter Facebook
2112.01565,dataset,19,,,"• Facebook (Leskovec & Mcauley, 2012): A social network dataset from the SNAP project."
2112.01565,dataset,38,,,Datasets. We test SparRL using graphs from a variety of domains. The details of these graphs are listed as follows while the number of nodes and edges of each graph is summarized in Table 1.
2112.01565,dataset,54,,,"• YouTube (Mislove et al., 2007): A social network dataset where users establish connections. Its community ground truth is deﬁned by the groups created by the users. In our experiment, we consider the top-100 communities for evaluation, which consist of 4,890 nodes and 20,787 edges."
2112.01565,dataset,118,,,"When training the model for all experiments, we set Tmax = 8 and train until negligible improvements are found over pruning 10% of the edges. Training the SparRL networks typically takes 1h–4h for the smaller graphs and roughly 6h–12h for the larger graphs using an Intel Core i9-11900k CPU and RTX 3090 GPU. However, the run-time complexity when evaluating is simply O(|H| ∗ T ) as we are predicting over a subgraph of length |H| for a total of T times. When evaluating on various test datasets, we act greedily w.r.t the learned DQN policy and prune the edge that corresponds to the maximum Q-value in the output."
2112.01565,"github, code",14,,,The source code of SparRL can be found at https://github.com/rwickman/ SparRL-PyTorch.
2112.01565,open-source,17,,,"Mathieu Bastian, Sebastien Heymann, and Mathieu Jacomy. Gephi: An open source software for"
2112.01565,python,61,,,"Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. Exploring network structure, dynamics, and function using networkx. In Ga¨el Varoquaux, Travis Vaught, and Jarrod Millman (eds.), Proceedings of the 7th Python in Science Conference, pp. 11 – 15, Pasadena, CA USA, 2008."
2112.01565,"used dataset, dataset",10,,,Table 1: Graph datasets used in the experiments.
2112.01790,data,31,,,"Pseudo Label Generation Assume parts of training data have labels, deﬁne initial label embedding matrix as O ∈ RC×N , where C denotes the total number of classes. For"
2112.01790,data,53,,,"methods, including ADDL [19], FDDL [20], LC-KSVD [4], LC-PDL [21], LEDL [5], CDLF [2]. We show the recognition results with 40% labeled training data in Table 1 and have the following observations."
2112.01790,data,59,,,p-Laplacian Attention Hypergraph GraphHypergraph      Fig. 2: The Self-Supervised Dictionary Learning framework. There are two steps: i) Employ the pAHL block to generate pseudo label F for the unlabeled data. ii) Embed the pseudo label into the dictionary learning model to obtain the dictionary D. More details please refer to section 2.
2112.01790,data,62,,,"In this section, we introduce the details of the self-supervised First, we introduce pdictionary learning algorithm. Laplacian based Attention Hypergraph to generate pseudo labels for the unlabeled training data. Then, we embed the pseudo label information into the standard dictionary learning framework. Figure 2 shows the ﬂowchart, and Algorithm 1 elaborates the algorithm procedure."
2112.01790,data,94,,,"Fortunately, the development of Self-Supervised Learning (SSL) provides us a novel perspective to solve this challenge. The core idea of SSL is to set a pretext task to generate a universal model for the downstream task. SSL has been demonstrated to effectively address the problem caused by inadequate labeled data in the training process. Combined with SSL, we propose a Self-Supervised Dictionary Learning (SSDL) framework. Like most SSL-based methods, the critical point of the challenge is setting up an appropriate pretext task."
2112.01790,data,107,,,"where X = [x1, x2, . . . , xN ] ∈ Rdim×N denotes the training data, xi (i = 1, 2, . . . ) denotes the feature embedding of the ith sample, dim denotes the dimension size of each sample, N is the number of training samples. D ∈ Rdim×K represents the to-be-learned dictionary, K is the dictionary base size. B ∈ RC×K represents the to-be-learned classiﬁer, C denotes the class number. S ∈ RK×N denotes the sparse codes for dictionary. α and γ are the positive scalar constants."
2112.01790,data,118,,,"This paper proposes a p-Laplacian Attention Hypergraph Learning (pAHL) based pretext task to generate a pseudo label matrix and then employ it in the downstream task (e.g., DL methods). Hypergraph learning was ﬁrst proposed by Zhou et al.[6] in 2007. It is capable of predicting labels according to mining and aggregating high-order relations within data. A hypergraph is composed of a vertex set and hyperedge set. Each hyperedge can connect any number of vertices. Compared with the simple graph, which is only able to reﬂect the pair-wise relations among vertices, hypergraph is more ﬂexible and can mine deeper relations of data."
2112.01790,"data, dataset",80,,,"i) One of our approach’s main contributions is to reduce the dependence on labeled data for dictionary learning. Thus, we design an ablation study on the UIUC-SE dataset to observe the effect of label rates. From Figure 4(a), we can see that, with the decrease of label rates, the performances of the two methods are decreasing, but our method is much slower than the other one."
2112.01790,"data, dataset",155,,,"For all the datasets, we employ standard Resnet to extract feature embedding with 2, 048 dimensions, select 70% for training, the rest for testing, and only 40% training data has labels. For the p and λ in pretext task, they play the key roles to obtain a suitable pseudo label matrix for dictionary learning. We ﬁx them to 1.8, 0.1 for Stanford40, and 2.2, 0.1 for UIUC-SE. There is a trick to tune the two parameters, for more details, please refer to section 3.3. In dictionary learning, we set the dictionary size K to half the number of training samples for the two datasets, and α = 2−14, γ = 2−12 for Stanford40 dataset, α = 2−12, γ = 2−12 for UIUC-SE dataset. The details are also discussed in section 3.3."
2112.01790,"data, dataset",241,,,"ii) There are mainly four parameters (p, λ, α, γ) inﬂuence the results. We set all the evaluated experiments to 40% label rate on the UIUC-SE dataset. Here, we ﬁrst discuss the p and λ in the pretext task. We adjust p and λ to obtain a pseudo label matrix. Usually, we ﬁne-tune the two parameters according to the ﬁnal results (as an example, in our paper, we can adjust the two parameters by the recognition accuracy). Here, we give a trick to easier ensure the two optimal parameters. Speciﬁcally, we ﬁrst use the training data to generate a model with p and λ. Then employ the training model to compute the cross-entropy loss of testing data. At last, adjust the parameters until achieving the minimum loss. The inﬂuence of p and λ are separately shown in Figure 4(b), 4(c). The yaxis denotes the testing data’s loss. We obtain the minimum loss near p = 2.2 and λ = 0.1. For α and γ, they interact with each other. Thus we explore the impact of these two parameters simultaneously. Figure 4(d) shows the experimental results. The proposed SSDL approach is not sensitive to these two parameters."
2112.01790,dataset,1,,,Methods\Datasets
2112.01790,dataset,19,,,Fig. 3: Comparison results about pAHL-LEDL and pAHLCDLF on Stanford40 dataset with 40% label rates.
2112.01790,dataset,91,,,"can be embedded into any standard dictionary learning algorithm, such as LC-KSVD, LC-PDL, LEDL, CDLF. That is to say, we may achieve higher recognition accuracies if we try to embed our pAHL block into these models. To evaluate this statement, we expand pAHL block to LEDL and CDLF on the Stanford40 dataset. The results are shown in Figure 3. Obviously see that, compared with original methods, the pAHL-embedded LEDL and CDLF can achieve more powerful performances than SSDL."
2112.01790,dataset,108,,,"Dictionary learning has been widely applied in many ﬁelds. Here we evaluate the learned dictionary in human activity recognition tasks. There are two datasets, including Stanford 40 Actions (Stanford40) [12] dataset and UIUC Sports Event (UIUC-SE) [13] dataset. We ﬁrst introduce the experimental setup. Then compare the proposed SSDL with state-of-the-art methods. Next, we try to embed the proposed pAHL block into other classical methods to evaluate the model-agnostic ability. Following, we conduct ablation studies to analyze our method. At last, we discuss something about the pretext task."
2112.01790,dataset,146,,,"From Table 1, we can see that our SSDL can outperform all other methods at least 1.1% and 0.7% on the Stanford40 and UIUC-SE datasets, respectively. Compared with the traditional methods, our SSDL has signiﬁcant improvements, but we need to consume more resources when training the dictionary. Compared with other state-of-the-art dictionary learning based approaches, SSDL has at least 0.7% improvement. For the label-embedded dictionary learning methods (LC-KSVD, LC-PDL, LEDL, CDLF), SSDL’s recognition accuracies can exceed them at least 2.6%. This phenomenon has demonstrated the efﬁciency of our method to some extent. However, our SSDL just embeds the pAHL based pretext task into a basic dictionary learning model. As mentioned in section 1, the pAHL block is a model-agnostic method that"
2112.01790,dataset,169,,,"The label-embedded dictionary learning (DL) algorithms generate inﬂuential dictionaries by introducing discriminative information. However, there exists a limitation: All the labelembedded DL methods rely on the labels due that this way merely achieves ideal performances in supervised learning. While in semi-supervised and unsupervised learning, it is no longer sufﬁcient to be effective. Inspired by the concept of self-supervised learning (e.g., setting the pretext task to generate a universal model for the downstream task), we propose a Self-Supervised Dictionary Learning (SSDL) framework to address this challenge. Speciﬁcally, we ﬁrst design a p-Laplacian Attention Hypergraph Learning (pAHL) block as the pretext task to generate pseudo soft labels for DL. Then, we adopt the pseudo labels to train a dictionary from a primary label-embedded DL method. We evaluate our SSDL on two human activity recognition datasets. The comparison results with other state-of-the-art methods have demonstrated the efﬁciency of SSDL."
2112.03471,data,42,,,"[13] Y. Hou and L. Zheng. Multiview detection with shadow transformer (and view-coherent data augmentation). In Proceedings of the 29th ACM International Conference on Multimedia, pages 1673–1682, 2021. 1, 2, 6"
2112.03471,data,139,,,"Multiple views based detection. To detect object under heavy occlusion, multiple perspective based methods have been developed in the past few years. Fleuret et al. [9] proposes Probabilistic occupancy map to estimate the probabilities of occupancy using Multi-view streams jointly. Second, in order to aggregate spatial neighbor feature, Conditional Random Field (CRF) [3, 26] are exploited. Hou et al. [14] adopts feature perspective transformation to aggregate multi-view data and regress pedestrian occupancy as a Gaussian distribution. Lima et al. [19] presents a multicamera detection model without any training that tends to integrate multiple views via graph-based method. Song et al. [29] proposes SHOT to alleviate the projection errors by multi-height-level homography transformation."
2112.03471,data,165,,,"Remarkable achievements have been made in 3D object detection over recent years. Due to the lack of depth information in the images, researchers have proposed fusion-based methods [8, 15, 18, 30]. For instance, MV3D [8] encodes the sparse point cloud with multi-view representation and performs region-based feature fusion. Pointfusion [30] uses two networks to process images and raw point cloud data respectively, and fuses them at feature level. In addition to predicting depth by fusing point cloud, more and more approaches have been proposed to directly predict 3D Boxes using convolutional neural network. Some methods [6, 7] are prone to perform 2D/3D matching via exhaustively sampling and scoring 3D proposals as representative templates. And some methods [16, 17, 23] start with accurate 2D bounding boxes directly to roughly estimate 3D pose from geometric properties obtained by empirical observation."
2112.03471,dataset,1,,,Dataset
2112.03471,dataset,2,,,4.1.1 Datasets
2112.03471,dataset,19,,,"• A synthetic 3D multi-view dataset, MultiviewC, which includes cattle targets of diverse sizes and actions."
2112.03471,dataset,31,,,"In this section, we evaluate the performance of the proposed VFA on WildTrack [4], MultiviewX [14], MVM3D [11] and MultiviewC datasets."
2112.03471,dataset,38,,,"is similar to our proposed dataset, containing images collected from multiple views, bounding boxes including position and orientation, and obstacles with different heights. However, it does not consider targets with diverse sizes."
2112.03471,dataset,43,,,Table 3. Impact of using different number of camera views on the MultiviewC dataset. Three metrics under two IoU values are compared. The proposed VFA method is used. Clearly using more views offers us more higher detection accuracy.
2112.03471,dataset,43,,,Table 4. The impact of different voxels sizes (top) and number of layers (bottom) of voxel grids on VFA performance on Wildtrack and MultiviewX dataset. [#] denotes the number of layers in voxel grids.
2112.03471,dataset,56,,,Comparison with the state of the art. Results on WildTrack dataset and MultiviewX dataset: the evaluation is focused on how our proposed method deals with occlusion cases. Comparison between our VFA method and previous methods on multiview 2D detection is shown in Table 1. Our VFA method outperforms the previous state-of Methods
2112.03471,dataset,56,,,"Existing multiview detection datasets. The WildTrack dataset is a real-world dataset while the MultiviewX dataset is a synthetic dataset for multiview pedestrian detection. Both datasets do not include the orientation label of the object. Therefore, the validation on these datasets is focus on occlusion cases. The MVM3D for mobile robot detection"
2112.03471,dataset,59,,,"We perform extensive experiments on four datasets intended for multi-view detection, where we show our system yields very competitive accuracy compared with the stateof-the-art methods. Among the four datasets is a synthetic dataset for multi-view cattle detection, named MultiviewC, newly introduced by this paper. The points made in this paper are summarized below."
2112.03471,dataset,65,,,"Table 1. Comparing with the state-of-the-art methods on Wildtrack and MultiviewX datasets for 2D multiview detection. We report and compare MODA, MODP, Precision and recall, all in percentage numbers. Higher is better. For each metric, the best, the second best and the third best numbers are highlighted in red, blue and green, respectively."
2112.03471,dataset,67,,,"[4] T. Chavdarova, P. Baqu´e, S. Bouquet, A. Maksai, C. Jose, T. Bagautdinov, L. Lettry, P. Fua, L. Van Gool, and F. Fleuret. Wildtrack: A multi-camera hd dataset for dense unscripted In 2018 IEEE/CVF Conference on pedestrian detection. Computer Vision and Pattern Recognition, pages 5030– 5039, 2018. 1, 5"
2112.03471,dataset,69,,,"Figure 5. Sample images and their ground truths of the newly introduced MultiviewC dataset, designed for multiview 3D detection of cattle. Row 1: images from 6 camera views with 3D bounding boxes and cattle IDs, and action categories (the last two are not utilized in this paper). Row 2: visualization of cattle bounding boxes on bird’s-eye-view plane."
2112.03471,dataset,74,,,"Table 2. Method comparison on MVM3D and MultiviewC datasets for 3D multiview detection. Apart from comparing with two recent approaches [11, 23], we also evaluate our system with different ground-truth modeling schemes: point distribution (w/ PD), spherical Gaussian distribution (w/ GD) and oriented Gaussian distribution (w/ OGD). ∗ indicate results obtained through our re-implementation on the MultiviewC dataset."
2112.03471,dataset,99,,,"Comparison with [29]. The SHOT method [29] is the closest work to ours, which also addresses the projection distortion problem. To demonstrate the effectiveness of our proposed VFA, we change SHOT by replacing its 2D3D feature projection by VFA component and call this as “SHOT with VFA”. We then compare SHOT and “SHOT with VFA” under the same experiment settings on the MultiviewX dataset. Fig. 6 A shows that the “SHOT with VFA” method outperforms the original SHOT on all metrics."
2112.03471,dataset,111,,,"3D voxels onto the 2D multi-view images, allowing features along the same vertical line to be projected onto the same position on the ground plane. VFA puriﬁes the aggregated features on the ground plane and thus improves the system accuracy. On a side contribution, we use an oriented Gaussian modeling method for objects that do not have a square ground-truth on the ground plane, like cattle. This method allows for more efﬁcient training process. We validate this system on four datasets (including a newly introduced cattle detection dataset), where we report very competitive detection accuracy compared with the state-of-the-art system."
2112.03471,dataset,114,,,"where γ is a scaling factor from original movement range to designed conﬁdence map. In different tasks, γ is different. For instance, in MultiviewC dataset, γ = 3900 ÷ 156 = 25. To alleviate the class imbalance, the conﬁdence map S(x, y) is trained via focal loss [21] to regress object occupancy on bird’s-eye-view plane. In addition to location, orientation and dimension are also indispensable in 3D object detection. We therefore append two additional networks to predict the orientation and dimension. First, for the orientation head, different from previous works that propose multi-bins"
2112.03471,dataset,116,,,"For 3D object detection, we need to predict the location, orientation and dimension of each object bounding box. For location, we ﬁrst predict a conﬁdence map which is a reduced version of movement range of all objects. For instance, we design a conﬁdence map of size 156 × 156 for MultiviewC dataset that covers a 39m × 39m area. Each point value on the conﬁdence map indicates the possibility of object existence within its coverage. When zooming in, lots of coordinate conversions fail to be divisible. Thus, we introduce an additional output head to predict relative position offset ∆pos(x, y):"
2112.03471,dataset,131,,,"Hyper-parameter analysis. There are two primary hyper-parameters of VFA which are grid height and number of voxel layers. First, we assess the inﬂuence of grid height as shown in Fig. 6 B (and Table 4 in the Supplementary Material). It is clear that our VFA performs best with the height of 1.6 meters and the grids size of 0.1m × 0.1m × 0.2m for both Wildtrack and MultiviewX datasets. We then ﬁnd the optimal number of voxel layers while the grid height is ﬁxed to 1.6 meters as shown in the Fig. 6 C (and in Table 4 of Supplementary Material). Results show that more layers achieve better performance as well as higher computational complexity, and 8 layers"
2112.03471,dataset,135,,,"Figure 6. A: Experimental comparison between the proposed VFA method and [29] on the MultiviewX dataset. After integrating VFA on top of [29], we observe consistent improvement on all four metrics. B and C: impact of hyper-parameters on MultiviewX, i.e., voxel height and the number of voxel layers, respectively. Results are reported on the MultiviewX validation set. When studying one hyper-parameter, we ﬁx the others at a near optimal value. We report MODA (%) and inference time (frames per second, FPS). We select voxel height and number of voxel layers as 0.2 and 8, respectively for this dataset. We conduct similar analysis on the validation set of each dataset."
2112.03471,dataset,199,,,"During experiments, input images of other sizes are resized to 720 × 1, 280 pixel resolution to save memory. We employ a multi-scale feature extractor based on a ResNet-18 taking the output of the last three layers as the multi-scale feature. After feature extraction, before voxelized feature aggregation we adjust channel size of multi-scale features to 256. In VFA, size of voxel grid are dependent on plane size of scene and target’s highest height. For example, the plane size of ground area in MultiviewC dataset is 39 meters by 39 meters, the height of cattle is from 1.1 meters to 1.5 meters, if we quantize planes into a 156 × 156 × 5 grid and set the grid height to 1.6 meters, voxel size will be 0.25m × 0.25m × 0.32m. During training, we use SGD optimizer with L2-normalization of 5E − 4 and momentum of 0.5. One-cycle learning rate scheduler [27] with learning rate 0.02 is also adapted. We trained the network over 30 epochs with mini-batch size 1. All experiments were performed on RTX-3090 GPUs."
2112.03471,dataset,201,,,"the-art method MVDeTr by +12.5% MODP, +1.4% Recall on the Wildtrack dataset, and +5.1% MODP on the MultiviewX dataset and the results of other metrics are close. MVDeTr method utilizes transformer to recover projection distortion while we apply voxelized feature aggregation to reduce the projection distortion achieving similar competitive result. On MVM3D dataset and MultiviewC dataset, we focus on 3D detection, including position and orientation. The comparison between our proposed VFA method with MVM3Det method and a single view method, Deep3DBox [23] is shown in Table 2. Our VFA method is trained with 3 different encoding distributions: point distribution (PD), Gaussian distribution (GD), and oriented Gaussian distribution (OGD). The results show that although on par on MVM3D dataset, our VFA method outperforms Deep3DBox and MVM3Det by large margins on all metrics on MultiviewC dataset. The reason is that MultiviewC dataset contains cow elongated bodies and different poses such as standing and lying, therefore more challenging than MVM3D dataset. In case of encoding methods, our proposed OGD encoding can further enhance performance."
2112.03471,"github, code, dataset",235,,,"Multi-view detection incorporates multiple camera views to alleviate occlusion in crowded scenes, where the stateof-the-art approaches adopt homography transformations to project multi-view features to the ground plane. However, we ﬁnd that these 2D transformations do not take into account the object’s height, and with this neglection features along the vertical direction of same object are likely not projected onto the same ground plane point, leading to impure ground-plane features. To solve this problem, we propose VFA, voxelized 3D feature aggregation, for feature transformation and aggregation in multi-view detection. Speciﬁcally, we voxelize the 3D space, project the voxels onto each camera view, and associate 2D features with these projected voxels. This allows us to identify and then aggregate 2D features along the same vertical line, alleviating projection distortions to a large extent. Additionally, because different kinds of objects (human vs. cattle) have different shapes on the ground plane, we introduce the oriented Gaussian encoding to match such shapes, leading to increased accuracy and efﬁciency. We perform experiments on multiview 2D detection and multiview 3D detection problems. Results on four datasets (including a newly introduced MultiviewC dataset) show that our system is very competitive compared with the state-ofthe-art approaches. Code and MultiviewC are released at https://github.com/Robert-Mar/VFA."
2112.03471,"github, data available, data https, dataset",274,,,"Our new MultiviewC dataset. To address the need of more diverse test cases for multiview 3D detection, we present our new challenging dataset MultiviewC generated using Unreal Engine 4.26 [1]. The scene is ﬁxed on a cattle farm with sunny lighting conditions. MultiviewC dataset covers a square of 39 meters by 39 meters. We quantize the ground plane into a 3, 900 × 3, 900 grid. There are 7 cameras, with 4 located in the 4 corners of the area and 3 located on top of a drinking water trough. All cameras are calibrated, each of which captures every two seconds a 720 × 1, 280 pixel resolution image. We collect 560 images from each camera, so 3, 920 images in total for 7 cameras. Our interested targets are 15 cows with ﬁve random actions: sleeping, grazing, running, walking and lazing. To simulate more occlusion situations, the cows hold their current movement for one second when they encounter another cow. Different from existing datasets, the targets in MultiviewC have diverse sizes and aspect ratios. The height, width and length of each cow are distributed as follows: 1.1 ∼ 1.5 meters, 1.1 ∼ 1.5 meters and 2.48 ∼ 2.78 meters. Actions also affect their size, for example, a sleeping cow has lower height and longer length. The visualization of this dataset is shown in Fig. 5. More details can be found at our dataset page https://github.com/RobertMar/MultiviewC."
2112.11194,data,50,,,"presented by its constituent unit cell elements. These twodimensional structures could easily be integrated into our built environment, to extend the coverage of mobile base stations to blind spots as well as selectively smoothing out achievable data rates at cell edges [4][5]."
2112.11194,data,96,,,"The 5G NR standard employs OFDM on uplink and downlink for its high spectral efﬁciency and resilience to fading. In each experiment performed here, similar to previous ﬁeld trial works [9][11], we utilised an OFDM signal with 20 MHz bandwidth and 312.5 kHz subcarrier spacing. The transmitted data was a randomly generated bit stream, with the signal processing performed by GNURadio Companion (GRC) software on laptop PCs. A block diagram of the RISaided communication link can be seen in Fig. 5."
2112.11194,data,209,,,"The unit cell employed in this work is the multi-bit columndriven planar design depicted in Fig. 1, which was recently introduced by the authors [14]. Each unit cell consists of 5 patches connected by 3 PIN diodes and a capacitor, mounted on a 5mm PTFE-based F4BM-2 substrate with relative permittivity (cid:15)r = 2.65 and loss tangent tanδ = 0.001. Unit cells are biased in a column-wise fashion by applying DC voltages to 3 of the patches, with the remaining two patches connected to ground. The patch widths and spacing were optimised through a particle swarm optimisation algorithm in order to maximise the achievable phase resolution whilst keeping the average reﬂection loss below 1dB. The components used in this unit cell design are Skyworks SMP1321-040LF PIN diodes and an AVX U-Series 3.6 pF 0402 capacitor. In order to increase the accuracy of simulations during design, the S-parameter data from the manufacturers of the PIN diodes and capacitors were incorporated in the simulations. The logic 0 state was simulated as that of 0 V PIN diode reverse bias voltage, whilst the logic 1 state is that of forward biased with 5 mA current."
2112.14162,data,44,,,"[1] F. Assous, J. Chaskalovic, Indeterminate Constants in Numerical Approximations of PDE’s: a Pilot Study Using Data Mining Techniques, J. Comput Appl. Math., Vol 270, pp. 462-470, (2014)."
2112.14162,data,63,,,"Owing to this lack of information, several heuristic approaches were considered, so as to investigate new possibilities, which rely on a probabilistic approach. Such new possibilities ﬁnally enable one to classify numerical methods in which the associated data are ﬁxed, and not asymptotic (for a review, see [6]-[10]) ."
2112.15448,data,13,,,"‘A note on data-splitting for the evaluation of signiﬁcance levels’,"
2112.15448,data,99,,,"The data used in this study is the daily stock prices of 505 equities in the USA Standard S&P500 index, for the period 02 August till 2013-02 July In 2018; which consist of 1259 record. The data is taken from Kaggle. this paper, the LASSO method [1] is combined with Post-selection inference Jason et al. (2016) to reduce the dimensionality of S&P500 . Jason et al. (2016),Hastie et al. (2015) shows that the LASSO parameters are bounded"
2112.15448,data,342,,,"ranges the model parameters by shrinking (regularization) the regression coeﬃcients and diminishing some of them to zero. The feature selection takes place after the shrinking phase, each non-zero value is chosen for use in the model. First LASSO method is used for dimension reduction of S&P500, then post selection inference method used to see which predictors are more eﬀective and less uncertain for their eﬀect on the index and they are called principal components in the context of principal component analysis(PCA). There are many methods to solve the lasso problem as described in Hastie et al. (2015). Benidis et al. (2018) uses majorization minimization (MM) but the present paper uses coordinate descent although the choice of the solver is not the issue for the current paper. Once the LASSO algorithm or any other linear regression method is implemented, the next important step is to see which of these coeﬃcients are more certain in the framework of statistical inference. Exact post-selection inference explained in Jason et al. (2016) is one of the relative recent ideas for inference which unlike traditional approach, inference is done after selection by data. The approach that is used in present paper for post-selection inference(POSI) is called Exact POSI and is described in Jason et al. (2016). There are other methods for POSI such as data splitting[Cox (1975)] which uses half of data for inference and the other half for selection. Although many statisticians use this method, but it reduces the statistical power since half of data is neglected and inferences are only valid for data that is used in selection. Tian & Taylor (2018) have introduced randomized response method and have shown it has smaller typeII error in comparison with methods like data splitting or data carving but is computationally challenging."
2201.00732,"code, database",279,,,"To generate the database of turbulent ﬂows on a rotating reference frame, we have performed a set of Direct Numerical Simulations (DNS) of the NSE for incompressible ﬂuid in a triply periodic domain of size L = 2π with a resolution of N = 256 gird-points per spatial direction. We used a fully dealiased parallel 3d pseudospectral code, with time integration implemented as a second-order Adams-Bashforth scheme and the viscous term implicitly 2 to reach a stationary integrated. A linear friction term, 0.1∆− 6, 4u, with ν = 1.6 state. At the same time, viscous dissipation, ν to increase the inertial range of scales dominated by the non-linear energy transfer. The forcing mechanism, f , is a Gaussian process delta-correlated in time, with support in wavenumber space around kf = 4. The total energy time evolution for six simulations varying Ω is presented in Fig. 3(a), while in the panel (b) of the same ﬁgure we show the energy spectra, E(k) = 1 2, averaged over time in the stationary regime for the same set of six ˆu(k) 2 | simulations. From the energy spectra we can observe that while for Ω = 2 (in the direct cascade regime) there is a < kf , for Ω values above the transition the split cascade regime, depletion of energy at wavenumbers smaller than also the small wavenumbers are very energetic thanks to the inverse cascade suggesting the presence of large-scales structures in the ﬂow."
2201.00732,data,18,,,"data clustering,” IEEE Transactions on Knowledge and Data Engineering 23, 1406–1418 (2011)."
2201.00732,data,18,,,"physics-informed data assimilation and spectral nudging,” Physical Review Fluids 3, 104604 (2018)."
2201.00732,data,19,,,"equations for data assimilation of turbulent ﬂows,” Physical Review X 10, 011023 (2020)."
2201.00732,data,24,,,"[36] Patricio Clark Di Leoni, Andrea Mazzino, and Luca Biferale, “Synchronization to big data: Nudging the navier-stokes"
2201.00732,data,24,,,"[37] Michele Buzzicotti and Patricio Clark Di Leoni, “Synchronizing subgrid scale models of turbulence to data,” Physics of"
2201.00732,data,25,,,"[21] Michele Buzzicotti, Fabio Bonaccorso, P Clark Di Leoni, and Luca Biferale, “Reconstruction of turbulent data with deep"
2201.00732,data,26,,,"[6] Karthik Duraisamy, Gianluca Iaccarino, and Heng Xiao, “Turbulence modeling in the age of data,” Annual Review of"
2201.00732,data,42,,,FIG. 4. Visual representation of the DCNN input-output setup used in the regression problem. The neural network is trained to map a plane of velocity module extracted from 3d data to the rotation frequency of the reference frame.
2201.00732,data,44,,,"[10] Eugenia Kalnay, Atmospheric modeling, data assimilation and predictability (Cambridge university press, 2003). [11] Michele Buzzicotti, Benjamin A Storer, Stephen M Griﬃes, and Hussein Aluie, “A coarse-grained decomposition of"
2201.00732,data,45,,,"[17] Ian Goodfellow, Yoshua Bengio, and Aaron Courville, Deep learning (MIT press, 2016). [18] Steven L Brunton and J Nathan Kutz, Data-driven science and engineering: Machine learning, dynamical systems, and"
2201.00732,data,45,,,"[2] Marc Bocquet, Julien Brajard, Alberto Carrassi, and Laurent Bertino, “Data assimilation as a learning tool to infer ordinary diﬀerential equation representations of dynamical models,” Nonlinear Processes in Geophysics 26, 143–162 (2019)."
2201.00732,data,46,,,"[1] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz, “Discovering governing equations from data by sparse identi ﬁcation of nonlinear dynamical systems,” Proceedings of the national academy of sciences 113, 3932–3937 (2016)."
2201.00732,data,50,,,"[12] Alberto Carrassi, Michael Ghil, Anna Trevisan, and Francesco Uboldi, “Data assimilation as a nonlinear dynamical systems problem: Stability and convergence of the prediction-assimilation system,” Chaos: An Interdisciplinary Journal of Nonlinear Science 18, 023112 (2008)."
2201.00732,data,50,,,• For each of the ten simulations we have dumped a number of roughly 600 snapshots of the full 3d velocity ﬁeld with a temporal separation large enough to decrease correlations between two successive data-points (see Fig. 3(a) for the total energy evolution).
2201.00732,data,54,,,"[22] Julien Brajard, Alberto Carrassi, Marc Bocquet, and Laurent Bertino, “Combining data assimilation and machine learning to emulate a dynamical model from sparse and noisy observations: a case study with the lorenz 96 model,” Geoscientiﬁc Model Development Discussions , 1–21 (2019)."
2201.00732,data,85,,,"−10−505105101520253000.10.20.30.40.551015202530h(Ωpred−Ωtrue)iΩtrueΩBayΩ∗BayDCNNh(Ωpred−Ωtrue)2iΩ2trueΩtruewhere they can be evaluated objectively and compared quantitatively on much more challenging tasks than the ones of general objects recognition or classiﬁcation. It is far from us to think that ML can always oﬀer best solutions to data analysis, but instead we aim to bring an example of how ML and physics can beneﬁts from each other, hoping that this work will lead to further and deeper investigations in such direction."
2201.00732,data,124,,,"FIG. 10. PDF of rotation value predicted by the neural network, Ωpred (DCNN, solid lines full triangles), compared Bay (red histograms), Bayes estimations using joint-likelihood with the mean, ΩBay (black histograms), and most likely, Ω∗ P ((cid:104)u2(cid:105), (cid:104)(∂xu)2(cid:105)|Ωi), on the validation set. For the sake of data presentation each of the PDF measured on the ten diﬀerent Ω values are reported in diﬀerent panels. As indicated in the axis, the PDF are all normalized by their corresponding Ωtrue indicate in the labels of each panel."
2201.00732,data,251,,,"Extracting information from the observation of a natural system is a fundamental problem in science. Except in the few cases where laboratory or numerical experiments are performed, inferring physical parameters is never easy [1, 2]. Measurements are always limited and our theoretical knowledge sometimes is inadequate to extract from them a correct description of the system. A signiﬁcant example are turbulent ﬂows [3–6]. Here because of chaos, any error in the observations increases exponentially in time, and quickly compromises our possibility to understand, predict, hence model turbulent behaviour. Consequences of such limitations are clear in ﬁelds like meteorology, oceanography and climate modeling [7–12]. In this work we aim to analyse turbulent ﬂows in a diﬀerent way. In particular, we use tools of Machine Learning (ML) developed in the ﬁeld of computer vision such as Deep Convolutional Neural Network (DCNN) [13–16], to bring new perspectives in the data assimilation and analysis of complex physical systems [17– 24]. The setup we consider is 3d turbulence under rotation [25]. There are multiple reasons for this choice, from the fundamental interest given by the high challenge to classify multiscale non-Gaussian ﬁelds to the speciﬁc relevance for geophysics and many applied ﬂows realizations. Rotating turbulence is described by the Navier-Stokes equations (NSE) equipped with the Coriolis force,"
2201.00732,"data, dataset",106,,,"The full dataset, in this way, is composed of more than 1 million diﬀerent velocity module planes, out of which the 80% of them are used as training set while 20% are used as validation set. To be sure that validation data were never seen during training, none of the planes used in the validation set is obtained from a random shift of a plane contained in the training set. Instead, the two sets are built using the same protocol but from diﬀerent temporal realization of the ﬂow splitting its time evolution in two separate parts."
2201.00732,"data, dataset",227,,,"We design a machine learning technique to solve the general problem of inferring physical parameters from the observation of turbulent ﬂows, a relevant exercise in many theoretical and applied ﬁelds, from engineering to earth observation and astrophysics. Our approach is to train the machine learning system to regress the rotation frequency of the ﬂow’s reference frame, from the observation of the ﬂow’s velocity amplitude on a 2d plane extracted from the 3d domain. The machine learning approach consists of a Deep Convolutional Neural Network (DCNN) of the same kind developed in computer vision. The training and validation datasets are produced by means of fully resolved direct numerical simulations. This study shows interesting results from two diﬀerent points of view. From the machine learning point of view it shows the potential of DCNN, reaching good results on such a particularly complex problem that goes well outside the limits of human vision. Second, from the physics point of view, it provides an example on how machine learning can be exploited in data analysis to infer information that would be inaccessible otherwise. Indeed, by comparing DCNN with the other possible Bayesian approaches, we ﬁnd that DCNN yields to a much higher inference accuracy in all the examined cases."
2201.00732,"data, dataset",245,,,"Each of these limitation strongly compromises our knowledge of the systems, however, as discussed above the ﬁrst one is the most stringent. Indeed, up to our knowledge, there are not previous attempts based on Machine Learning techniques to attack the problem without any knowledge on the equations of motion. For comparison, in this work, we have repeated the same investigation using standard Bayesian Inference (BI) analysis. Results highlight the superiority of DCNN in solving regression problems and show how DCNN can be used to improve current data analysis capability. At the same time, in this work we tested DCNN on a much more challenging task than the standard image classiﬁcation performed to simulate human vision [38–45]. We have also attempted to perform the same classiﬁcation, using the unsupervised Gaussian Mixture Models (GMM) classiﬁcation [46–48]. However we did not ﬁnd any relation between the unsupervised classiﬁcation and the rotation value, so it has not been included in the comparison with DCNN and BI. The paper is organized as follow. In Sect. II there is a detailed description of the datasets used in this work. In Sect. III we describe all methodologies, namely the DCNN and the BI. Results are presented in Sect. IV, and in Sect. V we draw our concluding remarks."
2201.00732,"data, dataset",343,,,"As argued, these two assumptions make the problem diﬃcult enough that no other tools of analysis are known to be able to achieve such goal under the same conditions, and allowed us to show how Machine Learning can push forward our data analysis capability. Indeed, results showed that even a simple DCNN properly trained has been able to solve such problem with unprecedented accuracy. For comparison, the only other possible way to repeat the same inference without using temporal information, was to use a standard Bayesian inference based on diﬀerent observables. For us, the natural choice from the type of problem and the available data, was to measure moments of the velocity ﬁeld and of the velocity gradients. As discussed all results have highlighted the potential of DCNN, that were able to outperform any other inference attempt. As brieﬂy discussed in the introduction, an unsupervised classiﬁcation of the same dataset has been attempted using a GMM classiﬁcation algorithm, however no relation between the classes identiﬁed and the rotation value has been found. Indeed GMM, as more in general k-means algorithms [56], are mainly sensitive to the number of vortexes inside each plane and to their spatial location, hence, to observables that are not closely connected to the rotation value but can only be used to distinguish the dynamics above and below the transition threshold from direct to inverse cascade. For this reason a detailed analysis using this tool has been omitted in the paper. From this study we want to bring two main messages. The ﬁrst is, from a physics point of view, that Machine Learning provides tools that can be highly beneﬁcial in problems of data analysis of complex systems, the work presented in [24] is another example. Second, on the other hand, is to show how physical systems can be ideal testing grounds for new ML algorithms,"
2201.00732,"data, dataset",348,,,"We have implemented a Deep Convolutional Neural Network (DCNN) inspired by the computer vision community to analyze data of turbulent ﬂows under rotation with the goal to infer the rotation frequency of the reference frame from the resulting ﬂow velocity ﬁeld. To put our self in a realistic scenario we assumed to have just a limited knowledge of the ﬂow, and in particular we imagined to have access only to a bidimensional cut of the full 3d domain and to have the velocity only at a ﬁxed time during the evolution. As argued, these two assumptions make the problem diﬃcult enough that no other tools of analysis are known to be able to achieve such goal under the same conditions, and allowed us to show how Machine Learning can push forward our data analysis capability. Indeed, results showed that even a simple DCNN properly trained has been able to solve such problem with unprecedented accuracy. For comparison, the only other possible way to repeat the same inference without using temporal information, was to use a standard Bayesian inference based on diﬀerent observables. For us, the natural choice from the type of problem and the available data, was to measure moments of the velocity ﬁeld and of the velocity gradients. As discussed all results have highlighted the potential of DCNN, that were able to outperform any other inference attempt. As brieﬂy discussed in the introduction, an unsupervised classiﬁcation of the same dataset has been attempted using a GMM classiﬁcation algorithm, however no relation between the classes identiﬁed and the rotation value has been found. Indeed GMM, as more in general k-means algorithms [56], are mainly sensitive to the number of vortexes inside each plane and to their spatial location, hence, to observables that are not closely connected to the rotation value but can only be used to distinguish the dynamics above and below the transition threshold from direct to inverse cascade."
2201.00732,"data, python, dataset",297,,,"which is the mean-squared-error between the target value Ωtrue and the predicted value Ωpred. E is the mean over a mini-batch of 256 diﬀerent planes. To implement the DCNN we used TensorFlow [50] libraries with a Python interface, optimized to run on GPUs. As described in Fig. 5 the overall architecture is composed of three convolutional layers that encode the input plane of velocity module on 64 diﬀerent planes of size 16 16. This ﬁrst part of the network is the one asked to identify all the relevant features of the velocity planes relevant to the regression problem. The data/features are then reshaped into a single vector of 512 elements. The second half of the network, is composed by two fully connected layers that transform the 512-dimensional vector into the real number representing the network prediction, Ωpred. The training of these two ﬁnal fully connected layers is performed using a dropout of 60% to reduce overﬁtting issues [51]. All layers are followed by a non-linear ReLu activation functions. In the main panel of Fig. 6 we show the evolution of the loss, , measured on the training and validation data as a function of the number of epochs. From their evolution we see that there is no evidence of overﬁtting with the two losses remaining always of the same order. We considered training converged after roughly 250 epochs where the cost of the validation set starts to saturate. Using a mini-batch of 256 planes, and a training dataset of Ntra = 844800 planes, each epoch consists of 3300 back-propagation steps. The backpropagation has been performed with an Adam optimizer 5."
2201.00732,"data, python, dataset",338,,,"The training of these two ﬁnal fully connected layers is performed using a dropout of 60% to reduce overﬁtting issues [51]. All layers are followed by a non-linear ReLu activation functions. In the main panel of Fig. 6 we show the evolution of the loss, , measured on the training and validation data as a function of the number of epochs. From their evolution we see that there is no evidence of overﬁtting with the two losses remaining always of the same order. We considered training converged after roughly 250 epochs where the cost of the validation set starts to saturate. Using a mini-batch of 256 planes, and a training dataset of Ntra = 844800 planes, each epoch consists of 3300 back-propagation steps. The backpropagation has been performed with an Adam optimizer 5. To achieve a good training with the learning equipped with a Nesterov momentum [52] using a learning rate of 10− protocol just introduced, we have normalized the input data to be in the [0; 1] range, namely we rescaled the module intensity of each gird-point by normalizing by it by the maximum value calculated over the whole training set. The same normalization constant has been used to rescale the validation set. In the same way the training labels Ωtrue have been rescaled by their maximum value in the training. In the inset of Fig. 6, we show the Probability Density Function (PDF) of predicting the correct rotation value using the DCNN on the validation set at diﬀerent epochs during training. Here Ωtrue = 20 is used, but similar behaviours are observed for all the other Ω. At epoch 350 the PDF becomes close to a delta-like distribution, suggesting very good accuracy of the network, with errors only every few cases over one-thousand, and always within 20% the correct value."
2201.00732,database,20,,,"generative models for semantic inpainting from turb-rot database,” Physical Review Fluids 6, 050503 (2021)."
2201.00732,database,26,,,"[49] Luca Biferale, Fabio Bonaccorso, Michele Buzzicotti, and Patricio Clark di Leoni, “TURB-Rot. A large database of 3d"
2201.00732,dataset,3,,,B. Dataset extraction
2201.00732,dataset,3,,,II. DATASET
2201.00732,dataset,23,,,"datasets and state of the art,” Foundations and Trends® in Computer Graphics and Vision 12, 1–308 (2020)."
2201.00732,dataset,43,,,"P (Ωi) is the prior probability, in our case for all Ωi equal to 0.1 because the dataset is composed by the same number of planes for each of the ten Ωi. P ( of a generic observable for"
2201.00732,dataset,46,,,"The ML training and validation datasets, accessible at the webpage http://smart-turb.roma2.infn.it, are extracted from the DNS described above, following [49], such as to match all assumption discussed in the introduction (Sect. I), namely;"
2201.00732,dataset,62,,,"• To increase the dataset, we shift each of the 16 planes in 11 diﬀerent ways, by choosing randomly a new center of the plane and using periodic boundary conditions, such as to obtain a total number of 176 planes at each 176 = 105600 planes for each Ω value. instant of time and a total of 600"
2201.00732,dataset,105,,,"Fig. 4 schematizes the ML problem set-up. A plane of velocity module randomly extracted from the dataset of ten simulations is given in input to a DCNN with the goal to infer the Ω value of the simulation from which the plane has been extracted. The training is performed in a supervised setting, having pre-labelled all planes with their corresponding value, Ωtrue. As solution of the regression problem, the DCNN gives in output a real number corresponding to the predicted rotation value, Ωpred. In this way the inferring task is not limited to a predeﬁned set"
2201.00732,dataset,106,,,"FIG. 11. (Main panel) Mean deviation between the predicted and true value of Ω for each of the ten classes considered, the errorbars show the root mean square of the diﬀerence between Ωpred and Ωtrue. (Inset) Mean squared diﬀerence between prediction and true Ω normalized to the square of the correct rotation value for each of the ten classes. The average is taken over the validation dataset, while the predictions are obtained as in Fig. 10, by the neural network, DCNN, or as the mean, ΩBay and most likely, Ω∗"
2201.00732,dataset,328,,,"For each of these observables we have calculated the likelihood P ( for each Ωi of the training dataset. Namely we have measured; P ( Ωi) and the joint-likelihood, (cid:104) Ωi) on the training set. At this point, measuring the same observables on the validation dataset, P ( ) (Eq. (5)) and than and knowing the likelihoods for each Ωi, we could estimate the posterior probability P (Ωi|O infer ΩBay( ) (Eq. (6)), on the validation planes. Before moving to the results, to highlight the diﬃculties of such problem in the left panel of Fig. 7 we show a scatterplot of the fourth vs the eight order moments of the velocity module for each of the 20 thousand planes of the validation set. Diﬀerent colours indicate diﬀerent rotation values as indicated by the arrows and labels, while the intensity of the colours is proportional to the density of points accumulated in that region. From the scatterplot we can diﬀerentiate three main clusters, the ﬁrst is composed by the planes in the direct cascade regime, while the other two are made of planes from simulations around or above the transition. Inside these three main clusters it is not possible to distinguish any speciﬁc Ωi. Furthermore, moments amplitude is not a monotonic function of the Ωi. The right panel of Fig. 7 shows the joint-likelihood of the two observables introduce above, mean energy and mean gradients. From this plot we can see that measuring gradients does not allow to have a better statistical separation of the diﬀerent Ω and only the points below-above the transition are always possible to be inferred. In the next section we present and discuss results of Ω inference via DCNN and standard Bayesian approach."
2201.00732,dataset,342,,,"where in the last step the denominator of (5) is neglected because it does not aﬀect the maximization on Ωi. For our speciﬁc set-up we have chosen two diﬀerent observables that could be relevant for the inference of the correct u2 Ω and at the same time accessible from the analysis of the dataset. The ﬁrst is the mean energy, , and the (cid:104) (∂xu)2 , where u = , are the average over the second is the mean squared gradient, (cid:105) Ωi) from the 80 thousand planes plane surface. For each of these observables we have calculated the likelihood P ( for each Ωi of the training dataset. Namely we have measured; P ( Ωi) and the joint-likelihood, (cid:104) Ωi) on the training set. At this point, measuring the same observables on the validation dataset, P ( ) (Eq. (5)) and than and knowing the likelihoods for each Ωi, we could estimate the posterior probability P (Ωi|O infer ΩBay( ) (Eq. (6)), on the validation planes. Before moving to the results, to highlight the diﬃculties of such problem in the left panel of Fig. 7 we show a scatterplot of the fourth vs the eight order moments of the velocity module for each of the 20 thousand planes of the validation set. Diﬀerent colours indicate diﬀerent rotation values as indicated by the arrows and labels, while the intensity of the colours is proportional to the density of points accumulated in that region. From the scatterplot we can diﬀerentiate three main clusters, the ﬁrst is composed by the planes in the direct cascade regime, while the other two are made of planes from simulations around or above the transition. Inside these three main clusters it is not possible to distinguish any speciﬁc Ωi."
2201.00732,dataset,344,,,"As already pointed out, guessing the correct Ω value from the observation of the velocity planes is a challenge much beyond human vision capability. To appreciate it, in Fig. 8 we present a random selection of 12 planes extracted from our validation dataset. Above each panel we have reported the exact Ω value of the simulation, and the corresponding Ωi). prediction obtained by the DCNN analysis and the Bayes inference, using the joint-likelihood P ( These ﬁrst results are already suggesting that the DCNN prediction is closer to the correct Ω than the Bayes one. In particular, the network’s uncertainties, (the neural network outputs a real number and not a class), are generally small enough to allow the correct classiﬁcation on the corresponding Ω, on the contrary the Bayes inference leads to a miss-classiﬁcation of the rotation parameter. A quantitative comparison of the two techniques is presented in Fig. 9, where in the four panels we report a scatterplot between the correct rotation value, Ωtrue and the corresponding prediction obtained via the DCNN (top left panel), and the Bayes inference with the three diﬀerent likelihoods, namely the one based on the mean energy (top right panel), on the mean square gradients (bottom left panel) and on the joint likelihood combining energy and gradients (bottom right panel). The four panels are all obtained with the same validation dataset of roughly 20 thousands planes for each of the ten Ω values. The symbols size and color intensity are proportional to the density of points in the speciﬁc region of the scatterplot, while the open hexagon in black represents the mean value measure over all planes for each Ω. The DCNN estimates are the more accurate both in terms of its mean and variance, being most of the time close to the identity (dotted) line indicating the"
2201.10154,data,9,,,the data is the following probability transition matrix:
2201.10154,data,22,,,function Equation 18 is just the negative log-likelihood or cross-entropy of the observed data under the given form of the distribution.
2201.10154,data,23,,,"The problem can be formulated as: Problem 3: For any time series data {xt}, identifying the existence of"
2201.10154,data,23,,,"With Theorem 2, we can understand what happens when the neural squeezer framework is trained by data in an intuitive way."
2201.10154,data,30,,,We test our model on several data sets. All the data is generated by simulated dynamical models. And the models include continuous dynamics and discrete Markovian dynamics.
2201.10154,data,33,,,"Theorem 3(Mutual information of the model will be closed to the data for a well trained framework): If the neural networks in NIS framework are well-trained, then:"
2201.10154,data,34,,,"[16] S. L¨owe, D. Madras, R. Zemel, M. Welling, Amortized causal discovery: Learning to infer causal graphs from time-series data, ArXiv (2020) 2006.10833."
2201.10154,data,36,,,"Proof. Because both ψ, χq, and f are Markovian, so X → X (cid:48) → U → V forms a Markov chain. Thus the data processing inequality holds:"
2201.10154,data,37,,,"[22] B. Chen, K. Huang, S. Raghupathi, I. Chandratreya, Q. Du, H. Lipson, Discovering state variables hidden in experimental data, arXiv preprint arXiv:2112.10755 (2021)."
2201.10154,data,41,,,"With all these notions, we can deﬁne our ﬁrst problem formally. Problem 1: Finding out the eﬀective q dimensional coarse-graining strategy and macro-dynamics as deﬁned in Deﬁnition 5 according to the time series data {xt}."
2201.10154,data,41,,,"[13] M. Reichstein, G. Camps-Valls, B. Stevens, M. Jung, J. Denzler, N. Carvalhais, Prabhat, Deep learning and process understanding for data-driven earth system science, Nature 566 (2019) 195–204."
2201.10154,data,42,,,"By training the dynamics learner in an end-to-end manner, we can avoid estimating the markov transitional probabilities from the data to reduce biases because neural networks always have much better ability to ﬁt the data and generalize to unseen cases."
2201.10154,data,42,,,"Finally, we should identify causal emergence. Deﬁnition 10(Causal Emergence in Time Series Data): For the time series of micro-states {xt}, we say there is causal emergence if the following inequality is satisﬁed:"
2201.10154,data,42,,,"First, we know as the neural networks are trained, the output of the entire framework ˆxt+1 is closed to the real data xt+1 under any given xt, so do the mutual information, that is the following theorem:"
2201.10154,data,48,,,"Proof. According to the objective function, i.e., Equation 18, we know that if the neural networks in NIS framework are well-trained, that means the conditional probability distribution of the model will be closed to the one on the data, that is:"
2201.10154,data,52,,,"If the neural networks are well-trained, the mutual information on the macrodynamics will approach to the information in the data {xt}. So no matter how small q is (or how large is the scale), the mutual information of the macrodynamics fβ will keep constant."
2201.10154,data,55,,,"Finally, although an explicit expression for EI on macro-dynamics has been derived under the neural information squeezer framework, we still cannot directly predict the causal emergence in the data. We believe that a more concise analytic results on the EI should be derived by setting some constraints on the data."
2201.10154,data,58,,,"From Theorem 5, we can clearly see that when the neural networks are well trained the eﬀective information of the macro-dynamics fβ is only dependent on the bijector ψα and the conditional distribution G in the time series data. Therefore, the bijector ψα actually plays a signiﬁcant role for the causal strength of fβ."
2201.10154,data,58,,,"[9] F. E. Rosas, P. A. M. Mediano, H. J. Jensen, A. K. Seth, A. B. Barrett, R. L. Carhart-Harris, D. Bor, Reconciling emergences: An informationtheoretic approach to identify causal emergence in multivariate data, PLoS Computational Biology 16 (12) (2020) e1008289."
2201.10154,data,59,,,"After coarse-graining, we obtain a new time series data of macro-states denoted by y1 = φq(x1), y2 = φq(x2), · · ·, yT = φq(xT ), we may use another dynamical model(or a markov chain) fφq to describe the evolution of yt:"
2201.10154,data,72,,,"The system has 8 states, and seven of them can transfer each other. The last state is standalone. We use a one-hot vector to encode the states. Therefore, for example, state 2 will be represented as (0, 1, 0, 0, 0, 0, 0, 0). We sample the initial state for 50,000 batches to generate data."
2201.10154,data,82,,,"In this paper, we propose a novel neural network framework, Neural Information Squeezer, for discovering coarse-graining strategy, macro-dynamics and emergent causality in time series data. We ﬁrst deﬁne eﬀective coarse-graining strategy and macro-dynamics by constraining the coarse-graining strategies as decomposable. We then use an invertible neural network incorporating with the projection operation to realize the decomposition. Decomposablity also allows us to analyze the properties of the entire NIS framework. By treating the frame 19"
2201.10154,data,105,,,"We always want to reconstruct g according to the observable micro-states. However, an informative dynamics g with strong causal connections is always hardly to be reconstructed from the micro-states when noise ξ is strong. While we can ignore some information in the micro-states data and convert it into macro-state time series. In this way we may reconstruct a macro-dynamics with stronger causality to describe the evolution of the system. This is the basic idea behind causal emergence[5, 6]. We formalize the information ignoring process as a coarse-graining strategy(or mapping, method)."
2201.10154,data,117,,,"Furthermore, in some other works of machine learning, although the terms like coarse-graining or renormalization are never used, similar ideas are also shared[28, 29, 30, 31, 32]. For example, Word2Vec tries to encode words in natural language into a latent state space (word vector space) in which the predictions can be made accordingly, and the dimension of latent space is always much smaller than the original space. Representation learning methods try to extract meaningful multi-scale representations from the original data to perform the down-stream tasks[33]. However, causality and emergence are never discussed by these methods."
2201.10154,data,125,,,"An alternative way to identify causal emergence and even other types of emergence is based on partial information decomposition given by [9]. Although this method can avoid the discussion on coarse-graining strategies, time consuming searching on sub sets space of the system state is also needed. And this method can not give the coarse-graining strategy and the corresponding macrodynamics which are useful in practice. Furthermore, another common shortage shared by the two methods is that an explicit markov transition matrix for both macro- and micro- dynamics are needed, and the transitional probabilities should be estimated from data. As a result, large bias on rare events can hardly be avoided, particularly for continuous data."
2201.10154,data,185,,,"The classic studies of causal emergence have revealed that in some Markovian dynamical systems, far stronger causal connections can be found on the higher-level descriptions than the lower-level of the same systems if we coarse-grain the system states in an appropriate way. However, identifying this emergent causality from the data is still a hard problem that has not been solved because the correct coarse-graining strategy can not be found easily. This paper proposes a general machine learning framework called Neural Information Squeezer to automatically extract the eﬀective coarse-graining strategy and the macro-state dynamics, as well as identify causal emergence directly from the time series data. By decomposing a coarse-graining operation into two processes: information conversion and information dropping out, we can not only exactly control the width of the information channel, but also can derive some important properties analytically including the exact expression of the eﬀective information of a macro-dynamics. We also show how our framework can extract the dynamics on diﬀerent levels and identify causal emergence from the data on several exampled systems."
2201.10154,data,194,,,"In this paper, we propose a general mathematical tractable framework called Neural Information Squeezer (NIS) to learn the coarse-graining strategy and macro-state dynamics from time series data, and identify if causal emergence takes place. By forcing any coarse-graining mapping from Rp to Rq (q ≤ p) as an information conversion invertible process and an information dropping out process, the NIS framework can squeeze the information of micro-states to form macro-states and make prediction on future micro-states according to the macrostates. Finally, an eﬀective coarse-graining strategy and macro-state dynamics with emergent causality can be learned. This minimal framework can not only allow us to control information conversion and discarding in a precise way, but also enable us to mathematically analyze the whole framework in a tractable way. We prove a series of mathematical theorems to reveal the properties of the NIS, and we analytically derive an expression of eﬀective information for causal emergence without the explicit macro-dynamics. At last, we show how NIS can learn the eﬀective coarse-graining strategy and macro-state dynamics numerically on a set of examples."
2201.10154,data,209,,,"We sample the one step state transition of the entire network for 50,000 batches and each batch contains 100 diﬀerent initial conditions which are randomly sampled from the possible state space evenly, and we then feed these data to the NIS model. By systematically search for diﬀerent q, we found that the eﬀective causal emergence information peaks at q = 1 as shown in Figure 4.2.3. Under this condition, we can visualize the coarse-graining strategy by Figure 4.2.3, on which the x-coordinate is the decimal coding for the binary microstates (e.g., 5 denotes for the state 0101), and the y-coordinate represents the codes for macro-states. The data points can be clearly classiﬁed into 4 clusters according to their y-coordinate. This means the NIS network found 4 discrete macro-states although the states are continuous real numbers. Interestingly, we found that the mapping between the 16 micro states and 4 macro states are identical as the coarse-graining strategy shown in the example in ref [5]. However, any prior information neither the method on how to group the nodes nor the coarse graining strategy, nor the dynamics are known by our algorithm."
2201.10154,data,226,,,"The results are shown in Figure 4.2.1. To test if the neural networks can learn the real latent(macro) state, we directly plot the predicted and the real latent states. As shown in Figure 4.2.1(a), the predicted and the real curves collapse together which means the neural network can recover the macro state in the data although it is unknown for the neural network. As a comparison, the feed-forward neural network cannot recover the macro state. We can also check if the neural squeezer can learn the dynamic of the macro states by plotting the derivatives of the states (dz/dt, dv/dt) against the macro state variables (v, z). If the learned dynamics follows Equation 27, then two cross-over lines for dz/dt = v and dv/dt = −z can be observed as shown in Figure 4.2.1(c). However, the same pattern can not be reproduced on the common feed-forward network as shown in Figure 4.2.1(d). We also test the well-trained NIS by multiple-step prediction as shown in Figure 4.2.1(e). Although there are larger and larger deviation from the prediction and the real data, the general trends can be captured by NIS model."
2201.10154,data,230,,,"On the other hand, machine learning methods powered by neural networks have been developed in recent years, many cross-disciplinary applications have been made[11, 12, 13, 14]. Equipped with this method, automated discovery of causal relationships and even dynamics of complex systems in a data driven way becomes possible[15, 16, 17, 18, 19, 20, 21, 22]. Machine learning and neural networks can also help us to ﬁnd good coarse-graining strategies[23, 24, 25, 26, 27]. If we treat a coarse-graining mapping(or being called renormalization in physics community) as a function from micro-states to macro-states, then we can certainly approximate this function by a parameterized neural network. For example, [26] and [24] used normalized ﬂow model equipped with invertible neural network to learn how to renormalize a multi-dimensional ﬁeld (quantum ﬁeld, images or joint probability distributions) to a set of independent Gaussian noise, and how to generate the same ﬁeld from sampling a set of independent Gaussian noise in the same time merely via running the same model in an inverse way. Therefore, both the coarse-graining strategy and the generative model can be learned from data automatically."
2201.10154,data,240,,,"work as a squeezed information channel, we can prove four important theorems. The results show that if the causal connection in the data is strong, then as we train the neural networks, the macro-dynamics will increase its informativeness. And during this process, the determinant of the Jacobian of the bijector and the entropy of the macro-state will increase in the same time. We also found a mathematical expression for the eﬀective information of the macro-dynamics without the explicit dependence on the macro-dynamics, and it is determined solely by the bijector and the data when the whole framework is well trained. Furthermore, if the framework has been trained in a suﬃcient time, the mutual information of the macro-dynamics will keep a constant no matter the scale q is. However, as q decreases, the mutual information or the bandwidth on the encoder part also decreases and closed to the information limitation on the entire channel such that it can make correct prediction for the future micro-states. Thus, the task becomes harder for the encoder because more eﬀective information must be encoded and pass to the dynamics learner such that it can make correct prediction with less information. Numerical experiments show that our framework can reconstruct the dynamics in diﬀerent scales and also can discover emergent causality in data on several classic causal emergence examples."
2201.10154,data,273,,,"emergence merely from data is still lack[9]. One of the diﬃculty is how to search all possible coarse-graining methods, on which the causal emergence can be found[9], in a systematic way. On a networked complex system, a coarse-graining strategy includes the way of grouping nodes and the method of mapping the micro-states within a group to a macro-state of the macro-level node[10]. The existing methods solve the problem by ﬁxing the mapping function of states, and searching all the grouping methods by heuristic optimization algorithms[5, 10]. However, there is no reason why some strategies of state coarse-graining are preferred but not others. Therefore, we should search on the space of all possible coarse-graining strategies such that the most informative causal mechanism on dynamics can be identiﬁed, however, the searching space is too large[9]. Another headache we must confront once we consider all possible coarse-graning strategies is that some trivial mapping between states of micro- and macro may be unavoidable. To show this, we consider a possible coarse-graining method is to map all the micro-states to an identical value as the macro-state. In this way, the macroscopic dynamics is only an identical mapping which will have large eﬀective information(EI) measure. However, it can not be called an informative causal emergence because all the information is eliminated by the coarse-graining method itself. Thus, we must ﬁnd a way to exclude such trivial strategies."
2201.10154,"data, dataset",54,,,"There are several weak points in our framework. First, it can only work on small data set. The major reason is the invertible neural network is very diﬃcult to train on large data set. Therefore, we will use some special techniques to optimize the architecture in the future."
2201.10154,"data, used dataset, dataset",158,,,"where, ξ ∼ N (0, σ) is a random number following two dimensional Gaussian distribution, and σ is the vector of the standard deviations for position and velocity. Although there is noise to disturb the measurement of the state, it can be easily eliminated by adding the measurements on the two channels together. Therefore, if the neural network can discover a macro-state which is the addition of the two measurements, then it can easily obtain the correct dynamics. We sample the data for 10,000 batches, and in each batch, we randomly generate 100 random initial states and perform one step dynamic to get the state at the next time step. We use these data to train the neural network. To compare, we also use the same data set to train an ordinary feed-forward neural network with the same number of parameters."
2202.01863,code,22,,,"[Co-1] Code is made available, or if not, is justified within manuscript as to why it is omitted"
2202.01863,code,34,,,"In general, review of code could be the subject of an entire paper itself. However, it is important that the details regarding code availability or lack thereof be clearly conveyed."
2202.01863,code,41,,,Reviewer Reviewer-1 Total Intro Methods Results Discussion Conclusion Code Reviewer-2 Total Intro Methods Results Discussion Conclusion Code Reviewer-3 Total Intro Methods Results Discussion Conclusion Code Reviewer-4 Total Intro Methods Results Discussion Conclusion Code Reviewer-5 Total Intro Methods Results Discussion Conclusion Code
2202.01863,code,197,,,"For each item, the reviewer should provide a score in the appropriate field as follows: 0 = Item not covered, 1 = Item covered but not sufficient, 2 = Item sufficiently covered. The maximum possible score a submission can receive is 70. In general, individual items receiving a score of 0 are points warranting major revision or cause for rejection. Items receiving a score of 1 can also be highlighted in the review and likely can be easily addressed in a revision. It also may not be possible to share code, which should be considered closely by the reviewer. In general, a guide for applying these metrics is as follows: <50 = manuscript will likely not achieve level necessary for publication even through revision and should therefore be rejected, 50-60 = manuscript needs major revision, >60 = manuscript needs minor or no revision to be considered suitable for publication. Note also that some sections may not be appropriate for some manuscripts. Use of N/A and reduction from the total possible score should be incorporated in such cases."
2202.01863,"code available, code",1,,,Code
2202.01863,"code available, code",26,,,"D-3 C-1 C-2 C-3 C-4 Co-1 Code is made available, or if not, is justified within manuscript as to why it is omitted."
2202.01863,data,8,,,[R-3] Data Availability Statement is present
2202.01863,data,11,,,Is the test data not representative of routinely acquired data?
2202.01863,data,11,,,"[M-9] Data augmentation strategies are described, if used"
2202.01863,data,14,,,"[M-2] If data is original, appropriate oversight committee approvals are detailed"
2202.01863,data,15,,,[R-2] A table with the clinical information of each data subset is shown
2202.01863,data,18,,,"The methods section should provide sufficient detail regarding data sources, approaches, and the software utilized."
2202.01863,data,19,,,"The strategy and methods for annotating the data needs to be clear and its rationale explained, including:"
2202.01863,data,24,,,“Data were sampled during training such that the proportion of each class in 1 epoch was the same across all classes.”
2202.01863,data,27,,,"Does the study focus on improved performance? If so, then the work should compare existing models to the proposed approach on the same data."
2202.01863,data,33,,,"Data privacy methods should be described (anonymization, pseudo-anonymization or de-identification) with mention to any technological tool or software used (such as encryption tools or DICOM anonymization software)."
2202.01863,data,44,,,"● Markups used ● Reports used ● Level of annotation: pixel/voxel, bounding boxes, slice-level or study-level (for 3D data) ● Time tracking ● Annotation software used ● Number and experience of human annotators ● Inter/intra rater variabilities of annotations"
2202.01863,data,64,,,"Details regarding how the images were acquired, and whether multiple vendors and/or institutional data were used should be clarified. State how data uniformity was established and the exclusion criteria to meet the protocol specifications. Provide detail regarding the data curation process to ensure wrong images were not included.  A minimum set of information should be stated for each modality:"
2202.01863,data,64,,,"For many studies it is critical to know the inherent variability of the task, particularly for human readers. This gives the AI practitioner an idea for when a model is performing as well, or possibly better than human readers. It is also often not possible to have this information, particularly for studies that scrape data from prior reports."
2202.01863,data,67,,,"Authors may use data augmentation methods to discourage model sensitivity to known invariants and to help improve model generalization. Authors should state which methods were performed and their corresponding parameterizations e.g. rotations of up to 15 degrees, random crops of 224x224 pixels with 0-padding as needed,etc. Some common augmentation approaches may not be justifiable in the area of medical imaging."
2202.01863,data,73,,,Sensitivity analyses were performed to test the model’s robustness to various assumptions. A STARD-like diagram is provided A table with the clinical information of each data subset is shown  Data Availability Statement is present Sufficient analysis is performed Explainability methods are used to demonstrate a reasonable and/or new explanation Failure analysis with reasonable hypotheses for incorrect model predictions is performed The results are summarized in a concise and coherent manner
2202.01863,data,130,,,"Data preprocessing is critical to understand how the study can be reproduced and compared in the future. Different considerations need to be given for different modalities. For instance, CT intensities are much more standardized than those from MR. Any process that changes pixel/voxels values and numbers from the original image prior to being input to the deep learning algorithm is considered pre-processing, and such image manipulations should be specifically described. Pre-processing may include such manipulations as changing image resolution (up-sampling or down-sampling), windowing (in the case of CT), signal intensity modification and windowing (for MRI), cropping images, standardizing using the mean and/or standard deviation of image intensities, or many other image processing techniques."
2202.01863,data,259,,,"A key part of science is reproducibility. Authors must describe their methodologies and results in sufficient detail to enable readers to assess the rigor and generalizability of the work. To the extent that a work cannot be appropriately evaluated and/or reproduced, it does not advance the field. This article aims to provide a granular set of best practices to practitioners, reviewers, and editorial boards related to publishing machine learning papers related to medical image classification problems. Additional entries in the series are planned, focusing on different application areas germane to medical imaging AI. A typical classification problem in this area uses imaging data to make a prediction. For example, the prediction might try to predict tumor type, treatment response, or chances for survival. Our goal is to enhance reproducibility and replicability1 of published manuscripts by providing a checklist and scoring rubric to guide reviewers. Other high-level guides2–5 have been already published elsewhere and were considered when creating this guide, but in our view did not provide a quantitative way to rank a paper. We believe that this screening rubric is unique to our checklist and will highlight areas of strengths and weakness of a particular manuscript. Moreover, this scoring system may identify and confirm specific sections were there are deficiencies across multiple manuscripts. This type of analyses will allow develop strategies and resources to address these limitations and further promote the reproducibilty of AI research in medical imaging"
2202.01863,"data available, data, data repository, repo",17,,,“This data is available at <repo link> under <license> license.”
2202.01863,"data available, data, dataset",101,,,"Explain the dataset creation methodology and strategy (e.g., single training, validation and test split, K-fold crossvalidation, etc.). State the size of each partition. Explain the rationale for the dataset size including data availability and access limitations, and statistical power estimation. If the creation of synthetic images has been used, it should be stated and the methodology explained. It is important to note the importance of using K-fold cross-validation when the dataset is small, given the large variance that can occur in performance estimation with single fold validation."
2202.01863,"data, code",150,,,"The Machine Learning Education Subcommittee of SiiM has established this primer series with the goal of providing resources for improving manuscripts that are published in the field of medical imaging using machine learning techniques. The vision for this series is to provide a checklist for reviewers of manuscripts to help standardize the review process and identify important components that all publications should address. The first primer in this series is an overview of the key issues in study design, data curation and model training and focuses on algorithms that are developed forimage classification tasks. Future primer papers will focus in detail on other tasks such as image segmentation, as well as other areas of review (e.g., code review). We hope that this series will be useful for both the reviewers, as well as authors preparing papers in this field."
2202.01863,"data, dataset",32,,,"A table with the clinical information of each data subset is shown, with appropriate statistical comparison. If crossvalidation is performed, clinical information can be shown for the entire dataset"
2202.01863,"data, dataset",46,,,"If data was acquired from multiples scanners/vendors, the range and distribution of acquisition parameters should be described. As protocol heterogeneity usually leads to more robust and generalizable models, it is usually a good sign to have a dataset with diverse acquisition parameters."
2202.01863,"data, dataset",52,,,"Ideally, authors should try to make their datasets available. This includes the labels and the train, validation and test set indices. Note however that this may not be possible in the case of institutional data. In either case, a data availability statement should be mentioned:"
2202.01863,"data, dataset",303,,,"With the recent advances in A.I. methodologies and their application to medical imaging, there has been an explosion of related research programs utilizing these techniques to produce stateof-the-art classification performance. Ultimately, these research programs culminate in submission of their work for consideration in peer reviewed journals. To date, the criteria for acceptance vs. rejection is often subjective; however, reproducible science requires reproducible review. The Machine Learning Education Sub-Committee of SIIM has identified a knowledge gap and a serious need to establish guidelines for reviewing these studies. Although there have been several recent papers with this goal, this present work is written from the machine learning practitioners standpoint. In this series, the committee will address the best practices to be followed in an A.I.-based study and present the required sections in terms of examples and discussion of what should be included to make the studies cohesive, reproducible, accurate, and self-contained. This first entry in the series focuses on the task of image classification. Elements such as dataset curation, data pre-processing steps, defining an appropriate reference standard, data partitioning, model architecture and training are discussed.. The sections are presented as they would be detailed in a typical manuscript, with content describing the necessary information that should be included to make sure the study is of sufficient quality to be considered for publication. The goal of this series is to provide resources to not only help improve the review process for A.I.-based medical imaging papers, but to facilitate a standard for the information that is presented within all components of the research study. We hope to provide quantitative metrics in what otherwise may be a qualitative review process."
2202.01863,"data, dataset provided",19,,,[M-6] The software and method for data annotation is clear and experience of those annotating is provided
2202.01863,"data, used dataset",33,,,"The method for splitting the data into training, validation (development), and test sets is clearly defined. It should be clear if an independent test set was used."
2202.01863,dataset,7,,,[M-1] Dataset origin is well-defined
2202.01863,dataset,8,,,Was the dataset of an appropriate size?
2202.01863,dataset,12,,,[M-7] Dataset is broken up into training/validation and test sets
2202.01863,dataset,34,,,The dataset split is done at the patient level if there is more than 1 image from a specific patient in the dataset to ensure that patients are not present across multiple partitions.
2202.01863,dataset,41,,,"“Influenza viral pneumopathy, atypical bacterial pneumopathy, and cryptogenic organizing pneumonia chest CT cases have been included in the dataset because they have similar appearances to SARS-VoV-2 viral pneumopathy, for which we are developing this model.”"
2202.01863,dataset,50,,,"Strategies for handling class imbalance (weighted loss, balanced sampling) are described, if used. The rationale of the different classes included in the dataset should be explained. If a specific dataset enrichment has been performed, it should be described and its rationale explained."
2202.01863,dataset,69,,,"This section should also include the description of advanced anonymization techniques such as defacing on head cross sectional imaging and optical character recognition, and if human verification of each image anonymization has been performed. In addition, if metadata have been partially removed to retain useful information for training, the authors should describe which metadata have been removed and which remain available in the dataset."
2202.01863,dataset,129,,,"“All MR acquisitions were acquired at our institution on a ‘VendorName’ 3T scanner (‘Specific scanner details’) in the supine position utilizing a multichannel surface coil. No intravenous contrast was used. The sequences included in the imaging protocol were conventional single-shot fast spin echo (SSFSE) axial, coronal and sagittal scout images followed by the T1-weighted MR images used in the study (spoiled gradient sequence, with TR=80ms, TE=3.2ms, 25º flip angle, and reconstructed voxel resolution in plane of 1.5mm, and slice thickness of 3mm). The images were acquired under a single breath-hold. A manual review guaranteed that sequences different from the ones described were not included in the dataset”"
2202.01863,"dataset provided, data, dataset",169,,,"Item A sufficient level of background information related to the relevant use case is given  Discussion of relevant and related work is comprehensive  Details regarding the gap the current study fills is provided There is a clear summary of the study objectives Dataset origin is well-defined If data is original, appropriate oversight committee approvals are detailed Steps to protect patient privacy should be outlined Specifics regarding image acquisition parameters are clear Reference standard is clearly defined and justified The software and method for data annotation is clear and experience of those annotating is provided Dataset is broken up into training/validation and test sets The method of pre-processing is clearly defined and justified Data augmentation strategies are described, if used Discussion of the model architecture is clear Hyperparameter choice and training protocol are presented Strategies for handling class imbalance are described Core software versions and libraries are detailed Assessment of inter/intra variabilities is provided, if possible A primary performance metric should be identified, with appropriate rationale"
2202.01863,"dataset, open-source data, used dataset, open-source",13,,,“This study used an open-source dataset from <citation>.”
2202.01863,"publicly available, data, dataset",161,,,"The origin of the dataset needs to be clearly detailed (original and proprietary, or public datasets). The collection and data transfer steps, in case of multi-site study, and the storage medium (on premise, cloud, internal or external) should be described. The way the quality of the dataset has been assessed should be described (human supervised or software), and finally the format of the images should be stated (DICOM, PNG, JPEG, TIFF, NIFTI, etc.). The investigators should explain the rationale of using the medical imaging exam type and specifics for US, MRI sequences, or CT phases for the study. “The process and tools used to identify, query and extract data from Electronic Medical Records/RIS/PACS should be described.” The authors should provide the date range of the dataset and the specific inclusion/ exclusion criteria."
2202.01863,"publicly available, dataset",16,,,“The datasets created in this study are not publicly available because <reasons>”
2202.01863,python,12,,,“Python 3.7.6 was used with PyTorch 1.4.0 and scikit-learn 0.22”
2202.12674,"code available, code, provide implementation",164,,,"We have developed the PLSSVM library to solve both issues. First, we resort to the formulation of the SVM as a least squares problem. Training an SVM then boils down to solving a system of linear equations for which highly parallel algorithms are known. Second, we provide a hardware independent yet efﬁcient implementation: PLSSVM uses different interchangeable backends—OpenMP, CUDA, OpenCL, SYCL—supporting modern hardware from various vendors like NVIDIA, AMD, or Intel on multiple GPUs. PLSSVM can be used as a drop-in replacement for LIBSVM. We observe a speedup on CPUs of up to 10 compared to LIBSVM and on GPUs of up to 14 compared to ThunderSVM. Our implementation scales on many-core CPUs with a parallel speedup of 74.7 on up to 256 CPU threads and on multiple GPUs with a parallel speedup of 3.71 on four GPUs. The code, utility scripts, and documentation are all available"
2202.12674,data,4,,,A. Data Layout
2202.12674,data,4,,,data.py (2022-01-20)
2202.12674,data,7,,,B. Data Sets and Experimental Setup
2202.12674,data,8,,,2https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/#libsvm for dense data
2202.12674,data,11,,,Figure 1c shows the runtimes for 28 to 215 data points
2202.12674,data,12,,,• read: Reads the input training data ﬁle and parses its
2202.12674,data,16,,,(b) CPU runtimes relative to the number of features with 213 data points.
2202.12674,data,16,,,(c) GPU runtimes relative to the number of data points with 212 features.
2202.12674,data,16,,,(d) GPU runtimes relative to the number of features with 215 data points.
2202.12674,data,20,,,by the fact that the runtime per CG iteration stays the same for data sets of the same size.
2202.12674,data,22,,,(a) GPU runtime behavior of the individual PLSSVM components depending on the number of data points with 212 features.
2202.12674,data,22,,,(b) GPU runtime behavior of the individual PLSSVM components depending on the number of features with 215 data points.
2202.12674,data,25,,,(a) Scaling behavior of the different PLSSVM components with respect to the number of cores for 212 data points and 211 features.
2202.12674,data,25,,,(b) Scaling behavior of the different PLSSVM components with respect to the number of GPUs for 216 data points and 214 features.
2202.12674,data,26,,,TABLE I RUNTIME EXAMPLES FOR THE DIFFERENT BACKENDS ON DIFFERENT GPUS FOR 215 DATA POINTS WITH 212 FEATURES EACH REACHING APPROX. 93.76 % ACCURACY.
2202.12674,data,27,,,"but generally holds for all our GPU backends, since they all have similar runtime behaviors with respect to the number of data points and features."
2202.12674,data,29,,,29211213215# data points2623202326runtime [s]readtransformcgwritetotal2729211213# features2623202326runtime [s]readtransformcgwritetotal(a) Runtime and number of CG iterations relative to the relative residual.
2202.12674,data,36,,,• transform: Transforms the previously created 2D training data structure into a 1D vector in Structure-of-Arrays (SoA) layout for a better caching efﬁciency. This is only relevant for the GPU implementations.
2202.12674,data,39,,,"If the data cannot be separated linearly, this inequality cannot be solved. In order to train such problems as well, a vector (cid:126)ξ of positive slip variables ξi is introduced:"
2202.12674,data,41,,,"Fig. 3. Runtime, accuracy, and number of CG iterations in relation to the relative residual of the CG method’s epsilon. Measured on the GPU machine with 215 data points and 212 features each."
2202.12674,data,41,,,"To classify a data point, the SVM computes on which side of the previously learned hyperplane the data point lies. This requires the normal vector (cid:126)w and the bias b to compute: (cid:16)"
2202.12674,data,48,,,"First, we discuss the overall runtime of our implementation using the CPU and GPU for training the SVM; see Figure 1. We only consider the linear kernel for the synthetic data sets, since the behavior can be projected one-to-one to the other kernels."
2202.12674,data,49,,,"[3] V. Singh et al., “Prediction of covid-19 corona virus pandemic based on time series data using support vector machine,” J. Discret. Math. Sci. Cryptogr., vol. 23, no. 8, pp. 1583–1597, 2020."
2202.12674,data,52,,,"Since ˜Q has (m − 1)2 entries, it is not feasible for large training data sets to store the matrix completely in memory. Therefore, the matrix is only represented implicitly: each entry ˜Qi,j is recalculated for each use according to Equation 13:"
2202.12674,data,58,,,"Finally, both ThunderSVM and LIBSVM support sparse data representations, which are used in their internal calcu lations. PLSSVM so far only supports sparse data representations when reading and writing. When parsing sparse data, we allocate memory for all features including those that are zero, resulting in a dense data representation internally."
2202.12674,data,59,,,"Fig. 2. Double logarithmic runtime scaling behavior of the individual PLSSVM components on a single GPU. The total runtime depends on the number of CG iterations, which are chosen so that the model reaches approximately 97 % accuracy on the training data or after convergence of the accuracy in the ﬁrst three decimal places."
2202.12674,data,67,,,"Fig. 1. Double logarithmic runtime comparison for sparse/dense LIBSVM, ThunderSVM, and PLSSVM on both CPU and GPU in relation to the number of data points and features. The runtimes were measured with an epsilon, such that the resulting models reach approximately 97 % accuracy on the training data or after convergence of the accuracy in the ﬁrst three decimal places."
2202.12674,data,68,,,"In Figure 1b, we examine the scalability with respect to the number of features, while the number of data points was ﬁxed to 213. Increasing the number of features, PLSSVM scales (up to 210) slightly better than LIBSVM and signiﬁcantly better than ThunderSVM. The behavior for the LIBSVM variants changes at 212, but this has not been investigated further."
2202.12674,data,73,,,"50 000 times. A survey conducted by Kaggle in 2017 showed that 26 % of the data mining and machine learning practitioners use SVMs [2]. Examples of current research topics are COVID-19 pandemic predictions [3], automatic COVID-19 lung image classiﬁcation [4], forecasting of carbon prices [5], face detection [6], or propaganda text recognition [7]."
2202.12674,data,82,,,"While the data points are initially read into an irregular 2D data structure, they are later transformed into a 1D vector laid out consecutively in memory. Since the data is accessed dimension-wise during execution, the data vector is not created arranging the individual points in row-major order, but it is constructed in column-major order. This has the advantage that it is signiﬁcantly more cache-efﬁcient for our indexing scheme on GPUs and thus results in better performance."
2202.12674,data,87,,,"The two most common tasks in supervised machine learning are classiﬁcation and regression. Classiﬁcation predicts to which set of categories/classes objects or situations belong, while regression estimates the function value of a functional dependency, e.g., a statistical process. Both can be solved using the same algorithms, by matching a function value to two or more discrete classes. Moreover, both share the challenge to scale in the size of the data in order to cope with massive data sets."
2202.12674,data,89,,,"In its least squares form, the secondary condition in Equation 3 is no longer understood as an inequality but as an equality. In contrast to the classic SVM, not only a few data points are rated for classiﬁcation, but the LS-SVM determines a weighting proportional to the distance between each data point and the hyperplane. Therefore, in the LS-SVM all data points are interpreted as support vectors. In this case, the weighting can also be negative in contrast to the classical"
2202.12674,data,90,,,"The GPU implementations have a small overhead accessing the GPU(s). Therefore, the CPU implementations are better suited for learning classiﬁers for very small data sets. Note that PLSSVM is currently implemented using a dense CG implementation. In the case of very sparse data sets with many features, it is therefore better to use ThunderSVM. PLSSVM clearly outperforms both LIBSVM and ThunderSVM for large non-trivial classiﬁcation tasks by orders of magnitude, making use of the high parallelization potential of LS-SVMs."
2202.12674,data,107,,,"In this paper, we describe our C++17 PLSSVM library, which efﬁciently brings SVMs to massively parallel accelerators, and we present and analyze some of our ﬁrst performance results. We have implemented the basic functionality of the most widely used SVM library, LIBSVM [1], as a dropin alternative with signiﬁcant acceleration aided by GPUs. However, sparse data sets, where all but a few feature entries are zero, are treated as if they would represent dense data, i.e., explicitly representing zeros where necessary. As of now, our implementation only supports binary classiﬁcation."
2202.12674,data,108,,,"In this work, we introduced the new SVM library PLSSVM. It is based on the rarely used least squares approach. In contrast to the SMO method that is used in most state-of-the-art implementations, it includes all data points as support vectors to compute the class-separating hyperplane. While SMO has limited parallelization potential, the LS-SVM approach is wellsuited for massively parallel hardware and thus large data sets. PLSSVM is the very ﬁrst SVM implementation supporting multiple backends—OpenMP, CUDA, OpenCL, and SYCL— to be able to target different hardware platforms from various vendors, being it CPUs or GPUs."
2202.12674,data,111,,,"Suykens and Vandewalle [23, 24] developed the least squares formulation of the SVM problem. In this formulation, training an SVM is reduced to solving a system of linear equations, a problem for which highly parallel algorithms exist. Suykens et al. improved their Least Squares Support Vector Machine (LS-SVM) further by introducing weights [25] and extending their implementation to also support sparse data structures [26]. Furthermore, they introduced the multi-class classiﬁcation for LS-SVMs [27]. An exact correlation between SVMs using SMO and LS-SVMs was investigated by Ye and Xiong [28]."
2202.12674,data,127,,,"We demonstrated with classiﬁcation tasks for artiﬁcial and real-world dense data sets of different sizes that we are capable of competing with well-established SMO implementations such as LIBSVM or ThunderSVM on the CPU although our basic implementation is currently still very much open to optimizations. Since there are already many widely distributed implicit matrix-vector multiplication implementations available that we have not considered yet, this shows that there is much potential for further work on a highly scalable multinode LS-SVM implementation. On the GPU our library shows its main strength and severely outperforms ThunderSVM by a factor of 14. We veriﬁed that our implementation scales well on many-core CPUs and multiple GPUs achieving a parallel speedup of 3.71 on four NVIDIA A100 GPUs."
2202.12674,data,131,,,"Abstract—Machine learning algorithms must be able to efﬁciently cope with massive data sets. Therefore, they have to scale well on any modern system and be able to exploit the computing power of accelerators independent of their vendor. In the ﬁeld of supervised learning, Support Vector Machines (SVMs) are widely used. However, even modern and optimized implementations such as LIBSVM or ThunderSVM do not scale well for large nontrivial dense data sets on cutting-edge hardware: Most SVM implementations are based on Sequential Minimal Optimization, an optimized though inherent sequential algorithm. Hence, they are not well-suited for highly parallel GPUs. Furthermore, we are not aware of a performance portable implementation that supports CPUs and GPUs from different vendors."
2202.12674,data,132,,,"Canonical next steps include the optimization of the CPU implementation, to consider sparse data structures for the CG solver, and to target multi-node multi-GPU systems to be able to use even larger data sets as currently possible and to report the scaling behavior of our library on more GPUs and CPU cores. In future work, we will investigate the advantages and disadvantages of the different backends, and provide extensive scalability studies on non-NVIDIA GPUs. As a long-term future task, we want to extend all PLSSVM kernels to support multi-node multi-GPU execution including load balancing on heterogeneous hardware. Finally, we intend to extend PLSSVM to provide all the standard functionality of LIBSVM to our users. This includes multi-class classiﬁcations and regression tasks."
2202.12674,data,135,,,"Looking at proﬁling results using NVIDIA’s Nsight Compute proﬁling tool, we noticed that ThunderSVM spawns a plethora of small compute kernels on the device (over 1600 in our proﬁled scenario with 214 data points and 212 features), most of them running signiﬁcantly less than one millisecond. However, the compute kernel with the highest compute intensity only reaches approximately 233 GFLOPS which only amounts to 2.4 % of the A100’s theoretical FP64 peak performance. In contrast, our implementation only spawns 3 compute kernels that each have a much higher compute intensity than ThunderSVM’s kernels. The kernel responsible for the implicit matrix-vector multiplication inside the CG algorithm reaches over 3.1 TFLOPS using double precision resulting in 32 % FP64 peak performance."
2202.12674,data,141,,,"Therefore, it is necessary to employ some form of blocking to utilize the faster shared memory. For each thread block, blocksize · 2 many data points are needed to calculate blocksize2 many kernel operations (e.g., scalar products for the linear kernel; see subsection II-E). We start by loading a few features of the data points, which are needed for the ﬁrst operations, into the shared memory. To utilize the full bandwidth of the shared memory, which is much higher compared to the bandwidth of the global memory, it becomes important to evenly distribute the memory reads over all warps. Next, we proceed with the actual calculations, followed by loading the next features of the data points into the shared memory, and so on."
2202.12674,data,151,,,"The runtime behavior for PLSSVM, ThunderSVM, and LIBSVM on the CPU, for a ﬁxed number of features and a varying number of data points, is shown in Figure 1a. All three SMO implementations behave very similarly, with the dense implementation of LIBSVM having a slight advantage over its sparse implementation. ThunderSVM performs better than LIBSVM in the medium size range, while in the extremes the LIBSVM variants are faster. However, all SMO implementations exhibit a steeper slope, compared to the LSSVM method used in PLSSVM. We out-scale both LIBSVM variants, starting with 211 data points. For 214 data points in 210 dimensions we need approximately 56.2 s to train the PLSSVM while LIBSVM already needs 7.0 min (dense) re (a) CPU runtimes relative to the number of data points with 210 features."
2202.12674,data,159,,,"Since the methods have different termination criteria, it is not trivial to compare the runtimes with each other. We compare the runtimes by adjusting the epsilon of the loss function for the SMO methods and the epsilon of the relative residual used in the LS-SVM algorithm. We start with 0.1 and increment the epsilon in steps of ×0.1 (i.e., 0.01, 0.001, etc.) until an accuracy of more than 97 % was reached on the training data. If the training data was non-separable, i.e., we were not able to reach a minimum accuracy of 97 %, we compared the runs that converged in accuracy in the ﬁrst three digits. Except for the epsilon, the default values of the libraries were retained. This implies that no special optimizations with regard to the used hardware were performed for any of the three used SVM implementations."
2202.12674,data,170,,,"To learn an SVM classiﬁcation model a d-dimensional data set T with m data points (cid:126)xi and their corresponding labels yi is required for training. Based on T , an SVM learns to distinguish between different classes. We assume a binary classiﬁcation problem and, w.l.o.g., that classes are labeled yi = ±1. The actual learning process is to ﬁnd the hyperplane (cid:104) (cid:126)w, (cid:126)xi(cid:105) + b = 0 that divides the two classes best. Therefore, the normal vector (cid:126)w is scaled in a way that the margin, and 2 (cid:107) (cid:126)w(cid:107) . The bias b indicates, thus the width of the separation, is b with (cid:107) (cid:126)w(cid:107) , the distance of the hyperplane to the origin. This can be expressed by the following inequality:"
2202.12674,data,182,,,"The two generated clusters are adjacent to each other and overlap with a low probability in a few points. Additionally, one percent of the labels were set randomly to ensure some noise in the generated data. The SMO approach would be a better ﬁt for well separable data. However, real-world data is rarely free of noise and in the rarest cases also well and clearly separable. The more noisy and ambiguous the training data is, the better the LS-SVM is suited (for the theoretical background, see [28]). Therefore, the data for this analysis were selected with a little noise but still relatively well separable. This ensures that the data suits the SMO approach while representing the structure of real-world-like data. The number of data points and features for all synthetically generated data sets are power of twos. However, our library is not limited or speciﬁcally optimized to sizes of power of twos, it is rather used for convenience in the logarithmic plots."
2202.12674,data,206,,,"The differences become larger if we ﬁx the number of data points to 215 and vary the number of features from 26 to 214. This time, PLSSVM has a slightly ﬂatter slope than ThunderSVM. ThunderSVM needs 241 s for training a data set with 211 features. Using PLSSVM this can be reduced by a factor of 14.2 to only 17 s. This additionally indicates that our implementation scales better with the number of features compared to ThunderSVM. Increasing the number of features by a factor of four from 211 to 213 using PLSSVM, increases the runtime by a factor of roughly 5.5. Actually, we had expected that the factor would be larger, since a four-fold increase of the number of features means an eight-fold increase of the matrix entries. However, in the selected scenario, the complexity of the problem to learn remains the same. It can thus be stated that, despite more features, fewer CG iterations are needed to solve the system of linear equations to a similar accuracy. Note that the observed performance advantage of PLSSVM over ThunderSVM thus depends to some extent on the actual classiﬁcation problem."
2202.12674,data,292,,,"runtime are due to parts that are not displayed here, such as the overhead when accessing the GPU for the ﬁrst time or the cleanup at the end of the program execution. Figure 2a shows that “read”, “transform”, and “write” do scale better than “cg” for an increasing number of data points. They can therefore also be neglected for the runtime of even larger training sets. The “cg” component has the worst complexity: doubling the number of data points increases the runtime by a factor of 3.3. In Figure 2b we proceed as before and ﬁx the number of data points to 215 while varying the number of features from 26 to 214. The result stays the same: solving the system of linear equations is again responsible for over 92 % of the total runtime. Doubling the number of features increases the runtime by roughly a factor of 2.11. Here, a factor of two can be justiﬁed by the fact that the effort for implicitly calculating each matrix entry doubles, since the vectors used in the scalar products have twice the size. Furthermore, the problem becomes more complex in higher dimension, so that it can be observed that more CG iterations are needed to reach a similar accuracy. Again, this factor is problem dependent. In any case, a factor larger than two is to be expected, provided that the additional features actually lift the problem into a higher dimension. First tests have indicated that the factor is close to two if the vectors are only extended with zeros."
2202.12674,"data available, data, dataset",164,,,"5) Distribute Blocks Across Multiple GPUs: Our implementation is capable of utilizing multiple GPUs for the linear kernel. In order to achieve an even utilization of all GPUs, we do not divide the data set itself, but we split all data points feature-wise. For example, if the data is in a ten-dimensional space (i.e., each data point has ten features) and two GPUs are available, each data point is split into two ﬁve-dimensional data points and each GPU is assigned one of these. Due to the linearity of the linear kernel, only the result vectors of the single devices have to be summed up. The polynomial and radial kernels do not currently support multi-GPU execution. The remaining calculations stay the same, no matter how many devices are used. This approach also reduces the memory usage per GPU, so that larger data sets can be learned."
2202.12674,"data, dataset",7,,,D. The SAT-6 Airbone Real-World Data Set
2202.12674,"data, dataset",43,,,Figure 3 displays the runtime and achieved accuracy based on the chosen epsilon using a single GPU. The epsilon is used as a termination criterion in the CG algorithm. The data set contains 215 data points with 212 features each.
2202.12674,"data, dataset",72,,,"Figure 4a shows the speedup for the “read”, “cg”, and “write” components on a CPU with 2 · 64 physical cores and 2 · 128 hyper-threads using the OpenMP backend. The “transform” component is omitted since the 2D to 1D transformation is only applied for the GPU backends. The classiﬁcation data set contains 212 data points with 211 features."
2202.12674,"data, dataset",80,,,"Using the real-world data set, we observe a similar behavior. On the SAT-6 Airbone training data set, we achieved the highest accuracy with the radial basis function kernel. Training the SVM classiﬁer using a single GPU takes 23.5 min for our PLSSVM implementation, resulting in an accuracy of 95 % on the test data set. ThunderSVM needs 40.6 min for 94 % accuracy and is thus a factor of 1.73 slower than PLSSVM."
2202.12674,"data, dataset",98,,,"However, it should not become as large as the factor of ThunderSVM in any case, since the complexity of the problem increase drastically with an increasing generally does not number of data points for an LS-SVM. In this example, the number of required CG iterations dropped from an average of 30.5 iterations for a data set with 210 points and 210 features to only 26 iterations for 215 data points and the same number of features. Using ThunderSVM the runtime drastically increases by a factor of 239 compared to the CPU runtimes."
2202.12674,"data, dataset",132,,,"210212214# data points2022242628210runtime [s]PLSSVMThunderSVMLIBSVMLIBSVM-DENSE2729211# features212427210213runtime [s]PLSSVMThunderSVMLIBSVMLIBSVM-DENSE29211213215# data points212325runtime [s]PLSSVMThunderSVM2729211213# features21232527runtime [s]PLSSVMThunderSVMwith 212 features each. Up to 211 data points, the runtime of PLSSVM does not increase, indicating a signiﬁcant static overhead using a GPU. However, for large data sets, this overhead is negligible (also see Figure 2a). The slopes of the SVM implementations indicate that both have approximately the same overall complexity, with PLSSVM having a drastically smaller constant factor. Training the SVM model using a data set with 214 data points needs 10 s using PLSSVM. ThunderSVM takes 72 s, resulting in a runtime increase of a factor of 7.2."
2202.12674,"data, dataset",136,,,Figure 1c and 1d show the same experimental setup but now using a single GPU instead of a CPU. The runtimes for PLSSVM using the GPU are orders of magnitude faster than their CPU counterpart (see Figure 1a). PLSSVM needs about 58 s to train a data set with 214 data points and 210 features on the CPU. Using a single GPU reduces this runtime by a factor of 24 to only 2.4 s. This is surprising since both systems have a comparable computational power with 9.7 TFLOPS theoretical peak performance for the NVIDIA A100 GPU and with 4.6 TFLOPS (base) to 6.96 TFLOPS (boost) for the two AMD EPYC 7742 CPUs. This underlines how much optimization potential there still is in our OpenMP implementation.
2202.12674,"data, dataset",147,,,"10151011107103epsilon204060 runtime [s]010203040 cg iterations10151011107103epsilon60708090 accuracy [%]010203040 cg iterations21232527# cores21232527speedupreadcgwritetotaloptimal202122# GPUs202122speedupreadtransformcgwritetotaloptimalFigure 4b shows the scaling behavior of the same three components together with the “transform” component on up to four A100 GPUs using the CUDA backend. Here, the data set contains 216 data points with 214 features. Increasing the number of GPUs does not result in better runtimes for the “read”, “transform”, and “write” components. This is rather obvious, since none of them uses the GPUs. The “cg” component, which dominates the total runtime, is executed on the GPUs. Using four GPUs reduces the total runtime by a factor of 3.71 from 13.49 min to 3.72 min. This nicely demonstrates good scaling on multiple GPUs."
2202.12674,"data, dataset",147,,,"Note that the state-of-the-art SMO algorithm and the least squares approach solve slightly different optimization problems. Therefore, for a given data set, the termination criterion (epsilon) can not be reused between the two methods. This makes comparing the overall runtimes between the two approaches more difﬁcult. In general, straightforward problems are better suited for SMO, as only few support vectors can be sufﬁcient to compute the separating hyperplane. Difﬁcult classiﬁcation problems with plenty of data points and complex clusters that require many support vectors are better suited for the LS-SVM approach. The termination criterion for the iterative solver of our PLSSVM has to be selected suitably. However, all of our classiﬁcation tasks so far have shown that the runtime is not very sensitive with respect to the termination criterion epsilon for high accuracies."
2202.12674,"data, dataset",149,,,"In addition to these synthetically generated data sets, we also used the real-world SAT-6 Airbone data set [38]. This data set consists of images displaying six different land cover classes. Since we currently only support binary classiﬁcation, we mapped the labels of all man-made structures (buildings and roads) to −1 and the labels of the remaining classes (barren land, trees, grassland, and water bodies) to 1. The data set is split into training data with 324 000 (193 729 with label −1 and 130 271 with label 1) images of size 28 × 28 with 4 color channels (RGB-IR) resulting in 3136 features per image and test data with 81 000 images. All features are scaled to values between [−1, 1] using LIBSVM’s svm-scale."
2202.12674,"data, dataset",185,,,"• write: Creates and writes the resulting model ﬁle to disk. • total: This represents the runtime of a complete training run including “read”, “transform”, “cg”, “write” and remaining parts like initializing the backend and hardware. Figure 2a shows the components’ runtimes for data set sizes from 28 up to 215 using 212 features each. It can be seen that the total runtime is heavily dominated by the solution of the system of linear equations using the CG algorithm if the data set is sufﬁciently large enough. For data sets with less than 212 data points, the IO components “read” and “write” contribute more to the total runtime than solving the system of linear equations. However, for a data set size of 215, the CG algorithm is responsible for 92 % of the total runtime, the other measured components combined are only accountable for 5 % and, therefore, negligible. The remaining 3 % of the"
2202.12674,"data, dataset",244,,,"We developed the PLSSVM library for maximum performance and portability. Therefore, we have not only implemented the LS-SVM using a single framework but with multiple ones: we cover OpenMP, CUDA, OpenCL, and SYCL (hipSYCL and DPC++). This opens up the possibility to target a broad variety of different hardware architectures, in particular CPUs and GPUs from NVIDIA, AMD, and Intel. All backends are optional, i.e., they are only conditionally included if the necessary hardware and software stack is available. The actual used backend can be selected at runtime. We use C++17 focusing on modern best practices and support switching between double and single precision ﬂoating point types by changing a single template parameter. The training of PLSSVM can be split into four steps: (1) read the whole training data set used to set up the system of linear equations (Equation 14), (2) load the training data onto the device, (3) solve the system of linear equations and writeback the solution to the host memory, and (4) save the learned support vectors with the corresponding weights to a model ﬁle. Currently, our implementation only supports dense data for calculations. If sparse data sets are used, they are at ﬁrst converted into a dense representation by ﬁlling in zeros."
2202.12674,"data, python",50,,,"In order to report scaling runtimes with respect to a growing number of data points and features, we decided to focus on synthetically generated dense data sets. The training ﬁles are created by Python’s scikit-learn single-label generator make_classification [37]. These synthetic data sets"
2202.12674,"data, python",87,,,"Do et al. [29, 30] developed an LS-SVM implementation using CUDA to support NVIDIA GPUs. However, they only investigated their implementation on data sets with less than 200 dimensions. Our PLSSVM implementation can easily cope with data sets having more than 65 000 data points with more than 16 000 features, as shown in Section IV. Other LS-SVM implementations were developed, e.g., an LS-SVM purely written in Python using PyTorch by Drumond [31]."
2202.12674,"data, python",130,,,"ThunderSVM [14] is a rather new SVM implementation supporting CPUs and NVIDIA Graphics Processing Units (GPUs) using the Compute Uniﬁed Device Architecture (CUDA). Additionally, there have been various other attempts to port SVMs efﬁciently to GPUs, e.g., with CUDA [15, 16, 17], but also using other languages and frameworks such as the Open Computing Language (OpenCL) [18], Python [19, 20], OpenACC [21], or SYCL [22]. However, all approaches mentioned so far are based on SMO, an inherently sequential algorithm, and, therefore, not very well suited for high performance GPU implementations and vast data sets."
2202.12674,"data, used dataset, dataset",111,,,"The total amount of memory used for the given data set on a single GPU is 8.15 GiB. Using four GPUs reduces the amount of memory used to 2.14 GiB per GPU. While this results only in a reduction of a factor of 3.6, instead of the optimal factor of 4, it nevertheless shows that using multiple GPUs not only allows us to train SVM classiﬁers more efﬁcient, but also to process data sets that would not ﬁt on a single GPU. In contrast, using the same data set with ThunderSVM results in a memory consumption of 13.08 GiB on a single GPU."
2202.12674,github,4,,,3https://github.com/SC-SGS/PLSSVM/blob/v1.0.1/utility scripts/generate
2202.12674,github,7,,,on GitHub: https://github.com/SC-SGS/PLSSVM.
2202.12674,github,13,,,"[17] S. Herrero, “multisvm,” https://github.com/sergherrero/"
2202.12674,github,40,,,"[21] V. Codreanu et al., “Evaluating automatically parallelized versions of the support vector machine,” Concurrency and Computation, vol. 28, no. 7, pp. 2274–2294, 2014. https://github.com/codeplaysoftware/"
2202.12674,"github, data",45,,,can be reproduced with the generate_data.py script located in the PLSSVM GitHub3 using the problem type “planes”. We have generated separate new data ﬁles for each individual run to average over a variety of different spatial arrangements of the same problem.
2202.12674,"github, data, data https, python",59,,,"[18] H. E. L. Cagnin et al., “A portable OpenCL-based IEEE, 2015. [19] S. Raschka et al., “Machine learning in python: Main developments and technology trends in data science, machine learning, and artiﬁcial intelligence,” MDPI, 2020. https://github.com/ “svm-gpu,”"
2202.12674,"github, python",57,,,"[31] R. Drumond, “Lssvm in python,” https://github.com/ RomuloDrumond/LSSVM, accessed: 2022-01-20. [32] M. R. Hestenes and E. Stiefel, “Methods of conjugate gradients for solving linear systems,” JRITEF, vol. 49, no. 6, p. 409, 1952."
2202.12674,package,73,,,"[15] A. Carpenter, “cusvm: A cuda implementation of support vector classiﬁcation and regression,” http:// patternsonascreen.net/cusvm.html, pp. 1–9, 2009. [16] Q. Li et al., “Gpusvm: a comprehensive cuda based support vector machine package,” Central European Journal of Computer Science, vol. 1, no. 4, pp. 387–405, 2011."
2202.12674,provide implementation,42,,,Machine (PLSSVM) library on the least squares formulation of Suykens and Vandewalle [23]. Our goal is to provide a High Performance Computing (HPC) implementation of a LSSVM suitable for modern massively parallel heterogeneous hardware.
2202.12674,python,15,,,"Python,” JMLR, vol. 12, pp. 2825–2830, 2011."
2202.12861,data,42,,,"[12] Remonda, A., Krebs, S., Veas, E., Luzhnica, G. & Kern, R. Formula rl: Deep reinforcement learning for autonomous racing using telemetry data. ArXiv Preprint arXiv:2104.11106. (2021)"
2202.12861,github,3,,,1https://www.github.com/ribsthakkar/
2202.13290,"code available, code",148,,,"computed based on 20 ms frames with a hop size of 10 ms, and a 320-point discrete Fourier transform. We use a stack of two gated recurrent unit layers, each of size 322 nodes, followed by a fullyconnected layer with a sigmoid activation function. The model has 1.3 million parameters. The estimated mask is point-wise multiplied with the magnitude spectrogram of the microphone signal to suppress the far end signal. Finally, to resynthesize the enhanced signal, an inverse short-time Fourier transform is used on the phase of the microphone signal and the estimated magnitude spectrogram. We use a mean squared error loss between the clean and enhanced magnitude spectrograms. The Adam optimizer with a learning rate of 0.0003 is used to train the model. The model and the inference code is available in the challenge repository.4"
2202.13290,"data available, dataset",168,,,"For the far end single talk case, there is only the loudspeaker signal (far end) played back to the users and users remain silent (no near end speech). For the near end single talk case, there is no far end signal and users are prompted to speak, capturing the near end signal. For double talk, both the far end and near end signals are active, where a loudspeaker signal is played and users talk at the same time. Echo path changes were incorporated by instructing the users to move their device around or bring themselves to move around the device. The RT60 distribution for 4387 desktop environments in the real dataset for which impulse response measurements were available is estimated using a method by Karjalainen et al. [12] and shown in Figure 1. For 1251 mobile environments the RT60 distribution shown was estimated blindly from speech recordings [13]."
2202.13290,"data https, open-source, data open-source , dataset provided, dataset",164,,,"The ICASSP 2022 Acoustic Echo Cancellation Challenge is intended to stimulate research in acoustic echo cancellation (AEC), which is an important area of speech enhancement and still a top issue in audio communication. This is the third AEC challenge and it is enhanced by including mobile scenarios, adding speech recognition word accuracy rate as a metric, and making the audio 48 kHz. We open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 10,000 real audio devices and human speakers in real environments, as well as a synthetic dataset. We open source an online subjective test framework and provide an online objective metric service for researchers to quickly test their results. The winners of this challenge were selected based on the average Mean Opinion Score (MOS) achieved across all scenarios and the word accuracy rate."
2202.13290,"data, code",154,,,based on MOS estimates with high accuracy. It is a neural networkbased model that is trained using the ground truth human ratings obtained using our online subjective evaluation framework. The audio data used to train the AECMOS model is gathered from the numerous subjective tests that we conducted in the process of improving the quality of our AECs as well as the ﬁrst two AEC challenge results. The performance of AECMOS on AEC models is given in Table 2 compared with subjective human ratings on the 18 submitted models. We note that this model had not seen any mobile nor fullband data during training. The next version of AECMOS will have mobile and fullband data in its training data. A more detailed description of AECMOS is given in [25]. Sample code and details of the evaluation API can be found on https://aka.ms/aec-challenge.
2202.13290,"data, dataset",19,,,"• The dataset has increased from 5,000 devices and environ ments to 10,000 to provide additional training data."
2202.13290,database,40,,,"[17] J. Thiemann, N. Ito, and E. Vincent, “The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings,” Montreal, Canada, 2013."
2202.13290,database,144,,,"To simulate a far end signal, we pick a random speaker from a pool of 1,627 speakers, randomly choose one of the clips from the speaker, and sample 10 seconds of audio from the clip. For the near end signal, we randomly choose another speaker and take 37 seconds of audio which is then zero-padded to 10 seconds. Of the selected far end and near end speakers, 71% and 67% are male, respectively. To generate an echo, we convolve a randomly chosen room impulse response from a large internal database with the far end signal. The room impulse responses are generated by using Project Acoustics technology3 and the RT60 ranges from 200 ms to 1200 ms. In 80% of the cases, the far end signal is processed by"
2202.13290,dataset,4,,,2. TRAINING DATASETS
2202.13290,dataset,4,,,2.1. Real dataset
2202.13290,dataset,4,,,2.2. Synthetic dataset
2202.13290,dataset,4,,,Params Real-time Additional Datasets
2202.13290,dataset,37,,,"The ﬁrst dataset was captured using a large-scale crowdsourcing effort. This dataset consists of more than 50,000 recordings from over 10,000 different real environments, audio devices, and human speakers in the following scenarios:"
2202.13290,dataset,49,,,"Two test sets are included, one at the beginning of the challenge and a blind test set near the end. Both consist of 800 real world recordings, between 30-45 seconds in duration. The datasets include the following scenarios that make echo cancellation more challenging:"
2202.13290,dataset,50,,,"While the results of this challenge continue to improve over previous challenges, there is still signiﬁcant room for improvement, especially with the mobile scenario. We hope this challenge, dataset, test set, and test framework stimulate research in this important area of speech enhancement."
2202.13290,dataset,53,,,"[10] K. Sridhar, R. Cutler, A. Saabas, T. Parnamaa, M. Loide, H. Gamper, S. Braun, R. Aichner, and S. Srinivasan, “ICASSP 2021 Acoustic Echo Cancellation Challenge: Datasets, Testing Framework, and Results,” in ICASSP, 2021."
2202.13290,dataset,61,,,"teams use linear AEC’s and DNN’s, and all 5 use the STFT domain. In addition, all 5 models perform noise suppression in addition to AEC. There is a wide range of model sizes and complexities, and it wasn’t necessary to use external datasets to do well in the challenge."
2202.13290,dataset,64,,,"[16] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio Set: An ontology and human-labeled dataset for audio events,” in ICASSP, New Orleans, LA, Mar. 2017, pp. 776–780, IEEE."
2202.13290,dataset,65,,,"The training dataset is described in Section 2, and the test set in Section 3. We describe a DNN-based AEC method in Section 4. The online subjective evaluation framework is discussed in Section 5, and the objective service in Section 6. The challenge metric is given in Section 7 and the challenge rules are described in https: //aka.ms/aec-challenge."
2202.13290,dataset,77,,,"[9] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, P. Rana, S. Srinivasan, and J. Gehrke, “The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets, Subjective Testing Framework, and Challenge Results,” in INTERSPEECH 2020. Oct. 2020, pp. 2492–2496, ISCA."
2202.13290,dataset,122,,,"With the advent of deep learning techniques, many supervised learning algorithms for AEC have shown better performance compared to their classical counterparts, e.g., [2, 3, 4]. Some studies have also shown good performance using a combination of classical and deep learning methods such as using adaptive ﬁlters and recurrent neural networks (RNNs) [4, 5] but only on synthetic datasets. While these approaches are promising, they lack evidence of their performance on real-world datasets with speech recorded in diverse noise and reverberant environments. This makes it difﬁcult for researchers in the industry to choose a good model that can perform well on a representative real-world dataset."
2202.13290,dataset,148,,,"a nonlinear function to mimic loudspeaker distortion. For example, the transformation can be clipping the maximum amplitude, using a sigmoidal function as in [18], or applying learned distortion functions, the details of which we will describe in a future paper. This signal gets mixed with the near end signal at a signal to echo ratio uniformly sampled from -10 dB to 10 dB. The signal to echo ratio is calculated based on the clean speech signal (i.e., a signal without near end noise). The far end and near end signals are taken from the noisy dataset in 50% of the cases. The ﬁrst 500 clips can be used for validation as these have a separate list of speakers and room impulse responses. Detailed metadata information can be found in the repository."
2202.13290,dataset,164,,,"For far end signals, we use both clean speech and real world recordings. For clean speech far end signals, we use the speech segments from the Edinburgh dataset [14]. This corpus consists of short single speaker speech segments (1 to 3 seconds). We used a long short term memory (LSTM) based gender detector to select an equal number of male and female speaker segments. Further, we combined 3 to 5 of these short segments to create clips of length between 9 and 15 seconds in duration. Each clip consists of a single gender speaker. We create a gender-balanced far end signal source comprising of 500 male and 500 female clips. Recordings are saved at the maximum sampling rate supported by the device and in 32-bit ﬂoating point format; in the released dataset we down-sample to 48 kHz and 16bit using automatic gain control to minimize clipping."
2202.13290,dataset,177,,,"The second dataset provides 10,000 synthetic scenarios, each including single talk, double talk, near end noise, far end noise, and various nonlinear distortion scenarios. Each scenario includes a far end speech, echo signal, near end speech, and near end microphone signal clip. We use 12,000 cases (100 hours of audio) from both the clean and noisy speech datasets derived in [9] from the LibriVox project1 as source clips to sample far end and near end signals. The LibriVox project is a collection of public domain audiobooks read by volunteers. [9] used the online subjective test framework ITU-T P.808 to select audio recordings of good quality (4.3 ≤ MOS ≤ 5) from the LibriVox project. The noisy speech dataset was created by mixing clean speech with noise clips sampled from Audioset [16], Freesound2 and DEMAND [17] databases at signal to noise ratios sampled uniformly from [0, 40] dB."
2202.13290,"dataset, dataset provided, open-source",150,,,"This AEC challenge is designed to stimulate research in the AEC domain by open sourcing a large training dataset, test set, and subjective evaluation framework. We provide two new open source datasets for training AEC models. The ﬁrst is a real dataset captured using a large-scale crowdsourcing effort. This dataset consists of real recordings that have been collected from over 10,000 diverse audio devices and environments. The second dataset is synthesized from speech recordings, room impulse responses, and background noise derived from [9]. An initial test set will be released for the researchers to use during development and a blind test set near the end, which will be used to decide the ﬁnal competition winners. We believe these datasets are large enough to facilitate deep learning and representative enough for practical usage in shipping telecommunication products."
2202.13290,"download, dataset",64,,,We use Amazon Mechanical Turk as the crowdsourcing platform and wrote a custom HIT application that includes a custom tool that users download and execute to record the six scenarios described above. The dataset includes Microsoft Windows and Android devices. Each scenario includes the microphone and loopback signal (see Figure 2). Even though our application uses the WASAPI raw
2202.13290,github,3,,,4https://github.com/microsoft/AEC-Challenge/tree/main/baseline/
2202.13290,github,33,,,The subjective test framework with an AEC extension is available at https://github.com/microsoft/P.808. A more detailed description of the test framework and its validation is given in [24].
2202.13290,"github, dataset, open-source",26,,,"The challenge will include two new open source datasets, one real and one synthetic. The datasets are available at https://github.com/ microsoft/AEC-Challenge."
2202.13290,open-source,31,,,"[20] B. Naderi and R. Cutler, “An Open Source Implementation of ITU-T Recommendation P.808 with Validation,” INTERSPEECH, pp. 2862–2866, Oct. 2020."
2202.13290,open-source,71,,,We have extended the open source P.808 Toolkit [20] with methods for evaluating the echo impairments in subjective tests. We followed the Third-party Listening Test B from ITU-T Rec. P.831 [21] and ITU-T Rec. P.832 [22] and adapted them to our use case as well as for the crowdsourcing approach based on the ITU-T Rec. P.808 [23] guidance.
2202.13290,"used dataset, dataset",162,,,"where y(n) is the microphone signal, and e(n) is the residual echo after cancellation. ERLE is only appropriate when measured in a quiet room with no background noise and only for single talk scenarios (not double talk), where we can use the processed microphone signal as an estimate for e(n). PESQ has also been shown to not have a high correlation to subjective speech quality in the presence of background noise [8]. Using the datasets provided in this challenge we show that ERLE and PESQ have a low correlation to subjective tests (Table 1). In order to use a dataset with recordings in real environments, we can not use ERLE and PESQ. A more reliable and robust evaluation framework is needed that everyone in the research community can use, which we provide as part of the challenge."
2203.02480,data,7,,,LSTM. Useful data augmentation techniques.
2203.02480,data,23,,,"Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowl edge and Data Engineering, 2021."
2203.02480,data,34,,,"Alexandra Vella and Patrizia Paggio. Overlaps in maltese: a comparison between map task dialogues and multimodal conversational data. In 4th Nordic Symposium on Multimodal Communication, pages 21–29, 2013."
2203.02480,data,39,,,"Richard A Guzzo, Alexis A Fink, Eden King, Scott Tonidandel, and Ronald S Landis. Big data recommendations for industrial–organizational psychology. Industrial and Organizational Psychology, 8(4):491–508, 2015."
2203.02480,data,49,,,"Michael Murray, Nick Walker, Amal Nanavati, Patricia Alves-Oliveira, Nikita Filippov, Allison Sauppe, Bilge Mutlu, and Maya Cakmak. Learning backchanneling behaviors for a social robot via data augmentation from human-human conversations. 5th Annual Conference on Robot Learning, 6 2021."
2203.02480,data,49,,,"Taras Kucherenko, Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, and Gustav Eje Henter. A large, crowdsourced evaluation of gesture generation systems on common data: The genea challenge 2020. In 26th International Conference on Intelligent User Interfaces, pages 11–21, 2021."
2203.02480,data,77,,,"In addition to the interaction context, the speech, voice tone or other information related to the person of interest or the others may inﬂuence their behavior. Naturally, such multimodal data needs to be exploited in a speciﬁc way in order to fully proﬁt from it. Therefore, we distinguish between unimodal and multimodal methods, which combine the visual modality with at least another modality as input to make their predictions."
2203.02480,data,88,,,"an earlier anticipation as no intentional or behavioral information was encoded. In fact, the authors claimed that the generation of fully autonomous behavior in groupal humanrobot interactions is still beyond their capabilities. One of the main reasons behind such pessimistic point of view lays on the numerous particularities of behavior forecasting. For example, backchanneling periods in a conversation are short and infrequent, which leads to a huge imbalanced problem. Murray et al. (2021) proposed a data augmentation method"
2203.02480,data,117,,,"In contrast to the previous highly complex approaches, Wang et al. (2021a) recently proposed an unimodal and context-blinded method which beat its multimodal and contextaware competitors in a multi-person motion prediction benchmark (Adeli et al., 2020, 2021). They used the work of Mao et al. (2019) as backbone, which consisted of cascaded Graph Convolutional Networks (GCNs) applied to the DCT of the joints. They proved that using several training tricks such as interpolation of invisible/missing joints, data augmentation, boundary ﬁltering, or curriculum learning, among many others, may be more eﬀective than leveraging more complex networks."
2203.02480,data,321,,,"On one side, works that predict high-level behavioral representations such as social cues or signals often need to compare single or multiple discrete predictions to a single observed ground truth (one-to-one). For univariate classiﬁcation, the Area Under the Curve (AUC) of the Receiver Operating Curve (ROC) is a common choice, as it very well describes the overall performance of the model. The classic accuracy, precision, recall, and F1-score are valid alternatives for both single and multi-class classiﬁcation. On the other side, low-level behavioral representations are often continuous and therefore conceived as regression tasks. For those, the L1 and L2 distances have traditionally been the golden standard. Variants of L2-based metrics have been used depending on the ﬁeld of application. For example, in human motion forecasting with joints, the Mean-Per-Joint-Position Error (MPJPE), the Percentage of Correct Keypoints (PCK), or the cosine similarity are very frequent options. In this context, Adeli et al. (2021) raised the common problem of missing data (e.g., occluded or out-of-view joints). To address this, they proposed metrics that evaluated the models performance under several visibility scenarios: the Visibility-Ignored Metric (VIM), the Visibility-Aware Metric (VAM) and the Visibility Score Metric (VSM). Diﬀerently, in behavior forecasting with raw image or audio, quality metrics like the Mean-Squared Error (MSE), the Structural Similarity Index Measure (SSIM) (Wang et al., 2004), or the Peak Signal to Noise Ratio (PSNR) are better suited. Indeed, this task shares many similarities with video prediction, so any metric from that ﬁeld can be leveraged for ours (Oprea et al., 2020)."
2203.02480,data,323,,,"or virtual agents (Huang et al., 2020; Murray et al., 2021). For example, these tests have been used to prove the great capacities of models that generate backchannel responses when it comes to successfully keeping the user engaged during human-robot interactions (Murray et al., 2021). For low-level representations though, there is still a lack of extensive studies that assess the transferability of the results to the target scenario. This is mainly due to the extra constraints posed by these low-level representations. In fact, some works highlight the current possibilities and limitations of behavior forecasting with such representations. For example, future behavior in competitive interactions like fencing strongly depends on the player’s decisions in answer to the competitor’s. Within this context, Honda et al. (2020) showed inferior performance for the rapid and highly stochastic motion of the dominant arm (PCKs of 71.8% and 66.4% for dominant hand and elbow, respectively) than for the other parts of the body (average PCK of 77.1%). This has been consistent in the literature for other less interactive scenarios like face-to-face conversations. Barquero et al. (2022) showed superior accuracy for behavior forecasting for face and upper body torso (errors of 12.70 and 5.75 pixels in average for a prediction of 2 seconds) than for hands (25.15 pixels). This represents an important bottleneck for hands forecasting, where research is almost nonexistent despite of their importance in human communication. The authors also showed superior performance (error of 5.34 pixels) than a naive but strong baseline (6.00 pixels) for the short term (<400ms). This opens new possibilities for providing humanrobot interactive agents with fairly accurate anticipation capabilities."
2203.02480,data,330,,,"As a matter of fact, Goswami et al. (2020) found few performance diﬀerences between a random forest and a ResNet when predicting disengagement in the context of storytelling with children, who are prone to disengage very easily. In this work, they also predicted whether a low or a high degree of backchanneling was needed to keep the listener engaged. They assessed the prediction capabilities of many visual cues never used before for this task such as pupil dilation, blink rate, head movements, or facial AUs. Interestingly, they found that the gaze features and the speech pitch were among the most important features to be considered for disengagement prediction. These ﬁndings are consistent with those from the work of M¨uller et al. (2020), who found eye contact and speaker turns to be the most informative modalities when it comes to anticipating averted gaze during dyadic interactions. In their work, the authors also tested other less powerful modalities including face- and gaze-related attributes, expressions and speaker information. As observed in the course of the survey, there is an important heterogeneity with respect to methodologies used for social signal/cues forecasting. Joo et al. (2019a) tried to establish a generic deﬁnition which homogenized all methodologies: a social signal prediction (SSP) model. The SSP model deﬁned a framework to model the dynamics of social signals exchanged among interacting individuals in a data-driven way. It consisted in using the target’s past behavior and their interactants’ to predict its future behavior. However, their deﬁnition implied that a separate function was learnt for every person. Raman et al. (2021) solved this issue in their formulation of social processes (SP), in which a simultaneous prediction of the behavior of all the individuals was considered."
2203.02480,data,337,,,"Disengagement is another very important social signal to be considered when designing interactive agents. If identiﬁed with enough anticipation, the speaker can prevent it from happening with backchannel responses or by making an engaging hands gesture, for example. Van Doorn (2018) made an attempt to anticipate whether a user would leave the conversation and when. Unfortunately, in the ﬁrst task, they only slightly outperformed the random baseline with one of the many models trained (AdaBoost), and were not successful at the second. Similarly in a human-robot interaction scenario, Ben-Youssef et al. (2021) aimed at anticipating a premature ending of an interaction. As part of their experiments, a logistic regression classiﬁer was trained with all the possible multimodal combinations. They found that the best results were achieved with the combination of the distance to the robot, the gaze and the head motion, as well as facial expressions and acoustic features. Surprisingly, the choice of the classiﬁer had little eﬀect over the ﬁnal results (logistic regression, random forest, multi-layer perceptron, or linear-discriminant analysis). The small inﬂuence of the classiﬁer choice has been consistent along all works reviewed in this section. We hypothesize that this is due to the little need of further processing for such simple and already high-level representations. As a matter of fact, Goswami et al. (2020) found few performance diﬀerences between a random forest and a ResNet when predicting disengagement in the context of storytelling with children, who are prone to disengage very easily. In this work, they also predicted whether a low or a high degree of backchanneling was needed to keep the listener engaged. They assessed the prediction capabilities of many visual cues never used before for this task such as pupil dilation, blink rate, head movements, or facial AUs."
2203.02480,data,347,,,"In fact, some works highlight the current possibilities and limitations of behavior forecasting with such representations. For example, future behavior in competitive interactions like fencing strongly depends on the player’s decisions in answer to the competitor’s. Within this context, Honda et al. (2020) showed inferior performance for the rapid and highly stochastic motion of the dominant arm (PCKs of 71.8% and 66.4% for dominant hand and elbow, respectively) than for the other parts of the body (average PCK of 77.1%). This has been consistent in the literature for other less interactive scenarios like face-to-face conversations. Barquero et al. (2022) showed superior accuracy for behavior forecasting for face and upper body torso (errors of 12.70 and 5.75 pixels in average for a prediction of 2 seconds) than for hands (25.15 pixels). This represents an important bottleneck for hands forecasting, where research is almost nonexistent despite of their importance in human communication. The authors also showed superior performance (error of 5.34 pixels) than a naive but strong baseline (6.00 pixels) for the short term (<400ms). This opens new possibilities for providing humanrobot interactive agents with fairly accurate anticipation capabilities. For example, the proper activation of the actuators of a robot may beneﬁt from any extra milliseconds of In general though, we think that landmarks-based behavior forecasting is anticipation. still immature, and will strongly beneﬁt from further research eﬀorts. Another concerning issue related to this topic lies on the typology of the data leveraged for forecasting. Only models that make their predictions solely leveraging automatically retrieved data can be successfully applied to real life scenarios. Actually, and similarly to the low-level scenario, we expect the forecasting of high-level behavioral representations to greatly beneﬁt from the development of new accurate methods to automatically retrieve social cues/signals from raw image/audio data."
2203.02480,"data available, data, dataset",301,,,"Social cues. Backchannel responses are among the most explored social cues1 in humanrobot interaction scenarios. Such cues can be vocal (e.g., ’Mmmh!’, ’Well...!’) or visual (e.g., head nodding), and are of utmost importance in order to keep the interacting user engaged (Krauss et al., 1977). Earlier classic approaches built handcrafted sets of rules that triggered generic backchannel responses (Al Moubayed et al., 2009; Poppe et al., 2010). In fact, forecasting backchannel subtypes (generic, agreement, disagreement, surprise, fear, etc.) had traditionally required diﬀerent levels of semantic processing. Blache et al. (2020) proposed a novel single-route backchannel predictive model that revisited the rule-based classic paradigm and predicted backchannels in real time at a ﬁne-grained level. Their method used prosodic, discourse, semantic, syntax, and gesture features. In contrast with previous approaches that used an observation window as long as the last utterance, they proposed to extract features from bigger semantic units by means of discourse markers. More recently, the collection, annotation and release of bigger datasets favored the appearance of data-driven automated multimodal methods for backchannel prediction. For example, Boudin et al. (2021) used a logistic classiﬁer that was trained on visual cues, prosodic and lexico-syntactic features in order to predict not only the backchannel opportunity but also their subtype associated (generic, positive, or expected). The choice of such a simple classiﬁer was driven by the small dataset available. They showed the superior performance of the multimodal combinations in both tasks."
2203.02480,"data available, dataset",40,,,"Lucien Maman, Eleonora Ceccaldi, Nale Lehmann-Willenbrock, Laurence Likforman-Sulem, Mohamed Chetouani, Gualtiero Volpe, and Giovanna Varni. Game-on: A multimodal dataset for cohesion and group analysis. IEEE Access, 8:124185–124203, 2020."
2203.02480,"data, data https",93,,,"capabilities. Guo et al. (2021) and Katircioglu et al. (2021) incorporated motion attention to propagate observed motions to the future, theoretically even when the motion has not been seen in training time. However, both considered small histories of 2 seconds, which only favors the propagation of short repetitive motion. We are not aware of methods that consider much longer historical data, or that learn in an online and adaptive fashion the unique characteristics of each person’s behavior."
2203.02480,"data, database",68,,,"Ellen Douglas-Cowie, Roddy Cowie, Ian Sneddon, Cate Cox, Orla Lowry, Margaret McRorie, Jean-Claude Martin, Laurence Devillers, Sarkis Abrilian, Anton Batliner, Noam Amir, and Kostas Karpouzis. The humaine database: Addressing the collection and annotation of naturalistic and induced emotional data. In International conference on aﬀective computing and intelligent interaction, pages 488–500, 2007."
2203.02480,"data, dataset",35,,,"S´everin Lemaignan, Charlotte Edmunds, Emmanuel Senft, and Tony Belpaeme. The pinsoro dataset: Supporting the data-driven study of child-child and child-robot social dynamics. PLoS ONE, 13, 2018."
2203.02480,"data, dataset",47,,,"Shahid Nawaz Khan, Maitree Leekha, Jainendra Shukla, and Rajiv Ratn Shah. Vyaktitv: A multimodal peer-to-peer hindi conversations based dataset for personality assessment. 2020 IEEE Sixth International Conference on Multimedia Big Data (BigMM), pages 103– 111, 2020."
2203.02480,"data, dataset",53,,,"Cheul Young Park, Narae Cha, Soowon Kang, Auk Kim, Ahsan H. Khandoker, Leontios J. Hadjileontiadis, Alice H. Oh, Yong Jeong, and Uichin Lee. K-emocon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations. Scientiﬁc Data, 7, 2020."
2203.02480,"data, dataset",254,,,"The forecasting of low-level representations like landmarks or facial action units has been recently tackled with deep learning methods such as recurrent neural networks, graph neural networks, and transformers. The usage of such deep and data-hungry models has been encouraged by the recent availability of large-scale multi-view datasets, often annotated in a semi-automatic way (Joo et al., 2019b; Palmero et al., 2022). The increasing accuracy of monocular and multi-view automated methods for face, pose, and hands estimation has contributed in reducing the annotation eﬀort. Still, the largest available datasets that provide thousands of hours of audiovisual material and feature the widest spectrum of behaviors do not provide such annotations (Carreira et al., 2019; Zhao et al., 2019; Monfort et al., 2020; Grauman et al., 2021). In contrast, the automated methods for high-level representations recognition such as feedback responses or atomic action labels are not accurate enough to signiﬁcantly help in their annotation procedures. Consequently, such annotations are scarce, and are only available for small datasets, as shown in our survey. Accordingly, recent works have opted for classic methods such as SVM, AdaBoost, and simple recurrent neural networks, which have traditionally worked fairly well with small datasets. We expect future work on high-level behavior forecasting to also explore semi- and weakly-supervised approaches (Jain and Leekha, 2021)."
2203.02480,"data, dataset",285,,,"that tried to mitigate this issue when predicting head nodding. The data augmentation focused on the frequency acoustic features and consisted in warping them over time, masking blocks of utterances, and masking blocks of consecutive frequency channels. A similar technique was explored for the head pose, which was also warped in space and time, and masked over time. The experiments showed important improvements when forecasting head nodding with the combination of these strategies and an LSTM. Another big challenge is the extremely time-consuming annotation of social datasets. In the backchannel scenario, the highly multimodal nature of an interaction requires the annotator to pay attention to the audio, speech, and visual content before making a decision. This process is tedious and prone to errors. Very recently, Jain and Leekha (2021) proposed a semi-supervised method for identiﬁcation of listener backchannels that was able to detect up to the 90% of the backchannels with only a small subset of labeled data (25%). More importantly, it identiﬁed the type of the signals associated around 85% of the times. The authors showed that models trained on such noisy labels were able to keep a 93% and a 96% of the performance with respect to those trained with the cleaned annotations for the tasks of backchannel opportunity prediction and signal classiﬁcation, respectively. Their general methodology can be adapted for other conversational datasets. Although its validation in other datasets is still pending, it represents an important ﬁrst step to speed-up the annotation processes and reduce the workforce that they require."
2203.02480,"data, dataset",339,,,"The most frequent low-level annotations that the datasets provide are the participants’ body poses and facial expressions (Douglas-Cowie et al., 2007; Rehg et al., 2013; Bilakhia et al., 2015; Vandeventer et al., 2015; Naim et al., 2015; Edwards et al., 2016; Cafaro et al., 2017; Feng et al., 2017; Georgakis et al., 2017; Paggio and Navarretta, 2017; Bozkurt et al., 2017; Andriluka et al., 2018; von Marcard et al., 2018; Mehta et al., 2018; Lemaignan et al., 2018; Joo et al., 2019b; Kossaiﬁ et al., 2019; Schiphorst et al., 2020; Doyran et al., 2021; Palmero et al., 2022). Given their annotation complexity, they are usually automatically retrieved with tools like OpenPose (Cao et al., 2019), and manually ﬁxed or discarded. Others use more complex retrieval systems like motion capture, or mocap (Edlund et al., 2010; Maman et al., 2020; Yang et al., 2021). However, the characteristics of the mocap recording setup and the special suit that participants wear could unintentionally bias the elicited behaviors during the interaction. Finally, the annotation of high-level social signals is often led by the needs of the study for which the dataset was collected. Indeed, some of the datasets have been complementary annotated and added in posterior studies. As a result, most common high-level labels consist of elicited emotions (McCowan et al., 2005; Douglas-Cowie et al., 2007; van Son et al., 2008; McKeown et al., 2010; Naim et al., 2015; Vandeventer et al., 2015; Chou et al., 2017; Paggio and Navarretta, 2017; Maman et al., 2020; Doyran et al.,"
2203.02480,"data, dataset",352,,,"Regarding the content released, although originally available, some of them did not release the audio of the videos showcased due to privacy concerns. This is especially frequent for egocentric videos, as the unconstrained recording of the interactions obstructs the collection of consent forms. Other common content typologies consist of psychological data (e.g., personality questionnaires), biosignals monitoring (e.g., heart rate, electrocardiogram, electroencephalograms) and transcriptions. The latter is considerably less frequent due to its tedious manual annotation process (McCowan et al., 2005; Douglas-Cowie et al., 2007; McKeown et al., 2010; L¨ucking et al., 2012; Vella and Paggio, 2013; Vandeventer et al., 2015; Naim et al., 2015; Chou et al., 2017; Paggio and Navarretta, 2017; Cafaro et al., 2017; Joo et al., 2019b; Kossaiﬁ et al., 2019; Chen et al., 2020; Khan et al., 2020; Palmero et al., 2022). The most frequent low-level annotations that the datasets provide are the participants’ body poses and facial expressions (Douglas-Cowie et al., 2007; Rehg et al., 2013; Bilakhia et al., 2015; Vandeventer et al., 2015; Naim et al., 2015; Edwards et al., 2016; Cafaro et al., 2017; Feng et al., 2017; Georgakis et al., 2017; Paggio and Navarretta, 2017; Bozkurt et al., 2017; Andriluka et al., 2018; von Marcard et al., 2018; Mehta et al., 2018; Lemaignan et al., 2018; Joo et al., 2019b; Kossaiﬁ et al., 2019; Schiphorst et al., 2020; Doyran et al., 2021; Palmero et al., 2022). Given their annotation complexity, they are usually automatically retrieved with tools like OpenPose (Cao et al., 2019), and manually ﬁxed or discarded."
2203.02480,"data, dataset provided",343,,,"We have discussed many applications for good where non-verbal social behavior forecasting might be valuable. Personalized pedagogical agents (Davis, 2018) that maximize learner’s attention and learning, empathetic assistive robots for hospital patients or dependent people (Andrist et al., 2015; Esterwood and Robert, 2021), and collaborative robots for industrial processes or even surgeries (Sexton et al., 2018) are few examples. However, each new technology comes with its own pitfalls and limitations. In fact, these algorithms may unintentionally hold important biases that lead to unfairness in the task being performed. For example, the implementation of behavior forecasting algorithms in security borders or migration controls might lead to undesired outcomes (McKendrick, 2019) interfering with human rights (Akhmetova and Harris, 2021). Furthermore, the interacting user should always be aware of the presence of such forecasting systems, the possible manipulations or persuasion techniques attached, and their ultimate goal. Unfortunately, providing the user with these descriptions is not always easy because most of the times such systems are neither transparent nor explainable. Therefore, the incorporation of speciﬁc techniques to promote such interpretability is of utmost importance in order to build trust with the user. On the other side, it is also important to consider the potential vulnerabilities that such systems may have and how users might exploit them driven by unethical purposes. This is especially important for assistive or collaborative robots, which often involve very sensitive scenarios. Finally, although data protection regulations vary across countries (Guzzo et al., 2015), data privacy and data protection must ensure informational self-determination and consensual use of the information that can be extracted with the methods presented herein. In this sense, frameworks such as the EU General Data Protection Regulation (GDPR2) provide excellent safeguards for establishing ethical borders that should not be crossed."
2203.02480,database,24,,,"4d cardiﬀ conversation database (4d ccdb): a 4d database of natural, dyadic conversations. Auditory-Visual Speech Processing, 2015."
2203.02480,database,31,,,"Christos Georgakis, Yannis Panagakis, Stefanos Zafeiriou, and Maja Pantic. The conﬂict escalation resolution (confer) database. Image and Vision Computing, 65:37–48, 2017."
2203.02480,database,33,,,"Sanjay Bilakhia, Stavros Petridis, Anton Nijholt, and Maja Pantic. The mahnob mimicry database: A database of naturalistic human interactions. Pattern Recognition Letters, 66:52–61, 2015."
2203.02480,database,37,,,"Jainendra Shukla, Miguel Barreda- ´Angeles, Joan Oliver, and Domenec Puig. Muderi: Multimodal database for emotion recognition among intellectually disabled individuals. In The Eight International Conference on Social Robotics, 2016."
2203.02480,database,41,,,"Elif Bozkurt, Hossein Khaki, Sinan Ke¸ceci, Bekir Berker Turker, Y¨ucel Yemez, and Engin Erzin. The jestkod database: an aﬀective multimodal database of dyadic interactions. Language Resources and Evaluation, 51:857–872, 2017."
2203.02480,database,51,,,"Angelo Cafaro, Johannes Wagner, Tobias Baur, Soumia Dermouche, Mercedes Torres Torres, Catherine Pelachaud, Elisabeth Andr´e, and Michel F. Valstar. The noxi database: multimodal recordings of mediated novice-expert interactions. Proceedings of the 19th ACM International Conference on Multimodal Interaction, 2017."
2203.02480,database,52,,,"Laura Schiphorst, Metehan Doyran, Sabine Molenaar, A. A. Salah, and Sjaak Brinkkemper. Video2report: A video database for automatic reporting of medical consultancy sessions. 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020), pages 552–556, 2020."
2203.02480,database,66,,,"Jean Kossaiﬁ, Robert Walecki, Yannis Panagakis, Jie Shen, Maximilian Schmitt, Fabien Ringeval, Jing Han, Vedhas Pandit, Antoine Toisoul, Bj¨orn Schuller, et al. Sewa db: A rich database for audio-visual emotion and sentiment research in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(3):1022–1040, 2019."
2203.02480,dataset,1,,,Dataset
2203.02480,dataset,3,,,3. Datasets
2203.02480,dataset,11,,,"kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019."
2203.02480,dataset,12,,,Figure 3: Classiﬁcation of audiovisual datasets featuring non-acted social interactions.
2203.02480,dataset,16,,,Figure 4: Samples representative of the types of dataset scenarios included in this survey.
2203.02480,dataset,28,,,"We summarize and compare all datasets reviewed in Table 3. They appear classiﬁed into ﬁrst-person (egocentric), third-person (mid-distance camera), and computer-mediated"
2203.02480,dataset,32,,,"Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012."
2203.02480,dataset,33,,,"David A. Salter, Amir Tamrakar, Behjat Siddiquie, Mohamed R. Amer, Ajay Divakaran, Brian Lande, and Darius Mehri. The tower game dataset: A multimodal dataset for"
2203.02480,dataset,33,,,"Michael Edwards, Jingjing Deng, and Xianghua Xie. From pose to activity: Surveying datasets and introducing converse. Computer Vision and Image Understanding, 144: 73–105, 2016."
2203.02480,dataset,33,,,"Oya Celiktutan, Efstratios Skordos, and Hatice Gunes. Multimodal human-human-robot interactions (mhhri) dataset for studying personality and engagement. IEEE Transactions on Aﬀective Computing, 10:484–497, 2019."
2203.02480,dataset,39,,,"Girmaw Abebe, Andreu Catal`a, and Andrea Cavallaro. A ﬁrst-person vision dataset of oﬃce activities. In IAPR Workshop on Multimodal Pattern Recognition of Social Signals in Human-Computer Interaction, pages 27–37, 2018."
2203.02480,dataset,41,,,"Fangkai Yang, Yuan Gao, Ruiyang Ma, Sahba Zojaji, Ginevra Castellano, and Christopher E. Peters. A dataset of human and robot approach behaviors into small free-standing conversational groups. PLoS ONE, 16, 2021."
2203.02480,dataset,43,,,"Hang Zhao, Zhicheng Yan, Heng Wang, and Lorenzo Torresani. Hacs: Human action clips and segments dataset for recognition and temporal localization. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8667–8677, 2019."
2203.02480,dataset,43,,,"Huili Chen, Yue Zhang, Felix Weninger, Rosalind Picard, Cynthia Breazeal, and Hae Won Park. Dyadic speech-based aﬀect recognition using dami-p2c parent-child multimodal interaction dataset. Proceedings of the 2020 International Conference on Multimodal Interaction, 2020."
2203.02480,dataset,47,,,"A recurrent problem observed in our survey and one of the main challenges of non-verbal social behavior forecasting is the lack of large annotated datasets. In order to provide the reader with an overview of the currently available datasets, we brieﬂy go through them in"
2203.02480,dataset,50,,,"Xavier Alameda-Pineda, Jacopo Staiano, Subramanian Ramanathan, Ligia Maria Batrinca, Elisa Ricci, B. Lepri, Oswald Lanz, and N. Sebe. Salsa: A novel dataset for multimodal group behavior analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38:1707–1720, 2016."
2203.02480,dataset,55,,,"Roberto Mart´ın-Mart´ın, Mihir Patel, Hamid Rezatoﬁghi, Abhijeet Shenoi, JunYoung Gwak, Eric Frankel, Amir Sadeghian, and Silvio Savarese. Jrdb: A dataset and benchmark of egocentric robot visual perception of humans in built environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, PP, 2021."
2203.02480,dataset,57,,,"Gilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori, Siddhartha S. Srinivasa, and Yaser Sheikh. Talking with hands 16.2m: A large-scale dataset of synchronized body-ﬁnger motion and audio for conversational motion analysis and synthesis. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 763–772, 2019."
2203.02480,dataset,57,,,"Laura Cabrera-Quiros, Andrew Demetriou, Ekin Gedik, Leander van der Meij, and Hayley Hung. The matchnmingle dataset: a novel multi-sensor resource for the analysis of social interactions and group dynamics in-the-wild during free-standing conversations and speed dates. IEEE Transactions on Aﬀective Computing, 12(1):113–130, 2018."
2203.02480,dataset,61,,,"Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa M. Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, and Aude Oliva. Moments in time dataset: One million videos for event understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:502–508, 2020."
2203.02480,dataset,61,,,"Metehan Doyran, Arjan Schimmel, Pinar Baki, K¨ubra Ergin, Batikan T¨urkmen, Almila Akdag Salah, Sander Bakkes, Heysem Kaya, Ronald Poppe, and A. A. Salah. Mumbai: multi-person, multimodal board game aﬀect and interaction analysis dataset. Journal on Multimodal User Interfaces, 15(4):373–391, 2021."
2203.02480,dataset,66,,,"behavior forecasting. Interestingly, their best results were obtained by only leveraging the prediction of one part of the body at a time (face, pose, and hands). They hypothesized that it could be due to the signiﬁcant behavioral diﬀerences among parts of the body. They also underlined the need of larger datasets to model such high dimensional problems."
2203.02480,dataset,108,,,"Cristina Palmero, German Barquero, Julio C. S. Jacques Junior, Albert Clap´es, Johnny N´u˜nez, David Curto, Sorina Smeureanu, Javier Selva, Zejian Zhang, David Saeteros, David Gallardo-Pujol, Georgina Guilera, David Leiva, Feng Han, Xiaoxue Feng, Jennifer He, Wei-Wei Tu, Thomas B. Moeslund, Isabelle Guyon, and Sergio Escalera. Chalearn LAP self-reported personality recognition and social behavior forecasting challenges applied on a dyadic interaction scenario: Dataset, design, and results. In Understanding Social Behavior in Dyadic and Small Group Interactions, Proceedings of Machine Learning Research, 2022."
2203.02480,dataset,156,,,"In this survey, we provided an overview of the recent approaches proposed for non-verbal social behavior forecasting. We formulated a taxonomy that comprises and uniﬁes recent (since 2017) attempts of forecasting low- or high-level representations of non-verbal social behavior up to date. By means of this taxonomy, we identiﬁed and described the main challenges of the problem, and analyzed how the recent literature has addressed them from both the sociological and the computer vision perspectives. We also presented all audiovisual datasets related to social behavior publicly released up to date in a summarized, structured, and friendly way. Finally, we described the most commonly used metrics, and the controversy that they often raise. We hope this survey can help bring the human motion prediction and the social signal forecasting worlds together in order to jointly tackle the main challenges of this ﬁeld."
2203.02480,dataset,179,,,"With regards to the setting, participants might interact while standing or while seated. Some datasets may include videos with both conﬁgurations (e.g., several independent interactive groups). The most frequent scenario consists in dyadic interactions due to their special interest for human-robot interaction and human behavior understanding, and their lower behavioral complexity when compared to bigger social groups. Triadic (SanchezCortes et al., 2012; Mehta et al., 2018; Celiktutan et al., 2019; Joo et al., 2019b; Yang et al., 2021) and bigger social gatherings (McCowan et al., 2005; Hung and Chittaranjan, 2010; Beyan et al., 2016; Joo et al., 2019b; Yang et al., 2021), which are referred to as groups, are less commonly showcased scenarios. Datasets featuring several simultaneous groups of interactions, as long as they showed focused interactions, are also included (Alameda-Pineda et al., 2016; Cabrera-Quiros et al., 2018)."
2203.02480,dataset,213,,,"Abstract Non-verbal social human behavior forecasting has increasingly attracted the interest of the research community in recent years. Its direct applications to human-robot interaction and socially-aware human motion generation make it a very attractive ﬁeld. In this survey, we deﬁne the behavior forecasting problem for multiple interactive agents in a generic way that aims at unifying the ﬁelds of social signals prediction and human motion forecasting, traditionally separated. We hold that both problem formulations refer to the same conceptual problem, and identify many shared fundamental challenges: future stochasticity, context awareness, history exploitation, etc. We also propose a taxonomy that comprises methods published in the last 5 years in a very informative way and describes the current main concerns of the community with regard to this problem. In order to promote further research on this ﬁeld, we also provide a summarized and friendly overview of audiovisual datasets featuring non-acted social interactions. Finally, we describe the most common metrics used in this task and their particular issues. Keywords: Behavior forecasting, Human motion prediction, Social signal prediction, Social robots, Socially interactive agents, Dyadic interactions, Triadic interactions, Multiparty interactions, Backchanneling, Engagement"
2203.02480,dataset,220,,,"Whole body. Few works have attempted to jointly model the behavior of body and face (Grafsgaard et al., 2018; Joo et al., 2019a). However, they do not fall within the scope of this work as all of them used future information of either another modality (e.g., text, speech) or the interactant. Very recently, a behavior forecasting competition leveraging whole-body landmarks was held within the ChaLearn LAP DYAD@ICCV’21 workshop (Palmero et al., 2022). The common trend observed during the competition coincides with the classic path for body pose forecasting: recurrent encoder-decoder architectures with adversarial losses that ensure realism. Although none of the teams beat the competition baseline, the organizers identiﬁed some of the main challenges. The usage of noisy labels, the highly stochastic nature of the hands, or the mostly static nature of the dataset (seated dyadic conversations) are some examples. Motivated by this workshop’s benchmark, Barquero et al. (2022) proposed several state-of-the-art methodologies that outperformed the competition’s baseline. Consistently to the recent ﬁndings in body pose forecasting, they also found that Transformer-like architectures provided the best results in whole-body"
2203.02480,dataset,223,,,"how past works have addressed them (Sections 2.1 to 2.5). Then, we thoroughly review the state-of-the-art methodologies proposed for non-verbal social behavior forecasting. In particular, we split the survey into methods predicting low-level (e.g., landmarks, facial action units) and high-level (social cues and signals) representations of non-verbal behavior, in Sections 2.6.1 and 2.6.2, respectively. Then, in Section 3, an extensive collection of datasets featuring audiovisual social interactions is presented. Datasets are classiﬁed according to the interaction scenario (dyadic, triadic, group, or several groups), task (conversational, collaborative, or competitive), and setting (participants are standing or seated). We also provide a summary table that allows the reader to easily compare the low- and high-level behavioral annotations provided as part of each dataset. Section 4 presents the most popular metrics for assessing the accuracy, diversity, and realism of the predicted behavior. In Section 5, we provide a discussion on general trends and current challenges, as well as possible future research directions for non-verbal social behavior forecasting. Finally, in Section 6, we review the ethical concerns regarding non-verbal social behavior forecasting and its real-world applications."
2203.02480,dataset,241,,,"(e.g., video-conferences) recording setups due to their signiﬁcant diﬀerences regarding their possible applications. Datasets recorded from very distant third-person views, or egocentric views where the camera is carried by a non-interacting agent were discarded due to their poor social behavior content. Usually, third-view datasets consist of structured interactions where participants need to follow basic directives which favor spontaneous and ﬂuent interactions. Despite the fact that conversations are the most common interaction structure, there are datasets which aim at fostering speciﬁc social signals like leadership, competitiveness, empathy, or aﬀect, and therefore engage the participants in competitive/cooperative scenarios (Hung and Chittaranjan, 2010; Sanchez-Cortes et al., 2012; Rehg et al., 2013; Ringeval et al., 2013; Vella and Paggio, 2013; Bambach et al., 2015; Salter et al., 2015; Edwards et al., 2016; Beyan et al., 2016; Georgakis et al., 2017; Yang et al., 2021; Doyran et al., 2021; Palmero et al., 2022). Other datasets, instead, record in-the-wild interactions during the so-called cocktail parties (Alameda-Pineda et al., 2016; Cabrera-Quiros et al., 2018) and represent very interesting benchmarks to study group dynamics. Thanks to the camera portability during the collection, egocentric datasets can record social behavior in"
2203.02480,dataset,242,,,"However, this simpliﬁcation comes at a high cost: the predictive model is penalized for generating plausible and realistic behaviors which do not match the ones observed in the dataset. To alleviate this, many works reduce the dimensionality of the forecasting objective (e.g., action labels) (Sanghvi et al., 2020; Airale et al., 2021), or provide extensive contextual information in order to narrow the future space and therefore reduce its stochasticity (Corona et al., 2020b; Adeli et al., 2020, 2021). Still, some works forecasting low-level behavioral representations (e.g., landmarks) report a strong regression-to-the-mean eﬀect in the predictions (Barquero et al., 2022). Some works tried to tackle this problem in several contexts. For example, Feng et al. (2017) proposed building a speciﬁc high-frequencies predictor which made the generated facial expressions more realistic. In the context of social signals forecasting, Raman et al. (2021) reasoned that such eﬀect was linked to the availability of similar future signals triggered at diﬀerent future points. To mitigate it, they proposed to inject the time oﬀset at which social signals triggered into the past encoding. In general though, deterministic works complement the quantitative evaluation with qualitative visualizations that help assess the realism and smoothness of the predictions."
2203.02480,dataset,250,,,"Finally, we want to raise awareness of what we consider one of the main bottlenecks of behavior forecasting: the evaluation metrics. An evaluation metric must always illustrate how well a method does for the target task. While this sentence may seem trivial when thinking of classic classiﬁcation or regression tasks, it is an important source of controversy in the behavior forecasting ﬁeld. For instance, the distance between the generated and the ground truth futures does not describe the coherence of the pose in all future steps, neither the realism of the movements. In fact, it does not even guarantee that a method with low error performs poorly, as the predictions may simply not match the ground truth, which is a sample of the multiple and equally plausible set of futures. Although one would conclude that a proper evaluation always contains a qualitative analysis, multiple behavioral dimensions may escape from human raters and therefore bias it. For instance, it is not trivial to build a qualitative analysis that also assesses the coherence of the predicted behavior with respect to the behavioral patterns speciﬁc to the subject, the context, or even the events from the mid- to long-term past. We hope that the recent appearance of behavior forecasting benchmarks and speciﬁc datasets will encourage the community to ﬁnd better-suited metrics and evaluation protocols that will boost the research progress in this ﬁeld."
2203.02480,dataset,319,,,"Sanghvi et al. (2020) proposed a model that used the visual features (e.g., location, gaze orientation) from all interactants to predict the target’s future actions (e.g., speak, listen, leave). To do so, a GRU encoded the features of each interactant concatenated with those from the target person. Then, similarly to the attention-based methods reviewed in Section 2.6.1, their method applied attention across all social encodings, which were then fed to a pooling layer so that an arbitrary number of individuals could be handled. Finally, two dense layers converted the output of the pooling layer to the probability distribution deﬁning the next conversational action. They reported that, thanks to the social attention, group annotations were not required. Airale et al. (2021) also deﬁned the non-verbal behavior forecasting as a discrete multi-sequence generation problem. Their methodology was radically diﬀerent though: a GAN conditioned on the observed interaction, which was encoded with an LSTM. In the generative stage, a socially aware hidden state was computed at each timestep. The strategy consisted in using a pooling module to update the hidden states of each person’s LSTM decoder and convert them to new socially aware LSTM hidden states for the next decoding step. As a result, the decoded actions were obtained in a coherent way with respect to the actions generated for all surrounding subjects. Additionally, they presented a two-streams novel adversarial discriminator. The ﬁrst branch corresponded to the classic one, so it favored the realism of individual action sequences disregarding any contextual information. The second one combined the predicted actions sequences with a pooling module of all individuals in the scene to ensure that the generated interactions as a"
2203.02480,dataset,333,,,"Pose. The very ﬁrst attempt to forecast non-verbal body behavior in social interactions was carried out for robot learning of social aﬀordance (Shu et al., 2016). In their work, Shu et al. presented a Markov Chain Monte Carlo (MCMC) based algorithm that iteratively discovered latent subevents, important joints, and their functional grouping. Their method also considered past trajectories of objects to successfully predict the agent’s behavior while performing handshakes, high-ﬁves, or object handovers. Favored by the appearance of new and bigger datasets featuring social interactions (von Marcard et al., 2018; Andriluka et al., 2018; Joo et al., 2019a), Recurrent Neural Networks (RNNs) quickly became the standard in human motion forecasting (Martinez et al., 2017; Hua et al., 2019; Honda et al., 2020). However, Honda et al. (2020) observed that recurrent models used for single human motion forecasting are not suitable for highly interactive situations like fencing. In their work, they presented a general framework that provided single human motion forecasting methods with the ability to model interpersonal dynamics. To do so, both encoder and decoder LSTMs received as input the previous skeleton (either observed or predicted) concatenated with the hidden state of the opponent in the previous timestep. As a result, the simultaneous behavior forecasting of both players encoded the interpersonal dynamics of the interaction, making the predicted movements more accurate and coherent in the context of competitive fencing. While previous approaches focused on scenarios strongly driven by interpersonal dynamics, Ahuja et al. (2019) emphasized the imbalance between intrapersonal and interpersonal dynamics in dyadic conversations, with considerably less instances from the later. They warned that, in such scenarios, interpersonal dynamics could end up being ignored."
2203.02480,dataset,335,,,"Very recently, several approaches introduced Transformer-like architectures (Vaswani et al., 2017) which outperformed previous RNN-based ones (Yasar and Iqbal, 2021; Wang et al., 2021b; Guo et al., 2021; Katircioglu et al., 2021). Yasar and Iqbal (2021) proposed to encode individually the multiple agents’ joints positions, velocities, and accelerations. Then, cross-agent attention was applied among the latent space to generate socially aware representations, which followed two subsequent paths. First, these representations went through a two-streams adversarial discriminator that sampled discrete and continuous latent variables. The authors reported that such conﬁguration favored the latent space interpretability. Their analysis on such variables showed that their method eﬀectively captured the underlying dynamics of human motion. Finally, the socially aware latent representations underwent individual recurrent decoders that autoregressively predicted the future sequence of poses. The independent generation of poses represented its main limitation, as generated poses might not be socially coherent. Wang et al. (2021b) proposed to encode local- and global-range dependencies (intra- and inter-personal dependencies, respectively) with two specialized transformer encoders. The past motion of the person of interest was transformed by means of a Discrete Cosine Transform (DCT) (Ahmed et al., 1974), which was then fed to the local-range transformer performing self-attention. At the same time, the global-range transformer encoder applied self-attention across diﬀerent subjects and diﬀerent time steps. A spatial positional encoding was added to the global encodings to help the network cluster diﬀerent individuals in diﬀerent social interaction groups. Finally, the transformer decoder leveraged the last observed pose as the query, and the local- and global-range encodings as both keys and values in order to generate the whole predicted sequence at once, which was then fed to a linear and an Inverse DCT layer."
2203.02480,dataset,341,,,"However, Honda et al. (2020) observed that recurrent models used for single human motion forecasting are not suitable for highly interactive situations like fencing. In their work, they presented a general framework that provided single human motion forecasting methods with the ability to model interpersonal dynamics. To do so, both encoder and decoder LSTMs received as input the previous skeleton (either observed or predicted) concatenated with the hidden state of the opponent in the previous timestep. As a result, the simultaneous behavior forecasting of both players encoded the interpersonal dynamics of the interaction, making the predicted movements more accurate and coherent in the context of competitive fencing. While previous approaches focused on scenarios strongly driven by interpersonal dynamics, Ahuja et al. (2019) emphasized the imbalance between intrapersonal and interpersonal dynamics in dyadic conversations, with considerably less instances from the later. They warned that, in such scenarios, interpersonal dynamics could end up being ignored. To mitigate this issue, they proposed a dyadic residual-attention model (DRAM) that smoothly transitioned between monadic- and dyadic-driven behavior generation. Results showed that their model successfully identiﬁed non-verbal social cues like head nod mirroring or torso pose switching and generated proper reactions. Hua et al. (2019) also supported the use of the partner’s cues but restricted it to the modeling of the listener’s behavior. In their approach, they presented a human-robot body gesture interaction system built similarly to the system of Chen et al. (2019) for facial gestures synthesis. Similarly, they also leveraged two specialized methods for the speaking (cospeech generator) and listening (behavior forecasting) phases of the interaction. In contrast to Chen et al. (2019) though, they incorporated the speaker’s speech transcription as an extra predictive feature for the listener’s behavior."
2203.02480,dataset,343,,,"Clearly, the prediction of backchannel responses has attracted a lot of attention. However, they are not the only type of non-verbal behavioral social cues. In a more general framework, few works have tried to predict the future development of an interaction leveraging low-level action labels (e.g., speaking, idling, laughing), motivated by the recent release of annotated audiovisual datasets (Alameda-Pineda et al., 2016; Cabrera-Quiros et al., 2018). Sanghvi et al. (2020) proposed a model that used the visual features (e.g., location, gaze orientation) from all interactants to predict the target’s future actions (e.g., speak, listen, leave). To do so, a GRU encoded the features of each interactant concatenated with those from the target person. Then, similarly to the attention-based methods reviewed in Section 2.6.1, their method applied attention across all social encodings, which were then fed to a pooling layer so that an arbitrary number of individuals could be handled. Finally, two dense layers converted the output of the pooling layer to the probability distribution deﬁning the next conversational action. They reported that, thanks to the social attention, group annotations were not required. Airale et al. (2021) also deﬁned the non-verbal behavior forecasting as a discrete multi-sequence generation problem. Their methodology was radically diﬀerent though: a GAN conditioned on the observed interaction, which was encoded with an LSTM. In the generative stage, a socially aware hidden state was computed at each timestep. The strategy consisted in using a pooling module to update the hidden states of each person’s LSTM decoder and convert them to new socially aware LSTM hidden states for the next decoding step. As a result, the decoded actions were obtained in a coherent way with respect to the actions generated for all surrounding subjects."
2203.02480,dataset,343,,,"First, these representations went through a two-streams adversarial discriminator that sampled discrete and continuous latent variables. The authors reported that such conﬁguration favored the latent space interpretability. Their analysis on such variables showed that their method eﬀectively captured the underlying dynamics of human motion. Finally, the socially aware latent representations underwent individual recurrent decoders that autoregressively predicted the future sequence of poses. The independent generation of poses represented its main limitation, as generated poses might not be socially coherent. Wang et al. (2021b) proposed to encode local- and global-range dependencies (intra- and inter-personal dependencies, respectively) with two specialized transformer encoders. The past motion of the person of interest was transformed by means of a Discrete Cosine Transform (DCT) (Ahmed et al., 1974), which was then fed to the local-range transformer performing self-attention. At the same time, the global-range transformer encoder applied self-attention across diﬀerent subjects and diﬀerent time steps. A spatial positional encoding was added to the global encodings to help the network cluster diﬀerent individuals in diﬀerent social interaction groups. Finally, the transformer decoder leveraged the last observed pose as the query, and the local- and global-range encodings as both keys and values in order to generate the whole predicted sequence at once, which was then fed to a linear and an Inverse DCT layer. Additionally, an adversarial loss was used to ensure the realism of the generated behavior. The authors argued that, by predicting the whole motion sequence at once, they prevented generating freezing motion. They reported state-of-the-art and qualitative impressive results in various datasets with several prediction window lengths (up to 3 seconds) and synthetically generated crowded scenarios (up to 15 people). Guo et al. (2021) provided the motion attention concept originally proposed by Mao et al. (2020) for single human motion prediction with a"
2203.02480,"publicly available, dataset",83,,,"Table 3: Datasets that feature audiovisual non-acted social interactions and are publicly available. They are presented grouped by recording setup (third-person, egocentric, and computer-mediated). Abbreviations: Conv. conversational; Collab., collaborative, Comp.; competitive, A, audiovisual; P, psychological; B, biosignals; T, transcriptions; Q, questionnaires; IMU, Inertial measurement unit; ?, value not found; *, robot interaction."
2203.02480,"publicly available, dataset",86,,,"Table 3: (Continuation) Datasets that feature audiovisual non-acted social interactions and are publicly available. They are presented grouped by recording setup (thirdperson, egocentric, and computer-mediated). Abbreviations: Conv. conversational; Collab., collaborative, Comp.; competitive, A, audiovisual; P, psychological; B, biosignals; T, transcriptions; Q, questionnaires; IMU, Inertial measurement unit; ?, value not found; *, robot interaction."
2203.02480,"publicly available, dataset",87,,,"this section. Note that we restrict the survey to publicly available datasets that feature audiovisual non-acted social interactions. The taxonomy that we present, see Figure 3, groups them into scenarios (dyadic, triadic, group, and >1 groups), tasks (conversational, collaborative, and competitive), and settings (standing, or seating) that elicit diﬀerent behavioral patterns. Figure 4 shows illustrative examples of the scenarios considered in this part of the survey."
2203.02480,"publicly available, dataset",209,,,"In the past years, research on non-verbal social behavior forecasting has followed distinct paths for social signal prediction and computer vision ﬁelds, although they share most of their fundamental concerns. For example, the human motion forecasting ﬁeld does not usually refer to any social signal forecasting work (Mourot et al., 2021), even though some of them predict visual social cues, or action labels. And vice versa. This survey wants to unify non-verbal social behavior forecasting for both ﬁelds, describe its main challenges, and analyze how they are currently being addressed. To do so, we establish a taxonomy which comprises all methodologies applied to multi-agent (human-human, or human-robot/virtual agent) scenarios and presented in the most recent years (2017-2021). In particular, we focus on works that exploit at least one visual cue. We also engage in a discussion where we foresee some methodological gaps that might become future trends in this ﬁeld. Besides, we summarize the publicly available datasets of social interactions into a comprehensive and friendly survey. Finally, we present and discuss on the usual evaluation metrics for non-verbal social behavior forecasting."
2203.02776,code,144,,,"During the ﬁrst phase DNF2LTL generates an initial procedural description in four steps. In the ﬁrst step, the algorithm extracts potential subroutines from the inputted DNF formula. In the second step, the algorithm determines the order in which those subroutines should be performed. In the third step, the algorithm computes the logical conditions for transitioning from each step to the next. Finally, in the fourth step, our method connects the subroutines with the appropriate conditions into a complete procedural description and outputs the result. Algorithm 1 presents a pseudo code that implements the ﬁrst phase of DNF2LTL and the following paragraph provides a technical description of each of these four steps. Readers who are primarily interested in the big picture and the application to boosting human planning can skip these technical details."
2203.02776,code,192,,,"Participants were randomly assigned to a static descriptions or a procedural instructions condition. Both conditions started with an instructions block, in which the task was introduced and motivated as a pass-code test required to enter an extraterrestrial planet that can only be accessed by following speciﬁc instructions. This was followed by an attention check consisting of 3 multiple-choice questions, a block explaining the procedure and the instructions and ﬁnal attention check consisting of 3 multiple-choice questions. After this, participants engaged in a single practice trial in which they were instructed to click locations marked with orange. They received feedback on the correctness of their clicks and when to end the trial. Lastly, there was a block of 10 test trials and a small demographic survey. The minimum time required to spent on a trial was ten seconds. Participants were informed that they would start with a bonus of $2 and lose 20 cents for each trial in which they made an inconsistent click or quit early. In addition, all participants received a base payment of $1."
2203.02776,data,227,,,"changed from true to false at the same moment when the value of cj changed from false to true. The transition graph is used to generate maximum length sequences of conjunctions ci1 ci2 . . . cin in order to capture the possibly fullest transition evidenced in the data. The last predicate in this sequence (i.e., cin ) either has no outgoing connections in the transition graph or connects to one of the cij s in which case the sequence ends with a special loop symbol that indicates which ij that is. The resulting maximum length sequences (with potential loop symbols at the end) are used to deﬁne equivalence classes for the trajectories. These equivalence classes represent potential dynamics of how the conjunctions change their truth values so that the full DNF formula was satisﬁed. To ﬁnd a small subset of equivalences classes that is suﬃcient to describe all of the trajectories, each trajectory, treated as a sequence of conjunctions of the DNF formula, is assigned to one equivalence class. Namely, for trajectory τ , the ﬁrst encountered equivalence class is chosen, for which its sequence contains a subsequence representing τ . For instance if τ is represented by sequence c1c3, it could be assigned to equivalence class c1c2c3c4LOOP c2."
2203.02776,"github, code available, code",26,,,Code availability (software application or custom code) The code for the algorithm introduced in the paper is available at https://github.com/RationalityEnhancement/ InterpretableStrategyDiscovery/tree/master/DNF2LTL.
2203.02776,"github, data, code, dataset",32,,,"Availability of data and material (data transparency) The datasets analyzed for this study, alongside the code for the analysis can be found at https://github. com/RationalityEnhancement/InterpretableStrategyDiscovery."
2203.09450,code,70,,,"We present the main experiment results in Tab. 1. The last two columns give the average TIL/CIL results of each system/row. For A-RPS, CCG, and Co2L, we copy the results from their original papers as their codes are not released to the public or the public code cannot run on our system. The rows are grouped by CIL and TIL methods."
2203.09450,"code, used dataset, dataset",47,,,"For the baselines, we use the best hyper-parameters reported in their original papers or in their code. If some hyperparameters are unknown, e.g., the baseline did not use a particular dataset, we search for the hyper-parameters as we do for CLOM."
2203.09450,data,26,,,"• CLOM needn’t to save any replay data. If some past data is saved for output calibration, it performs even better."
2203.09450,data,57,,,Contrastive Loss for Feature Learning. This is step 1 in Fig. 1(b). Supervised contrastive learning is used to try to repel data of different classes and align data of the same class more closely to make it easier to classify them. A key operation is data augmentation via transformations.
2203.09450,data,76,,,"C Details about Augmentations We follow (Chen et al. 2020; Tack et al. 2020) for the choice of data augmentations. We ﬁrst apply horizontal ﬂip, color change (color jitter and grayscale), and Inception crop (Szegedy et al. 2015), and then four rotations (0◦, 90◦, 180◦, and 270◦). The details about each augmentation are the following."
2203.09450,data,81,,,"After training the network f for the current task t, we freeze the model and use the saved data in M to ﬁnd the scaling and shifting parameters (σ, µ) ∈ Rt×Rt to calibrate the after-ensemble classiﬁcation output f (h(x, k)) (Eq. 8) (i.e., using σkf (h(x, k))+µk) for each task k by minimizing the cross-entropy loss,"
2203.09450,data,92,,,"Data Augmentation. For baselines, we use data augmentations used in their original papers. For CLOM, following (Chen et al. 2020; Tack et al. 2020), we use three initial augmentations (see Sec. 3) (i.e., horizontal ﬂip, color change (color jitter and grayscale), and Inception crop (Szegedy et al. 2015)) and four rotations (see Sec. 3). Speciﬁc details about these transformations are given in Appendix C."
2203.09450,data,98,,,"For other hyper-parameters in CLOM, we use 10% of training data as the validation data and select the set of hyperparameters that gives the highest CIL accuracy on the validation set. We train the output calibration parameters (σ, µ) for 160 iterations with learning rate 0.01 and batch size 32. The following are experiment speciﬁc hyper-parameters found with hyper-parameter search. • For MNIST-5T, batch size = 256, the hard attention regularization hyper-parameters are λ1 = 0.25, and λ2 = · · · = λ5 = 0.1."
2203.09450,data,98,,,"There are also many other approaches, e.g., a network of experts (Aljundi, Chakravarty, and Tuytelaars 2017) and generalized CL (Mi et al. 2020), etc. PASS (Zhu et al. 2021) uses data rotation and regularizes them. Co2L (Cha, Lee, and Shin 2021) is a replay method that uses contrastive loss on old samples. CLOM also uses rotation and constrative loss, but its CF handling is based on masks. None of the existing methods uses OOD detection."
2203.09450,data,99,,,"Learning the Classiﬁer. This is step 2 in Fig.1(b). Given the feature extractor h trained with the loss in Eq. 3, we freeze h and only ﬁne-tune the linear classiﬁer f , which is trained to predict the classes of task t and the augmented rotation classes. f maps the feature representation to the label space in R4|Ct|, where 4 is the number of rotation classes including the original data with 0◦ rotation and |Ct| is the number of original classes in task t. We minimize the cross-entropy loss,"
2203.09450,data,104,,,"As in existing works, we evaluate each method by two metrics: average classiﬁcation accuracy on all classes after training the last task, and average forgetting rate (Liu et al. 2020b), F t = 1 is the j’th task’s act−1 curacy of the network right after the j’th task is learned and At j is the accuracy of the network on the j’th task data after learning the last task t. We report the forgetting rate after the ﬁnal task t. Our results are averages of 5 random runs."
2203.09450,data,105,,,"CLOM without OOD detection. In this case, CLOM uses contrastive learning and data augmentation, but does not use the rotation classes in classiﬁcation. Note that the rotation classes are basically regarded as OOD data in training and for OOD detection in testing. CLOM (w/o OOD) in Tab. 2 represents this CLOM variant. We see that CLOM (w/o OOD) is much weaker than the full CLOM. This indicates that the improved results of CLOM over baselines are not only due to contrastive learning and data augmentation but also signiﬁcantly due to OOD detection."
2203.09450,data,120,,,"3http://yann.lecun.com/exdb/mnist/ 4https://www.cs.toronto.edu/ kriz/cifar.html 5https://www.cs.toronto.edu/ kriz/cifar.html 6http://tiny-imagenet.herokuapp.com 7iTAML (Rajasegaran et al. 2020b) is not included as they require a batch of test data from the same task to predict the task-id. When each batch has only one test sample, which is our setting, it is very weak. For example, iTAML TIL/CIL accuracy is only 35.2%/33.5% on CIFAR100 10 tasks. Expert Gate (EG) (Aljundi, Chakravarty, and Tuytelaars 2017) is also weak. For example, its TIL/CIL accuracy is 87.2/43.2 on MNIST 5 tasks. Both iTAML and EG are much weaker than many baselines."
2203.09450,data,142,,,"For all experiments, we use 10% of training data as the validation set to grid-search for good hyperparameters. For minimizing the contrastive loss, we use LARS (You, Gitman, and Ginsburg 2017) for 700 epochs with initial learning rate 0.1. We linearly increase the learning rate by 0.1 per epoch for the ﬁrst 10 epochs until 1.0 and then decay it by cosine scheduler (Loshchilov and Hutter 2016) after 10 epochs without restart as in (Chen et al. 2020; Tack et al. 2020). For ﬁne-tuning the classiﬁer f , we use SGD for 100 epochs with learning rate 0.1 and reduce the learning rate by 0.1 at 60, 75, and 90 epochs. The full set of hyperparameters is given in Appendix D."
2203.09450,data,157,,,"Output Calibration with Memory. The proposed method can make incorrect CIL prediction even with a perfect OOD model (rejecting every test sample that does not belong any class of the task). This happens because the task models are trained independently, and the outputs of different tasks may have different magnitudes. We use output calibration to ensure that the outputs are of similar magnitudes to be comparable by using some saved examples in a memory M with limited budget. At each task k, we store a fraction of the validation data {(x, y)} into Mk for output calibration and update the memory M ← update(M, Mk) by maintaining an equal number of samples per class. We will detail the memory budget in Sec. 4. Basically, we save the same number as the existing replay-based TIL/CIL methods."
2203.09450,data,193,,,"Learning Each Task as OOD Detection We borrow the latest OOD ideas based on contrastive learning (Chen et al. 2020; He et al. 2020) and data augmentation due to their excellent performance (Tack et al. 2020). Since this section focuses on how to learn a single task based on OOD detection, we omit the task-id unless necessary. The OOD training process is similar to that of contrastive learning. It consists of two steps: 1) learning the feature representation by the composite g ◦ h, where h is a feature extractor and g is a projection to contrastive representation, and 2) learning a linear classiﬁer f mapping the feature representation of h to the label space. In the following, we describe the training process: contrastive learning for feature representation learning (1), and OOD classiﬁer building (2). We then explain how to make a prediction based on an ensemble method for both TIL and CIL settings, and how to further improve prediction using some saved past data."
2203.09450,data,194,,,"Effect of data augmentations. For data augmentation, we use three initial augmentations (i.e., horizontal ﬂip, color change (color jitter and grayscale), Inception crop (Szegedy et al. 2015)), which are commonly used in contrastive learning to build a single model. We additionally use rotation for OOD data in training. To evaluate the contribution of each augmentation when task models are trained sequentially, we train CLOM using one augmentation. We do not report their effects on forgetting as we experience rarely any forgetting (Fig. 2 and Tab. 3). Tab. 4 shows that the performance is lower when only a single augmentation is applied. When all augmentations are applied, the TIL/CIL accuracies are higher. The rotation always improves the result when it is combined with other augmentations. More importantly, when we use crop and rotation, we achieve higher CIL accuracy (79.4/60.3% for CIFAR10-5T/CIFAR100-10T) than we use all augmentations without rotation (74.0/50.2%). This shows the efﬁcacy of rotation in our system."
2203.09450,data,207,,,"2019) improves GEM’s efﬁciency. ADP decomposes parameters into shared and adaptive parts to construct an order robust TIL system. CCLL uses task-adaptive calibration on convolution layers. Orthog-Subspace learns each task in subspaces orthogonal to each other. HyperNet initializes task-speciﬁc parameters conditioned on task-id. PackNet, CPG and SupSup ﬁnd an isolated sub-network for each task and use it at inference. HAT and CAT protect previous tasks by masking the important parameters. Our CLOM also uses this general approach, but its model building for each task is based on OOD detection, which has not been used by existing TIL methods. It also performs well in the CIL setting (see Sec. 4). Related work on out-of-distribution (OOD) detection (also called open set detection) is also extensive. Excellent surveys include (Bulusu et al. 2020; Geng, Huang, and Chen 2020). The model building part of CLOM is inspired by the latest method in (Tack et al. 2020) based on data augmentation (He et al. 2020) and contrastive learning (Chen et al. 2020)."
2203.09450,data,216,,,"OOD detection is stated as follows (Bulusu et al. 2020): Given a training set of n classes, called the in-distribution (IND) training data, we want to build a model that can assign correct classes to IND test data and reject or detect OOD test data that do not belong to any of the n IND training classes. The OOD rejection capability makes a TIL model effective for CIL problem because during testing, if a test sample does not belong to any of the classes of a task, it will be rejected by the model of the task. Thus, only the task that the test sample belongs to will accept it and classify it to one of its classes. No task-id is needed. The OOD detection algorithm in CLOM is inspired by several recent advances in self-supervised learning, data augmentation (He et al. 2020), contrastive learning (Oord, Li, and Vinyals 2018; Chen et al. 2020), and their applications to OOD detection (Golan and El-Yaniv 2018; Hendrycks et al. 2019; Tack et al. 2020). The main contributions of this work are as follows:"
2203.09450,data,295,,,"Given a batch of N samples, each sample x is ﬁrst duplicated and each version then goes through three initial augmentations (also see Data Augmentation in Sec. 4) to generate two different views x1 and x2 (they keep the same class label as x). Denote the augmented batch by B, which now has 2N samples. In (Hendrycks et al. 2019; Tack et al. 2020), it was shown that using image rotations is effective in learning OOD detection models because such rotations can effectively serve as out-of-distribution (OOD) training data. For each augmented sample x ∈ B with class y of a task, we rotate x by 90◦, 180◦, 270◦ to create three images, which are assigned three new classes y1, y2, and y3, respectively. This results in a larger augmented batch ˜B. Since we generate three new images from each x, the size of ˜B is 8N . For each original class, we now have 4 classes. For a sample x ∈ ˜B, let ˜B(x) = ˜B\{x} and let P (x) ⊂ ˜B\{x} be a set consisting of the data of the same class as x distinct from x. The contrastive representation of a sample x is zx = g(h(x, t))/(cid:107)g(h(x, t))(cid:107), where t is the current task. In learning, we minimize the supervised contrastive loss (Khosla et al. 2020) of task t."
2203.09450,data,339,,,"2 Related Work Many approaches have been proposed to deal with CF in CL. Using regularization (Kirkpatrick et al. 2017) and knowledge distillation (Li and Hoiem 2016) to minimize the change to previous models are two popular approaches (Jung et al. 2016; Camoriano et al. 2017; Fernando et al. 2017; Rannen Ep Triki et al. 2017; Seff et al. 2017; Zenke, Poole, and Ganguli 2017; Kemker and Kanan 2018; Ritter, Botev, and Barber 2018; Schwarz et al. 2018; Xu and Zhu 2018; Castro et al. 2018; Dhar et al. 2019; Hu et al. 2019; Lee et al. 2019; Liu et al. 2020a). Memorizing some old examples and using them to adjust the old models in learning a new task is another popular approach (called replay) (Rusu et al. 2016; LopezPaz and Ranzato 2017; Rebufﬁ, Kolesnikov, and Lampert 2017; Chaudhry et al. 2019; de Masson d’Autume et al. 2019; Hou et al. 2019; Wu et al. 2019; Rolnick et al. 2019; Buzzega et al. 2020; Zhao et al. 2020; Rajasegaran et al. 2020a; Liu, Schiele, and Sun 2021). Several systems learn to generate pseudo training data of old tasks and use them to jointly train the new task, called pseudo-replay (Gepperth and Karaoguz 2016; Kamra, Gupta, and Liu 2017; Shin et al. 2017; Wu et al. 2018; Seff et al. 2017; Wu et al. 2018; Kemker and Kanan 2018; Hu et al. 2019; Hayes et al. 2019; Rostami, Kolouri, and Pilly 2019; Ostapenko et al. 2019)."
2203.09450,data,342,,,"Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. ImageNet Classiﬁcation with Deep Convolutional Neural Networks. In NIPS. Le, Y.; and Yang, X. 2015. Tiny ImageNet Visual Recognition Challenge. Lee, K.; Lee, K.; Shin, J.; and Lee, H. 2019. Overcoming catastrophic forgetting with unlabeled data in the wild. In CVPR. Li, Z.; and Hoiem, D. 2016. Learning Without Forgetting. In ECCV, 614–629. Springer. Liang, S.; Li, Y.; and Srikant, R. 2018. Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks. In ICLR. Liu, Y.; Parisot, S.; Slabaugh, G.; Jia, X.; Leonardis, A.; and Tuytelaars, T. 2020a. More Classiﬁers, Less Forgetting: A Generic Multi-classiﬁer Paradigm for Incremental Learning. In ECCV, 699–716. Springer International Publishing. Liu, Y.; Schiele, B.; and Sun, Q. 2021. Adaptive Aggregation Networks for Class-Incremental Learning. In CVPR. Liu, Y.; Su, Y.; Liu, A.-A. ; Schiele, B.; and Sun, Q. 2020b. Mnemonics Training: Multi-Class Incremental Learning Without Forgetting. In CVPR. Lopez-Paz, D.; and Ranzato, M. 2017. Gradient Episodic Memory for Continual Learning. In NeurIPS, 6470–6479. Loshchilov, I.; and Hutter, F. 2016. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983. Mallya, A.; and Lazebnik, S. 2017. PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning. arXiv preprint arXiv:1711.05769. McCloskey, M.; and Cohen, N. J. 1989. Catastrophic interference in connectionist networks: The sequential learning In Psychology of learning and motivation, volproblem. ume 24, 109–165. Elsevier."
2203.09450,data,345,,,"2019; Liu et al. 2020a). Memorizing some old examples and using them to adjust the old models in learning a new task is another popular approach (called replay) (Rusu et al. 2016; LopezPaz and Ranzato 2017; Rebufﬁ, Kolesnikov, and Lampert 2017; Chaudhry et al. 2019; de Masson d’Autume et al. 2019; Hou et al. 2019; Wu et al. 2019; Rolnick et al. 2019; Buzzega et al. 2020; Zhao et al. 2020; Rajasegaran et al. 2020a; Liu, Schiele, and Sun 2021). Several systems learn to generate pseudo training data of old tasks and use them to jointly train the new task, called pseudo-replay (Gepperth and Karaoguz 2016; Kamra, Gupta, and Liu 2017; Shin et al. 2017; Wu et al. 2018; Seff et al. 2017; Wu et al. 2018; Kemker and Kanan 2018; Hu et al. 2019; Hayes et al. 2019; Rostami, Kolouri, and Pilly 2019; Ostapenko et al. 2019). CLOM differs from these approaches as it does not replay any old task data to prevent forgetting and can function with/without saving some old data. Parameter isolation is yet another popular approach, which makes different subsets (which may overlap) of the model parameters dedicated to different tasks using masks (Fernando et al. 2017; Serr`a et al. 2018; Ke, Liu, and Huang 2020) or ﬁnding a sub-network for each task by pruning (Mallya and Lazebnik 2017; Wortsman et al. 2020; Hung et al. 2019). CLOM uses parameter isolation, but it differs from these approaches as it combines the idea of parameter isolation and OOD detection, which can solve both TIL and CIL problems effectively."
2203.09450,data,348,,,"Liu, Y.; Parisot, S.; Slabaugh, G.; Jia, X.; Leonardis, A.; and Tuytelaars, T. 2020a. More Classiﬁers, Less Forgetting: A Generic Multi-classiﬁer Paradigm for Incremental Learning. In ECCV, 699–716. Springer International Publishing. Liu, Y.; Schiele, B.; and Sun, Q. 2021. Adaptive Aggregation Networks for Class-Incremental Learning. In CVPR. Liu, Y.; Su, Y.; Liu, A.-A. ; Schiele, B.; and Sun, Q. 2020b. Mnemonics Training: Multi-Class Incremental Learning Without Forgetting. In CVPR. Lopez-Paz, D.; and Ranzato, M. 2017. Gradient Episodic Memory for Continual Learning. In NeurIPS, 6470–6479. Loshchilov, I.; and Hutter, F. 2016. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983. Mallya, A.; and Lazebnik, S. 2017. PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning. arXiv preprint arXiv:1711.05769. McCloskey, M.; and Cohen, N. J. 1989. Catastrophic interference in connectionist networks: The sequential learning In Psychology of learning and motivation, volproblem. ume 24, 109–165. Elsevier. Mi, F.; Kong, L.; Lin, T.; Yu, K.; and Faltings, B. 2020. Generalized Class Incremental Learning. In CVPR. Oord, A.; Li, Y.; and Vinyals, O. 2018. Representation learning with contrastive predictive coding. arXiv:1807.03748. Ostapenko, O.; Puscas, M.; Klein, T.; Jahnichen, P.; and Nabi, M. 2019. Learning to remember: A synaptic plasticity driven framework for continual learning. In CVPR, 11321–11329. Parisi, G. I.; Kemker, R.; Part, J. L.; Kanan, C.; and Wermter, S. 2019. Continual lifelong learning with neural networks: A review."
2203.09450,data,349,,,"Loshchilov, I.; and Hutter, F. 2016. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983. Mallya, A.; and Lazebnik, S. 2017. PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning. arXiv preprint arXiv:1711.05769. McCloskey, M.; and Cohen, N. J. 1989. Catastrophic interference in connectionist networks: The sequential learning In Psychology of learning and motivation, volproblem. ume 24, 109–165. Elsevier. Mi, F.; Kong, L.; Lin, T.; Yu, K.; and Faltings, B. 2020. Generalized Class Incremental Learning. In CVPR. Oord, A.; Li, Y.; and Vinyals, O. 2018. Representation learning with contrastive predictive coding. arXiv:1807.03748. Ostapenko, O.; Puscas, M.; Klein, T.; Jahnichen, P.; and Nabi, M. 2019. Learning to remember: A synaptic plasticity driven framework for continual learning. In CVPR, 11321–11329. Parisi, G. I.; Kemker, R.; Part, J. L.; Kanan, C.; and Wermter, S. 2019. Continual lifelong learning with neural networks: A review. Neural Networks. Rajasegaran, J.; Hayat, M.; Khan, S.; Khan, F. S.; Shao, L.; and Yang, M.-H. 2020a. An Adaptive Random Path Selection Approach for Incremental Learning. arXiv:1906.01120. Rajasegaran, J.; Khan, S.; Hayat, M.; Khan, F. S.; and Shah, iTAML: An Incremental Task-Agnostic MetaM. 2020b. learning Approach. In CVPR. Rannen Ep Triki, A.; Aljundi, R.; Blaschko, M.; and Tuytelaars, T. 2017. Encoder based lifelong learning. In ICCV. Rebufﬁ, S.-A. ; Kolesnikov, A.; and Lampert, C. H. 2017. iCaRL: Incremental classiﬁer and representation learning. In CVPR, 5533–5542."
2203.09450,"data, dataset",188,,,"tasks Continual <1, 2, ..., k, ...> incrementally. Each task k has its dataset Dk = {(xi k ∈ X is a data sample in task k and yi k and Yk is the set of classes of task k. The key challenge of CL is catastrophic forgetting (CF) (McCloskey and Cohen 1989), which refers to the situation where the learning of a new task may signiﬁcantly change the network weights learned for old tasks, degrading the model accuracy for old tasks. Researchers have mainly worked on two CL problems: class incremental/continual learning (CIL) and task incremental/continual learning (TIL) (Dhar et al. 2019; van de Ven and Tolias 2019). The main difference between CIL and TIL is that in TIL, the task-id k is provided for each test sample x during testing so that only the model for task k is used to classify x, while in CIL, the task-id k for each test sample is not provided."
2203.09450,"data, dataset",213,,,"4 Experiments Evaluation Datasets: Four image classiﬁcation CL benchmark datasets are used in our experiments. (1) MNIST :3 handwritten digits of 10 classes (digits) with 60,000 examples for training and 10,000 examples for testing. (2) CIFAR10 (Krizhevsky and Hinton 2009):4 60,000 32x32 color images of 10 classes with 50,000 for training and 10,000 for testing. (3) CIFAR-100 (Krizhevsky and Hinton 2009):5 60,000 32x32 color images of 100 classes with 500 images per class for training and 100 per class for testing. (4) TinyImageNet (Le and Yang 2015):6 120,000 64x64 color images of 200 classes with 500 images per class for training and 50 images per class for validation, and 50 images per class for testing. Since the test data has no labels in this dataset, we use the validation data as the test data as in (Liu et al. 2020a). Baseline Systems: We compare our CLOM with both the classic and the most recent state-of-the-art CIL and TIL methods. We also include CLOM(-c), which is CLOM without calibration (which already outperforms the baselines)."
2203.09450,dataset,102,,,"CIL Results Comparison. Tab. 1 shows that CLOM and CLOM(-c) achieve much higher CIL accuracy except for MNIST for which CLOM is slightly weaker than CCG by 0.4%, but CLOM’s result on CIFAR10-5T is about 18% greater than CCG. For other datasets, CLOM improves by similar margins. This is in contrast to the baseline TIL systems that are incompetent at the CIL setting when classes are predicted using Eq. 2. Even without calibration, CLOM(-c) already outperforms all the baselines by large margins."
2203.09450,dataset,112,,,"Comparison of Forgetting Rate. Fig. 2 shows the average forgetting rate of each method in the TIL setting. The CIL systems suffer from more forgetting as they are not designed for the TIL setting, which results in lower TIL accuracy (Tab. 1). The TIL systems are highly effective at preserving previous within-task knowledge. This results in higher TIL accuracy on large dataset such as T-ImageNet, but they collapse when task-id is not provided (the CIL setting) as shown in Tab. 1. CLOM is robust to forgetting as a TIL system and it also functions well without task-id."
2203.09450,dataset,117,,,"We use the same backbone architecture for CLOM and baselines, except for OWM and HyperNet, where we use the same architecture as in their original papers. OWM uses an Alexnet-like structure for all datasets. OWN has difﬁculty to work with ResNet-18 because it is not obvious how to deal with batch normalization in OWM. HyperNet uses a fully-connected network for MNIST and ResNet-32 for other datasets. We found it very hard to change HyperNet because the network initialization requires some arguments which were not explained in the paper. In Tab. 6, we report the network parameter sizes after the ﬁnal task in each experiment has been trained."
2203.09450,dataset,118,,,"TIL Results Comparison. The gains by CLOM and CLOM(-c) over the baselines are also great in the TIL setting. CLOM and CLOM(-c) are the same as the output calibration does not affect TIL performance. For the two large datasets CIFAR100 and T-ImageNet, CLOM gains by large margins. This is due to contrastive learning and the OOD model. The replay based CIL methods (LwF.R, iCaRL, Mnemonics, BiC, and DER++) perform reasonably in the TIL setting, but our CLOM and CLOM(-c) are much better due to task masks which can protect previous models better with little CF."
2203.09450,dataset,134,,,"Table 1: Average accuracy over all classes after the last task is learned. -xT: x number of tasks. †: In their original paper, PASS and Mnemonics use the ﬁrst half of classes to pre-train before CL. Their results are 50.1% and 53.5% on CIFAR100-10T respectively, but they are still lower than CLOM without pre-training. In our experiments, no pre-training is used for fairness. ∗: iCaRL and Mnemonics give both the ﬁnal average accuracy as here and the average incremental accuracy in the original papers. We report the average incremental accuracy and network size in Appendix A and B, respectively. The last two columns show the average TIL and CIL accuracy of each method over all datasets."
2203.09450,dataset,145,,,"In the paper, we reported the accuracy after all tasks have been learned. Here we give the average incremental accuracy. Let Ak be the average accuracy over all tasks seen so far right after the task k is learned. The average incremental accuracy is deﬁned as A = (cid:80)t k=1 Ak/t, where t is the last task. It measures the performance of a method throughout the learning process. Tab. 5 shows the average incremental accuracy for the TIL and CIL settings. Figures 3 and 4 plot the TIL and CIL accuracy Ak at each task k for every dataset, respectively. We can clearly see that our proposed method CLOM and CLOM(-c) outperform all others except for MNIST-5T, for which a few systems have the same results."
2203.09450,github,4,,,https://github.com/yaoyao-liu/class incremental-learning
2203.09450,github,10,,,• iCaRL: https://github.com/yaoyao-liu/class-incremental learning • Mnemonics:
2203.09450,github,26,,,• OWM: https://github.com/beijixiong3510/OWM • MUC: https://github.com/liuyudut/MUC • PASS: https://github.com/Impression2805/CVPR21 PASS • LwF.R: https://github.com/yaoyao-liu/class-incremental learning
2203.09450,github,30,,,• BiC: https://github.com/sairin1202/BIC • DER++: https://github.com/aimagelab/mammoth • HAT: https://github.com/joansj/hat • HyperNet: https://github.com/chrhenning/hypercl • SupSup: https://github.com/RAIVNLab/supsup
2203.09450,"github, code available, code",176,,,"Existing continual learning techniques focus on either task incremental learning (TIL) or class incremental learning (CIL) problem, but not both. CIL and TIL differ mainly in that the task-id is provided for each test sample during testing for TIL, but not provided for CIL. Continual learning methods intended for one problem have limitations on the other problem. This paper proposes a novel uniﬁed approach based on out-of-distribution (OOD) detection and task masking, called CLOM, to solve both problems. The key novelty is that each task is trained as an OOD detection model rather than a traditional supervised learning model, and a task mask is trained to protect each task to prevent forgetting. Our evaluation shows that CLOM outperforms existing state-of-the-art baselines by large margins. The average TIL/CIL accuracy of CLOM over six experiments is 87.6/67.9% while that of the best baselines is only 82.4/55.0%. The code of our system is available at https://github.com/k-gyuhak/CLOM."
