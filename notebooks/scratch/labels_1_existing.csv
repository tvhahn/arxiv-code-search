id,pattern,update_date,label,para
2011.05411,dataset,04/21/22,1,"2. Local Computation: Once receiving the global ML model from the server, the participants updates its current local ML model and then trains the updated model using the local dataset resided in the device. This step is operated at local nodes, and it requires end-usersâ€™ devices to install an FL client program to perform training algorithms such as FederatedSGD and Federated Averaging, as well as to receive the global model updates and send the local ML model parameters from/to the server."
2011.05411,dataset provided,04/22/22,2,5.3. Rights of Data Subject
2011.05411,dataset,04/23/22,3,"Although gradient descent-based optimisation methods were successfully engaged in various ML algorithms, they have recently re-gained much attention since the emergence of large-scale distributed learning, including FL [16, 29]. In these scenarios, a complex model, e.g., a deep neural network (DNN) with millions of parameters, is trained on a very large dataset across multiple nodes. These nodes are"
2011.05411,dataset,04/24/22,4,"Another approach to preserve data privacy and security in ML is to utilise homomorphic encryption techniques, particularly in centralised systems, e.g., cloud servers, wherein data is collected and trained at a server without disclosing the original information. Homomorphic encryption enables the ability to perform computation on an encrypted form of data without the need for the secret key to decrypt the ciphertext [44]. Results of the computation are in encrypted form and can only be decrypted by the requester of the computation. In addition, homomorphic encryption ensures that the decrypted output is the same as the one computed on the original unencrypted dataset."
2011.05411,dataset,04/25/22,5,"As aforementioned, a trained ML model contains unintended features that can be utilised to extract personal information. Thus, local ML model parameters from a federated optimisation algorithm can be exploited by an adversary to infer personal information, particularly when combining with related information such as model data structure and meta-data. This information can be either original training data samples (i.e., reconstruction attack) [38, 111, 81, 4, 57, 96, 112, 6, 93, 130, 43] or membership tracing (i.e., to check if a given data point belongs to a training dataset) [15, 112, 86]."
2011.05411,dataset,04/26/22,6,"Data anonymisation or de-identiï¬cation is a technique to hide (e.g., hashing) or remove sensitive attributes, such as personally identiï¬able information (PII), so that a data subject cannot be identiï¬ed within the modiï¬ed dataset (i.e., the anonymous dataset) [92]. As a consequence, data anonymisation has to balance well between privacy-guarantee and utility because hiding or removing information may reduce the utility of the dataset. Furthermore, when combined with auxiliary information from other anonymous datasets, a data subject might be re-identiï¬ed, subjected to a privacy attack called linkage attack [40]. To prevent from linkage attack, numerous techniques have been proposed such as k-anonymity [116], l-diversity [79], a k-anonymity-based method, and tcloseness - a technique built on both k-anonymity and l-diversity that preserves the distribution of sensitive attributes in a dataset so that it reduces the risk of re-identifying a data subject in a same quasi-identiï¬er group [73]."
2011.05411,dataset,04/27/22,7,"FL is well-suited for sorts of ML models that are formulated as minimisation of some objective functions (loss functions) on a training dataset for parameter estimation, particularly for gradient-based optimisation algorithms [70]. The minimisation objective can be formulated as follows:"
2011.05411,data available,04/28/22,8,"FL settings. Training data in FL is unbalanced and non-IID, which is scattered across millions of personal mobile devices with signiï¬cant higher-latency, lower-throughput connections compared to the traditional techniques working on a cloud-centric data server. In addition, the data and computing resources in personal devices are only intermittently available for training. Therefore, to actualise FL, optimisation algorithms must be well adapted and eï¬ƒciently performed for federated settings (i.e., federated optimisation [70])."
2011.05411,dataset,04/29/22,9,"Furthermore, the local nodes can leverage the perturbation method to prevent a coordination server and other adversaries from disclosing model parameters updates and original training dataset. The idea of employing perturbation technique to FL is that a local node adds random noise to its local model parameters in order to obscure certain sensitive attributes of the model before sharing. As a result, adversaries, in case it can successfully derive such model parameters, is unable to accurately reconstruct the original training data or infer some related information. In other words, the perturbation method could prevent adversaries from carrying out inference attacks on a local model trained by a particular client. This privacy-preservation method typically adopts diï¬€erential privacy technique that adds random noises to either training dataset or model parameters, oï¬€ering statistical privacy guarantees for individual data [35, 33, 7]. Indeed, before the proposal of FL, diï¬€erential privacy with SMC has been suggested as a privacy-preserving technique"
2011.05411,dataset,04/30/22,10,"Generally, there are three gradient descent methods that are categorised based on the amount of training data used in the gradient calculation of the objective function ğ‘“ (ğœƒ) [103]. The ï¬rst category is batch gradient descent, in which the gradients are computed over the entire training dataset îˆ° for one update. The second category is stochastic gradient descent (SGD), that, in contrast to batch gradient descent, randomly selects a sample (or a subset) from îˆ° and performs the parameters update based on the gradient of this sample only (one sample per step, the whole process sweeps through the entire dataset). The third one is mini-batch gradient descent in which the dataset is subdivided into mini-batches of ğ‘› training samples (ğ‘› is the batch-size); the parameters update is then performed on every mini-batch (single minibatch per step)."
2011.05411,dataset,05/01/22,11,"Geyer et al. in [48] have developed another method to implement diï¬€erential privacy for federated optimisation in FL settings that conceals the participation of a user in a training task; as a result, the whole local training dataset of the user is protected against diï¬€erential attacks. This approach is diï¬€erent from the batch-level one, which aims at protecting a single data point in a training task. The proposed method utilises a similar concept of privacy accounting from [1] that allows a coordination server to monitor the accumulated privacy budget by observing the moment accountant and privacy loss proposed in [1]. The training process is halted once the accumulated privacy budget reaches a pre-deï¬ned threshold, implying that the privacy guarantee is no further tolerated. The Gaussian mechanism is also used to generate random noise which is then added to distort the sum of gradients updates to protect the whole training data. The proposed method has been experimented on MNIST dataset, and the results show that with a suï¬ƒciently large number of participants (e.g., about 10,000 clients), the accuracy of the FL trained model almost achieves as high as the nondiï¬€erential-privacy baseline while a certain level of privacy guarantee over the local training data still holds."
2011.05411,dataset provided,05/02/22,12,"In this article, we conduct a survey on existing FL studies with an emphasis on privacy-preserving techniques from the GDPR-compliance perspective. Firstly, we brieï¬‚y review the challenges on data privacy preservation in conventional centralised ML approaches (Section 2) and introduce FL as a potential approach to address the challenges (Section 3). Secondly, the state-of-the-art privacy-preserving techniques for centralised FL are described with the analysis of how these solutions can mitigate data security and privacy risks (Section 4). Thirdly, we provide an insightful deliberation with potential solution approaches of how an FL system can be implemented in order to comply with the EU/UK GDPR (Section 5). Unsolved challenges hindering an FL system from complying with the GDPR are also speciï¬ed along with the future research directions."
2011.05411,dataset,05/03/22,13,"In this regard, FL is an alternative for the cloud-centric ML technique that facilitates an ML model to be trained collaboratively while retaining original personal data on their devices, thus potentially mitigates data privacy-related vulnerabilities. It is a cross-disciplinary technique covering multiple computer science aspects including ML, distributed computing, data privacy and security that enables end-usersâ€™ devices (i.e., local nodes) to locally train a shared ML model on local data. Only parameters in the training process are exchanged for the model aggregation and updates. The diï¬€erence between FL and the standard distributed learning is that in distributed learning, local training datasets in compute nodes are assumed to be independent and identically distributed data (IID) whose their sizes are roughly the same. FL is, thus, as an advancement of distributed learning as it is designed to work with unbalanced and non-independent identically-distributed data (non-IID) whose sizes may span several orders of magnitude. Such heterogeneous datasets are resided at a massive number of scattering mobile devices under unstable connectivity and limited communication bandwidth [81, 80, 65]."
2011.05411,dataset,05/04/22,14,"In traditional ML approaches, this sort of algorithms performs a vast number of fast iterations over a large dataset homogeneously partitioned in data servers. Such algorithms require super low-latency and high-throughput connections to the training data [80]. Therefore, solving this optimisation problem in the context of FL is diï¬€erent from the traditional ML approaches as such conditions do not hold in"
2011.05411,dataset,05/05/22,15,"It is worth to emphasise that the separation of the four steps in the cycle is not a strict requirement in every training round. For instance, an asynchronous SGD algorithm can be used in which results of the local training can be immediately applied to update the local model before obtaining updates from other participants [21]. This asynchronous approach is typically utilised in distributed training for deep learning models on a large-scale dataset as it maximises the rate of updates [29, 25]. However, in FL settings, the synchronous approach, which requires the coordination from a centralised server, has substantial advantages over the asynchronous ones in terms of both communication eï¬ƒciency and security because it allows advanced technologies to be integrated such as aggregation compression, secure aggregation with SMC, and diï¬€erential privacy [80, 71, 55, 120]."
2011.05411,data available,05/06/22,16,"Normally, DPAs might require a variety of information with a detailed explanation from Data Controller to perform the analysis including documents of organisational and technical measures related to the implementation the GDPR requirements as well as independent DPIA and PIA reports frequently conducted by the Data Controller. DPAs may also require to be given access to data server infrastructure and management system including personal data that is being processed. In this respect, besides the legal basis such as consents from end-users, an FL service provider can only provide documentation of how FL-related mechanisms are implemented along with privacy-preserving technical measures such as secure aggregation, diï¬€erential privacy, and homomorphic encryption. Other inquiries from DPAs such as direct access to the FL model training operations and inspection of individual local model parameters from a particular end-user are technically infeasible for any FL systems."
2011.05411,dataset,05/07/22,17,"Proposed by Dwork et al. in 2006, diï¬€erential privacy [34] is an advanced solution of the perturbation privacy-preserving technique in which random noise is added to true outputs using rigorous mathematical measures [40]. As a result, it is statistically indistinguishable between an original aggregate dataset and a diï¬€erentially additive-noise one. Thus, a single individual cannot be identiï¬ed as any (statistical) query results to the original dataset is practically the same regardless of the existence of the individual [34, 33, 35]. However, there is a trade-oï¬€ between privacy-guarantee and utility as adding too much noise and improper random Nguyen Truong et al.: Preprint submitted to Elsevier"
2011.05411,dataset,05/08/22,18,"Reconstruction attacks using MI and GANs are only feasible if and only if all class members in an ML model are analogous which entails a similarity between the MI/GANreconstructed outputs and the training data (e.g., facial recognition of a speciï¬c person, or MNIST dataset for handwritten digits6 used in [4]). Fortunately, this precondition is less practical in most of the FL scenarios."
2011.05411,dataset,05/09/22,19,"Regarding the Fairness and Transparency requirements, as AI/ML algorithms like deep learning are normally operated in a black-box fashion, it is limited of transparency of how certain decisions are made, as well as limited understanding of the bias in data samples and training process [30, 83, 3, 91]. An FL system is not an exception. Generally, if the training data is poorly collected or intentionally prejudicial and fed to an ML, including FL, system, the results apparently turn out to be biased. If the trained model is then utilised for an automated decision-making system, then it probably leads to discrimination and injustice. Furthermore, the nature of preventing service providers from accessing original training dataset as well as the inability to inspect individualsâ€™ locally trained ML model due to Secure Aggregation mechanism ampliï¬es the lack of transparency and fairness in FL systems. As a result, an FL system ï¬nds it problematic to transparently execute the training operations as well as to ensure any automated decisions from the system are impartially performed. This, consequently, induces the impracticality for any FL systems and fails to fully comply with the GDPR requirements of fairness and transparency."
2011.05411,dataset,05/10/22,20,"SMC is beneï¬cial to data privacy preservation in distributed learning wherein compute nodes collaboratively perform model training on their local dataset without revealing such dataset to others. Indeed, SMC has been employed in numerous ML algorithms such as secure two-party computation (S2C) in linear regression [31], Iterative Dichotomiser3 (ID3) decision tree learning algorithm [78], and k-means clustering algorithm for distributed data mining [62]. However,most of SMC protocols impose non-trivial overheads which require further eï¬ƒciency improvements with practical deployment."
2011.05411,dataset,05/11/22,21,"SMC, also known as multi-party computation (MPC) or privacy-preserving computation, was ï¬rstly introduced by Yao in 1986 [126] and further developed by numerous researchers. Its catalyst is that a function can be collectively computed over a dataset owned by multiple parties using their own inputs (i.e., a subset of the dataset) so that any party learns nothing about othersâ€™ data except the outputs [51, 18, 27]. Speciï¬cally, ğ‘› parties ğ‘ƒ1, ğ‘ƒ2, .., ğ‘ƒğ‘› own ğ‘› pieces of pri, respectively to collectively comvate data ğ‘‹1, ğ‘‹2, ..., ğ‘‹ğ‘› pute a public function ğ‘“ (ğ‘‹1, ğ‘‹2, .., ğ‘‹ğ‘›) = (ğ‘Œ1, ğ‘Œ2, .., ğ‘Œğ‘›). The only information each party can obtain from the computation is the result (ğ‘Œ1, ğ‘Œ2, .., ğ‘Œğ‘›) and its own inputs ğ‘‹ğ‘– . Classical secret sharing such as Shamirâ€™s secret sharing [109, 17] and veriï¬able secret sharing (VSS) schemes [26] are the groundwork for most of the SMC protocols."
2011.05411,dataset,05/12/22,22,"Shokri and Shmatikov in [111] have proposed a communication eï¬ƒcient privacy-preserving SGD algorithm for deep learning in distributed settings in which local gradient parameters are asynchronously shared among participants with an option of adding noise to such updates for the differentially private protection of the individual model parameters. In this algorithm, participants can choose a fraction of parameters (randomly selected or following a strategy) to be updated at each round so that their local optimal can converge faster while being more accurate. In order to integrate diï¬€erential privacy technique into the algorithm, the ğœ€ total privacy budget parameter and the sensitivity of gradient are taken into account to control Î”ğ‘“ğ‘– the trade-oï¬€ between the diï¬€erential privacy protection and the model accuracy. Laplacian mechanism is used to generate noise during both parameter selection and exchange processes based on the estimation of the Î”ğ‘“ğ‘– sensitivity and the allocated ğœ€ privacy budget. The proposed algorithm has experimented on MNIST and SVHN datasets showing the trade-oï¬€ between strong diï¬€erential privacy guarantees and high accuracy of the training model. However, with a large number of participants sharing a large fraction of gradients, the accuracy of the proposed algorithm with diï¬€erential privacy is better than the standalone baseline. It is worth noting that in this algorithm, local gradients can be exchanged directly or via a central server, which can feasibly be implemented in the FL settings."
2011.05411,dataset provided,05/13/22,23,"The GDPR clearly diï¬€erentiates three participant roles, namely: Data Subject, Data Controller and Data Processor, along with associated requirements and obligations under the EU/UK data protection law. While serving as a better privacy and security framework, the GDPR also aims at protecting data ownership by obligating Data Controllers to provide fundamental rights for Data Subjects to control over their data (""How?"" in Fig. 1). For these purposes, the GDPR introduces and sets high-standard for the consent lawful basis in which Data Controller shall obtain consent from Data Subject in order to process data. Data Controller takes full responsibility to regulate the purposes for which and the methods in which, personal data is processed under the Terms and Conditions deï¬ned in the consent."
2011.05411,dataset provided,05/14/22,24,"The GDPR diï¬€erentiates three participant roles, namely Data Subject, Data Controller and Data Processor, and designates associated obligations for these roles under the EU data protection law. Data Controllers are subject to comply with the GDPR by determining the purposes for which, and the method in which, personal data is processed by Data Processors - who will be responsible for processing the data on behalf of Data Controllers. Furthermore, Data Controllers should take appropriate measures to provide Data Subjects with information related not only to how data is shared but also to how data is processed in the manner ensuring security and privacy of personal data. The GDPR also clearly speciï¬es rights of Data Subjects, giving data owners the rights to inspect information about how the personal data is being processed (e.g., Right to be informed and Right of access) as well as to fully control the data (e.g., Right of rectiï¬cation and erasure, and Right to restriction of processing)."
2011.05411,publicly available,05/15/22,25,"The GDPR establishes supervisory authorities in each member state which are independent public authorities called Data Protection Authorities (DPAs). DPAs are responsible for supervising and inspecting whether a Data Controller is compliant with the data protection regulations whilst the Data Controller is responsible for demonstrating the compliance. The questions are judiciously raised: How can an FL system be investigated and validated by DPAs, and how can it demonstrate the compliance?"
2011.05411,dataset,05/16/22,26,"The authors in [1] have proposed an SGD algorithm integrated with diï¬€erential privacy performing over some batches (a group) of data samples. This algorithm estimates the gradient of the group by taking the average of the gradient loss of these batches and adds noise (generated by Gaussian mechanism) to the group to protect the privacy. This algorithm is implemented to train on the MNIST and CIFAR-10 datasets showing sensible results as it achieves only 1.3% and 7% less accurate compared to the non-diï¬€erentially private conventional baseline algorithms on the same datasets, respectively. Similar to the mechanism proposed by Shokri and Shmatikov in [111], the authors have proposed a mechanism to monitor the total privacy budget (i.e., privacy accounting)"
2011.05411,publicly available,05/17/22,27,"The new GDPR legislation has come into force from May 2018 in all European Union (EU) countries which is a major update to the EU Data Protection Directive (95/46/EC) (DPD-95) introduced in the year 1995. The GDPR aims to protect personal data (more comprehensive range depicted in ""Which?"" - Fig. 1) with the impetus that ""personal data can only be gathered legally, under strict conditions, for a legitimate purpose"". The full regulation is described in detail across 99 articles covering principles, and both technical and admin requirements around how organisations need to process personal data. The GDPR creates a legal data protection framework throughout the EU/UK member states which has impacted commercial and public organisations worldwide processing EU/UK residentsâ€™ data (""Global"" in Fig. 1)."
2011.05411,dataset provided,05/18/22,28,"To meet stringent requirements of the GDPR, conventional ML-based applications and services are required to implement measures that eï¬€ectively protect and manage personal data adhering to the six data protection principles in the GDPR, as well as to provide mechanisms for data subjects to fully control their data. Although ML-based systems are strengthened by several privacy-preserving methods, implementing these obligations in a centralised MLbased system is non-trivial, sometimes technologically impractical [119, 53]."
2011.05411,dataset,05/19/22,29,"To overcome such challenges, Federated Learning (FL), proposed by Google researchers in 2016, has appeared as a promising solution and attracted attention from both industry and academia [70, 71, 81, 80]. Generally, FL is a technique to implement an ML algorithm in decentralised collaborative learning settings wherein the algorithm is executed on multiple local datasets stored at isolated data sources (i.e., local nodes) such as smart phones, tablet, PCs, and wearable devices without the need for collecting and processing the training data at a centralised data server. FL allows local nodes to collaboratively train a shared ML model while retaining both training dataset and computation at internal sites [70]. Only results of the training (i.e., parameters) are exchanged at a certain frequency, which requires a central server to coordinate the training process (centralised FL) or utilises a peer-to-peer underlying network infrastructure (i.e., decentralised FL) to aggregate the training results and calculate the global model."
2011.05411,dataset,05/20/22,30,"[92] Narayanan, A., Shmatikov, V., 2008. Robust de-anonymization of large sparse datasets, in: 2008 IEEE Symposium on Security and Privacy (sp 2008), IEEE. pp. 111â€“125."
2011.05411,dataset,05/21/22,31,"as accumulated privacy loss by observing privacy loss random variables. Based on the experiment, the authors also indicate that privacy loss is minimal for large group size (with a large number of datasets)."
2011.05411,"data available, dataset",05/22/22,32,"called compute nodes and grouped into clusters. For efï¬ciency, the calculations in the training process should be parallelised using concurrency methods such as model parallelism and data parallelism [24]. Model parallelism distributes an ML model into diï¬€erent computing blocks; available computing nodes are then be assigned to compute some speciï¬c blocks only. Model parallelism requires mini-batch data is replicated at computing nodes in a cluster, as well as regular communication and synchronisation among such nodes [29]. Data parallelism, instead, keeps the completeness of the model on each computing node but partitions the training dataset into smaller equal size shards (also known as sharding), which are then distributed to computing nodes in each cluster [8]. The computing nodes then train the model on their subset as a mini-batch, which is especially eï¬€ective for SGD variants because most operations over mini-batches are independent in these algorithms. Data parallelism can be found in numerous modern ML frameworks including TensorFlow3 and Pytorch4. The two parallelism techniques can also be combined (so-called Hybrid parallelism) to intensify the advantages while mitigating the drawbacks of each one; as a result, a hybrid system can achieve better eï¬ƒciency and scalability [25]."
2011.05411,dataset,05/23/22,33,"learning system target two main objectives: (i) privacy of the training dataset and (ii) privacy of the local model parameters (from an optimisation algorithm such as a gradient descent variant) which are exchanged with other nodes and/or a centralised server [111]. In this respect, prominent privacy-preserving techniques in ML include data anonymisation [92], diï¬€erential privacy [34], secure multi-party computation (SMC) [126], and homomorphic encryption [44]."
2011.05411,dataset,05/24/22,34,"ness will signiï¬cantly depreciate reliability and usability of the dataset [33, 35, 40]."
2011.05411,dataset,05/25/22,35,"where the training dataset is in form of a set of input-output pairs (ğ‘¥ğ‘–, ğ‘¦ğ‘–), ğ‘¥ğ‘– âˆˆ â„ğ‘‘ and ğ‘¦ğ‘– âˆˆ â„, âˆ€ğ‘– âˆˆ {1, 2, .., ğ‘›}. In Equation 2, ğ‘› is the number of samples in the dataset, ğ‘¤ âˆˆ â„ğ‘‘ is the parameter vector, and ğ‘“ğ‘–(ğ‘¤) is a loss function. This formulation covers both linear and logistic regressions, support vector machines, as well as complicated non-convex problems in Artiï¬cial Neural Networks (ANN) including Deep Learning [70]. This problem requires an optimisation process that can be eï¬ƒciently computed by using a gradient descent algorithm with back-propagation technique [105, 101] for minimising the overall loss with respect to each model parameters."
2103.12883,github,05/26/22,36,1https://github.com/ricardoGrando/hydrone_deep_rl_icra
2103.12883,python,05/27/22,37,"The whole system was implemented using ROS and Gazebo frameworks. The Deep-RL approaches were implemented using Python programming language, while the vehiclesâ€™ related plugins were partially implemented in C++ and Python. The implementation of the neural networks was carried out with the PyTorch2 library. The performance of our approaches can also be observed in a complementary video3."
2103.12883,package,05/28/22,38,"[31] M. M. M. ManhËœaes, S. A. Scherer, M. Voss, L. R. Douat, and T. Rauschenbach, â€œUuv simulator: A gazebo-based package for underwater intervention and multi-robot simulation,â€ in OCEANS MTS/IEEE Monterey. IEEE, 2016, pp. 1â€“8."
2103.12883,package,05/29/22,39,"â€¢ We demonstrate that, with our approaches, the robot is capable to arrive at the desired target avoiding collisions. However, with a geometrically dependent tracking controller, the robot is unable to bypass the drilling risers. We also provide a completely built ROS package with a real-world described HUAUV."
2108.09408,dataset,05/30/22,40,"Abstract. Deep-learning based salient object detection methods achieve great improvements. However, there are still problems existing in the predictions, such as blurry boundary and inaccurate location, which is mainly caused by inadequate feature extraction and integration. In this paper, we propose a Multi-scale Edge-based U-shape Network (MEUN) to integrate various features at diï¬€erent scales to achieve better performance. To extract more useful information for boundary prediction, U-shape Edge Network modules are embedded in each decoder units. Besides, the additional down-sampling module alleviates the location inaccuracy. Experimental results on four benchmark datasets demonstrate the validity and reliability of the proposed method. Multi-scale Edgebased U-shape Network also shows its superiority when compared with 15 state-of-the-art salient object detection methods."
2108.09408,dataset,05/31/22,41,Datasets
2108.09408,dataset,06/01/22,42,"Datasets and implementation details. The model is trained on the DUTSTR with 10553 images. In detail, we trained the model using the SGD optimizer with initial learning rate 3e-5, 0.9 momentum, 5e-4 weight decay, and batch size 16. Because the ResNet-50 parameters are pre-trained on ImageNet, the learning rate of this part is a tenth of the randomly initialized parts which is set as 3e-5. Then, the trained model is tested on ï¬ve datasets, including DUTS-TE with 5019 images, DUT-OMROM with 5168 images, HKU-IS with 4447 images, ECSSD with 1000 images and PASCAL-S with 850 images."
2108.09408,dataset,06/02/22,43,Datasets mF MAE Sm Em mF MAE Sm Em mF MAE Sm Em mF MAE Sm Em Metrics .745 .049 .862 .860 .868 .044 .911 .914 .692 .064 .809 .837 .871 .038 .907 .937 BMPM[30] .751 .059 .839 .861 .889 .059 .893 .914 .713 .062 .814 .846 .874 .045 .888 .931 RAS[3] .785 .057 .834 .867 .914 .040 .910 .929 .747 .063 .815 .850 .893 .036 .895 .939 R3Net[5] PiCANet[13] .749 .051 .867 .852 .885 .044 .917 .910 .710 .065 .835 .834 .870 .039 .908 .934 MLMSNet[26] .799 .045 .856 .882 .914 .038 .911 .925 .735 .056 .817 .846 .892 .034 .901 .945 .777 .051 .854 .869 .906 .042 .912 .920 .736 .066 .824 .853 .882 .037 .903 .940 PAGE[23] .805 .043 .869 .886 .917 .037 .918 .925 .747 .056 .825 .866 .891 .034 .905 .944 CPD[28] .756 .048 .866 .884 .880 .037 .916 .921 .756 .056 .836 .869 .895 .032 .909 .946 BASNet[19] .840 .035 .888 .902 .925 .033 .924 .927 .766 .053 .838 .870 .840 .062 .855 .859 F3Net[24] .799 .040 .879 .881 .910 .042 .917 .921 .739 .055 .832 .858 .885 .032 PoolNet[12] .941 .767 .048 .865 .879 .880 .040 .918 .922 .739 .059 .837 .854 .878 .038 .907 .942 TDBU[22] .815 .039 .875 .891 .920 .041 .918 .927 .755 .052 .818 .867 .898 .031 .918 .948 EGNet[31] .792 .044 .861 .886 .892 .033 .928 .924 .761 .054 .847 .871 .896 .031 .916 .948 U2Net[18] .828 .037 .884 .917 .924 .033 .925 .953 .756 .055 .833 .873 .908 .028 .920 .961 MINet[16] .855 .034 .892 .910 .930 .034 .924 .925 .773 .051 .838 .873 .914 .027 .919 .954 LDF[25] .870 .031 .904 .917 .936 .028 .934 .929 .790 .052 .851 .881 .917 .026 .925 .956 Ours
2108.09408,dataset,06/03/22,44,"Quantitative comparison. Table. 3 shows the quantitative evaluation results of the SOTA methods mentioned above and our model in terms of mF , M AE, Sm, and Em. The proposed method consistently performs better than all the competitors across four metrics on four datasets. In terms of Em, our method achieves the second best overall performance, which is slightly inferior to MINet. It is worth noting that MEUNet achieves the best performance in terms of the mean F-measure and structure quality evaluation Sm."
2108.09408,dataset,06/04/22,45,Table 3. Quantitative comparison with state-of-the-art methods on ï¬ve datasets. The best results are highlighted in bold. The best and the second best results are highlighted in red and green respectively.
2108.09408,dataset,06/05/22,46,"â€¢ We build an eï¬ƒcient framework to fully combine and fuse edge information, detailed information and semantic clues. Many experiments are conducted to illustrate the validity of our algorithm and this model could surpass most models on four large-scale salient object detection datasets."
